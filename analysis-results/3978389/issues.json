[{"url": "https://api.github.com/repos/triton-inference-server/server/issues/5520", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5520/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5520/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5520/events", "html_url": "https://github.com/triton-inference-server/server/issues/5520", "id": 1629709001, "node_id": "I_kwDOCQnI4s5hI2LJ", "number": 5520, "title": "Triton not working with Inferentia in AWS", "user": {"login": "saraRaris", "id": 20050415, "node_id": "MDQ6VXNlcjIwMDUwNDE1", "avatar_url": "https://avatars.githubusercontent.com/u/20050415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saraRaris", "html_url": "https://github.com/saraRaris", "followers_url": "https://api.github.com/users/saraRaris/followers", "following_url": "https://api.github.com/users/saraRaris/following{/other_user}", "gists_url": "https://api.github.com/users/saraRaris/gists{/gist_id}", "starred_url": "https://api.github.com/users/saraRaris/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saraRaris/subscriptions", "organizations_url": "https://api.github.com/users/saraRaris/orgs", "repos_url": "https://api.github.com/users/saraRaris/repos", "events_url": "https://api.github.com/users/saraRaris/events{/privacy}", "received_events_url": "https://api.github.com/users/saraRaris/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "jbkyang-nvi", "id": 80359429, "node_id": "MDQ6VXNlcjgwMzU5NDI5", "avatar_url": "https://avatars.githubusercontent.com/u/80359429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbkyang-nvi", "html_url": "https://github.com/jbkyang-nvi", "followers_url": "https://api.github.com/users/jbkyang-nvi/followers", "following_url": "https://api.github.com/users/jbkyang-nvi/following{/other_user}", "gists_url": "https://api.github.com/users/jbkyang-nvi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbkyang-nvi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbkyang-nvi/subscriptions", "organizations_url": "https://api.github.com/users/jbkyang-nvi/orgs", "repos_url": "https://api.github.com/users/jbkyang-nvi/repos", "events_url": "https://api.github.com/users/jbkyang-nvi/events{/privacy}", "received_events_url": "https://api.github.com/users/jbkyang-nvi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jbkyang-nvi", "id": 80359429, "node_id": "MDQ6VXNlcjgwMzU5NDI5", "avatar_url": "https://avatars.githubusercontent.com/u/80359429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbkyang-nvi", "html_url": "https://github.com/jbkyang-nvi", "followers_url": "https://api.github.com/users/jbkyang-nvi/followers", "following_url": "https://api.github.com/users/jbkyang-nvi/following{/other_user}", "gists_url": "https://api.github.com/users/jbkyang-nvi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbkyang-nvi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbkyang-nvi/subscriptions", "organizations_url": "https://api.github.com/users/jbkyang-nvi/orgs", "repos_url": "https://api.github.com/users/jbkyang-nvi/repos", "events_url": "https://api.github.com/users/jbkyang-nvi/events{/privacy}", "received_events_url": "https://api.github.com/users/jbkyang-nvi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2023-03-17T17:52:10Z", "updated_at": "2023-04-27T19:55:56Z", "closed_at": "2023-04-27T19:55:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI have been trying to deploy a model compiled in Neuron to Triton but I'm running into issues each time. \r\nFor this, I followed the [tutorial](https://github.com/triton-inference-server/python_backend/tree/main/inferentia). The model I have used has been Yolov5 from [Ultralytics repo](https://github.com/ultralytics/yolov5) in Pytorch. I have also tried to follow the framework compatibilities guidelines documented in the [Frameworks Support Matrix](https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html).\r\n\r\n**Triton Information**\r\nI have tried the following configurations:\r\n\r\nConfiguration 1:\r\nTriton container: 22.10-py3\r\nOS: Ubuntu 20.04\r\nAMI: Deep Learning Base Neuron AMI (Ubuntu 20.04) 20221012 - Inf1 configured by myself and confirmed to be working outside of the Triton container\r\n\r\nConfiguration 2: \r\nTriton container: 23.02.py3\r\nOS: Ubuntu 20.04\r\nAMI: Deep Learning Base Neuron AMI (Ubuntu 20.04) 20221012\r\n\r\nConfiguration 3: (As indicated in the tutorial - This config looks strange since at beginning of the tutorial it's stated that Triton + Inf only work from 21.11 release )\r\nTriton container: 21.10\r\nOS: Ubuntu 18.04\r\nAMI: Deep Learning AMI (Ubuntu 18.04) Version 63.0\r\n\r\n**To Reproduce**\r\nThe model config file is the following:\r\n\r\nname: \"yolov5_inf\"\r\nbackend: \"python\"\r\nmax_batch_size: 1\r\n\r\n```\r\ninput [\r\n  {\r\n    name: \"INPUT__0\"\r\n    data_type: TYPE_STRING\r\n    dims: [-1]\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"OUTPUT__0\"\r\n    data_type: TYPE_STRING\r\n    dims: [-1]\r\n  }\r\n]\r\n\r\ninstance_group [\r\n    {\r\n        kind: KIND_MODEL\r\n        count: 1\r\n    }\r\n]\r\nparameters: {key: \"COMPILED_MODEL\", value: {string_value: \"/home/ubuntu/model_weights/person-neuron.pt\"}}\r\nparameters: {key: \"NEURON_CORE_START_INDEX\", value: {string_value: \"0\"}}\r\nparameters: {key: \"NEURON_CORE_END_INDEX\", value: {string_value: \"0\"}}\r\nparameters: {key: \"NUM_THREADS_PER_CORE\", value: {string_value: \"1\"}}\r\n```\r\n\r\n\r\n**Expected behavior**\r\nAfter I run the docker container with the following command\r\n\r\n`docker run --device /dev/neuron0 -v /home/ubuntu/model_repository:/home/ubuntu/model_repository -v /home/ubuntu/python_backend:/home/ubuntu/python_backend -v /home/ubuntu/model_weights:/home/ubuntu/model_weights -v /lib/udev:/mylib/udev --shm-size=1g --ulimit memlock=-1 -p 8000:8000 -p 8001:8001 -p 8002:8002 -p 50050:50051 --ulimit stack=67108864 -ti nvcr.io/nvidia/tritonserver:22.10-py3`\r\n\r\nand follow the steps in the tutorial indicated above, the server is unable to start and hangs when trying to initialize the model. It doesn't give any error. \r\n\r\nMoreover, the setup configuration takes a very long time, over 1h.\r\n\r\nThese are the detailed logs I get\r\n\r\n```\r\nW0317 17:44:20.365466 15314 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\nI0317 17:44:20.365555 15314 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\nI0317 17:44:20.366849 15314 model_config_utils.cc:646] Server side auto-completed config: name: \"yolov5_inf\"\r\nmax_batch_size: 1\r\ninput {\r\n  name: \"INPUT__0\"\r\n  data_type: TYPE_STRING\r\n  dims: -1\r\n}\r\noutput {\r\n  name: \"OUTPUT__0\"\r\n  data_type: TYPE_STRING\r\n  dims: -1\r\n}\r\ninstance_group {\r\n  count: 1\r\n  kind: KIND_MODEL\r\n}\r\ndefault_model_filename: \"model.py\"\r\nparameters {\r\n  key: \"COMPILED_MODEL\"\r\n  value {\r\n    string_value: \"/home/ubuntu/model_weights/person-neuron-down-se.pt\"\r\n  }\r\n}\r\nparameters {\r\n  key: \"NEURON_CORE_END_INDEX\"\r\n  value {\r\n    string_value: \"0\"\r\n  }\r\n}\r\nparameters {\r\n  key: \"NEURON_CORE_START_INDEX\"\r\n  value {\r\n    string_value: \"0\"\r\n  }\r\n}\r\nparameters {\r\n  key: \"NUM_THREADS_PER_CORE\"\r\n  value {\r\n    string_value: \"1\"\r\n  }\r\n}\r\nbackend: \"python\"\r\n\r\nI0317 17:44:20.366990 15314 model_lifecycle.cc:459] loading: yolov5_inf:1\r\nI0317 17:44:20.367069 15314 backend_model.cc:348] Adding default backend config setting: default-max-batch-size,4\r\nI0317 17:44:20.367094 15314 shared_library.cc:112] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\r\nI0317 17:44:20.368693 15314 python_be.cc:1614] 'python' TRITONBACKEND API version: 1.11\r\nI0317 17:44:20.368716 15314 python_be.cc:1636] backend configuration:\r\n{\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\r\nI0317 17:44:20.368751 15314 python_be.cc:1766] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\r\nI0317 17:44:20.368917 15314 python_be.cc:2012] TRITONBACKEND_GetBackendAttribute: setting attributes\r\nI0317 17:44:20.368952 15314 python_be.cc:1814] TRITONBACKEND_ModelInitialize: yolov5_inf (version 1)\r\nI0317 17:44:20.369360 15314 model_config_utils.cc:1838] ModelConfig 64-bit fields:\r\nI0317 17:44:20.369376 15314 model_config_utils.cc:1840]         ModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\r\nI0317 17:44:20.369385 15314 model_config_utils.cc:1840]         ModelConfig::dynamic_batching::max_queue_delay_microseconds\r\nI0317 17:44:20.369394 15314 model_config_utils.cc:1840]         ModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\r\nI0317 17:44:20.369430 15314 model_config_utils.cc:1840]         ModelConfig::ensemble_scheduling::step::model_version\r\nI0317 17:44:20.369441 15314 model_config_utils.cc:1840]         ModelConfig::input::dims\r\nI0317 17:44:20.369453 15314 model_config_utils.cc:1840]         ModelConfig::input::reshape::shape\r\nI0317 17:44:20.369474 15314 model_config_utils.cc:1840]         ModelConfig::instance_group::secondary_devices::device_id\r\nI0317 17:44:20.369490 15314 model_config_utils.cc:1840]         ModelConfig::model_warmup::inputs::value::dims\r\nI0317 17:44:20.369501 15314 model_config_utils.cc:1840]         ModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\r\nI0317 17:44:20.369508 15314 model_config_utils.cc:1840]         ModelConfig::optimization::cuda::graph_spec::input::value::dim\r\nI0317 17:44:20.369540 15314 model_config_utils.cc:1840]         ModelConfig::output::dims\r\nI0317 17:44:20.369550 15314 model_config_utils.cc:1840]         ModelConfig::output::reshape::shape\r\nI0317 17:44:20.369559 15314 model_config_utils.cc:1840]         ModelConfig::sequence_batching::direct::max_queue_delay_microseconds\r\nI0317 17:44:20.369578 15314 model_config_utils.cc:1840]         ModelConfig::sequence_batching::max_sequence_idle_microseconds\r\nI0317 17:44:20.369596 15314 model_config_utils.cc:1840]         ModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\r\nI0317 17:44:20.369625 15314 model_config_utils.cc:1840]         ModelConfig::sequence_batching::state::dims\r\nI0317 17:44:20.369635 15314 model_config_utils.cc:1840]         ModelConfig::sequence_batching::state::initial_state::dims\r\nI0317 17:44:20.369658 15314 model_config_utils.cc:1840]         ModelConfig::version_policy::specific::versions\r\nI0317 17:44:20.396773 15314 stub_launcher.cc:251] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /home/ubuntu/model_repository/yolov5_inf/1/model.py triton_python_backend_shm_region_1 67108864 67108864 15314 /opt/tritonserver/backends/python 336 yolov5_inf\r\n/bin/bash: /home/ubuntu/miniconda/envs/test_conda_env/lib/libtinfo.so.6:\r\n\r\n\r\n\r\n```", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5520/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5520/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5503", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5503/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5503/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5503/events", "html_url": "https://github.com/triton-inference-server/server/issues/5503", "id": 1624256808, "node_id": "I_kwDOCQnI4s5g0DEo", "number": 5503, "title": "Request ID not in Response when accessed by In-Process Triton Server API", "user": {"login": "avickars", "id": 49044484, "node_id": "MDQ6VXNlcjQ5MDQ0NDg0", "avatar_url": "https://avatars.githubusercontent.com/u/49044484?v=4", "gravatar_id": "", "url": "https://api.github.com/users/avickars", "html_url": "https://github.com/avickars", "followers_url": "https://api.github.com/users/avickars/followers", "following_url": "https://api.github.com/users/avickars/following{/other_user}", "gists_url": "https://api.github.com/users/avickars/gists{/gist_id}", "starred_url": "https://api.github.com/users/avickars/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/avickars/subscriptions", "organizations_url": "https://api.github.com/users/avickars/orgs", "repos_url": "https://api.github.com/users/avickars/repos", "events_url": "https://api.github.com/users/avickars/events{/privacy}", "received_events_url": "https://api.github.com/users/avickars/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2023-03-14T20:32:37Z", "updated_at": "2023-03-21T01:57:58Z", "closed_at": "2023-03-21T01:57:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nA clear and concise description of what the bug is.\r\n\r\nWhen trying to get the request ID from a TRITONSERVER_InferenceResponse object using the TRITONSERVER_InferenceResponseId defined [here](https://github.com/triton-inference-server/core/blob/24e2d3afd9ceb7f328cbe2b754ba5d4e144622fb/include/triton/core/tritonserver.h#L1334), the returned request ID is empty.  For instance, in the following block of code that is a direct replacement of [this](https://github.com/triton-inference-server/server/blob/db3d08b46bfffc1fbf900b0a69f4e1ed6aa13206/src/simple.cc#L255)\r\n\r\n```\r\nvoid InferResponseComplete(TRITONSERVER_InferenceResponse* response, const uint32_t flags, void* userp) {\r\n    if (response != nullptr) {\r\n        const char* request_id = \"\";\r\n        TRITONSERVER_InferenceResponseId(response, &request_id);\r\n        std::cout << \"Request ID: \\\"\" << request_id << \"\\\"\"<< std::endl;\r\n\r\n        FAIL_IF_ERR(\r\n                TRITONSERVER_InferenceResponseDelete(response),\r\n                \"deleting inference response\");\r\n    }\r\n}\r\n```\r\n\r\nI get the following output:\r\n\r\n`\r\nResponse: \"\"\r\n`\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n\r\n23.01\r\n\r\nAre you using the Triton container or did you build it yourself?\r\n\r\nCurrently using the deepstream:6.2-triton container.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nRun [sample.cc](https://github.com/triton-inference-server/server/blob/db3d08b46bfffc1fbf900b0a69f4e1ed6aa13206/src/simple.cc#L255) but just replace the \"InferResponseComplete\" function with the one shown above.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\nI expected to get the following output:\r\n\r\n`\r\nRequest ID: \"my_request_id\"\r\n`\r\n\r\nAny help would be greatly appreciated as I do need to get the request id for a response.  Any work arounds would be appreciated!", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5503/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5503/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5483", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5483/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5483/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5483/events", "html_url": "https://github.com/triton-inference-server/server/issues/5483", "id": 1618320048, "node_id": "I_kwDOCQnI4s5gdZqw", "number": 5483, "title": "Failed to open the cudaIpcHandle when I call an ONNX / TRT backend from Python backend", "user": {"login": "BasharMahasen", "id": 4402789, "node_id": "MDQ6VXNlcjQ0MDI3ODk=", "avatar_url": "https://avatars.githubusercontent.com/u/4402789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BasharMahasen", "html_url": "https://github.com/BasharMahasen", "followers_url": "https://api.github.com/users/BasharMahasen/followers", "following_url": "https://api.github.com/users/BasharMahasen/following{/other_user}", "gists_url": "https://api.github.com/users/BasharMahasen/gists{/gist_id}", "starred_url": "https://api.github.com/users/BasharMahasen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BasharMahasen/subscriptions", "organizations_url": "https://api.github.com/users/BasharMahasen/orgs", "repos_url": "https://api.github.com/users/BasharMahasen/repos", "events_url": "https://api.github.com/users/BasharMahasen/events{/privacy}", "received_events_url": "https://api.github.com/users/BasharMahasen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "krishung5", "id": 43719498, "node_id": "MDQ6VXNlcjQzNzE5NDk4", "avatar_url": "https://avatars.githubusercontent.com/u/43719498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krishung5", "html_url": "https://github.com/krishung5", "followers_url": "https://api.github.com/users/krishung5/followers", "following_url": "https://api.github.com/users/krishung5/following{/other_user}", "gists_url": "https://api.github.com/users/krishung5/gists{/gist_id}", "starred_url": "https://api.github.com/users/krishung5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krishung5/subscriptions", "organizations_url": "https://api.github.com/users/krishung5/orgs", "repos_url": "https://api.github.com/users/krishung5/repos", "events_url": "https://api.github.com/users/krishung5/events{/privacy}", "received_events_url": "https://api.github.com/users/krishung5/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "krishung5", "id": 43719498, "node_id": "MDQ6VXNlcjQzNzE5NDk4", "avatar_url": "https://avatars.githubusercontent.com/u/43719498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krishung5", "html_url": "https://github.com/krishung5", "followers_url": "https://api.github.com/users/krishung5/followers", "following_url": "https://api.github.com/users/krishung5/following{/other_user}", "gists_url": "https://api.github.com/users/krishung5/gists{/gist_id}", "starred_url": "https://api.github.com/users/krishung5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krishung5/subscriptions", "organizations_url": "https://api.github.com/users/krishung5/orgs", "repos_url": "https://api.github.com/users/krishung5/repos", "events_url": "https://api.github.com/users/krishung5/events{/privacy}", "received_events_url": "https://api.github.com/users/krishung5/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2023-03-10T03:59:26Z", "updated_at": "2023-04-20T19:58:38Z", "closed_at": "2023-04-20T19:58:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI am getting the below error message when I call an ONNX / TRT backend from a Python backend.\r\n\"Failed to open the cudaIpcHandle error. . error: invalid argument\".\r\n\r\n\r\n**Description**\r\nI am receiving the above message whenever I try to to call an ONNX model from a Triton Python Backend. The server logs show that the ONNX model is being called and executed. However, it failed to returns back the results to Python backend. \r\n\r\nThe same behaviour happens for ONNX and TRT backends.\r\nI was able to test direct inference against the ONNX/ TRT model. However, my scenario requires a front end python to handle the pre/post processing tasks.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nI am facing this issue on 23.02-py3 and 23.01-py3 \r\n\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nI am using Triton Container.\r\n\r\n**To Reproduce**\r\n--------------------------------------------------------\r\nHere are the models definitions, the native client logs and the server logs\r\n\r\nname: \"py_main_model\"\r\nbackend: \"python\"\r\nmax_batch_size:10\r\n\r\ninput [\r\n  {\r\n    name: \"image_name\"\r\n    data_type: TYPE_STRING\r\n    dims: [-1]\r\n  }\r\n]\r\n\r\ninput [\r\n  {\r\n    name: \"image_bytes_b46\"\r\n    data_type: TYPE_STRING\r\n    dims: [-1]\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"result\",\r\n    data_type: TYPE_FP32,\r\n    dims: [ -1,1]\r\n\r\n  }\r\n  ]\r\n\r\n\r\ninstance_group: [\r\n        {\r\n            name: \"py_main_model\",\r\n            kind: KIND_CPU,\r\n            count: 1\r\n        }\r\n]\r\n\r\n--------------------------------------------------------\r\nimport json\r\nimport cv2\r\nfrom cv2.dnn import blobFromImage\r\nimport numpy as np\r\nimport triton_python_backend_utils as pb_utils\r\nimport time\r\nimport base64\r\n\r\n\r\nclass TritonPythonModel:\r\n\r\n\r\n    def initialize(self, args):\r\n        # You must parse model_config. JSON string is not parsed here\r\n        self.model_config = model_config = json.loads(args['model_config'])\r\n        self.model_std = 128.0\r\n        self.model_mean = 127.5\r\n        self.model_height = 640\r\n        self.model_width = 640\r\n        self.detection_model_name = \"onnx_scrfd_10g_bnkps_detection_model\"\r\n        self.logger = pb_utils.Logger\r\n\r\n\r\n    def execute(self, requests):\r\n        responses = []\r\n        for request in requests:\r\n\r\n            input0_tensor = pb_utils.get_input_tensor_by_name(request, \"image_name\")\r\n            input1_tensor = pb_utils.get_input_tensor_by_name(request, \"image_bytes_b46\")\r\n            \r\n            input0_data = input0_tensor.as_numpy()[0][0]\r\n            image_name = input0_data.decode(\"utf-8\")\r\n            input1_data = input1_tensor.as_numpy()[0][0]\r\n            img_jpg_bytes_b64 = input1_data.decode(\"utf-8\")\r\n            img_jpg_str = base64.b64decode(img_jpg_bytes_b64)\r\n            \r\n            image_data = cv2.imdecode(np.frombuffer(img_jpg_str, np.uint8), cv2.IMREAD_COLOR)  # as numpy array\r\n            processed_img, resize_scale = self.preprocess_for_detection(img=image_data)\r\n            img_blob = self.convert_to_blob(processed_img)\r\n            \r\n            # # create an input tensor for the detection model\r\n            input_tensor = pb_utils.Tensor(\"input.1\", np.random.random((1,3,640,640)).astype(\"float32\"))\r\n            infer_request = pb_utils.InferenceRequest(\r\n                request_id=str(1),\r\n                model_name=self.detection_model_name,\r\n                requested_output_names=self.detection_model_outputs_name,\r\n                inputs=[input_tensor])\r\n\r\n            infer_response = infer_request.exec()\r\n            if infer_response.has_error():\r\n                raise pb_utils.TritonModelException(\r\n                    infer_response.error().message())\r\n            inference_response = pb_utils.InferenceResponse(output_tensors=infer_response.output_tensors())\r\n            responses.append(inference_response)\r\n            self.logger.log_info(f\"Exec,{time.perf_counter() - t1:.3f},Prepare,{t1 - t0:.3f}\")\r\n            ...................\r\n            #convert responses ....\r\n            ...................\r\n            \r\n        return processed_responses\r\n        \r\n\r\n...................\r\n\r\n\r\n--------------------------------------------------------\r\n\r\nname: \"onnx_scrfd_10g_bnkps_detection_model\"\r\nplatform: \"onnxruntime_onnx\"\r\nmax_batch_size : 10\r\n\r\ninput [\r\n  {\r\n    name: \"input.1\"\r\n    data_type: TYPE_FP32\r\n    dims: [3, -1, -1]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"score_8\",\r\n    data_type: TYPE_FP32,\r\n    dims: [ -1,1]\r\n\r\n  }\r\n  ]\r\n\r\n  output [\r\n  {\r\n    name: \"bbox_8\",\r\n    data_type: TYPE_FP32,\r\n    dims: [-1, 4]\r\n\r\n  }]\r\n\r\noutput [\r\n  {\r\n    name: \"kps_8\",\r\n    data_type: TYPE_FP32,\r\n    dims: [ -1,10]\r\n\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"score_16\",\r\n    data_type: TYPE_FP32,\r\n    dims: [ -1,1]\r\n\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"bbox_16\",\r\n    data_type: TYPE_FP32,\r\n    dims: [-1, 4]\r\n\r\n  }\r\n ]\r\n\r\noutput [\r\n  {\r\n    name: \"kps_16\",\r\n    data_type: TYPE_FP32,\r\n    dims: [ -1,10]\r\n\r\n  }\r\n ]\r\n\r\noutput [\r\n  {\r\n    name: \"score_32\",\r\n    data_type: TYPE_FP32,\r\n    dims: [ -1,1]\r\n\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"bbox_32\",\r\n    data_type: TYPE_FP32,\r\n    dims: [ -1,4]\r\n\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"kps_32\",\r\n    data_type: TYPE_FP32,\r\n    dims:[-1,10]\r\n\r\n  }\r\n]\r\n\r\n\r\ninstance_group: [\r\n        {\r\n            name: \"onnx_scrfd_10g_bnkps_detection_model\",\r\n            kind: KIND_GPU,\r\n            count: 1\r\n\r\n        }\r\n]\r\n--------------------------------------------------------\r\n\r\n\r\n\r\n** Client Logs **\r\n-------------------------------------\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\projects\\FRApp-Triton\\Python\\client\\client_triton_native.py\", line 47, in <module>\r\n    response = client.infer(model_name,\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\fr_onnx_env\\lib\\site-packages\\tritonclient\\grpc\\__init__.py\", line 1446, in infer\r\n    raise_error_grpc(rpc_error)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\fr_onnx_env\\lib\\site-packages\\tritonclient\\grpc\\__init__.py\", line 76, in raise_error_grpc\r\n    raise get_error_grpc(rpc_error) from None\r\ntritonclient.utils.InferenceServerException: [StatusCode.INTERNAL] Failed to process the request(s) for model instance 'fr_main_model', message: TritonModelException: Failed to open the cudaIpcHandle. error: invalid argument\r\n\r\nAt:  /models/fr_main_model/1/model.py(86): execute\r\n\r\n\r\n** Server Logs **\r\n-------------------------------------\r\n\r\nI0310 03:49:01.579260 1 model_lifecycle.cc:428] AsyncLoad() 'fr_main_model'\r\nI0310 03:49:01.588920 1 model_lifecycle.cc:459] loading: fr_main_model:1\r\nI0310 03:49:01.589062 1 model_lifecycle.cc:509] CreateModel() 'fr_main_model' version 1\r\nI0310 03:49:01.593124 1 backend_model.cc:348] Adding default backend config setting: default-max-batch-size,4\r\nI0310 03:49:01.593206 1 python_be.cc:1814] TRITONBACKEND_ModelInitialize: fr_main_model (version 1)\r\nI0310 03:49:01.606389 1 stub_launcher.cc:251] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /models/fr_main_model/1/model.py triton_python_backend_shm_region_13 67108864 67108864 1 /opt/tritonserver/backends/python 336 fr_main_model\r\nI0310 03:49:03.199017 1 python_be.cc:1594] model configuration:\r\n{\r\n    \"name\": \"fr_main_model\",\r\n    \"platform\": \"\",\r\n    \"backend\": \"python\",\r\n    \"version_policy\": {\r\n        \"latest\": {\r\n            \"num_versions\": 1\r\n        }\r\n    },\r\n    \"max_batch_size\": 10,\r\n    \"input\": [\r\n        {\r\n            \"name\": \"image_name\",\r\n            \"data_type\": \"TYPE_STRING\",\r\n            \"format\": \"FORMAT_NONE\",\r\n            \"dims\": [\r\n                -1\r\n            ],\r\n            \"is_shape_tensor\": false,\r\n            \"allow_ragged_batch\": false,\r\n            \"optional\": false\r\n        },\r\n        {\r\n            \"name\": \"image_bytes_b46\",\r\n            \"data_type\": \"TYPE_STRING\",\r\n            \"format\": \"FORMAT_NONE\",\r\n            \"dims\": [\r\n                -1\r\n            ],\r\n            \"is_shape_tensor\": false,\r\n            \"allow_ragged_batch\": false,\r\n            \"optional\": false\r\n        }\r\n    ],\r\n    \"output\": [\r\n        {\r\n            \"name\": \"result\",\r\n            \"data_type\": \"TYPE_FP32\",\r\n            \"dims\": [\r\n                -1,\r\n                1\r\n            ],\r\n            \"label_filename\": \"\",\r\n            \"is_shape_tensor\": false\r\n        }\r\n    ],\r\n    \"batch_input\": [],\r\n    \"batch_output\": [],\r\n    \"optimization\": {\r\n        \"priority\": \"PRIORITY_DEFAULT\",\r\n        \"input_pinned_memory\": {\r\n            \"enable\": true\r\n        },\r\n        \"output_pinned_memory\": {\r\n            \"enable\": true\r\n        },\r\n        \"gather_kernel_buffer_threshold\": 0,\r\n        \"eager_batching\": false\r\n    },\r\n    \"instance_group\": [\r\n        {\r\n            \"name\": \"fr_main_model\",\r\n            \"kind\": \"KIND_CPU\",\r\n            \"count\": 1,\r\n            \"gpus\": [],\r\n            \"secondary_devices\": [],\r\n            \"profile\": [],\r\n            \"passive\": false,\r\n            \"host_policy\": \"\"\r\n        }\r\n    ],\r\n    \"default_model_filename\": \"model.py\",\r\n    \"cc_model_filenames\": {},\r\n    \"metric_tags\": {},\r\n    \"parameters\": {},\r\n    \"model_warmup\": []\r\n}\r\nI0310 03:49:03.204836 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: fr_main_model (CPU device 0)\r\nI0310 03:49:03.204888 1 backend_model_instance.cc:68] Creating instance fr_main_model on CPU using artifact 'model.py'\r\nI0310 03:49:03.219671 1 stub_launcher.cc:251] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /models/fr_main_model/1/model.py triton_python_backend_shm_region_14 67108864 67108864 1 /opt/tritonserver/backends/python 336 fr_main_model\r\nI0310 03:49:03.559618 1 model.py:53] FR Python Model is Loaded !!\r\nI0310 03:49:03.562520 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful fr_main_model (device 0)\r\nI0310 03:49:03.562663 1 backend_model_instance.cc:766] Starting backend thread for fr_main_model at nice 0 on device 0...\r\nI0310 03:49:03.563025 1 model_lifecycle.cc:694] successfully loaded 'fr_main_model' version 1\r\nI0310 03:49:03.563099 1 backend_model_instance.cc:789] Stopping backend thread for fr_main_model...\r\nI0310 03:49:03.563114 1 model_lifecycle.cc:285] VersionStates() 'fr_main_model'\r\nI0310 03:49:03.563225 1 python_be.cc:1998] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\n<class 'c_python_backend_utils.Logger'>\r\nCleaning up...\r\nI0310 03:49:04.776534 1 python_be.cc:1837] TRITONBACKEND_ModelFinalize: delete model state\r\nI0310 03:49:04.776595 1 model_lifecycle.cc:577] OnDestroy callback() 'fr_main_model' version 1\r\nI0310 03:49:04.776601 1 model_lifecycle.cc:579] successfully unloaded 'fr_main_model' version 1\r\nI0310 03:49:18.563314 1 server.cc:334] Polling model repository\r\nI0310 03:49:33.706748 1 server.cc:334] Polling model repository\r\nI0310 03:49:46.116211 1 grpc_server.cc:3848] Process for ModelInferHandler, rpc_ok=1, 0 step START\r\nI0310 03:49:46.116262 1 grpc_server.cc:3841] New request handler for ModelInferHandler, 0\r\nI0310 03:49:46.116272 1 model_lifecycle.cc:327] GetModel() 'fr_main_model' version -1\r\nI0310 03:49:46.116280 1 model_lifecycle.cc:327] GetModel() 'fr_main_model' version -1\r\nI0310 03:49:46.116303 1 infer_request.cc:729] [request id: 101] prepared: [0x0x7f8634004b00] request id: 101, model: fr_main_model, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7f8634005908] input: image_bytes_b46, type: BYTES, original shape: [1,1], batch + shape: [1,1], shape: [1]\r\n[0x0x7f863400a158] input: image_name, type: BYTES, original shape: [1,1], batch + shape: [1,1], shape: [1]\r\noverride inputs:\r\ninputs:\r\n[0x0x7f863400a158] input: image_name, type: BYTES, original shape: [1,1], batch + shape: [1,1], shape: [1]\r\n[0x0x7f8634005908] input: image_bytes_b46, type: BYTES, original shape: [1,1], batch + shape: [1,1], shape: [1]\r\noriginal requested outputs:\r\nresult\r\nrequested outputs:\r\nresult\r\n\r\nI0310 03:49:46.116410 1 python_be.cc:1094] model fr_main_model, instance fr_main_model, executing 1 requests\r\nI0310 03:49:46.170530 1 model_lifecycle.cc:327] GetModel() 'onnx_scrfd_10g_bnkps_detection_model' version -1\r\nI0310 03:49:46.175708 1 model_lifecycle.cc:327] GetModel() 'onnx_scrfd_10g_bnkps_detection_model' version -1\r\nI0310 03:49:46.175752 1 model_lifecycle.cc:327] GetModel() 'onnx_scrfd_10g_bnkps_detection_model' version -1\r\nI0310 03:49:46.176831 1 infer_request.cc:729] [request id: 1] prepared: [0x0x7f86d4001f50] request id: 1, model: onnx_scrfd_10g_bnkps_detection_model, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7f86d4002218] input: input.1, type: FP32, original shape: [1,3,640,640], batch + shape: [1,3,640,640], shape: [3,640,640]\r\noverride inputs:\r\ninputs:\r\n[0x0x7f86d4002218] input: input.1, type: FP32, original shape: [1,3,640,640], batch + shape: [1,3,640,640], shape: [3,640,640]\r\noriginal requested outputs:\r\nbbox_16\r\nbbox_32\r\nbbox_8\r\nkps_16\r\nkps_32\r\nkps_8\r\nscore_16\r\nscore_32\r\nscore_8\r\nrequested outputs:\r\nbbox_16\r\nbbox_32\r\nbbox_8\r\nkps_16\r\nkps_32\r\nkps_8\r\nscore_16\r\nscore_32\r\nscore_8\r\n\r\nI0310 03:49:46.179897 1 onnxruntime.cc:2672] model onnx_scrfd_10g_bnkps_detection_model, instance onnx_scrfd_10g_bnkps_detection_model_0, executing 1 requests\r\nI0310 03:49:46.179935 1 onnxruntime.cc:1469] TRITONBACKEND_ModelExecute: Running onnx_scrfd_10g_bnkps_detection_model_0 with 1 requests\r\n2023-03-10 03:49:46.272369552 [I:onnxruntime:log, bfc_arena.cc:26 BFCArena] Creating BFCArena for Cuda with following configs: initial_chunk_size_bytes: 1048576 max_dead_bytes_per_chunk: 134217728 initial_growth_chunk_size_bytes: 2097152 memory limit: 18446744073709551615 arena_extend_strategy: 0\r\n2023-03-10 03:49:46.275071883 [V:onnxruntime:log, bfc_arena.cc:62 BFCArena] Creating 21 bins of max chunk size 256 to 268435456\r\n2023-03-10 03:49:46.275389987 [I:onnxruntime:log, bfc_arena.cc:317 AllocateRawInternal] Extending BFCArena for Cuda. bin_num:14 (requested) num_bytes: 4915200 (actual) rounded_bytes:4915200\r\n2023-03-10 03:49:46.276985905 [I:onnxruntime:log, bfc_arena.cc:197 Extend] Extended allocation by 8388608 bytes.\r\n2023-03-10 03:49:46.277021006 [I:onnxruntime:log, bfc_arena.cc:200 Extend] Total allocated bytes: 8388608\r\n2023-03-10 03:49:46.277028406 [I:onnxruntime:log, bfc_arena.cc:203 Extend] Allocated memory at 0xb53c00000 to 0xb54400000\r\n2023-03-10 03:49:46.290202358 [I:onnxruntime:, sequential_executor.cc:176 Execute] Begin execution\r\n2023-03-10 03:49:46.293022590 [I:onnxruntime:log, bfc_arena.cc:317 AllocateRawInternal] Extending BFCArena for Cuda. bin_num:15 (requested) num_bytes: 11468800 (actual) rounded_bytes:11468800\r\n2023-03-10 03:49:46.293763699 [I:onnxruntime:log, bfc_arena.cc:197 Extend] Extended allocation by 16777216 bytes.\r\n2023-03-10 03:49:46.293805399 [I:onnxruntime:log, bfc_arena.cc:200 Extend] Total allocated bytes: 25165824\r\n2023-03-10 03:49:46.293815900 [I:onnxruntime:log, bfc_arena.cc:203 Extend] Allocated memory at 0xb54400000 to 0xb55400000\r\n2023-03-10 03:49:46.334326567 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:48.645325223 [I:onnxruntime:log, bfc_arena.cc:317 AllocateRawInternal] Extending BFCArena for Cuda. bin_num:11 (requested) num_bytes: 618624 (actual) rounded_bytes:618752\r\n2023-03-10 03:49:48.645912229 [I:onnxruntime:log, bfc_arena.cc:197 Extend] Extended allocation by 16777216 bytes.\r\n2023-03-10 03:49:48.645953630 [I:onnxruntime:log, bfc_arena.cc:200 Extend] Total allocated bytes: 41943040\r\n2023-03-10 03:49:48.645964130 [I:onnxruntime:log, bfc_arena.cc:203 Extend] Allocated memory at 0xb55400000 to 0xb56400000\r\n2023-03-10 03:49:48.648471459 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:48.709687565 [I:onnxruntime:log, bfc_arena.cc:317 AllocateRawInternal] Extending BFCArena for Cuda. bin_num:11 (requested) num_bytes: 643840 (actual) rounded_bytes:643840\r\n2023-03-10 03:49:48.710296172 [I:onnxruntime:log, bfc_arena.cc:197 Extend] Extended allocation by 33554432 bytes.\r\n2023-03-10 03:49:48.710336072 [I:onnxruntime:log, bfc_arena.cc:200 Extend] Total allocated bytes: 75497472\r\n2023-03-10 03:49:48.710346673 [I:onnxruntime:log, bfc_arena.cc:203 Extend] Allocated memory at 0xb66200000 to 0xb68200000\r\n2023-03-10 03:49:48.711128782 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:48.809422715 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\nI0310 03:49:48.856349 1 server.cc:334] Polling model repository\r\n2023-03-10 03:49:48.871865836 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:48.926574867 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:48.974499519 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:48.981579901 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:48.988586882 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.001684933 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.009385222 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.015513693 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.025537008 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.030749168 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.035117219 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.039637271 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.044339925 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.048902978 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.053202027 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.058668390 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.062754437 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.067573093 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.073085757 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.077894612 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.081744956 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.086769114 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.090807361 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.096415126 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.101409283 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.107921458 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.112702114 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.116719060 [I:onnxruntime:log, bfc_arena.cc:317 AllocateRawInternal] Extending BFCArena for CUDA_CPU. bin_num:0 (requested) num_bytes: 32 (actual) rounded_bytes:256\r\n2023-03-10 03:49:49.117431268 [I:onnxruntime:log, bfc_arena.cc:197 Extend] Extended allocation by 1048576 bytes.\r\n2023-03-10 03:49:49.117466768 [I:onnxruntime:log, bfc_arena.cc:200 Extend] Total allocated bytes: 1048576\r\n2023-03-10 03:49:49.117474269 [I:onnxruntime:log, bfc_arena.cc:203 Extend] Allocated memory at 0x7f85ea4f11a0 to 0x7f85ea5f11a0\r\n2023-03-10 03:49:49.118298678 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.126844577 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.132116837 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.136446887 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.140996940 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.145188588 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.148632128 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.156911923 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.160916970 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.164905516 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.169633570 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.174097622 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.181343305 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.188075083 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.195068364 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.199037509 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.205609585 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.210187938 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.215340897 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.220104052 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.224429302 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.232321393 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.237067248 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.243079017 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.248245377 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.253628739 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\n2023-03-10 03:49:49.258519795 [I:onnxruntime:log, bfc_arena.cc:267 Reserve] Reserving memory in BFCArena for Cuda size: 33554432\r\nI0310 03:49:49.263064 1 infer_response.cc:167] add response output: output: bbox_16, type: FP32, shape: [1,3200,4]\r\nI0310 03:49:49.265057 1 infer_response.cc:167] add response output: output: bbox_32, type: FP32, shape: [1,800,4]\r\nI0310 03:49:49.265146 1 infer_response.cc:167] add response output: output: bbox_8, type: FP32, shape: [1,12800,4]\r\nI0310 03:49:49.265207 1 infer_response.cc:167] add response output: output: kps_16, type: FP32, shape: [1,3200,10]\r\nI0310 03:49:49.265252 1 infer_response.cc:167] add response output: output: kps_32, type: FP32, shape: [1,800,10]\r\nI0310 03:49:49.265319 1 infer_response.cc:167] add response output: output: kps_8, type: FP32, shape: [1,12800,10]\r\nI0310 03:49:49.266020 1 infer_response.cc:167] add response output: output: score_16, type: FP32, shape: [1,3200,1]\r\nI0310 03:49:49.266107 1 infer_response.cc:167] add response output: output: score_32, type: FP32, shape: [1,800,1]\r\nI0310 03:49:49.266153 1 infer_response.cc:167] add response output: output: score_8, type: FP32, shape: [1,12800,1]\r\nI0310 03:49:49.669613 1 grpc_server.cc:4012] ModelInferHandler::InferResponseComplete, 0 step ISSUED\r\nI0310 03:49:49.669897 1 grpc_server.cc:3848] Process for ModelInferHandler, rpc_ok=1, 0 step COMPLETE\r\nI0310 03:49:49.670268 1 grpc_server.cc:2758] Done for ModelInferHandler, 0\r\nI0310 03:49:49.669914 1 grpc_server.cc:3566] ModelInferHandler::InferRequestComplete\r\nI0310 03:49:49.670586 1 python_be.cc:1980] TRITONBACKEND_ModelInstanceExecute: model instance name fr_main_model released 1 requests\r\nI0310 03:50:03.995909 1 server.cc:334] Polling model repository\r\nI0310 03:50:19.118287 1 server.cc:334] Polling model repository\r\nI0310 03:50:34.259362 1 server.cc:334] Polling model repository\r\nI0310 03:50:49.414143 1 server.cc:334] Polling model repository\r\nI0310 03:51:04.543423 1 server.cc:334] Polling model repository\r\nI0310 03:51:19.688254 1 server.cc:334] Polling model repository\r\nI0310 03:51:34.827078 1 server.cc:334] Polling model repository\r\nI0310 03:51:49.962146 1 server.cc:334] Polling model repository\r\n\r\n\r\n**Expected behavior**\r\nThe model should pass back the results to python model.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5483/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5471", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5471/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5471/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5471/events", "html_url": "https://github.com/triton-inference-server/server/issues/5471", "id": 1616295921, "node_id": "I_kwDOCQnI4s5gVrfx", "number": 5471, "title": "Inference result of single batch ONNX model contains all zeros and also emits \"Failed to open the cudaIpcHandle.\" error in additional inference calls", "user": {"login": "jackylu0124", "id": 28615340, "node_id": "MDQ6VXNlcjI4NjE1MzQw", "avatar_url": "https://avatars.githubusercontent.com/u/28615340?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jackylu0124", "html_url": "https://github.com/jackylu0124", "followers_url": "https://api.github.com/users/jackylu0124/followers", "following_url": "https://api.github.com/users/jackylu0124/following{/other_user}", "gists_url": "https://api.github.com/users/jackylu0124/gists{/gist_id}", "starred_url": "https://api.github.com/users/jackylu0124/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jackylu0124/subscriptions", "organizations_url": "https://api.github.com/users/jackylu0124/orgs", "repos_url": "https://api.github.com/users/jackylu0124/repos", "events_url": "https://api.github.com/users/jackylu0124/events{/privacy}", "received_events_url": "https://api.github.com/users/jackylu0124/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2023-03-09T02:38:38Z", "updated_at": "2023-04-25T15:24:58Z", "closed_at": "2023-04-25T15:24:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI am calling inference requests on multiple ONNX models using the ONNX Runtime CUDA backend (`KIND_GPU`) in the Python backend file. For my ONNX model that takes in input with fixed batch size of 1, the inference request returns a tensor that contains all zeros, which is different from the ONNX inference results using pure ONNX Runtime inference outside of Triton. For the purpose of reproduction of this issue, I have created two very simple ONNX models that contain only one convolution layer inside, one model (`conv_single_batch.onnx`) takes in an input with fixed size of `1x3x473x473` and the other model (`conv_dynamic_batch.onnx`) can take input with dynamic batch size (e.g. `Nx3x473x473`), and the only single convolution layer in both models have non-zero weights and biases, and the reproduction example tries to run these two models on an input tensor with all ones. The behavior I have observed is that the inference request on the `conv_single_batch.onnx` always returns a tensor with all zeros, and subsequent inference calls on it will lead Triton to emit the error message `\"tritonclient.utils.InferenceServerException: Failed to process the request(s) for model instance 'pipeline_0', message: TritonModelException: Failed to open the cudaIpcHandle. error: unknown error\"`. However, if I switch the inference backend from `KIND_GPU` to `KIND_CPU`, then sometimes it could return results that are not all zeros. On the other hand, for the `conv_dynamic_batch.onnx` model that can take in dynamic batch size, the first inference call to it could produce correct results, but likewise subsequent inference calls to it lead to the same error mesasge `\"tritonclient.utils.InferenceServerException: Failed to process the request(s) for model instance 'pipeline_0', message: TritonModelException: Failed to open the cudaIpcHandle. error: unknown error\"`. I have attached the entire zipped project with code and ONNX models below, and for others' convenience, I also pasted my code as well as screenshots of my ONNX models' structure below.\r\n\r\n**Zipped Folder Containing All Files And ONNX Models**\r\n[TritonDebug.zip](https://github.com/triton-inference-server/server/files/10927093/TritonDebug.zip)\r\n\r\n**Triton Information**\r\nI am using the `nvcr.io/nvidia/tritonserver:23.01-py3` Docker container.\r\n\r\n**To Reproduce**\r\nI have included a simple client file (`client.py`) in the zipped folder that makes inference requests to the Triton Inferenc Server. You can reproduce the issues mentioned above by running the client file after launching the Triton Inference Server.\r\n\r\n**Expected behavior**\r\nInference requests' results should not return tensors containing all zeros and additional inference request calls should not cause Triton Inference Server to emit the error message `\"tritonclient.utils.InferenceServerException: Failed to process the request(s) for model instance 'pipeline_0', message: TritonModelException: Failed to open the cudaIpcHandle. error: unknown error\"`.\r\n\r\n**Python Backend File (`model.py`)**\r\n```\r\nimport triton_python_backend_utils as pb_utils\r\nimport os\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torch.utils.dlpack import to_dlpack, from_dlpack\r\n\r\ndef get_response_tensor_by_name(response, name):\r\n    if response.has_error():\r\n        raise pb_utils.TritonModelException(response.error().message())\r\n    else:\r\n        pb_tensor = pb_utils.get_output_tensor_by_name(response, name)\r\n        if pb_tensor.is_cpu():\r\n            return pb_tensor.as_numpy()\r\n        else:\r\n            return from_dlpack(pb_tensor.to_dlpack()).cpu().numpy()\r\n\r\ndef run_single_batch_model(img):\r\n    input = pb_utils.Tensor(\"img\", img)\r\n    request = pb_utils.InferenceRequest(model_name=\"single_batch\", inputs=[input], requested_output_names=[\"out\"])\r\n    response = request.exec()\r\n    out = get_response_tensor_by_name(response, \"out\")\r\n    return out\r\n\r\ndef run_dynamic_batch_model(img):\r\n    input = pb_utils.Tensor(\"img\", img)\r\n    request = pb_utils.InferenceRequest(model_name=\"dynamic_batch\", inputs=[input], requested_output_names=[\"out\"])\r\n    response = request.exec()\r\n    out = get_response_tensor_by_name(response, \"out\")\r\n    return out\r\n\r\nclass TritonPythonModel:\r\n    def initialize(self, args):\r\n        self.logger = pb_utils.Logger\r\n        self.logger.log_info(\"Initialization completed.\")\r\n\r\n    def execute(self, requests):\r\n        responses = []\r\n        for request in requests:\r\n            img = pb_utils.get_input_tensor_by_name(request, \"img\").as_numpy()\r\n\r\n            single_batch_output = run_single_batch_model(img.copy())\r\n            self.logger.log_info(\"single_batch_output: \" + str(single_batch_output))\r\n\r\n            inference_response = pb_utils.InferenceResponse(output_tensors=[\r\n                pb_utils.Tensor(\r\n                    \"out\",\r\n                    single_batch_output\r\n                )\r\n            ])\r\n\r\n            # dynamic_batch_output = run_dynamic_batch_model(img.copy())\r\n            # self.logger.log_info(\"dynamic_batch_output: \" + str(dynamic_batch_output))\r\n\r\n            # inference_response = pb_utils.InferenceResponse(output_tensors=[\r\n            #     pb_utils.Tensor(\r\n            #         \"out\",\r\n            #         dynamic_batch_output\r\n            #     )\r\n            # ])\r\n            responses.append(inference_response)\r\n\r\n        return responses\r\n\r\n    def finalize(self):\r\n        self.logger.log_info(\"Finalization/clean-up completed.\")\r\n```\r\n\r\nPython Backend Config File\r\n```\r\nname: \"pipeline\"\r\nbackend: \"python\"\r\nmax_batch_size: 4\r\n\r\ninput [\r\n    {\r\n        name: \"img\"\r\n        data_type: TYPE_FP32\r\n        dims: [3, 473, 473]\r\n    }\r\n]\r\n\r\noutput [\r\n    {\r\n        name: \"out\"\r\n        data_type: TYPE_FP32\r\n        dims: [20, 471, 471]\r\n    }\r\n]\r\n\r\ninstance_group [\r\n    {\r\n        kind: KIND_GPU\r\n    }\r\n]\r\n```\r\n\r\n**Single Batch Model Config File (for `conv_single_batch.onnx`)**\r\n```\r\nname: \"single_batch\"\r\nplatform: \"onnxruntime_onnx\"\r\ndefault_model_filename: \"conv_single_batch.onnx\"\r\nmax_batch_size: 0\r\n\r\ninput [\r\n    {\r\n        name: \"img\"\r\n        data_type: TYPE_FP32\r\n        dims: [1, 3, 473, 473]\r\n    }\r\n]\r\n\r\noutput [\r\n    {\r\n        name: \"out\"\r\n        data_type: TYPE_FP32\r\n        dims: [1, 20, 471, 471]\r\n    }\r\n]\r\n\r\ninstance_group [\r\n    {\r\n        kind: KIND_GPU\r\n    }\r\n]\r\n\r\nmodel_warmup [\r\n    {\r\n        name: \"Random Sample\"\r\n        inputs {\r\n            key: \"img\"\r\n            value: {\r\n                data_type: TYPE_FP32\r\n                dims: [1, 3, 473, 473]\r\n                random_data: true\r\n            }\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n**Dynamic Batch Size Model Config File (for `conv_dynamic_batch.onnx`)**\r\n```\r\nname: \"dynamic_batch\"\r\nplatform: \"onnxruntime_onnx\"\r\ndefault_model_filename: \"conv_dynamic_batch.onnx\"\r\nmax_batch_size: 4\r\n\r\ninput [\r\n    {\r\n        name: \"img\"\r\n        data_type: TYPE_FP32\r\n        dims: [3, 473, 473]\r\n    }\r\n]\r\n\r\noutput [\r\n    {\r\n        name: \"out\"\r\n        data_type: TYPE_FP32\r\n        dims: [20, 471, 471]\r\n    }\r\n]\r\n\r\ninstance_group [\r\n    {\r\n        kind: KIND_GPU\r\n    }\r\n]\r\n\r\nmodel_warmup [\r\n    {\r\n        name: \"Random Sample\"\r\n        batch_size: 4\r\n        inputs {\r\n            key: \"img\"\r\n            value: {\r\n                data_type: TYPE_FP32\r\n                dims: [3, 473, 473]\r\n                random_data: true\r\n            }\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n**Client file for making inference requests (client.py)**\r\n```\r\nimport numpy as np\r\nfrom tritonclient.utils import *\r\nimport tritonclient.http as httpclient\r\n\r\ndef main():\r\n    client = httpclient.InferenceServerClient(url=\"localhost:8000\")\r\n    img = np.ones((1, 3, 473, 473), dtype=np.float32)\r\n    img_input = httpclient.InferInput(\"img\", img.shape,\r\n                                       np_to_triton_dtype(img.dtype))\r\n    img_input.set_data_from_numpy(img)\r\n\r\n    out = httpclient.InferRequestedOutput(\"out\")\r\n\r\n    response = client.infer(model_name=\"pipeline\",\r\n                            inputs=[img_input],\r\n                            outputs=[out])\r\n\r\n    image = response.as_numpy(\"out\")\r\n    print(image)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n**Screenshot of the `conv_single_batch.onnx` model in Netron**\r\n![image](https://user-images.githubusercontent.com/28615340/223901172-df9cab55-08fe-4fc9-aaa3-036de30d3202.png)\r\n\r\n**Screenshot of the `conv_dynamic_batch.onnx` model in Netron**\r\n![image](https://user-images.githubusercontent.com/28615340/223901311-93057ff2-67a0-435e-bd5b-1ea9442beb8c.png)", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5471/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5328", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5328/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5328/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5328/events", "html_url": "https://github.com/triton-inference-server/server/issues/5328", "id": 1574324614, "node_id": "I_kwDOCQnI4s5d1kmG", "number": 5328, "title": "Signal 11 received while stress testing", "user": {"login": "AndreWanga", "id": 114132971, "node_id": "U_kgDOBs2H6w", "avatar_url": "https://avatars.githubusercontent.com/u/114132971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreWanga", "html_url": "https://github.com/AndreWanga", "followers_url": "https://api.github.com/users/AndreWanga/followers", "following_url": "https://api.github.com/users/AndreWanga/following{/other_user}", "gists_url": "https://api.github.com/users/AndreWanga/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreWanga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreWanga/subscriptions", "organizations_url": "https://api.github.com/users/AndreWanga/orgs", "repos_url": "https://api.github.com/users/AndreWanga/repos", "events_url": "https://api.github.com/users/AndreWanga/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreWanga/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2023-02-07T13:21:23Z", "updated_at": "2023-02-27T16:10:04Z", "closed_at": "2023-02-27T16:10:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen I do stress test, tritonserver dumped with signal 11\r\n\r\n**Triton Information**\r\ntritonserver version: 2.24.0\r\n\r\nuse the Triton container\r\n\r\n**To Reproduce**\r\n1. start triton server with TensorRT model\r\n2. do stress test with 100 users by using grpc\r\n3. wait for servaral minitues\r\n![signal 11](https://user-images.githubusercontent.com/114132971/217255950-b10e1092-61cb-4abc-87bd-5e8af1746a4c.gif)\r\n\r\n\r\nSimbert(12-layer Bert) NER(4-layer Bert)\r\n\r\n**Expected behavior**\r\nFunctionlly well during stress test\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5328/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5328/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5296", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5296/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5296/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5296/events", "html_url": "https://github.com/triton-inference-server/server/issues/5296", "id": 1566273277, "node_id": "I_kwDOCQnI4s5dW279", "number": 5296, "title": "Wrong prometheus metrics reports while using horizontally placed nodes in an ensemble model DAG", "user": {"login": "julienripoche", "id": 12826149, "node_id": "MDQ6VXNlcjEyODI2MTQ5", "avatar_url": "https://avatars.githubusercontent.com/u/12826149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/julienripoche", "html_url": "https://github.com/julienripoche", "followers_url": "https://api.github.com/users/julienripoche/followers", "following_url": "https://api.github.com/users/julienripoche/following{/other_user}", "gists_url": "https://api.github.com/users/julienripoche/gists{/gist_id}", "starred_url": "https://api.github.com/users/julienripoche/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/julienripoche/subscriptions", "organizations_url": "https://api.github.com/users/julienripoche/orgs", "repos_url": "https://api.github.com/users/julienripoche/repos", "events_url": "https://api.github.com/users/julienripoche/events{/privacy}", "received_events_url": "https://api.github.com/users/julienripoche/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "rmccorm4", "id": 21284872, "node_id": "MDQ6VXNlcjIxMjg0ODcy", "avatar_url": "https://avatars.githubusercontent.com/u/21284872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmccorm4", "html_url": "https://github.com/rmccorm4", "followers_url": "https://api.github.com/users/rmccorm4/followers", "following_url": "https://api.github.com/users/rmccorm4/following{/other_user}", "gists_url": "https://api.github.com/users/rmccorm4/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmccorm4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmccorm4/subscriptions", "organizations_url": "https://api.github.com/users/rmccorm4/orgs", "repos_url": "https://api.github.com/users/rmccorm4/repos", "events_url": "https://api.github.com/users/rmccorm4/events{/privacy}", "received_events_url": "https://api.github.com/users/rmccorm4/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmccorm4", "id": 21284872, "node_id": "MDQ6VXNlcjIxMjg0ODcy", "avatar_url": "https://avatars.githubusercontent.com/u/21284872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmccorm4", "html_url": "https://github.com/rmccorm4", "followers_url": "https://api.github.com/users/rmccorm4/followers", "following_url": "https://api.github.com/users/rmccorm4/following{/other_user}", "gists_url": "https://api.github.com/users/rmccorm4/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmccorm4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmccorm4/subscriptions", "organizations_url": "https://api.github.com/users/rmccorm4/orgs", "repos_url": "https://api.github.com/users/rmccorm4/repos", "events_url": "https://api.github.com/users/rmccorm4/events{/privacy}", "received_events_url": "https://api.github.com/users/rmccorm4/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2023-02-01T15:01:46Z", "updated_at": "2023-03-22T23:37:42Z", "closed_at": "2023-03-22T23:31:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi triton server team !\r\nThanks for doing a great job ;)\r\n\r\nHere is small prometheus report error that came to me while using ensemble model.\r\n\r\n**Description**\r\nI've implemented an object detection model as an ensemble model on tritonserver like this : \r\n```mermaid\r\n  graph TD;\r\n      backbone-->anchor_generator;\r\n      backbone-->head_1;\r\n      anchor_generator-->postprocess_detections;\r\n      head_1-->postprocess_detections;\r\n```\r\nwhere all models are tensorRT engines but `postprocess_detections` which is an ONNX model.\r\n\r\nThe ensemble model that interests me has actually 9 heads like this :\r\n\r\n```mermaid\r\n  graph TD;\r\n      backbone-->anchor_generator;\r\n      backbone-->head_1;\r\n      backbone-->head_2;\r\n      backbone-->head_3;\r\n      backbone-->head_4;\r\n      backbone-->head_5;\r\n      backbone-->head_6;\r\n      backbone-->head_7;\r\n      backbone-->head_8;\r\n      backbone-->head_9;\r\n      anchor_generator-->postprocess_detections_1;\r\n      anchor_generator-->postprocess_detections_2;\r\n      anchor_generator-->postprocess_detections_3;\r\n      anchor_generator-->postprocess_detections_4;\r\n      anchor_generator-->postprocess_detections_5;\r\n      anchor_generator-->postprocess_detections_6;\r\n      anchor_generator-->postprocess_detections_7;\r\n      anchor_generator-->postprocess_detections_8;\r\n      anchor_generator-->postprocess_detections_9;\r\n      head_1-->postprocess_detections_1;\r\n      head_2-->postprocess_detections_2;\r\n      head_3-->postprocess_detections_3;\r\n      head_4-->postprocess_detections_4;\r\n      head_5-->postprocess_detections_5;\r\n      head_6-->postprocess_detections_6;\r\n      head_7-->postprocess_detections_7;\r\n      head_8-->postprocess_detections_8;\r\n      head_9-->postprocess_detections_9;\r\n```\r\nwhere postprocess_detections is the same model but run 9 times with different inputs.\r\n\r\nIn the first ensemble model with 1 head, the prometheus metrics are like this :\r\n\r\n```\r\n# HELP nv_inference_request_success Number of successful inference requests, all batch sizes\r\n# TYPE nv_inference_request_success counter\r\nnv_inference_request_success{model=\"model_head_1\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_inference_request_success{model=\"anchor_generator\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_inference_request_success{model=\"postprocess_detections\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_1\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_inference_request_success{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_inference_request_success{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_inference_request_success{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_inference_request_success{model=\"backbone\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_inference_request_success{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_inference_request_success{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_inference_request_failure Number of failed inference requests, all batch sizes\r\n# TYPE nv_inference_request_failure counter\r\nnv_inference_request_failure{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_inference_count Number of inferences performed (does not include cached requests)\r\n# TYPE nv_inference_count counter\r\nnv_inference_count{model=\"model_head_1\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_inference_count{model=\"anchor_generator\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_inference_count{model=\"postprocess_detections\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_1\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_inference_count{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_inference_count{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_inference_count{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_inference_count{model=\"backbone\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_inference_count{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_inference_count{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_inference_exec_count Number of model executions performed (does not include cached requests)\r\n# TYPE nv_inference_exec_count counter\r\nnv_inference_exec_count{model=\"model_head_1\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_inference_exec_count{model=\"anchor_generator\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_inference_exec_count{model=\"postprocess_detections\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_1\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_inference_exec_count{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_inference_exec_count{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_inference_exec_count{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_inference_exec_count{model=\"backbone\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_inference_exec_count{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_inference_exec_count{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_inference_request_duration_us Cumulative inference request duration in microseconds (includes cached requests)\r\n# TYPE nv_inference_request_duration_us counter\r\nnv_inference_request_duration_us{model=\"model_head_1\",version=\"1\"} 5557899.000000\r\nnv_inference_request_duration_us{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_inference_request_duration_us{model=\"anchor_generator\",version=\"1\"} 1729050.000000\r\nnv_inference_request_duration_us{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_inference_request_duration_us{model=\"postprocess_detections\",version=\"1\"} 1024797.000000\r\nnv_inference_request_duration_us{model=\"head_1\",version=\"1\"} 2007358.000000\r\nnv_inference_request_duration_us{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_inference_request_duration_us{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_inference_request_duration_us{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_inference_request_duration_us{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_inference_request_duration_us{model=\"backbone\",version=\"1\"} 2524156.000000\r\nnv_inference_request_duration_us{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_inference_request_duration_us{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_inference_request_duration_us{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_inference_queue_duration_us Cumulative inference queuing duration in microseconds (includes cached requests)\r\n# TYPE nv_inference_queue_duration_us counter\r\nnv_inference_queue_duration_us{model=\"model_head_1\",version=\"1\"} 69.000000\r\nnv_inference_queue_duration_us{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_inference_queue_duration_us{model=\"anchor_generator\",version=\"1\"} 8885.000000\r\nnv_inference_queue_duration_us{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_inference_queue_duration_us{model=\"postprocess_detections\",version=\"1\"} 8371.000000\r\nnv_inference_queue_duration_us{model=\"head_1\",version=\"1\"} 7996.000000\r\nnv_inference_queue_duration_us{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_inference_queue_duration_us{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_inference_queue_duration_us{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_inference_queue_duration_us{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_inference_queue_duration_us{model=\"backbone\",version=\"1\"} 8433.000000\r\nnv_inference_queue_duration_us{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_inference_queue_duration_us{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_inference_queue_duration_us{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_inference_compute_input_duration_us Cumulative compute input duration in microseconds (does not include cached requests)\r\n# TYPE nv_inference_compute_input_duration_us counter\r\nnv_inference_compute_input_duration_us{model=\"model_head_1\",version=\"1\"} 2856970.000000\r\nnv_inference_compute_input_duration_us{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_inference_compute_input_duration_us{model=\"anchor_generator\",version=\"1\"} 1328254.000000\r\nnv_inference_compute_input_duration_us{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_inference_compute_input_duration_us{model=\"postprocess_detections\",version=\"1\"} 575610.000000\r\nnv_inference_compute_input_duration_us{model=\"head_1\",version=\"1\"} 35120.000000\r\nnv_inference_compute_input_duration_us{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_inference_compute_input_duration_us{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_inference_compute_input_duration_us{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_inference_compute_input_duration_us{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_inference_compute_input_duration_us{model=\"backbone\",version=\"1\"} 917843.000000\r\nnv_inference_compute_input_duration_us{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_inference_compute_input_duration_us{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_inference_compute_input_duration_us{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_inference_compute_infer_duration_us Cumulative compute inference duration in microseconds (does not include cached requests)\r\n# TYPE nv_inference_compute_infer_duration_us counter\r\nnv_inference_compute_infer_duration_us{model=\"model_head_1\",version=\"1\"} 4314026.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_inference_compute_infer_duration_us{model=\"anchor_generator\",version=\"1\"} 351166.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_inference_compute_infer_duration_us{model=\"postprocess_detections\",version=\"1\"} 428012.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_1\",version=\"1\"} 1951941.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_inference_compute_infer_duration_us{model=\"backbone\",version=\"1\"} 1582762.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_inference_compute_infer_duration_us{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_inference_compute_output_duration_us Cumulative inference compute output duration in microseconds (does not include cached requests)\r\n# TYPE nv_inference_compute_output_duration_us counter\r\nnv_inference_compute_output_duration_us{model=\"model_head_1\",version=\"1\"} 49238.000000\r\nnv_inference_compute_output_duration_us{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_inference_compute_output_duration_us{model=\"anchor_generator\",version=\"1\"} 38270.000000\r\nnv_inference_compute_output_duration_us{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_inference_compute_output_duration_us{model=\"postprocess_detections\",version=\"1\"} 3656.000000\r\nnv_inference_compute_output_duration_us{model=\"head_1\",version=\"1\"} 3375.000000\r\nnv_inference_compute_output_duration_us{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_inference_compute_output_duration_us{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_inference_compute_output_duration_us{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_inference_compute_output_duration_us{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_inference_compute_output_duration_us{model=\"backbone\",version=\"1\"} 3781.000000\r\nnv_inference_compute_output_duration_us{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_inference_compute_output_duration_us{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_inference_compute_output_duration_us{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_cache_num_entries Number of responses stored in response cache\r\n# TYPE nv_cache_num_entries gauge\r\n# HELP nv_cache_num_lookups Number of cache lookups in response cache\r\n# TYPE nv_cache_num_lookups gauge\r\n# HELP nv_cache_num_hits Number of cache hits in response cache\r\n# TYPE nv_cache_num_hits gauge\r\n# HELP nv_cache_num_misses Number of cache misses in response cache\r\n# TYPE nv_cache_num_misses gauge\r\n# HELP nv_cache_num_evictions Number of cache evictions in response cache\r\n# TYPE nv_cache_num_evictions gauge\r\n# HELP nv_cache_lookup_duration Total cache lookup duration (hit and miss), in microseconds\r\n# TYPE nv_cache_lookup_duration gauge\r\n# HELP nv_cache_insertion_duration Total cache insertion duration, in microseconds\r\n# TYPE nv_cache_insertion_duration gauge\r\n# HELP nv_cache_util Cache utilization [0.0 - 1.0]\r\n# TYPE nv_cache_util gauge\r\n# HELP nv_cache_num_hits_per_model Number of cache hits per model\r\n# TYPE nv_cache_num_hits_per_model counter\r\nnv_cache_num_hits_per_model{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_cache_hit_lookup_duration_per_model Total cache hit lookup duration per model, in microseconds\r\n# TYPE nv_cache_hit_lookup_duration_per_model counter\r\nnv_cache_hit_lookup_duration_per_model{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_cache_num_misses_per_model Number of cache misses per model\r\n# TYPE nv_cache_num_misses_per_model counter\r\nnv_cache_num_misses_per_model{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_cache_miss_lookup_duration_per_model Total cache miss lookup duration per model, in microseconds\r\n# TYPE nv_cache_miss_lookup_duration_per_model counter\r\nnv_cache_miss_lookup_duration_per_model{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_cache_miss_insertion_duration_per_model Total cache miss insertion duration per model, in microseconds\r\n# TYPE nv_cache_miss_insertion_duration_per_model counter\r\nnv_cache_miss_insertion_duration_per_model{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_gpu_utilization GPU utilization rate [0.0 - 1.0)\r\n# TYPE nv_gpu_utilization gauge\r\nnv_gpu_utilization{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 0.240000\r\n# HELP nv_gpu_memory_total_bytes GPU total memory, in bytes\r\n# TYPE nv_gpu_memory_total_bytes gauge\r\nnv_gpu_memory_total_bytes{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 25769803776.000000\r\n# HELP nv_gpu_memory_used_bytes GPU used memory, in bytes\r\n# TYPE nv_gpu_memory_used_bytes gauge\r\nnv_gpu_memory_used_bytes{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 16931356672.000000\r\n# HELP nv_gpu_power_usage GPU power usage in watts\r\n# TYPE nv_gpu_power_usage gauge\r\nnv_gpu_power_usage{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 156.564000\r\n# HELP nv_gpu_power_limit GPU power management limit in watts\r\n# TYPE nv_gpu_power_limit gauge\r\nnv_gpu_power_limit{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 350.000000\r\n# HELP nv_energy_consumption GPU energy consumption in joules since the Triton Server started\r\n# TYPE nv_energy_consumption counter\r\nnv_energy_consumption{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 3601.027000\r\n```\r\n\r\nIn the second ensemble model with 9 heads, the prometheus metrics are like this :\r\n\r\n```\r\n# HELP nv_inference_request_success Number of successful inference requests, all batch sizes\r\n# TYPE nv_inference_request_success counter\r\nnv_inference_request_success{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_inference_request_success{model=\"head_3\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"anchor_generator\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_4\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"postprocess_detections\",version=\"1\"} 900.000000\r\nnv_inference_request_success{model=\"head_1\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_5\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_6\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_2\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"backbone\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_7\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_8\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"head_9\",version=\"1\"} 100.000000\r\nnv_inference_request_success{model=\"model_all_heads\",version=\"1\"} 100.000000\r\n# HELP nv_inference_request_failure Number of failed inference requests, all batch sizes\r\n# TYPE nv_inference_request_failure counter\r\nnv_inference_request_failure{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_inference_request_failure{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_inference_count Number of inferences performed (does not include cached requests)\r\n# TYPE nv_inference_count counter\r\nnv_inference_count{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_inference_count{model=\"head_3\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"anchor_generator\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_4\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"postprocess_detections\",version=\"1\"} 900.000000\r\nnv_inference_count{model=\"head_1\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_5\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_6\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_2\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"backbone\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_7\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_8\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"head_9\",version=\"1\"} 100.000000\r\nnv_inference_count{model=\"model_all_heads\",version=\"1\"} 100.000000\r\n# HELP nv_inference_exec_count Number of model executions performed (does not include cached requests)\r\n# TYPE nv_inference_exec_count counter\r\nnv_inference_exec_count{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_inference_exec_count{model=\"head_3\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"anchor_generator\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_4\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"postprocess_detections\",version=\"1\"} 900.000000\r\nnv_inference_exec_count{model=\"head_1\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_5\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_6\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_2\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"backbone\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_7\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_8\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"head_9\",version=\"1\"} 100.000000\r\nnv_inference_exec_count{model=\"model_all_heads\",version=\"1\"} 100.000000\r\n# HELP nv_inference_request_duration_us Cumulative inference request duration in microseconds (includes cached requests)\r\n# TYPE nv_inference_request_duration_us counter\r\nnv_inference_request_duration_us{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_inference_request_duration_us{model=\"head_3\",version=\"1\"} 14311666.000000\r\nnv_inference_request_duration_us{model=\"anchor_generator\",version=\"1\"} 5910979.000000\r\nnv_inference_request_duration_us{model=\"head_4\",version=\"1\"} 14363945.000000\r\nnv_inference_request_duration_us{model=\"postprocess_detections\",version=\"1\"} 58913533.000000\r\nnv_inference_request_duration_us{model=\"head_1\",version=\"1\"} 14378334.000000\r\nnv_inference_request_duration_us{model=\"head_5\",version=\"1\"} 14336454.000000\r\nnv_inference_request_duration_us{model=\"head_6\",version=\"1\"} 14347145.000000\r\nnv_inference_request_duration_us{model=\"head_2\",version=\"1\"} 14310618.000000\r\nnv_inference_request_duration_us{model=\"backbone\",version=\"1\"} 2460564.000000\r\nnv_inference_request_duration_us{model=\"head_7\",version=\"1\"} 14343129.000000\r\nnv_inference_request_duration_us{model=\"head_8\",version=\"1\"} 14389983.000000\r\nnv_inference_request_duration_us{model=\"head_9\",version=\"1\"} 14491176.000000\r\nnv_inference_request_duration_us{model=\"model_all_heads\",version=\"1\"} 29087514.000000\r\n# HELP nv_inference_queue_duration_us Cumulative inference queuing duration in microseconds (includes cached requests)\r\n# TYPE nv_inference_queue_duration_us counter\r\nnv_inference_queue_duration_us{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_inference_queue_duration_us{model=\"head_3\",version=\"1\"} 12510.000000\r\nnv_inference_queue_duration_us{model=\"anchor_generator\",version=\"1\"} 10022.000000\r\nnv_inference_queue_duration_us{model=\"head_4\",version=\"1\"} 11754.000000\r\nnv_inference_queue_duration_us{model=\"postprocess_detections\",version=\"1\"} 46213640.000000\r\nnv_inference_queue_duration_us{model=\"head_1\",version=\"1\"} 9433.000000\r\nnv_inference_queue_duration_us{model=\"head_5\",version=\"1\"} 12068.000000\r\nnv_inference_queue_duration_us{model=\"head_6\",version=\"1\"} 12850.000000\r\nnv_inference_queue_duration_us{model=\"head_2\",version=\"1\"} 11972.000000\r\nnv_inference_queue_duration_us{model=\"backbone\",version=\"1\"} 8074.000000\r\nnv_inference_queue_duration_us{model=\"head_7\",version=\"1\"} 12434.000000\r\nnv_inference_queue_duration_us{model=\"head_8\",version=\"1\"} 10090.000000\r\nnv_inference_queue_duration_us{model=\"head_9\",version=\"1\"} 13365.000000\r\nnv_inference_queue_duration_us{model=\"model_all_heads\",version=\"1\"} 116.000000\r\n# HELP nv_inference_compute_input_duration_us Cumulative compute input duration in microseconds (does not include cached requests)\r\n# TYPE nv_inference_compute_input_duration_us counter\r\nnv_inference_compute_input_duration_us{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_inference_compute_input_duration_us{model=\"head_3\",version=\"1\"} 482239.000000\r\nnv_inference_compute_input_duration_us{model=\"anchor_generator\",version=\"1\"} 4182645.000000\r\nnv_inference_compute_input_duration_us{model=\"head_4\",version=\"1\"} 381857.000000\r\nnv_inference_compute_input_duration_us{model=\"postprocess_detections\",version=\"1\"} 5768877.000000\r\nnv_inference_compute_input_duration_us{model=\"head_1\",version=\"1\"} 384371.000000\r\nnv_inference_compute_input_duration_us{model=\"head_5\",version=\"1\"} 443877.000000\r\nnv_inference_compute_input_duration_us{model=\"head_6\",version=\"1\"} 616120.000000\r\nnv_inference_compute_input_duration_us{model=\"head_2\",version=\"1\"} 438818.000000\r\nnv_inference_compute_input_duration_us{model=\"backbone\",version=\"1\"} 918382.000000\r\nnv_inference_compute_input_duration_us{model=\"head_7\",version=\"1\"} 576845.000000\r\nnv_inference_compute_input_duration_us{model=\"head_8\",version=\"1\"} 444612.000000\r\nnv_inference_compute_input_duration_us{model=\"head_9\",version=\"1\"} 708303.000000\r\nnv_inference_compute_input_duration_us{model=\"model_all_heads\",version=\"1\"} 15347899.000000\r\n# HELP nv_inference_compute_infer_duration_us Cumulative compute inference duration in microseconds (does not include cached requests)\r\n# TYPE nv_inference_compute_infer_duration_us counter\r\nnv_inference_compute_infer_duration_us{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_3\",version=\"1\"} 13609076.000000\r\nnv_inference_compute_infer_duration_us{model=\"anchor_generator\",version=\"1\"} 1700379.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_4\",version=\"1\"} 13734642.000000\r\nnv_inference_compute_infer_duration_us{model=\"postprocess_detections\",version=\"1\"} 6860768.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_1\",version=\"1\"} 13737362.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_5\",version=\"1\"} 13656193.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_6\",version=\"1\"} 13470334.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_2\",version=\"1\"} 13639434.000000\r\nnv_inference_compute_infer_duration_us{model=\"backbone\",version=\"1\"} 1499234.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_7\",version=\"1\"} 13477953.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_8\",version=\"1\"} 13639134.000000\r\nnv_inference_compute_infer_duration_us{model=\"head_9\",version=\"1\"} 13397781.000000\r\nnv_inference_compute_infer_duration_us{model=\"model_all_heads\",version=\"1\"} 132423230.000000\r\n# HELP nv_inference_compute_output_duration_us Cumulative inference compute output duration in microseconds (does not include cached requests)\r\n# TYPE nv_inference_compute_output_duration_us counter\r\nnv_inference_compute_output_duration_us{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_inference_compute_output_duration_us{model=\"head_3\",version=\"1\"} 197679.000000\r\nnv_inference_compute_output_duration_us{model=\"anchor_generator\",version=\"1\"} 14340.000000\r\nnv_inference_compute_output_duration_us{model=\"head_4\",version=\"1\"} 225947.000000\r\nnv_inference_compute_output_duration_us{model=\"postprocess_detections\",version=\"1\"} 26516.000000\r\nnv_inference_compute_output_duration_us{model=\"head_1\",version=\"1\"} 238129.000000\r\nnv_inference_compute_output_duration_us{model=\"head_5\",version=\"1\"} 214396.000000\r\nnv_inference_compute_output_duration_us{model=\"head_6\",version=\"1\"} 238709.000000\r\nnv_inference_compute_output_duration_us{model=\"head_2\",version=\"1\"} 211102.000000\r\nnv_inference_compute_output_duration_us{model=\"backbone\",version=\"1\"} 3424.000000\r\nnv_inference_compute_output_duration_us{model=\"head_7\",version=\"1\"} 265899.000000\r\nnv_inference_compute_output_duration_us{model=\"head_8\",version=\"1\"} 286779.000000\r\nnv_inference_compute_output_duration_us{model=\"head_9\",version=\"1\"} 362117.000000\r\nnv_inference_compute_output_duration_us{model=\"model_all_heads\",version=\"1\"} 2285966.000000\r\n# HELP nv_cache_num_entries Number of responses stored in response cache\r\n# TYPE nv_cache_num_entries gauge\r\n# HELP nv_cache_num_lookups Number of cache lookups in response cache\r\n# TYPE nv_cache_num_lookups gauge\r\n# HELP nv_cache_num_hits Number of cache hits in response cache\r\n# TYPE nv_cache_num_hits gauge\r\n# HELP nv_cache_num_misses Number of cache misses in response cache\r\n# TYPE nv_cache_num_misses gauge\r\n# HELP nv_cache_num_evictions Number of cache evictions in response cache\r\n# TYPE nv_cache_num_evictions gauge\r\n# HELP nv_cache_lookup_duration Total cache lookup duration (hit and miss), in microseconds\r\n# TYPE nv_cache_lookup_duration gauge\r\n# HELP nv_cache_insertion_duration Total cache insertion duration, in microseconds\r\n# TYPE nv_cache_insertion_duration gauge\r\n# HELP nv_cache_util Cache utilization [0.0 - 1.0]\r\n# TYPE nv_cache_util gauge\r\n# HELP nv_cache_num_hits_per_model Number of cache hits per model\r\n# TYPE nv_cache_num_hits_per_model counter\r\nnv_cache_num_hits_per_model{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_cache_num_hits_per_model{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_cache_hit_lookup_duration_per_model Total cache hit lookup duration per model, in microseconds\r\n# TYPE nv_cache_hit_lookup_duration_per_model counter\r\nnv_cache_hit_lookup_duration_per_model{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_cache_hit_lookup_duration_per_model{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_cache_num_misses_per_model Number of cache misses per model\r\n# TYPE nv_cache_num_misses_per_model counter\r\nnv_cache_num_misses_per_model{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_cache_num_misses_per_model{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_cache_miss_lookup_duration_per_model Total cache miss lookup duration per model, in microseconds\r\n# TYPE nv_cache_miss_lookup_duration_per_model counter\r\nnv_cache_miss_lookup_duration_per_model{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_cache_miss_lookup_duration_per_model{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_cache_miss_insertion_duration_per_model Total cache miss insertion duration per model, in microseconds\r\n# TYPE nv_cache_miss_insertion_duration_per_model counter\r\nnv_cache_miss_insertion_duration_per_model{model=\"model_head_1\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_3\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"anchor_generator\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_4\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"postprocess_detections\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_1\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_5\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_6\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_2\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"backbone\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_7\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_8\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"head_9\",version=\"1\"} 0.000000\r\nnv_cache_miss_insertion_duration_per_model{model=\"model_all_heads\",version=\"1\"} 0.000000\r\n# HELP nv_gpu_utilization GPU utilization rate [0.0 - 1.0)\r\n# TYPE nv_gpu_utilization gauge\r\nnv_gpu_utilization{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 0.000000\r\n# HELP nv_gpu_memory_total_bytes GPU total memory, in bytes\r\n# TYPE nv_gpu_memory_total_bytes gauge\r\nnv_gpu_memory_total_bytes{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 25769803776.000000\r\n# HELP nv_gpu_memory_used_bytes GPU used memory, in bytes\r\n# TYPE nv_gpu_memory_used_bytes gauge\r\nnv_gpu_memory_used_bytes{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 17082351616.000000\r\n# HELP nv_gpu_power_usage GPU power usage in watts\r\n# TYPE nv_gpu_power_usage gauge\r\nnv_gpu_power_usage{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 142.772000\r\n# HELP nv_gpu_power_limit GPU power management limit in watts\r\n# TYPE nv_gpu_power_limit gauge\r\nnv_gpu_power_limit{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 350.000000\r\n# HELP nv_energy_consumption GPU energy consumption in joules since the Triton Server started\r\n# TYPE nv_energy_consumption counter\r\nnv_energy_consumption{gpu_uuid=\"GPU-2d050232-24ef-5168-4fc7-6700ac827bce\"} 15561.756000\r\n```\r\n\r\nThe problem is that there is several incoherence in the logs.\r\nFor example, in the first case the cumulative time on 100 requests of the inference request duration for the head_1 is\r\n```\r\nnv_inference_request_duration_us{model=\"head_1\",version=\"1\"} 2007358.000000\r\n```\r\nWhile in the second case of 9 heads, this time is\r\n```\r\nnv_inference_request_duration_us{model=\"head_1\",version=\"1\"} 14378334.000000\r\n```\r\nwhich is more than 7 times bigger (almost 9).\r\nAnd I think it's just an error while computing metrics of ensemble model.\r\n\r\nAnother more concrete example.\r\nOn the second case with 9 heads, if we look at overall inference request duration and compute infer duration :\r\n```\r\nnv_inference_request_duration_us{model=\"model_all_heads\",version=\"1\"} 29087514.000000\r\nnv_inference_compute_infer_duration_us{model=\"model_all_heads\",version=\"1\"} 132423230.000000\r\n```\r\nwe see that the compute infer duration is far greater than the overall inference request duration.\r\nFurthermore the mean client-side infererence time is around 700ms which is two times lower than the compute inference duration.\r\n\r\nThus I think the computation of the prometheus timing for ensemble models does not reflect the real timings.\r\n\r\n**Triton Information**\r\nI'm using the nvcr.io/nvidia/tritonserver:22.12-py3 docker image.\r\n\r\n**To Reproduce**\r\nAn ensemble model where some tensorRT models (the heads) have the same inputs (from the backbone) and deliver their outputs to the same ONNX model (postprocess_detections).\r\n\r\n**Expected behavior**\r\nThe metrics should be coherent.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5296/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5278", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5278/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5278/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5278/events", "html_url": "https://github.com/triton-inference-server/server/issues/5278", "id": 1557149675, "node_id": "I_kwDOCQnI4s5c0Dfr", "number": 5278, "title": "Incorrect output for a movinet model on a tensorflow backend - Triton 22.04", "user": {"login": "dpostolovski", "id": 43391137, "node_id": "MDQ6VXNlcjQzMzkxMTM3", "avatar_url": "https://avatars.githubusercontent.com/u/43391137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dpostolovski", "html_url": "https://github.com/dpostolovski", "followers_url": "https://api.github.com/users/dpostolovski/followers", "following_url": "https://api.github.com/users/dpostolovski/following{/other_user}", "gists_url": "https://api.github.com/users/dpostolovski/gists{/gist_id}", "starred_url": "https://api.github.com/users/dpostolovski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dpostolovski/subscriptions", "organizations_url": "https://api.github.com/users/dpostolovski/orgs", "repos_url": "https://api.github.com/users/dpostolovski/repos", "events_url": "https://api.github.com/users/dpostolovski/events{/privacy}", "received_events_url": "https://api.github.com/users/dpostolovski/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2023-01-25T19:16:04Z", "updated_at": "2023-03-01T21:04:24Z", "closed_at": "2023-03-01T21:04:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nThe output from a movinet model on a tensorflow backend is incorrect. No error message or warning is present.\r\nLatest version (Triton 22.12) works as expected. Looking at the compatibility matrix, triton 22.04 is not compatible with tensorflow 2.10, but as a good practice, if not compatible with the specific model, some error message should be expected.\r\n\r\nWhat is the best way to prevent bugs like these creeping in?\r\n\r\n**Triton Information**\r\n22.04\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nYes, using 22.04-py3 official image container.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nTrain a movinet model on tensorflow 2.10 and export it as a savedmodel. Load it in Triton 22.04, inference it and the result\r\nwill be different than when inferencing on native python with the latest version of tensorflow.\r\n\r\n**Expected behavior**\r\nModel outputs from tensorflow and triton are the same, or an error message about compatibility is present.\r\n\r\n\r\n@rsandler00", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5278/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5278/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5277", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5277/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5277/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5277/events", "html_url": "https://github.com/triton-inference-server/server/issues/5277", "id": 1557092092, "node_id": "I_kwDOCQnI4s5cz1b8", "number": 5277, "title": "Backend configs ignoring/not receiving some config flags in latest release", "user": {"login": "darintay", "id": 1653481, "node_id": "MDQ6VXNlcjE2NTM0ODE=", "avatar_url": "https://avatars.githubusercontent.com/u/1653481?v=4", "gravatar_id": "", "url": "https://api.github.com/users/darintay", "html_url": "https://github.com/darintay", "followers_url": "https://api.github.com/users/darintay/followers", "following_url": "https://api.github.com/users/darintay/following{/other_user}", "gists_url": "https://api.github.com/users/darintay/gists{/gist_id}", "starred_url": "https://api.github.com/users/darintay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/darintay/subscriptions", "organizations_url": "https://api.github.com/users/darintay/orgs", "repos_url": "https://api.github.com/users/darintay/repos", "events_url": "https://api.github.com/users/darintay/events{/privacy}", "received_events_url": "https://api.github.com/users/darintay/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2023-01-25T18:22:42Z", "updated_at": "2023-03-16T21:56:46Z", "closed_at": "2023-03-16T21:56:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nPython backend configs shm-growth-byte-size and shm-region-prefix-name are no longer working in the latest release.  They used to work in 22.04.  shm-default-byte-size still works.\r\n\r\nEdit: This also seems to apply to other backends e.g. tensorflow\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n22.12.  Used to work in 22.04.\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTriton container.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nCommand:\r\n```\r\n/opt/tritonserver/bin/tritonserver     --model-repository models     --model-control-mode=explicit   --backend-config=python,shm-default-byte-size=4500000     --backend-config=python,shm-growth-byte-size=10000000     --backend-config=python,shm-region-prefix-name=test-repro-shm-  --load-model slow_model\r\n```\r\n\r\nIn 22.12, note that it only picks up 1 of the 3 backend config flags I attempted to set:\r\n```\r\nI0125 17:58:01.158294 363 python_be.cc:1634] backend configuration:\r\n{\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"shm-default-byte-size\":\"4500000\",\"default-max-batch-size\":\"4\"}}\r\nI0125 17:58:01.158355 363 python_be.cc:1764] Shared memory configuration is shm-default-byte-size=4500000,shm-growth-byte-size=67108864,stub-timeout-seconds=30\r\n```\r\n\r\n```\r\nI0125 17:58:02.754897 428 stub_launcher.cc:259] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub models/slow_model/1/model.py triton_python_backend_shm_region_2 4500000 67108864 363 /opt/tritonserver/backends/python 336 slow_model\r\n```\r\n\r\n```\r\nI0125 17:58:03.034163 363 server.cc:590] \r\n+---------+-------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Backend | Path                                                  | Config                                                                                                                                                                                          |\r\n+---------+-------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| python  | /opt/tritonserver/backends/python/libtriton_python.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"shm-default-byte-size\":\"4500000\",\"default-max-batch-size\":\"4\"}} |\r\n+---------+-------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n```\r\n\r\n\r\n\r\n**Expected behavior**\r\nshm-growth-byte-size and shm-region-prefix-name should get passed to the backend.  They used to be in 22.04.\r\n\r\n22.04:\r\n```\r\n| python      | /opt/tritonserver/backends/python/libtriton_python.so                   | {\"cmdline\":{\"shm-default-byte-size\":\"4500000\",\"shm-growth-byte-size\":\"10000000\",\"shm-region-prefix-name\":\"test-repro-shm-\"}} |\r\n```\r\n\r\nI see the code for parsing these flags still exists, they just don't seem to get passed along.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5277/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5276", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5276/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5276/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5276/events", "html_url": "https://github.com/triton-inference-server/server/issues/5276", "id": 1556859397, "node_id": "I_kwDOCQnI4s5cy8oF", "number": 5276, "title": "Every other sequence_id is set to zero using pytorch backend with stateful TS model.", "user": {"login": "acinnes", "id": 22580340, "node_id": "MDQ6VXNlcjIyNTgwMzQw", "avatar_url": "https://avatars.githubusercontent.com/u/22580340?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acinnes", "html_url": "https://github.com/acinnes", "followers_url": "https://api.github.com/users/acinnes/followers", "following_url": "https://api.github.com/users/acinnes/following{/other_user}", "gists_url": "https://api.github.com/users/acinnes/gists{/gist_id}", "starred_url": "https://api.github.com/users/acinnes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acinnes/subscriptions", "organizations_url": "https://api.github.com/users/acinnes/orgs", "repos_url": "https://api.github.com/users/acinnes/repos", "events_url": "https://api.github.com/users/acinnes/events{/privacy}", "received_events_url": "https://api.github.com/users/acinnes/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2023-01-25T15:38:27Z", "updated_at": "2023-02-08T23:18:06Z", "closed_at": "2023-02-08T23:18:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nI'm trying to host stateful TS models directly in Triton using the pytorch backend, so that internal state can stay on the GPU and not be transmitted back and forth to the client. \r\n\r\nI've created a trivial version of a model that will initialize and track updates to internal state for multiple active sequences of inference requests, using the start/end/corr_id control inputs supplied by Triton. I have a simple test script that submits multiple concurrent inference sequences, using either the http or grpc tritonclient. Triton is configured to build batches from the sequences using the sequence batcher.\r\n\r\nThis works, except that by the time the inputs reach my model, roughly every other inference request has its sequence_id (corr_id) reset to zero. This is obviously broken for any actual use.\r\n\r\n**Triton Information**\r\nI'm currently testing with Triton 22.12, pulled directly from NGC.\r\n\r\n**To Reproduce**\r\n\r\nLoad test model from the attached model_test.zip in Triton:\r\n- docker run -it --rm -p8000:8000 -p8001:8001 -p8002:8002 -v$(realpath model_test):/models nvcr.io/nvidia/tritonserver:22.12-pyt-python-py3 tritonserver --model-repository=/models\r\n\r\nYou can update the TS model if you wish as follows:\r\n- python stateful_model.py --head_output_path stateful.test.ts\r\n- cp stateful.test.ts model_test/stateful_test/1/model.pt \r\n\r\nRun test script on same host (edit to switch from http to grpc):\r\n- python test_stateful_model.py\r\n\r\nThe model prints out the control inputs for each (dynamically batched) inference request it receives from Triton. It will also print warnings if it encounters sequences in an unexpected state, such as end of a sequence it isn't currently tracking, or a non-starting request from a sequence it hasn't seen before.\r\n\r\nThe test script is submitting 16 concurrent sequences, of 4 inference requests each. The sequence IDs start at 1000 and go up to 1015.\r\n\r\n**Expected behavior**\r\n\r\nThe model should be receiving batches where every sequence_id is a different number from range(1000, 1016), allowing the model to reference the correct state variables for that sequence. There should be no requests where sequence_id is zero (since the test script always supplies a non-zero value to the tritonclient object). There should be no warnings about sequence inputs being received in an unexpected state.\r\n\r\n**Actual (incorrect) behaviour**\r\n\r\nWhen I run this locally, I get output like this\r\n```code\r\n[8, 127, 15552]\r\n 1000     1     0\r\n    0     1     0\r\n 1001     1     0\r\n    0     1     0\r\n 1002     1     0\r\n    0     1     0\r\n 1003     1     0\r\n    0     1     0\r\n[ CPUIntType{8,3} ]\r\n[7, 127, 15552]\r\n 1008     1     0\r\n    0     1     0\r\n 1010     1     0\r\n    0     1     0\r\n 1009     1     0\r\n    0     1     0\r\n 1012     1     0\r\n[ CPUIntType{7,3} ]\r\n...\r\n[6, 127, 15552]\r\n 1010     0     0\r\n    0     0     0\r\n 1011     0     0\r\n    0     0     0\r\n 1012     0     0\r\n    0     0     0\r\n[ CPUIntType{6,3} ]\r\n** id 1011 is missing from state_map; initializing\r\n** id 1012 is missing from state_map; initializing\r\n...\r\n[7, 127, 15552]\r\n 1008     0     1\r\n    0     0     1\r\n 1009     0     1\r\n    0     0     1\r\n 1010     0     1\r\n    0     0     1\r\n 1011     0     1\r\n[ CPUIntType{7,3} ]\r\n** id 0 is missing from state_map; initializing\r\n** id 0 wasn't in state_map at seq_end\r\n** id 0 wasn't in state_map at seq_end\r\n```\r\n\r\nIf I run triton with `--log-verbose=1`, I see that it receives the individual inference requests with correct sequence_id values, so the corruption of some of the ids appears to be occurring somewhere inside the sequence/dynamic batchers or the pytorch backend.\r\n\r\n**Attachments**\r\n\r\n[model_test_script.zip](https://github.com/triton-inference-server/server/files/10500581/model_test_script.zip)\r\nThis contains the source code for the trivial stateful TS model, and the python test script to submit async sequences of dummy data.\r\n\r\n[model_test.zip](https://github.com/triton-inference-server/server/files/10500570/model_test.zip)\r\nThis contains the model repo for hosting the stateful test model in Triton using the pytorch backend.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5276/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5276/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5248", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5248/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5248/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5248/events", "html_url": "https://github.com/triton-inference-server/server/issues/5248", "id": 1531677040, "node_id": "I_kwDOCQnI4s5bS4lw", "number": 5248, "title": "Issue with system shared memory and OpenCL (failed reading shared memory buffer with clEnqueueWriteBuffer )", "user": {"login": "YoelBerger", "id": 7348263, "node_id": "MDQ6VXNlcjczNDgyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/7348263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YoelBerger", "html_url": "https://github.com/YoelBerger", "followers_url": "https://api.github.com/users/YoelBerger/followers", "following_url": "https://api.github.com/users/YoelBerger/following{/other_user}", "gists_url": "https://api.github.com/users/YoelBerger/gists{/gist_id}", "starred_url": "https://api.github.com/users/YoelBerger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YoelBerger/subscriptions", "organizations_url": "https://api.github.com/users/YoelBerger/orgs", "repos_url": "https://api.github.com/users/YoelBerger/repos", "events_url": "https://api.github.com/users/YoelBerger/events{/privacy}", "received_events_url": "https://api.github.com/users/YoelBerger/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-01-13T04:04:53Z", "updated_at": "2023-02-01T20:15:09Z", "closed_at": "2023-02-01T20:12:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI am writing a new backend for a new device that uses OpenCL (2.0) interface.\r\nWhen working without system-shared memory there is no issue, but when using with system shared-memory I received an error of \"bad address - Errno 14\" from calling clEnqueueWriteBuffer (Upload to device memory).\r\nAfter some investigation I found the root cause:\r\nThe shared memory manager maps the shared memory with \"mmap\" with only PROT_WRITE and without PROT_READ:\r\nhttps://github.com/triton-inference-server/server/blob/main/src/shared_memory_manager.cc#L153\r\n\r\nWhen it mapped with PROT_WRITE  and without PROT_READ there is no problem reading it in C++ code, but clEnqueueWriteBuffer failed to work.\r\nWhen I added the PROT_READ flag too it works.\r\n\r\nIs it possible to add the PROT_READ too?\r\n\r\n**Triton Information**\r\nUsing custom build of triton based on r22.12 and Ubuntu 18.04.\r\n(The accelerator vendor of the device I working on only supports ubuntu 18.04 right now).\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5248/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5223", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5223/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5223/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5223/events", "html_url": "https://github.com/triton-inference-server/server/issues/5223", "id": 1522204648, "node_id": "I_kwDOCQnI4s5auv_o", "number": 5223, "title": "Server crash running torchvision.io.decode_image  on GPU", "user": {"login": "alexzubiaga", "id": 17120045, "node_id": "MDQ6VXNlcjE3MTIwMDQ1", "avatar_url": "https://avatars.githubusercontent.com/u/17120045?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexzubiaga", "html_url": "https://github.com/alexzubiaga", "followers_url": "https://api.github.com/users/alexzubiaga/followers", "following_url": "https://api.github.com/users/alexzubiaga/following{/other_user}", "gists_url": "https://api.github.com/users/alexzubiaga/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexzubiaga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexzubiaga/subscriptions", "organizations_url": "https://api.github.com/users/alexzubiaga/orgs", "repos_url": "https://api.github.com/users/alexzubiaga/repos", "events_url": "https://api.github.com/users/alexzubiaga/events{/privacy}", "received_events_url": "https://api.github.com/users/alexzubiaga/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2023-01-06T08:50:03Z", "updated_at": "2023-04-14T00:04:28Z", "closed_at": "2023-04-14T00:04:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nThe triton server crashes with a core dump when executing the pytorch function torchvision.io.decode_image on a GPU, the same model works without issues when running in triton on CPU.\r\n\r\nThis is the log of the crash.\r\n```\r\nSignal (11) received.\r\n 0# 0x000055DE2E9BAC19 in tritonserver\r\n 1# 0x00007F03468D8090 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# 0x00007F0346A19E33 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 3# vision::image::decode_image(at::Tensor const&, long) in /opt/tritonserver/backends/pytorch/libtorchvision.so\r\n 4# c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >::operator()(at::Tensor const&, long) in /opt/tritonserver/backends/pytorch/libtorchvision.so\r\n 5# c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, at::Tensor (at::Tensor const&, long)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, long) in /opt/tritonserver/backends/pytorch/libtorchvision.so\r\n 6# std::decay<c10::guts::infer_function_traits<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> > >::type::return_type>::type c10::impl::call_functor_with_args_from_stack_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true, 0ul, 1ul, at::Tensor const&, long>(c10::OperatorKernel*, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*, std::integer_sequence<unsigned long, 0ul, 1ul>, c10::guts::typelist::typelist<at::Tensor const&, long>*) in /opt/tritonserver/backends/pytorch/libtorchvision.so\r\n 7# std::decay<c10::guts::infer_function_traits<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> > >::type::return_type>::type c10::impl::call_functor_with_args_from_stack<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>(c10::OperatorKernel*, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) in /opt/tritonserver/backends/pytorch/libtorchvision.so\r\n 8# auto c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)::{lambda(auto:1)#1}::operator()<c10::guts::detail::_identity>(c10::guts::detail::_identity) const in /opt/tritonserver/backends/pytorch/libtorchvision.so\r\n 9# decltype(auto) c10::guts::detail::_if_constexpr<true>::call<c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)::{lambda(auto:1)#1}, c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)::{lambda()#2}, (void*)0>(c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)::{lambda(auto:1)#1}&&, c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)::{lambda()#2}&&) in /opt/tritonserver/backends/pytorch/libtorchvision.so\r\n10# decltype(auto) c10::guts::if_constexpr<true, c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)::{lambda(auto:1)#1}, c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)::{lambda()#2}>(c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)::{lambda(auto:1)#1}&&, c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)::{lambda()#2}&&) in /opt/tritonserver/backends/pytorch/libtorchvision.so\r\n11# c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long> >, true>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) in /opt/tritonserver/backends/pytorch/libtorchvision.so\r\n12# c10::Dispatcher::callBoxed(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const in /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n13# 0x00007F0135931DE7 in /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n14# torch::jit::InterpreterState::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) in /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n15# 0x00007F013590DA7D in /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n16# torch::jit::Method::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&) const in /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n17# 0x00007F03344E76AA in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n18# 0x00007F03344DF0C6 in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n19# 0x00007F03344E1965 in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n20# TRITONBACKEND_ModelInstanceExecute in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n21# 0x00007F03471891EA in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n22# 0x00007F0347189917 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n23# 0x00007F034724BF51 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n24# 0x00007F0347183787 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n25# 0x00007F0346CC9DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n26# 0x00007F0347EE0609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n27# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n\r\nSegmentation fault (core dumped)\r\n\r\n```\r\n\r\n**Triton Information**\r\nTried it with the Triton Docker container versions 22.07-py3 and 22.12-py3\r\n\r\n**To Reproduce**\r\nThe issue can be triggered running this model trace on the triton docker container using a GPU\r\n\r\n```\r\nimport torchvision\r\nimport torch\r\nfrom pathlib import Path\r\n\r\n@torch.jit.script\r\ndef decode_and_resize_images(image: torch.Tensor):\r\n\r\n    image = torchvision.io.decode_image(image, torchvision.io.ImageReadMode.RGB)\r\n\r\n    return image\r\n\r\n\r\nimage = Path(r\"sample.jpg\").read_bytes()\r\n\r\ntraced_model = torch.jit.trace(decode_and_resize_images, example_inputs=torch.frombuffer(image, dtype=torch.uint8))\r\ntorch.jit.save(traced_model, \"model.pt\")\r\n```\r\n\r\n**Expected behavior**\r\nImage decoding should work on both CPU and GPU.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5223/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5223/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5211", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5211/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5211/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5211/events", "html_url": "https://github.com/triton-inference-server/server/issues/5211", "id": 1516859167, "node_id": "I_kwDOCQnI4s5aaW8f", "number": 5211, "title": "Triton server stuck during initialization/reload of python models", "user": {"login": "eskomorokhov", "id": 3755983, "node_id": "MDQ6VXNlcjM3NTU5ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/3755983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eskomorokhov", "html_url": "https://github.com/eskomorokhov", "followers_url": "https://api.github.com/users/eskomorokhov/followers", "following_url": "https://api.github.com/users/eskomorokhov/following{/other_user}", "gists_url": "https://api.github.com/users/eskomorokhov/gists{/gist_id}", "starred_url": "https://api.github.com/users/eskomorokhov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eskomorokhov/subscriptions", "organizations_url": "https://api.github.com/users/eskomorokhov/orgs", "repos_url": "https://api.github.com/users/eskomorokhov/repos", "events_url": "https://api.github.com/users/eskomorokhov/events{/privacy}", "received_events_url": "https://api.github.com/users/eskomorokhov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-01-03T02:16:49Z", "updated_at": "2023-01-12T18:27:00Z", "closed_at": "2023-01-12T18:27:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nA clear and concise description of what the bug is.\r\nDuring server start with python models or model reload(poll) server load models stuck.\r\n\r\nStuck reloading model may appear if logs enabled or in error during load python models.\r\n\r\nStack trace suggests the issue introduced by python backend due to use logs with mtx in between fork() and exec\r\nhttps://github.com/triton-inference-server/python_backend/blob/5b2c1a159b33f8dc17fb884df07aef82b622a3a0/src/stub_launcher.cc#L243\r\n\r\n\r\n```sh\r\n\r\n#0  0x0000ffffb3ea32bc __lll_lock_wait\r\n#1  0x0000ffffb3e9acd8 __pthread_mutex_lock\r\n#2  0x0000ffffb412fd54 triton::common::Logger::Log(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)\r\n#3  0x0000ffffb412fed4 triton::common::LogMessage::~LogMessage()\r\n#4  0x0000ffffb407b04c TRITONSERVER_LogMessage\r\n#5  0x0000ffff8811002c triton::backend::python::StubLauncher::Launch()\r\n#6  0x0000ffff880d8328 triton::backend::python::ModelInstanceState::LaunchStubProcess()\r\n#7  0x0000ffff880d9bf0 TRITONBACKEND_ModelInstanceInitialize\r\n#8  0x0000ffffb3f7ab04 triton::core::TritonModelInstance::CreateInstance(triton::core::TritonModel*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long, TRITONSERVER_instancegroupkind_enum, int, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&, inference::ModelRateLimiter const&, bool, std::map<unsigned int, std::shared_ptr<triton::core::TritonModelInstance::TritonBackendThread>, std::less<unsigned int>, std::allocator<std::pair<unsigned int const, std::shared_ptr<triton::core::TritonModelInstance::TritonBackendThread> > > >*, std::vector<triton::core::TritonModelInstance::SecondaryDevice, std::allocator<triton::core::TritonModelInstance::SecondaryDevice> > const&)\r\n#9  0x0000ffffb3f7bd6c triton::core::TritonModelInstance::CreateInstances(triton::core::TritonModel*, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > const&, inference::ModelConfig const&, bool)\r\n#10 0x0000ffffb3f712c8 triton::core::TritonModel::Create(triton::core::InferenceServer*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, long, inference::ModelConfig, bool, std::unique_ptr<triton::core::TritonModel, std::default_delete<triton::core::TritonModel> >*)\r\n#11 0x0000ffffb3fffbf8 triton::core::ModelLifeCycle::CreateModel(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, long, triton::core::ModelLifeCycle::ModelInfo*, bool)\r\n#12 0x0000ffffb40052e4 std::_Function_handler<void (), triton::core::ModelLifeCycle::AsyncLoad(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, inference::ModelConfig const&, bool, std::shared_ptr<triton::core::TritonRepoAgentModelList> const&, std::function<void (triton::core::Status)>&&)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n#13 0x0000ffffb412e310 std::thread::_State_impl<std::thread::_Invoker<std::tuple<triton::common::ThreadPool::ThreadPool(unsigned long)::{lambda()#1}> > >::_M_run()\r\n#14 0x0000ffffb383afac\r\n#15 0x0000ffffb3e98624 start_thread\r\n#16 0x0000ffffb3def49c\r\n\r\nTriton main process waiting response from python process which stuck\r\n#0  0x0000ffffb3ea2268 do_futex_wait.constprop.0\r\n#1  0x0000ffffb3ea239c __new_sem_wait_slow.constprop.0\r\n#2  0x0000ffff8810d6c8 triton::backend::python::StubLauncher::ModelInstanceStubProcess()\r\n#3  0x0000ffff881109ec triton::backend::python::StubLauncher::Launch()\r\n#4  0x0000ffff880d8328 triton::backend::python::ModelInstanceState::LaunchStubProcess()\r\n#5  0x0000ffff880d9bf0 TRITONBACKEND_ModelInstanceInitialize\r\n#6  0x0000ffffb3f7ab04 triton::core::TritonModelInstance::CreateInstance(triton::core::TritonModel*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long, TRITONSERVER_instancegroupkind_enum, int, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&, inference::ModelRateLimiter const&, bool, std::map<unsigned int, std::shared_ptr<triton::core::TritonModelInstance::TritonBackendThread>, std::less<unsigned int>, std::allocator<std::pair<unsigned int const, std::shared_ptr<triton::core::TritonModelInstance::TritonBackendThread> > > >*, std::vector<triton::core::TritonModelInstance::SecondaryDevice, std::allocator<triton::core::TritonModelInstance::SecondaryDevice> > const&)\r\n#7  0x0000ffffb3f7bd6c triton::core::TritonModelInstance::CreateInstances(triton::core::TritonModel*, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > const&, inference::ModelConfig const&, bool)\r\n#8  0x0000ffffb3f712c8 triton::core::TritonModel::Create(triton::core::InferenceServer*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, long, inference::ModelConfig, bool, std::unique_ptr<triton::core::TritonModel, std::default_delete<triton::core::TritonModel> >*)\r\n#9  0x0000ffffb3fffbf8 triton::core::ModelLifeCycle::CreateModel(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, long, triton::core::ModelLifeCycle::ModelInfo*, bool)\r\n#10 0x0000ffffb40052e4 std::_Function_handler<void (), triton::core::ModelLifeCycle::AsyncLoad(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, inference::ModelConfig const&, bool, std::shared_ptr<triton::core::TritonRepoAgentModelList> const&, std::function<void (triton::core::Status)>&&)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n#11 0x0000ffffb412e310 std::thread::_State_impl<std::thread::_Invoker<std::tuple<triton::common::ThreadPool::ThreadPool(unsigned long)::{lambda()#1}> > >::_M_run()\r\n#12 0x0000ffffb383afac\r\n#13 0x0000ffffb3e98624 start_thread\r\n#14 0x0000ffffb3def49c\r\n\r\n\r\n```\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n22.11\r\n\r\nAre you using the Triton container or did you build it yourself?\r\n22.11-py3 container\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nuse repository with models, for example add_sub and add_sub2(copy) from https://github.com/triton-inference-server/python_backend/tree/main/examples\r\nrun `tritonserver --model-repository deployment_repo --model-control-mode=poll --repository-poll-secs=1 --log-verbose 3 --log-info true --log-file tis.log`\r\nrun in other terminal update models files, for example `while true; do touch repo/*/config.pbtxt; done`\r\nrun in other terminal metadata requests to server, for example `ab -t 300 -n 1000000 localhost:8000/v2/models/add_sub `\r\n\r\nCrucial to use logging levels enabled as it allow to reproduce issue faster.\r\n\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nServer continue to endlessly reload models.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5211/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5211/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5210", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5210/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5210/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5210/events", "html_url": "https://github.com/triton-inference-server/server/issues/5210", "id": 1516373402, "node_id": "I_kwDOCQnI4s5aYgWa", "number": 5210, "title": "python backend IPC makes triton-model QPS drop rapidly", "user": {"login": "spacegrass", "id": 3315048, "node_id": "MDQ6VXNlcjMzMTUwNDg=", "avatar_url": "https://avatars.githubusercontent.com/u/3315048?v=4", "gravatar_id": "", "url": "https://api.github.com/users/spacegrass", "html_url": "https://github.com/spacegrass", "followers_url": "https://api.github.com/users/spacegrass/followers", "following_url": "https://api.github.com/users/spacegrass/following{/other_user}", "gists_url": "https://api.github.com/users/spacegrass/gists{/gist_id}", "starred_url": "https://api.github.com/users/spacegrass/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/spacegrass/subscriptions", "organizations_url": "https://api.github.com/users/spacegrass/orgs", "repos_url": "https://api.github.com/users/spacegrass/repos", "events_url": "https://api.github.com/users/spacegrass/events{/privacy}", "received_events_url": "https://api.github.com/users/spacegrass/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2023-01-02T13:01:26Z", "updated_at": "2023-01-30T18:56:05Z", "closed_at": "2023-01-30T18:56:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWe test the triton-model QPS at a machine with T4 * 4.\r\n![image](https://user-images.githubusercontent.com/3315048/210233338-1c237eb8-e2cb-4954-a06c-87715088f957.png)\r\nThe CPU usage is blow 50%\r\nThere is more than 10GB memory available.\r\n\r\n![image](https://user-images.githubusercontent.com/3315048/210234795-b1af6fc7-5037-4bfa-8cd9-4f8c1c5287fc.png)\r\n\r\n**Triton Information**\r\nTriton inference server version is r21.10\r\n\r\nWe use triton inference server in a container.\r\n\r\n**To Reproduce**\r\n1. create an official r21.10 container.\r\n2. run triton inference server as model_start_xxxx.sh\r\n3. the triton model only have python backend model.\r\n4. use wget to download the triton model: http://ivip-projects.bj.bcebos.com/wanggaofei/tmp/model_ipc_qps.tar.gz?authorization=bce-auth-v1%2Fde0472630db3405fbab8c469aa03f05e%2F2023-01-02T12%3A58%3A03Z%2F-1%2Fhost%2F7d945cbf0838e012e51162a988bad4d0eb312470a1ddeedc5633f47efd0ed3a5\r\n\r\n\r\n**Expected behavior**\r\nIf we create 4 triton inference server, QPS of every server is 30.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5210/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5210/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5185", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5185/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5185/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5185/events", "html_url": "https://github.com/triton-inference-server/server/issues/5185", "id": 1504794244, "node_id": "I_kwDOCQnI4s5ZsVaE", "number": 5185, "title": "HTTP response from server does not include \"content-encoding\" header even if the response body is encoded", "user": {"login": "pkourouklidis", "id": 22411884, "node_id": "MDQ6VXNlcjIyNDExODg0", "avatar_url": "https://avatars.githubusercontent.com/u/22411884?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pkourouklidis", "html_url": "https://github.com/pkourouklidis", "followers_url": "https://api.github.com/users/pkourouklidis/followers", "following_url": "https://api.github.com/users/pkourouklidis/following{/other_user}", "gists_url": "https://api.github.com/users/pkourouklidis/gists{/gist_id}", "starred_url": "https://api.github.com/users/pkourouklidis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pkourouklidis/subscriptions", "organizations_url": "https://api.github.com/users/pkourouklidis/orgs", "repos_url": "https://api.github.com/users/pkourouklidis/repos", "events_url": "https://api.github.com/users/pkourouklidis/events{/privacy}", "received_events_url": "https://api.github.com/users/pkourouklidis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-12-20T15:56:28Z", "updated_at": "2023-04-19T20:38:57Z", "closed_at": "2023-04-19T20:38:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen sending an HTTP request to the server with the \"accept-encoding\" header set (e.g accept-encoding: gzip), the server encodes the response body but doesn't set the \"content-encoding\" header to the appropriate value (e.g content-encoding: gzip). As a result most clients will error out when trying to read the response. \r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n2.28.0\r\n\r\nAre you using the Triton container or did you build it yourself?\r\ncontainer version 22.11-py3\r\n\r\n**To Reproduce**\r\n1. Deploy a model. I used https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2 . You need to use the folder layout that triton accepts. /path/to/model/modelName/1/model.savedmodel/<everything downloaded from the link above>\r\n2. Send an inference request over HTTP example:\r\ncurl -v <server endpoint> -H \"Content-Type: application/json\" -H \"accept-encoding: gzip\" -d '{\"id\": \"1\",\"inputs\": [{\"name\": \"input_1\",\"shape\": [1,2,2,3],\"datatype\": \"FP32\",\"data\": [ [[0.5,0.5,0.5], [0.5,0.5,0.5]], [[0.5,0.5,0.5], [0.5,0.5,0.5]] ]}],\"outputs\": [{\"name\": \"output_1\"}]}'\r\n3. curl will error out trying to read the response. If you remove the \"accept-encoding\" header it will work fine.\r\n\r\n**Expected behavior**\r\nThe server should set the \"content-encoding\" header in the response if compression is used for the response body.\r\n\r\nMany thanks in advance to the people that will look into this issue!", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5185/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5150", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5150/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5150/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5150/events", "html_url": "https://github.com/triton-inference-server/server/issues/5150", "id": 1482822637, "node_id": "I_kwDOCQnI4s5YYhPt", "number": 5150, "title": "Change model namespacing to allow Triton to re-use model names across different model repos", "user": {"login": "nskool", "id": 10671803, "node_id": "MDQ6VXNlcjEwNjcxODAz", "avatar_url": "https://avatars.githubusercontent.com/u/10671803?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nskool", "html_url": "https://github.com/nskool", "followers_url": "https://api.github.com/users/nskool/followers", "following_url": "https://api.github.com/users/nskool/following{/other_user}", "gists_url": "https://api.github.com/users/nskool/gists{/gist_id}", "starred_url": "https://api.github.com/users/nskool/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nskool/subscriptions", "organizations_url": "https://api.github.com/users/nskool/orgs", "repos_url": "https://api.github.com/users/nskool/repos", "events_url": "https://api.github.com/users/nskool/events{/privacy}", "received_events_url": "https://api.github.com/users/nskool/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-12-07T20:51:38Z", "updated_at": "2023-03-29T19:10:42Z", "closed_at": "2023-03-29T19:10:41Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently, if model names repeat across multiple repositories (e.g., in multiple ensemble setup where each ensemble resides in its own repository), one runs into the following error:\r\n```\r\n... failed to poll model `add_sub_1`: not unique across all model repositories\r\n```\r\n\r\n**Describe the solution you'd like**\r\n1. While it's noted that Triton uses model names as unique identifies of a model and that in a flat namespace, all model names must be unique. However, it'll be useful to have Triton look for add_sub_1 first within the model repo in which it's called by ensemble proper, and then in a global namespace.\r\n2. I.e. customers can just clone their ensemble model and re-use the existing model names instead of making them unique. \r\n\r\n**Describe alternatives you've considered**\r\n1. Offline renaming of model names to make them unique, however, some customers may prefer to not do this as it's added maintenance\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5150/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5116", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5116/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5116/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5116/events", "html_url": "https://github.com/triton-inference-server/server/issues/5116", "id": 1466813444, "node_id": "I_kwDOCQnI4s5XbcwE", "number": 5116, "title": "Error while nginx proxy", "user": {"login": "AndreWanga", "id": 114132971, "node_id": "U_kgDOBs2H6w", "avatar_url": "https://avatars.githubusercontent.com/u/114132971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreWanga", "html_url": "https://github.com/AndreWanga", "followers_url": "https://api.github.com/users/AndreWanga/followers", "following_url": "https://api.github.com/users/AndreWanga/following{/other_user}", "gists_url": "https://api.github.com/users/AndreWanga/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreWanga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreWanga/subscriptions", "organizations_url": "https://api.github.com/users/AndreWanga/orgs", "repos_url": "https://api.github.com/users/AndreWanga/repos", "events_url": "https://api.github.com/users/AndreWanga/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreWanga/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2022-11-28T17:31:18Z", "updated_at": "2023-01-28T16:01:36Z", "closed_at": "2023-01-28T16:01:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen I use nginx to proxy triton server, there would be GOAWAY and Socket Close error when I got result\r\n\r\n**Triton Information**\r\n2.24.0\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nyes\r\n**To Reproduce**\r\n1. nginx config (the nginx version I use is 1.20.2 )\r\nevents {\r\n    work_connections 10000;\r\n    multi_accept on;\r\n    accept_mutex on;\r\n    use epoll;\r\n}\r\nhttp {\r\n    client_max_body_size 4000M;\r\n    grpc_read_timeout 400ms;\r\n    grpc_send_timeout 400ms;\r\n    grpc_buffer_size 100M;\r\n    grpc_next_upstream error timeout http_502 http_504;\r\n    grpc_next_upstream_tries 2;\r\n    underscores_in_headers on;\r\n    \r\n    upstream infer {\r\n        server XX.XX.XX.XX:8001 fail_timeout=10s max_fails=10;\r\n        server XX.XX.XX.XX:8002 fail_timeout=10s max_fails=10;\r\n    }\r\n    server {\r\n        listen 8000 http2;\r\n        location /inference.GRPCInferenceService/ModelInfer {\r\n            grpc_pass grpc://infer;\r\n        }\r\n    }\r\n}\r\n2. start triton server\r\ntritonserver --model-control-mode=poll --strict-model-config=false --model-repository=xx\r\n3. use python triton client async_infer function for query\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nNER(bert) tensorrt plan FP32\r\n**Expected behavior**\r\nNo GOAWAY and Socket Close when I use nginx to proxy\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5116/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5110", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5110/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5110/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5110/events", "html_url": "https://github.com/triton-inference-server/server/issues/5110", "id": 1463489704, "node_id": "I_kwDOCQnI4s5XOxSo", "number": 5110, "title": "Can not create a customized Python backend when using Python 3.11 for the Conda environment", "user": {"login": "luismarquezgft", "id": 102350703, "node_id": "U_kgDOBhm_bw", "avatar_url": "https://avatars.githubusercontent.com/u/102350703?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luismarquezgft", "html_url": "https://github.com/luismarquezgft", "followers_url": "https://api.github.com/users/luismarquezgft/followers", "following_url": "https://api.github.com/users/luismarquezgft/following{/other_user}", "gists_url": "https://api.github.com/users/luismarquezgft/gists{/gist_id}", "starred_url": "https://api.github.com/users/luismarquezgft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luismarquezgft/subscriptions", "organizations_url": "https://api.github.com/users/luismarquezgft/orgs", "repos_url": "https://api.github.com/users/luismarquezgft/repos", "events_url": "https://api.github.com/users/luismarquezgft/events{/privacy}", "received_events_url": "https://api.github.com/users/luismarquezgft/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-11-24T15:11:56Z", "updated_at": "2022-12-05T15:39:57Z", "closed_at": "2022-12-05T15:39:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nThe cmake - make operation that is required to create a customized Python environment does not work when the conda environment points to Python version 3.11. It works with Python 3.7, 3.8, 3.9 and 3.10.\r\n\r\n```\r\ncmake -DTRITON_ENABLE_GPU=OFF -DTRITON_BACKEND_REPO_TAG=r22.09 -DTRITON_COMMON_REPO_TAG=r22.09 -DTRITON_CORE_REPO_TAG=r22.09 -DCMAKE_INSTALL_PREFIX:PATH=`pwd`/install ..\r\nmake triton-python-backend-stub\r\n```\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nr22.09\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nThe host used to created the customized Python backend is using Ubuntu 20.04.\r\n\r\n**To Reproduce**\r\n- Create a file `conda_p311.yaml` with this content:\r\n```\r\nname: p311\r\nchannels:\r\n  - conda-forge\r\ndependencies:\r\n  - python=3.11\r\n```\r\n- Run the following commands:\r\n```\r\nconda env create --name p311 --file conda_p311.yaml\r\nexport PYTHONNOUSERSITE=True\r\nconda activate p311\r\ngit clone https://github.com/triton-inference-server/python_backend -b r22.09\r\ncd python_backend && mkdir build && cd build\r\ncmake -DTRITON_ENABLE_GPU=OFF -DTRITON_BACKEND_REPO_TAG=r22.09 -DTRITON_COMMON_REPO_TAG=r22.09 -DTRITON_CORE_REPO_TAG=r22.09 -DCMAKE_INSTALL_PREFIX:PATH=`pwd`/install ..\r\nmake triton-python-backend-stub\r\n```\r\nError received:\r\n```\r\n$ make triton-python-backend-stub\r\n\r\n[  2%] Building CXX object _deps/repo-common-build/CMakeFiles/triton-common-async-work-queue.dir/src/async_work_queue.cc.o\r\n[  5%] Building CXX object _deps/repo-common-build/CMakeFiles/triton-common-async-work-queue.dir/src/error.cc.o\r\n[  8%] Building CXX object _deps/repo-common-build/CMakeFiles/triton-common-async-work-queue.dir/src/thread_pool.cc.o\r\n[ 11%] Linking CXX static library libtritonasyncworkqueue.a\r\n[ 11%] Built target triton-common-async-work-queue\r\n[ 14%] Building CXX object _deps/repo-backend-build/CMakeFiles/triton-backend-utils.dir/src/backend_common.cc.o\r\n[ 17%] Building CXX object _deps/repo-backend-build/CMakeFiles/triton-backend-utils.dir/src/backend_input_collector.cc.o\r\n[ 20%] Building CXX object _deps/repo-backend-build/CMakeFiles/triton-backend-utils.dir/src/backend_memory.cc.o\r\n...\r\n...\r\n...\r\n[ 40%] No update step for 'boostorg'\r\n[ 42%] No patch step for 'boostorg'\r\n[ 45%] Performing configure step for 'boostorg'\r\n[ 48%] No build step for 'boostorg'\r\n[ 51%] No install step for 'boostorg'\r\n[ 54%] Completed 'boostorg'\r\n[ 54%] Built target boostorg\r\n[ 57%] Building CXX object CMakeFiles/triton-python-backend-stub.dir/src/pb_stub_utils.cc.o\r\nIn file included from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/a\r\nttr.h:13,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/p\r\nybind11.h:45,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/e\r\nmbed.h:12,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/src/pb_stub_utils.h:28,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/src/pb_stub_utils.cc:27:\r\n/home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/cast.h: In function \u2018st\r\nd::string pybind11::detail::error_string()\u2019:\r\n/home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/cast.h:446:36: error: i\r\nnvalid use of incomplete type \u2018PyFrameObject\u2019 {aka \u2018struct _frame\u2019}\r\n  446 |                 \"  \" + handle(frame->f_code->co_filename).cast<std::string>() +\r\n      |                                    ^~\r\nIn file included from /home/azureuser/anaconda3/envs/p311/include/python3.11/Python.h:42,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/d\r\netail/common.h:124,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/p\r\nytypes.h:12,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/c\r\nast.h:13,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/a\r\nttr.h:13,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/p\r\nybind11.h:45,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/e\r\nmbed.h:12,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/src/pb_stub_utils.h:28,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/src/pb_stub_utils.cc:27:\r\n/home/azureuser/anaconda3/envs/p311/include/python3.11/pytypedefs.h:22:16: note: forward declaration of \u2018PyFrameObject\u2019 {aka \r\n\u2018struct _frame\u2019}\r\n   22 | typedef struct _frame PyFrameObject;\r\n      |                ^~~~~~\r\nIn file included from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/a\r\nttr.h:13,\r\n                 from /home/azureuser/triton/custom_python311_stub/python_backend/build/_deps/pybind11-src/include/pybind11/p\r\nybind11.h:45,\r\n```\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nCompilation - make operations finish successfully.\r\nTip: When the conda file references Python version 3.10 or lower, the cmake - make process finishes successfully.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5110/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5084", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5084/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5084/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5084/events", "html_url": "https://github.com/triton-inference-server/server/issues/5084", "id": 1454171147, "node_id": "I_kwDOCQnI4s5WrOQL", "number": 5084, "title": "onnx model causes core dump in 22.08+, works with 22.06", "user": {"login": "dagardner-nv", "id": 96306125, "node_id": "U_kgDOBb2DzQ", "avatar_url": "https://avatars.githubusercontent.com/u/96306125?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dagardner-nv", "html_url": "https://github.com/dagardner-nv", "followers_url": "https://api.github.com/users/dagardner-nv/followers", "following_url": "https://api.github.com/users/dagardner-nv/following{/other_user}", "gists_url": "https://api.github.com/users/dagardner-nv/gists{/gist_id}", "starred_url": "https://api.github.com/users/dagardner-nv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dagardner-nv/subscriptions", "organizations_url": "https://api.github.com/users/dagardner-nv/orgs", "repos_url": "https://api.github.com/users/dagardner-nv/repos", "events_url": "https://api.github.com/users/dagardner-nv/events{/privacy}", "received_events_url": "https://api.github.com/users/dagardner-nv/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-11-17T23:40:16Z", "updated_at": "2023-01-03T20:04:34Z", "closed_at": "2023-01-03T20:04:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nIn Morpheus we have an [onnx model](https://github.com/nv-morpheus/Morpheus/tree/branch-22.11/models/triton-model-repo/phishing-bert-onnx) which was working with tritonserver 22.02 & 22.06 but causes a core dump in versions 22.08, 22.09 & 22.10\r\nhttps://github.com/nv-morpheus/Morpheus/issues/475\r\n\r\n**Triton Information**\r\n22.08\r\n\r\nContainer: nvcr.io/nvidia/tritonserver:22.08-py3\r\n\r\n**To Reproduce**\r\n```bash\r\ngit clone https://github.com/nv-morpheus/Morpheus\r\ncd Morpheus\r\n./scripts/fetch_data.py fetch models\r\ndocker run --rm -ti --gpus=all -p8000:8000 -p8001:8001 -p8002:8002 -v $PWD/models:/models nvcr.io/nvidia/tritonserver:22.08-py3 bash\r\ntritonserver --model-repository=/models/triton-model-repo --exit-on-error=false --log-info=true\r\n```\r\n\r\nFails with:\r\n```\r\n=============================\r\n== Triton Inference Server ==\r\n=============================\r\n\r\nNVIDIA Release 22.08 (build 42766143)\r\nTriton Server Version 2.25.0\r\n\r\nCopyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nroot@4af633826189:/opt/tritonserver# tritonserver --model-repository=/models/triton-model-repo --exit-on-error=false --log-info=true\r\nI1117 23:36:57.355277 91 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f23c6000000' with size 268435456\r\nI1117 23:36:57.355606 91 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\nI1117 23:36:57.361426 91 model_lifecycle.cc:459] loading: phishing-bert-trt:1\r\nI1117 23:36:57.361455 91 model_lifecycle.cc:459] loading: root-cause-binary-onnx:1\r\nI1117 23:36:57.361486 91 model_lifecycle.cc:459] loading: abp-nvsmi-xgb:1\r\nI1117 23:36:57.361505 91 model_lifecycle.cc:459] loading: phishing-bert-onnx:1\r\nI1117 23:36:57.361521 91 model_lifecycle.cc:459] loading: log-parsing-onnx:1\r\nI1117 23:36:57.361538 91 model_lifecycle.cc:459] loading: sid-minibert-onnx:1\r\nI1117 23:36:57.361579 91 model_lifecycle.cc:459] loading: sid-minibert-trt:1\r\nI1117 23:36:57.377033 91 tensorrt.cc:5441] TRITONBACKEND_Initialize: tensorrt\r\nI1117 23:36:57.377062 91 tensorrt.cc:5451] Triton TRITONBACKEND API version: 1.10\r\nI1117 23:36:57.377067 91 tensorrt.cc:5457] 'tensorrt' TRITONBACKEND API version: 1.10\r\nI1117 23:36:57.377153 91 tensorrt.cc:5500] backend configuration:\r\n{\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\r\nI1117 23:36:57.377182 91 tensorrt.cc:5552] TRITONBACKEND_ModelInitialize: phishing-bert-trt (version 1)\r\nI1117 23:36:57.585333 91 logging.cc:49] [MemUsageChange] Init CUDA: CPU +320, GPU +0, now: CPU 337, GPU 1161 (MiB)\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nbacktrace:\r\n```\r\nCore was generated by `tritonserver --model-repository=/models/triton-model-repo --exit-on-error=false'.\r\nProgram terminated with signal SIGSEGV, Segmentation fault.\r\n#0  0x00007f24003f342b in triton::backend::tensorrt::UseTensorRTv2API(std::shared_ptr<nvinfer1::ICudaEngine> const&) () from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n[Current thread is 1 (Thread 0x7f2402cec000 (LWP 95))]\r\n(gdb) bt\r\n#0  0x00007f24003f342b in triton::backend::tensorrt::UseTensorRTv2API(std::shared_ptr<nvinfer1::ICudaEngine> const&) () from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n#1  0x00007f24003d008e in triton::backend::tensorrt::ModelState::AutoCompleteConfigHelper(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) ()\r\n   from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n#2  0x00007f24003d2c60 in triton::backend::tensorrt::ModelState::AutoCompleteConfig() () from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n#3  0x00007f24003d3588 in triton::backend::tensorrt::ModelState::Create(TRITONBACKEND_Model*, triton::backend::tensorrt::ModelState**) () from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n#4  0x00007f24003d3a3a in TRITONBACKEND_ModelInitialize () from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n#5  0x00007f2412016fde in triton::core::TritonModel::Create(triton::core::InferenceServer*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > > > > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, long, inference::ModelConfig const&, std::unique_ptr<triton::core::TritonModel, std::default_delete<triton::core::TritonModel> >*) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#6  0x00007f24120d13a4 in triton::core::ModelLifeCycle::CreateModel(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, long, triton::core::ModelLifeCycle::ModelInfo*) ()\r\n   from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#7  0x00007f24120d7e38 in std::_Function_handler<void (), triton::core::ModelLifeCycle::AsyncLoad(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, inference::ModelConfig const&, std::shared_ptr<triton::core::TritonRepoAgentModelList> const&, std::function<void (triton::core::Status)>&&)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#8  0x00007f241220ab00 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<triton::common::ThreadPool::ThreadPool(unsigned long)::{lambda()#1}> > >::_M_run() () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#9  0x00007f2411b64de4 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#10 0x00007f2412eb5609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#11 0x00007f241184f133 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n```\r\n\r\n[config.pbtxt](https://github.com/nv-morpheus/Morpheus/blob/branch-22.11/models/triton-model-repo/phishing-bert-onnx/config.pbtxt) looks like:\r\n```\r\nname: \"phishing-bert-onnx\"\r\nplatform: \"onnxruntime_onnx\"\r\nbackend: \"onnxruntime\"\r\nmax_batch_size: 32\r\n\r\ninput [\r\n  {\r\n    name: \"input_ids\"\r\n    data_type: TYPE_INT64\r\n    dims: [ 128 ]\r\n  },\r\n  {\r\n    name: \"attention_mask\"\r\n    data_type: TYPE_INT64\r\n    dims: [ 128 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"output\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 2 ]\r\n  }\r\n]\r\n\r\ndynamic_batching {\r\n  preferred_batch_size: [ 1, 4, 8, 12, 16, 20, 24, 28, 32 ]\r\n  max_queue_delay_microseconds: 50000\r\n}\r\n\r\noptimization { execution_accelerators {\r\n  gpu_execution_accelerator : [ {\r\n    name : \"tensorrt\"\r\n    parameters { key: \"precision_mode\" value: \"FP16\" }\r\n    parameters { key: \"max_workspace_size_bytes\" value: \"1073741824\" }\r\n    }]\r\n}}\r\n```\r\n\r\n**Expected behavior**\r\nNot core dumping\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5084/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5073", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5073/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5073/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5073/events", "html_url": "https://github.com/triton-inference-server/server/issues/5073", "id": 1447003503, "node_id": "I_kwDOCQnI4s5WP4Vv", "number": 5073, "title": "docs/examples/stable_diffusion -> `scale_model_input` function should be called before `step`", "user": {"login": "nikolaydyankov", "id": 1205055, "node_id": "MDQ6VXNlcjEyMDUwNTU=", "avatar_url": "https://avatars.githubusercontent.com/u/1205055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikolaydyankov", "html_url": "https://github.com/nikolaydyankov", "followers_url": "https://api.github.com/users/nikolaydyankov/followers", "following_url": "https://api.github.com/users/nikolaydyankov/following{/other_user}", "gists_url": "https://api.github.com/users/nikolaydyankov/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikolaydyankov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikolaydyankov/subscriptions", "organizations_url": "https://api.github.com/users/nikolaydyankov/orgs", "repos_url": "https://api.github.com/users/nikolaydyankov/repos", "events_url": "https://api.github.com/users/nikolaydyankov/events{/privacy}", "received_events_url": "https://api.github.com/users/nikolaydyankov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-11-13T17:03:05Z", "updated_at": "2022-11-28T22:54:33Z", "closed_at": "2022-11-28T22:54:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "I followed /docs/examples/stable_diffusion/README.md and when I started the server and prompted the model I got this error in the server log:\r\n\r\n```\r\n0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/diffusers/schedulers/scheduling_lms_discrete.py:207: UserWarning: The `scale_model_input` function should be called before `step` to ensure correct denoising. See `StableDiffusionPipeline` for a usage example.\r\n  warnings.warn(\r\n50it [00:04, 12.40it/s]\r\nE1113 16:49:28.731224 1229 python_be.cc:1907] Stub process is unhealthy and it will be restarted.\r\n```\r\n\r\nAnd this in the client:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 289, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 982, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 824, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"client.py\", line 26, in generate\r\n    response = client.infer(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tritonclient/grpc/__init__.py\", line 1431, in infer\r\n    raise_error_grpc(rpc_error)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tritonclient/grpc/__init__.py\", line 62, in raise_error_grpc\r\n    raise get_error_grpc(rpc_error) from None\r\ntritonclient.utils.InferenceServerException: [StatusCode.INTERNAL] Failed to process the request(s) for model instance 'pipeline_0', message: Stub process is not healthy.\r\n```\r\n\r\nI found this [post](https://huggingface.co/CompVis/stable-diffusion-v1-4/discussions/126#635d5d8506a944e347beeae0) on the stable diffusion community forum:\r\n\r\n```\r\nuse image.sample instead of image.\r\n\r\ncredit: https://www.reddit.com/r/StableDiffusion/comments/xdxika/comment/is3pmfs/?utm_source=share&utm_medium=web2x&context=3\r\n```\r\n\r\nPerhaps the README.md needs to be updated?\r\n\r\nEDIT: From what I understand, line 130 of stable_diffusion/model_repository/pipeline/1/model.py needs to change to this:\r\n\r\n```\r\ndecoded_image = (decoded_image.sample / 2 + 0.5).clamp(0, 1)\r\n```", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5073/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5055", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5055/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5055/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5055/events", "html_url": "https://github.com/triton-inference-server/server/issues/5055", "id": 1442498657, "node_id": "I_kwDOCQnI4s5V-shh", "number": 5055, "title": "Stable-diffusion Example Inference Error due to Triton Server side's triton version update", "user": {"login": "tangkangqi", "id": 53213125, "node_id": "MDQ6VXNlcjUzMjEzMTI1", "avatar_url": "https://avatars.githubusercontent.com/u/53213125?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tangkangqi", "html_url": "https://github.com/tangkangqi", "followers_url": "https://api.github.com/users/tangkangqi/followers", "following_url": "https://api.github.com/users/tangkangqi/following{/other_user}", "gists_url": "https://api.github.com/users/tangkangqi/gists{/gist_id}", "starred_url": "https://api.github.com/users/tangkangqi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tangkangqi/subscriptions", "organizations_url": "https://api.github.com/users/tangkangqi/orgs", "repos_url": "https://api.github.com/users/tangkangqi/repos", "events_url": "https://api.github.com/users/tangkangqi/events{/privacy}", "received_events_url": "https://api.github.com/users/tangkangqi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-11-09T17:43:29Z", "updated_at": "2022-11-17T00:33:42Z", "closed_at": "2022-11-17T00:33:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nFollow the StableDiffusion github README.md steps https://github.com/triton-inference-server/server/tree/main/docs/examples/stable_diffusion,  after running server/model convert/client in docker, then run client.py will fail with log: Stub process is unhealthy.  \r\n\r\nChecked triton sever log shows very slower iteration (even comparing with running native .pkl model) and end up with \"Stub process is unhealthy and it will be restarted. \"\r\n\r\n![image](https://user-images.githubusercontent.com/53213125/200901212-8239d552-6903-47db-aea1-8f3f9fd9d56b.png)\r\n\r\n\r\n**Triton Information**\r\n2.25.0\r\n\r\nusing Triton container: \r\nserver: nvcr.io/nvidia/tritonserver:22.08-py3\r\n\r\n**To Reproduce**\r\nFollow the StableDiffusion github README.md steps https://github.com/triton-inference-server/server/tree/main/docs/examples/stable_diffusion,  after running server/model convert/client in docker, then run client.py will fail with log: Stub process is unhealthy.  \r\n\r\n**Expected behavior**\r\nIt should successfully generated image in less than 3 seconds. ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5055/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5039", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5039/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5039/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5039/events", "html_url": "https://github.com/triton-inference-server/server/issues/5039", "id": 1438063702, "node_id": "I_kwDOCQnI4s5VtxxW", "number": 5039, "title": "Unable to access GCS bucket with workload identity mechanism in GKE", "user": {"login": "jplu", "id": 959590, "node_id": "MDQ6VXNlcjk1OTU5MA==", "avatar_url": "https://avatars.githubusercontent.com/u/959590?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jplu", "html_url": "https://github.com/jplu", "followers_url": "https://api.github.com/users/jplu/followers", "following_url": "https://api.github.com/users/jplu/following{/other_user}", "gists_url": "https://api.github.com/users/jplu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jplu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jplu/subscriptions", "organizations_url": "https://api.github.com/users/jplu/orgs", "repos_url": "https://api.github.com/users/jplu/repos", "events_url": "https://api.github.com/users/jplu/events{/privacy}", "received_events_url": "https://api.github.com/users/jplu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2022-11-07T09:47:08Z", "updated_at": "2023-04-18T09:59:19Z", "closed_at": "2023-02-16T02:53:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nSince the version 22.08 it is not possible anymore to get access to the GCS buckets. Triton fails to start with the following error message:\r\n```\r\nI1107 09:25:28.889159 31 server.cc:259] No server context available. Exiting immediately.\r\nerror: creating server: Internal - Unable to create GCS client. Check account credentials.\r\n```\r\n\r\n**Triton Information**\r\nFrom Triton 22.08, official build.\r\n\r\n**To Reproduce**\r\nHere a minimal K8S manifest in order to be able to easily replicate the behavior:\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: workload-identity-test\r\n  namespace: \"default\"\r\n\r\n\r\nspec:\r\n  containers:\r\n  - image: nvcr.io/nvidia/tritonserver:22.08-py3\r\n    name: workload-identity-test\r\n    command: [\"sleep\",\"infinity\"]\r\n  serviceAccountName: <MY_SERVICE_ACCOUNT>\r\n```\r\nReplace `<MY_SERVICE_ACCOUNT>` with the name of the created service account to get access to GCS. Once the pod is deployed, run a shell on the pod with:\r\n```\r\nkubectl exec -it workload-identity-test --namespace default -- /bin/bash\r\n```\r\nOnce done run Triton with:\r\n```\r\ntritonserver --model-store=<GCS_BUCKET_ADDRESS>\r\n```\r\nReplace `<GCS_BUCKET_ADDRESS>` with the name of the bucket where the models are stored.\r\n\r\n**Expected behavior**\r\nBeing able to use the GKE workload identity mechanism.\r\n\r\nThanks in advance.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5039/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5039/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5000", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/5000/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/5000/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/5000/events", "html_url": "https://github.com/triton-inference-server/server/issues/5000", "id": 1417356886, "node_id": "I_kwDOCQnI4s5UeyZW", "number": 5000, "title": "Windows triton build cannot make https call because of a reduced default libcurl ", "user": {"login": "RandySheriffH", "id": 48490400, "node_id": "MDQ6VXNlcjQ4NDkwNDAw", "avatar_url": "https://avatars.githubusercontent.com/u/48490400?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RandySheriffH", "html_url": "https://github.com/RandySheriffH", "followers_url": "https://api.github.com/users/RandySheriffH/followers", "following_url": "https://api.github.com/users/RandySheriffH/following{/other_user}", "gists_url": "https://api.github.com/users/RandySheriffH/gists{/gist_id}", "starred_url": "https://api.github.com/users/RandySheriffH/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RandySheriffH/subscriptions", "organizations_url": "https://api.github.com/users/RandySheriffH/orgs", "repos_url": "https://api.github.com/users/RandySheriffH/repos", "events_url": "https://api.github.com/users/RandySheriffH/events{/privacy}", "received_events_url": "https://api.github.com/users/RandySheriffH/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-10-20T21:57:51Z", "updated_at": "2022-11-18T23:01:28Z", "closed_at": "2022-11-18T23:01:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "With either r20.03 or main branch, after build with triton client on windows and coded an exe base off cxx API:\r\nThe line where inference is happening failed:\r\n\r\n```\r\nstd::unique_ptr<tc::InferenceServerHttpClient> client;\r\nif (!tc::InferenceServerHttpClient::Create(&client, \"https://abc.com\", true).IsOk()) {\r\n\tthrow std::exception(\"Failed to create triton client!\");\r\n}\r\n...\r\nif(!client->Infer(&results, options, triton_inputs, triton_outputs, http_headers, tc::Parameters(),\r\n                          request_compression_algorithm, response_compression_algorithm).IsOk()) {\r\n\t\t\tthrow std::exception(\"Failed to send triton http request!\");\t\t\t\t\t\t\t\t\t\t\r\n}\r\n``` \r\n\r\nlibcurl reports:\r\n\r\n```\r\n* Protocol \"https\" not supported or disabled in libcurl\r\n* Closing connection -1\r\n```\r\n\r\nThe root cause of this is that my exe linked to libcurl.lib in triton client: \r\n\r\n```\r\nc:/triton/client/build/third-party/curl\r\n```\r\n\r\nThis pre-built  libcurl is compiled with zero ssl backend support such as openssl or schannel.\r\nSo I git-cloned curl and built with:\r\n\r\n```\r\ncmake .. -DCURL_USE_SCHANNEL=ON\r\n```\r\n\r\nAfter relinking, the https call succeed. \r\nTo sum up, I am expecting either 1 or 2:\r\n\r\n1.  document that customer might need to rebuild customized libcurl with ssl properly enabled;\r\n2.  when building on specific platform, enable ssl when building the default libcurl.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/5000/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/5000/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4997", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4997/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4997/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4997/events", "html_url": "https://github.com/triton-inference-server/server/issues/4997", "id": 1414277533, "node_id": "I_kwDOCQnI4s5UTCmd", "number": 4997, "title": "Model config and data transmitted is type F32, response in BYTES", "user": {"login": "csheaff", "id": 10719781, "node_id": "MDQ6VXNlcjEwNzE5Nzgx", "avatar_url": "https://avatars.githubusercontent.com/u/10719781?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csheaff", "html_url": "https://github.com/csheaff", "followers_url": "https://api.github.com/users/csheaff/followers", "following_url": "https://api.github.com/users/csheaff/following{/other_user}", "gists_url": "https://api.github.com/users/csheaff/gists{/gist_id}", "starred_url": "https://api.github.com/users/csheaff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csheaff/subscriptions", "organizations_url": "https://api.github.com/users/csheaff/orgs", "repos_url": "https://api.github.com/users/csheaff/repos", "events_url": "https://api.github.com/users/csheaff/events{/privacy}", "received_events_url": "https://api.github.com/users/csheaff/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-10-19T04:42:13Z", "updated_at": "2022-10-21T01:16:03Z", "closed_at": "2022-10-21T01:16:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a model configuration file like so:\r\n\r\n```\r\nname: \"claysmodel\"\r\nplatform: \"tensorflow_savedmodel\"\r\nmax_batch_size: 128\r\ndynamic_batching {\r\n    preferred_batch_size: [ 128 ]\r\n  }\r\ninput [\r\n  {\r\n    name: \"input_1\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 128, 128, 1 ]\r\n    format: FORMAT_NHWC\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"predictions\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 2 ]\r\n  }\r\n]\r\n```\r\nand have successfully sent data to the server for inference. However, I receive responses not with F32 precision:\r\n\r\n```\r\n(Pdb) responses[0].get_response()\r\n{'id': '1', 'model_name': 'claysmodel', 'model_version': '1', 'outputs': [{'name': 'predictions', 'datatype': 'BYTES', 'shape': [128, 2], 'data': ['0.999998:0', '0.000002:1', '0.999998:0', '0.000002:1', ...\r\n```\r\nThe script I am using for data sending and retrieval is mostly taken from `image_client.py`, and I've scrutinized it to see if I'm somehow asking for a string returned, but it doesn't appear to be the case. Is this likely a user error or is it expected behavior? Any tips would be much appreciated.\r\n\r\nEdit:  I have included `TF_ENABLE_ONEDNN_OPTS=0` in `docker run`", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4997/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4995", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4995/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4995/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4995/events", "html_url": "https://github.com/triton-inference-server/server/issues/4995", "id": 1414119538, "node_id": "I_kwDOCQnI4s5UScBy", "number": 4995, "title": "How to properly use dynamic input shape with Triton?", "user": {"login": "thiagoribeirodamotta", "id": 6556354, "node_id": "MDQ6VXNlcjY1NTYzNTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/6556354?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thiagoribeirodamotta", "html_url": "https://github.com/thiagoribeirodamotta", "followers_url": "https://api.github.com/users/thiagoribeirodamotta/followers", "following_url": "https://api.github.com/users/thiagoribeirodamotta/following{/other_user}", "gists_url": "https://api.github.com/users/thiagoribeirodamotta/gists{/gist_id}", "starred_url": "https://api.github.com/users/thiagoribeirodamotta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thiagoribeirodamotta/subscriptions", "organizations_url": "https://api.github.com/users/thiagoribeirodamotta/orgs", "repos_url": "https://api.github.com/users/thiagoribeirodamotta/repos", "events_url": "https://api.github.com/users/thiagoribeirodamotta/events{/privacy}", "received_events_url": "https://api.github.com/users/thiagoribeirodamotta/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-10-19T01:51:44Z", "updated_at": "2022-10-31T20:45:56Z", "closed_at": "2022-10-31T20:45:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI'm building an ensemble pbtxt where the first step is to apply preprocessing (custom TritonPythonModel implementation) to images of any size, so they get resized to match the second step object detection model size, but I've been unable to make this work.\r\n\r\nI know pbtxt works with dynamic input shape as follows: `dims: [ -1, -1, 3 ]`, but when passing an input image to `set_data_from_numpy`, so they can be inferred on a client application, it will yield and error stating that -1 shape is different from whatever actual size the image has, such as: `got unexpected numpy array shape [1080, 1920, 3], expected [-1, -1, 3]`\r\n\r\nHere's a snippet of some of the steps done along the way:\r\n```\r\nself.input_shape = list(self.model_config.config.input[0].dims)\r\nself.inputs.append(\r\n            grpcclient.InferInput(\r\n                self.model_config.config.input[0].name,\r\n                self.input_shape,\r\n                self.datatype,\r\n            ),\r\n        )\r\ninput_image = image.utils.load_img(\r\n        str(FLAGS.input),\r\n        color_mode=\"rgb\",\r\n    )   \r\n self.inputs[0].set_data_from_numpy(input_image )       \r\n results = self.triton_client.infer(\r\n            model_name=self.FLAGS.model,\r\n            inputs=self.inputs,\r\n            outputs=self.outputs,\r\n            client_timeout=self.FLAGS.client_timeout,\r\n        )\r\n```\r\n**Triton Information**\r\nWhat version of Triton are you using?  22.09-py3\r\n\r\nAre you using the Triton container or did you build it yourself? Using container\r\n\r\n**To Reproduce**\r\nCreate a pbtxt with variable size dims, such as `dims: [ -1, -1, 3 ]`, start the triton server and attempt to make an inference with an image of any size.\r\n\r\nDescribe the models (framework, inputs, outputs):\r\nThe following is the ensemble pbtxt model I've built so far\r\n```\r\nname: \"ensemble_yolox_m_coco\"\r\nplatform: \"ensemble\"\r\nmax_batch_size: 0\r\ninput [\r\n  {\r\n    name: \"input\"\r\n    data_type: TYPE_UINT8\r\n    dims: [ -1, -1, 3 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"output\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, 6 ]\r\n  }   \r\n]\r\nensemble_scheduling {\r\n  step [\r\n    {\r\n      model_name: \"preprocess\"\r\n      model_version: -1\r\n      input_map {\r\n        key: \"INPUT_0\"\r\n        value: \"input\"\r\n      }\r\n      output_map {\r\n        key: \"OUTPUT_0\"\r\n        value: \"preprocessed_image\"\r\n      }\r\n    },\r\n    {\r\n      model_name: \"yolox_m_coco\"\r\n      model_version: -1\r\n      input_map {\r\n        key: \"input_1\"\r\n        value: \"preprocessed_image\"\r\n      }\r\n      output_map {\r\n        key: \"outputs_fp32\"\r\n        value: \"detected_image\"\r\n      }\r\n    },\r\n    {\r\n      model_name: \"postprocess\"\r\n      model_version: -1\r\n      input_map {\r\n        key: \"INPUT_0\"\r\n        value: \"detected_image\"\r\n      }\r\n      output_map {\r\n        key: \"OUTPUT_0\"\r\n        value: \"output\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nPreprocess step is as follows:\r\n```\r\nname: \"preprocess\"\r\nbackend: \"python\"\r\nmax_batch_size: 0 \r\ninput [\r\n  {\r\n    name: \"INPUT_0\"\r\n    data_type: TYPE_UINT8\r\n    dims: [ -1, -1, 3 ]\r\n  }\r\n]\r\n \r\noutput [\r\n{\r\n    name: \"OUTPUT_0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1, 1280, 1280, 3 ]\r\n}\r\n]\r\n\r\ninstance_group [{ kind: KIND_CPU }]\r\n```\r\n**Expected behavior**\r\nImages of any size would be accepted and resized when needed during the preprocessing step.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4995/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4995/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4981", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4981/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4981/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4981/events", "html_url": "https://github.com/triton-inference-server/server/issues/4981", "id": 1408506821, "node_id": "I_kwDOCQnI4s5T9BvF", "number": 4981, "title": "Docker build fail on windows ", "user": {"login": "RandySheriffH", "id": 48490400, "node_id": "MDQ6VXNlcjQ4NDkwNDAw", "avatar_url": "https://avatars.githubusercontent.com/u/48490400?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RandySheriffH", "html_url": "https://github.com/RandySheriffH", "followers_url": "https://api.github.com/users/RandySheriffH/followers", "following_url": "https://api.github.com/users/RandySheriffH/following{/other_user}", "gists_url": "https://api.github.com/users/RandySheriffH/gists{/gist_id}", "starred_url": "https://api.github.com/users/RandySheriffH/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RandySheriffH/subscriptions", "organizations_url": "https://api.github.com/users/RandySheriffH/orgs", "repos_url": "https://api.github.com/users/RandySheriffH/repos", "events_url": "https://api.github.com/users/RandySheriffH/events{/privacy}", "received_events_url": "https://api.github.com/users/RandySheriffH/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2022-10-13T22:44:09Z", "updated_at": "2022-12-05T19:58:34Z", "closed_at": "2022-12-05T19:58:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nBuild docker image failed on \"Dockerfile.win10.min\"\r\n\r\n**Triton Information**\r\nmain \r\n\r\n**To Reproduce**\r\n`docker build -t win10-py3-min2 -f Dockerfile.win10.min .`\r\n\r\nGot:\r\n\r\n**Step 21/61** : RUN vcpkg.exe install openssl:x64-windows openssl-windows:x64-windows rapidjson:x64-windows re2:x64-windows boost-interprocess:x64-windows boost-stacktrace:x64-windows zlib:x64-windows pthread:x64-windows b64:x64-windows\r\n ---> Running in 4e03abdf511e\r\nComputing installation plan...\r\nA suitable version of cmake was not found (required v3.22.2). Downloading portable cmake v3.22.2...\r\nDownloading cmake...\r\n  https://github.com/Kitware/CMake/releases/download/v3.22.2/cmake-3.22.2-windows-i386.zip -> C:\\vcpkg\\downloads\\cmake-3.22.2-windows-i386.zip\r\nExtracting cmake...\r\nA suitable version of 7zip_msi was not found (required v21.7.0). Downloading portable 7zip_msi v21.7.0...\r\nDownloading 7zip_msi...\r\n  https://www.7-zip.org/a/7z2107-x64.msi -> C:\\vcpkg\\downloads\\7z2107-x64.msi\r\nExtracting 7zip_msi...\r\nThe following packages will be built and installed:\r\n    b64[core]:x64-windows -> 2.0.0.1\r\n  * boost-array[core]:x64-windows -> 1.78.0\r\n  * boost-assert[core]:x64-windows -> 1.78.0\r\n  * boost-build[core]:x64-windows -> 1.78.0#1\r\n  * boost-config[core]:x64-windows -> 1.78.0\r\n  * boost-container[core]:x64-windows -> 1.78.0\r\n  * boost-container-hash[core]:x64-windows -> 1.78.0\r\n  * boost-core[core]:x64-windows -> 1.78.0\r\n  * boost-detail[core]:x64-windows -> 1.78.0\r\n  * boost-integer[core]:x64-windows -> 1.78.0\r\n    boost-interprocess[core]:x64-windows -> 1.78.0\r\n  * boost-intrusive[core]:x64-windows -> 1.78.0\r\n  * boost-modular-build-helper[core]:x64-windows -> 1.78.0#3\r\n  * boost-move[core]:x64-windows -> 1.78.0\r\n  * boost-predef[core]:x64-windows -> 1.78.0\r\n  * boost-preprocessor[core]:x64-windows -> 1.78.0\r\n  * boost-smart-ptr[core]:x64-windows -> 1.78.0\r\n    boost-stacktrace[core]:x64-windows -> 1.78.0\r\n  * boost-static-assert[core]:x64-windows -> 1.78.0\r\n  * boost-throw-exception[core]:x64-windows -> 1.78.0\r\n  * boost-tuple[core]:x64-windows -> 1.78.0\r\n  * boost-type-traits[core]:x64-windows -> 1.78.0\r\n  * boost-uninstall[core]:x64-windows -> 1.78.0\r\n  * boost-unordered[core]:x64-windows -> 1.78.0\r\n  * boost-vcpkg-helpers[core]:x64-windows -> 1.78.0#1\r\n  * boost-winapi[core]:x64-windows -> 1.78.0\r\n    openssl[core]:x64-windows -> 3.0.2#2\r\n    openssl-windows[core]:x64-windows -> 1.1.1h#2\r\n    pthread[core]:x64-windows -> 3.0.0#1\r\n  * pthreads[core]:x64-windows -> 3.0.0#10\r\n    rapidjson[core]:x64-windows -> 2020-09-14#2\r\n    re2[core]:x64-windows -> 2021-11-01\r\n  * vcpkg-cmake[core]:x64-windows -> 2022-04-07\r\n  * vcpkg-cmake-config[core]:x64-windows -> 2022-02-06\r\n    zlib[core]:x64-windows -> 1.2.12\r\nAdditional packages (*) will be modified to complete this operation.\r\n**Error: in triplet x64-windows: Unable to find a valid Visual Studio instance**\r\nCould not locate a complete Visual Studio instance\r\nThe following paths were examined for Visual Studio instances:\r\n    C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\TestAgent\\VC\\Auxiliary/Build\\vcvarsall.bat\r\n    C:\\BuildTools\\VC\\Auxiliary/Build\\vcvarsall.bat\r\nThe command 'cmd /S /C vcpkg.exe install openssl:x64-windows openssl-windows:x64-windows rapidjson:x64-windows re2:x64-windows boost-interprocess:x64-windows boost-stacktrace:x64-windows zlib:x64-windows pthread:x64-windows b64:x64-windows' returned a non-zero code: 1\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4981/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4981/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4979", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4979/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4979/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4979/events", "html_url": "https://github.com/triton-inference-server/server/issues/4979", "id": 1408205591, "node_id": "I_kwDOCQnI4s5T74MX", "number": 4979, "title": "Build error on windows", "user": {"login": "RandySheriffH", "id": 48490400, "node_id": "MDQ6VXNlcjQ4NDkwNDAw", "avatar_url": "https://avatars.githubusercontent.com/u/48490400?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RandySheriffH", "html_url": "https://github.com/RandySheriffH", "followers_url": "https://api.github.com/users/RandySheriffH/followers", "following_url": "https://api.github.com/users/RandySheriffH/following{/other_user}", "gists_url": "https://api.github.com/users/RandySheriffH/gists{/gist_id}", "starred_url": "https://api.github.com/users/RandySheriffH/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RandySheriffH/subscriptions", "organizations_url": "https://api.github.com/users/RandySheriffH/orgs", "repos_url": "https://api.github.com/users/RandySheriffH/repos", "events_url": "https://api.github.com/users/RandySheriffH/events{/privacy}", "received_events_url": "https://api.github.com/users/RandySheriffH/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2022-10-13T17:53:19Z", "updated_at": "2022-11-28T18:36:29Z", "closed_at": "2022-11-28T18:36:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen buiilding on windows with non-docker, have multiple errors reported.\r\n\r\n**Triton Information**\r\nTested on latest main and r22.03\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nI am not with container\r\n\r\n**To Reproduce**\r\nJust follow build steps for non-docker windows.\r\n\r\n**Expected behavior**\r\nBuild error:\r\n\"C:\\triton\\client\\build\\cc-clients\\_deps\\repo-common-src\\include\\triton/common/triton_json.h(641,35): error C2039: 'GetO\r\nbjectA': is not a member of 'rapidjson::GenericValue<rapidjson::UTF8<char>,rapidjson::MemoryPoolAllocator<rapidjson::Cr\r\ntAllocator>>' [C:\\triton\\client\\build\\cc-clients\\library\\http-client-library.vcxproj] [C:\\triton\\client\\build\\cc-client\r\ns.vcxproj]\"\r\n\r\n-------------------------------------------------------\r\nSome update - there actually a few other build errors too:\r\n\r\nC:\\issue\\triton_ut2\\build\\triton_client-prefix\\src\\triton_client-build\\cc-clients\\_deps\\repo-common-src\\include\\triton/\r\ncommon/triton_json.h(595,1): error C2530: 'm': references must be initialized [C:\\issue\\triton_ut2\\build\\triton_client-\r\nprefix\\src\\triton_client-build\\cc-clients\\library\\http-client-library.vcxproj] [C:\\issue\\triton_ut2\\build\\triton_client\r\n-prefix\\src\\triton_client-build\\cc-clients.vcxproj] [C:\\issue\\triton_ut2\\build\\triton_client.vcxproj]\r\n\r\nC:\\issue\\triton_ut2\\build\\triton_client-prefix\\src\\triton_client\\src\\c++\\library\\http_client.cc(1645,53): error C2589:\r\n'(': illegal token on right side of '::' [C:\\issue\\triton_ut2\\build\\triton_client-prefix\\src\\triton_client-build\\cc-cli\r\nents\\library\\http-client-library.vcxproj] [C:\\issue\\triton_ut2\\build\\triton_client-prefix\\src\\triton_client-build\\cc-cl\r\nients.vcxproj] [C:\\issue\\triton_ut2\\build\\triton_client.vcxproj]", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4979/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4956", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4956/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4956/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4956/events", "html_url": "https://github.com/triton-inference-server/server/issues/4956", "id": 1402133367, "node_id": "I_kwDOCQnI4s5Tktt3", "number": 4956, "title": "Triton Server crash when upload a wrong serializationVersion model", "user": {"login": "bug-developer021", "id": 80243243, "node_id": "MDQ6VXNlcjgwMjQzMjQz", "avatar_url": "https://avatars.githubusercontent.com/u/80243243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bug-developer021", "html_url": "https://github.com/bug-developer021", "followers_url": "https://api.github.com/users/bug-developer021/followers", "following_url": "https://api.github.com/users/bug-developer021/following{/other_user}", "gists_url": "https://api.github.com/users/bug-developer021/gists{/gist_id}", "starred_url": "https://api.github.com/users/bug-developer021/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bug-developer021/subscriptions", "organizations_url": "https://api.github.com/users/bug-developer021/orgs", "repos_url": "https://api.github.com/users/bug-developer021/repos", "events_url": "https://api.github.com/users/bug-developer021/events{/privacy}", "received_events_url": "https://api.github.com/users/bug-developer021/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-10-09T03:14:45Z", "updated_at": "2022-10-10T21:50:13Z", "closed_at": "2022-10-10T21:50:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nI upload a trt engine with an  mistach serializationVersion  that Triton Server don't support. Even if i set the triton launch config `--strict-readiness=False` and `--exit-on-error=False`,  the mistach serializationVersion model still cause triton server crash\r\n\r\n\r\n\r\n```\r\nE1008 08:29:52.094179 1 logging.cc:43] 1: [stdArchiveReader.cpp::StdArchiveReader::40] Error Code 1: Serialization (Serialization assertion stdVersionRead == serializationVersion failed.Version tag does not match. Note: Current Version: 205, Serialized Engine Version: 213)\r\nE1008 08:29:52.094254 1 logging.cc:43] 4: [runtime.cpp::deserializeCudaEngine::50] Error Code 4: Internal Error (Engine deserialization failed.)\r\nSignal (11) received.\r\nI1008 08:29:52.265387 1 http_server.cc:3014] HTTP request: 0 /v2/health/ready\r\nI1008 08:29:52.341725 1 http_server.cc:3014] HTTP request: 0 /v2/health/live\r\n 0# 0x0000561E522EA549 in tritonserver\r\n 1# 0x00007FB7FE02A0C0 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# 0x00007FB71DC346FB in /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n 3# 0x00007FB71DC1009E in /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n 4# 0x00007FB71DC11DD4 in /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n 5# 0x00007FB71DC121AB in /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n 6# TRITONBACKEND_ModelInitialize in /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n 7# 0x00007FB7FEBCCA3A in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n 8# 0x00007FB7FEA5137A in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n 9# 0x00007FB7FEA5F351 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n10# 0x00007FB7FE41BDE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n11# 0x00007FB7FE898609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n12# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n```\r\n\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n2.20.0\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTriton container nvidia/tritonserver:22.03-py3\r\n\r\n**To Reproduce**\r\n\r\nI export yolov5s.pt to TensorRT engine with python library nvidia-tensorrt 8.4.3, and upload to Triton Server which TensorRT Version is 8.2.3. Finally I knonw the wrong TensorRT version, But I hope this situation will not cause the whole triton server to crash\r\n   \r\n\r\n\r\n**Expected behavior**\r\nOn the  condition of triton launch config `--strict-readiness=False` and `--exit-on-error=False`, the single model bug \r\nwill not affect other model services\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4956/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4956/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4934", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4934/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4934/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4934/events", "html_url": "https://github.com/triton-inference-server/server/issues/4934", "id": 1390148140, "node_id": "I_kwDOCQnI4s5S2_os", "number": 4934, "title": "RuntimeError: Error in dlopen: libtorch_cuda_linalg.so: cannot open shared object file: No such file or directory", "user": {"login": "zhaozhiming37", "id": 93253880, "node_id": "U_kgDOBY7w-A", "avatar_url": "https://avatars.githubusercontent.com/u/93253880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhaozhiming37", "html_url": "https://github.com/zhaozhiming37", "followers_url": "https://api.github.com/users/zhaozhiming37/followers", "following_url": "https://api.github.com/users/zhaozhiming37/following{/other_user}", "gists_url": "https://api.github.com/users/zhaozhiming37/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhaozhiming37/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhaozhiming37/subscriptions", "organizations_url": "https://api.github.com/users/zhaozhiming37/orgs", "repos_url": "https://api.github.com/users/zhaozhiming37/repos", "events_url": "https://api.github.com/users/zhaozhiming37/events{/privacy}", "received_events_url": "https://api.github.com/users/zhaozhiming37/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2022-09-29T02:10:24Z", "updated_at": "2023-02-21T21:01:11Z", "closed_at": "2023-02-21T21:01:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nIt seems cannot found `libtorch_cuda_linalg.so`\r\n\r\nserver\r\n```\r\nI0929 01:52:22.008062 5902 grpc_server.cc:3587] New request handler for ModelInferHandler, 0\r\nI0929 01:52:22.008083 5902 infer_request.cc:713] [request id: 1-1] prepared: [0x0x7fd828016730] request id: 1-1, model: face_iqa, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 0, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7fd828008058] input: LANDMARKS, type: FP32, original shape: [2,5,2], batch + shape: [2,5,2], shape: [2,5,2]\r\n[0x0x55929f3383b8] input: IMAGE, type: UINT8, original shape: [1,720,1280,3], batch + shape: [1,720,1280,3], shape: [1,720,1280,3]\r\noverride inputs:\r\ninputs:\r\n[0x0x55929f3383b8] input: IMAGE, type: UINT8, original shape: [1,720,1280,3], batch + shape: [1,720,1280,3], shape: [1,720,1280,3]\r\n[0x0x7fd828008058] input: LANDMARKS, type: FP32, original shape: [2,5,2], batch + shape: [2,5,2], shape: [2,5,2]\r\noriginal requested outputs:\r\nSCORES\r\nrequested outputs:\r\nSCORES\r\n\r\nI0929 01:52:22.008134 5902 infer_request.cc:713] [request id: 1-1] prepared: [0x0x7fd828018b50] request id: 1-1, model: face_iqa_preprocess, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 0, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7fd828019018] input: landmarks__1, type: FP32, original shape: [2,5,2], batch + shape: [2,5,2], shape: [2,5,2]\r\n[0x0x7fd828018e88] input: image__0, type: UINT8, original shape: [1,720,1280,3], batch + shape: [1,720,1280,3], shape: [1,720,1280,3]\r\noverride inputs:\r\ninputs:\r\n[0x0x7fd828018e88] input: image__0, type: UINT8, original shape: [1,720,1280,3], batch + shape: [1,720,1280,3], shape: [1,720,1280,3]\r\n[0x0x7fd828019018] input: landmarks__1, type: FP32, original shape: [2,5,2], batch + shape: [2,5,2], shape: [2,5,2]\r\noriginal requested outputs:\r\ncrops__0\r\nrequested outputs:\r\ncrops__0\r\n\r\nI0929 01:52:22.008203 5902 libtorch.cc:2142] model face_iqa_preprocess, instance face_iqa_preprocess_0_1, executing 1 requests\r\nI0929 01:52:22.008216 5902 libtorch.cc:992] TRITONBACKEND_ModelExecute: Running face_iqa_preprocess_0_1 with 1 requests\r\nI0929 01:52:22.008273 5902 pinned_memory_manager.cc:161] pinned memory allocation: size 2764800, addr 0x7fd9d2000090\r\nI0929 01:52:22.008507 5902 pinned_memory_manager.cc:161] pinned memory allocation: size 80, addr 0x7fd9d22a30a0\r\nI0929 01:52:22.033392 5902 grpc_server.cc:3758] ModelInferHandler::InferResponseComplete, 0 step ISSUED\r\nI0929 01:52:22.033527 5902 grpc_server.cc:3312] ModelInferHandler::InferRequestComplete\r\nI0929 01:52:22.033538 5902 grpc_server.cc:3594] Process for ModelInferHandler, rpc_ok=1, 0 step COMPLETE\r\nI0929 01:52:22.033545 5902 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7fd9d2000090\r\nI0929 01:52:22.033550 5902 grpc_server.cc:2504] Done for ModelInferHandler, 0\r\nI0929 01:52:22.033554 5902 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7fd9d22a30a0\r\n```\r\nclient\r\n```\r\nTraceback (most recent call last):\r\n  File \"/media/zzm/Code/work/cv_face_quality/face_iqa.py\", line 144, in <module>\r\n    FaceIqa.request(\r\n  File \"/media/zzm/Code/work/cv_face_quality/face_iqa.py\", line 118, in request\r\n    raise e\r\n  File \"/media/zzm/Code/work/cv_face_quality/face_iqa.py\", line 109, in request\r\n    response = client.infer(\r\n  File \"/home/zzm/anaconda3/envs/triton/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\", line 1322, in infer\r\n    raise_error_grpc(rpc_error)\r\n  File \"/home/zzm/anaconda3/envs/triton/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\", line 62, in raise_error_grpc\r\n    raise get_error_grpc(rpc_error) from None\r\ntritonclient.utils.InferenceServerException: [StatusCode.INTERNAL] in ensemble 'face_iqa', PyTorch execute failure: The following operation failed in the TorchScript interpreter.\r\nTraceback of TorchScript, serialized code (most recent call last):\r\n  File \"code/__torch__.py\", line 12, in forward\r\n    landmarks: Tensor) -> Tensor:\r\n    face_align = self.face_align\r\n    crops = (face_align).forward(img, landmarks, )\r\n             ~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    norm = self.norm\r\n    return (norm).forward(crops, )\r\n  File \"code/__torch__.py\", line 29, in forward\r\n    _1 = __torch__.torch.nn.functional.grid_sample\r\n    src_shape = torch.slice(torch.size(img), 1, 3)\r\n    theta = (self).get_similarity_transform(src_pts, src_shape, )\r\n             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    _2 = torch.permute(img, [0, 3, 1, 2])\r\n    _3 = [(torch.size(theta))[0], (torch.size(img))[3], (torch.size(img))[1], (torch.size(img))[2]]\r\n  File \"code/__torch__.py\", line 44, in get_similarity_transform\r\n    src = torch.pad(src_pts, [0, 1], \"constant\", 1.)\r\n    ref_pts = self.ref_pts\r\n    _7, _8, _9, _10 = torch.linalg_lstsq(ref_pts, src)\r\n                      ~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    tfm = torch.transpose(_7, 1, 2)\r\n    h1 = src_shape[0]\r\n\r\nTraceback of TorchScript, original code (most recent call last):\r\n  File \"/media/zzm/Code/work/cv-face-image-quality-assessment/fiqa_exp/ser_exp/pipeline.py\", line 188, in forward\r\n            faces: torch Tensor\r\n        \"\"\"\r\n        crops = self.face_align(img, landmarks)\r\n                ~~~~~~~~~~~~~~~ <--- HERE\r\n        crops = self.norm(crops)\r\n        return crops\r\n  File \"/media/zzm/Code/work/cv-face-image-quality-assessment/fiqa_exp/ser_exp/pipeline.py\", line 108, in forward\r\n    def forward(self, img, src_pts):\r\n        src_shape = img.shape[1:3]\r\n        theta = self.get_similarity_transform(src_pts, src_shape)\r\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        # print(f\"img shape: {img.shape}\")\r\n        imgs = img.permute(0, 3, 1, 2).\\\r\n  File \"/media/zzm/Code/work/cv-face-image-quality-assessment/fiqa_exp/ser_exp/pipeline.py\", line 95, in get_similarity_transform\r\n    def get_similarity_transform(self, src_pts: torch.Tensor, src_shape: List[int]):\r\n        src = F.pad(src_pts, (0, 1), \"constant\", 1.0)\r\n        res = torch.linalg.lstsq(self.ref_pts, src)\r\n              ~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        tfm = res.solution.transpose(1, 2)\r\n        h1, w1 = src_shape[0], src_shape[1]\r\nRuntimeError: Error in dlopen: libtorch_cuda_linalg.so: cannot open shared object file: No such file or directory\r\n```\r\n\r\n**Triton Information**\r\n\r\ntriton docker version\r\n`nvcr.io/nvidia/tritonserver:22.08-py3`\r\n\r\n\r\n**To Reproduce**\r\n\r\n\r\n**Expected behavior**\r\nno error\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4934/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4934/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4929", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4929/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4929/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4929/events", "html_url": "https://github.com/triton-inference-server/server/issues/4929", "id": 1388662509, "node_id": "I_kwDOCQnI4s5SxU7t", "number": 4929, "title": "Error Resuming when not pause", "user": {"login": "haritsahm", "id": 26606913, "node_id": "MDQ6VXNlcjI2NjA2OTEz", "avatar_url": "https://avatars.githubusercontent.com/u/26606913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/haritsahm", "html_url": "https://github.com/haritsahm", "followers_url": "https://api.github.com/users/haritsahm/followers", "following_url": "https://api.github.com/users/haritsahm/following{/other_user}", "gists_url": "https://api.github.com/users/haritsahm/gists{/gist_id}", "starred_url": "https://api.github.com/users/haritsahm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/haritsahm/subscriptions", "organizations_url": "https://api.github.com/users/haritsahm/orgs", "repos_url": "https://api.github.com/users/haritsahm/repos", "events_url": "https://api.github.com/users/haritsahm/events{/privacy}", "received_events_url": "https://api.github.com/users/haritsahm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-09-28T03:22:45Z", "updated_at": "2022-12-20T20:22:03Z", "closed_at": "2022-12-20T20:22:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nConnection reset by peer error similar to [#3353](https://github.com/triton-inference-server/server/issues/3353) . This rarely happens and sometimes there is also an error related to http. I've tried adding verbose but I haven't caught/reproduced the same error and ended up with storage filled by verbose log. The only way to solve this by restart the docker or sometimes have to reset the docker in systemctl.\r\n\r\n**Triton Information**\r\nTriton: NVIDIA Release 21.10 (build 28453983)\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nYes, for custom python_backend model with torch support.\r\n```\r\nARG TRITON_VERSION=21.10\r\nARG BASE_IMAGE=nvcr.io/nvidia/tritonserver:${TRITON_VERSION}-py3\r\n\r\nFROM $BASE_IMAGE\r\n\r\nRUN pip3 install --upgrade --no-cache-dir torch torchvision\r\n```\r\n**To Reproduce**\r\nI don't know how to reproduce this error since it's rarely happen\r\n\r\n**Expected behavior**\r\nNo connection error.\r\n\r\n**Docker log**\r\n```\r\n=============================\r\n== Triton Inference Server ==\r\n=============================\r\n\r\nNVIDIA Release 21.10 (build 28453983)\r\n\r\nCopyright (c) 2018-2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nI0719 03:50:13.054499 1 metrics.cc:298] Collecting metrics for GPU 0: Tesla T4\r\nI0719 03:50:13.054702 1 metrics.cc:298] Collecting metrics for GPU 1: Tesla T4\r\nI0719 03:50:13.054715 1 metrics.cc:298] Collecting metrics for GPU 2: Tesla T4\r\nI0719 03:50:13.054725 1 metrics.cc:298] Collecting metrics for GPU 3: Tesla T4\r\nI0719 03:50:13.054737 1 metrics.cc:298] Collecting metrics for GPU 4: Tesla T4\r\nI0719 03:50:13.316525 1 libtorch.cc:1092] TRITONBACKEND_Initialize: pytorch\r\nI0719 03:50:13.316554 1 libtorch.cc:1102] Triton TRITONBACKEND API version: 1.6\r\nI0719 03:50:13.316557 1 libtorch.cc:1108] 'pytorch' TRITONBACKEND API version: 1.6\r\n2022-07-19 03:50:13.663576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nI0719 03:50:13.702483 1 tensorflow.cc:2170] TRITONBACKEND_Initialize: tensorflow\r\nI0719 03:50:13.702514 1 tensorflow.cc:2180] Triton TRITONBACKEND API version: 1.6\r\nI0719 03:50:13.702517 1 tensorflow.cc:2186] 'tensorflow' TRITONBACKEND API version: 1.6\r\nI0719 03:50:13.702520 1 tensorflow.cc:2210] backend configuration:\r\n{}\r\nI0719 03:50:13.727620 1 onnxruntime.cc:1999] TRITONBACKEND_Initialize: onnxruntime\r\nI0719 03:50:13.727647 1 onnxruntime.cc:2009] Triton TRITONBACKEND API version: 1.6\r\nI0719 03:50:13.727653 1 onnxruntime.cc:2015] 'onnxruntime' TRITONBACKEND API version: 1.6\r\nI0719 03:50:13.750021 1 openvino.cc:1193] TRITONBACKEND_Initialize: openvino\r\nI0719 03:50:13.750070 1 openvino.cc:1203] Triton TRITONBACKEND API version: 1.6\r\nI0719 03:50:13.750073 1 openvino.cc:1209] 'openvino' TRITONBACKEND API version: 1.6\r\nI0719 03:50:14.469792 1 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fd3b8000000' with size 1073741824\r\nI0719 03:50:14.485167 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 1073741824\r\nI0719 03:50:14.485179 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\r\nI0719 03:50:14.485182 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\r\nI0719 03:50:14.485184 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\r\nI0719 03:50:14.485187 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 4 with size 67108864\r\nI0719 03:50:15.437258 1 model_repository_manager.cc:1022] loading: model-b-xx:2\r\nI0719 03:50:15.537698 1 model_repository_manager.cc:1022] loading: model-c-xx:1\r\nI0719 03:50:15.573242 1 tensorrt.cc:4925] TRITONBACKEND_Initialize: tensorrt\r\nI0719 03:50:15.573271 1 tensorrt.cc:4935] Triton TRITONBACKEND API version: 1.6\r\nI0719 03:50:15.573274 1 tensorrt.cc:4941] 'tensorrt' TRITONBACKEND API version: 1.6\r\nI0719 03:50:15.573365 1 tensorrt.cc:4984] backend configuration:\r\n{}\r\nI0719 03:50:15.581101 1 tensorrt.cc:5036] TRITONBACKEND_ModelInitialize: model-b-xx (version 2)\r\nI0719 03:50:15.589062 1 tensorrt.cc:5085] TRITONBACKEND_ModelInstanceInitialize: model-a-xx_0_0 (GPU device 0)\r\nI0719 03:50:16.016144 1 logging.cc:49] [MemUsageChange] Init CUDA: CPU +320, GPU +0, now: CPU 415, GPU 1514 (MiB)\r\nI0719 03:50:16.100782 1 logging.cc:49] Loaded engine size: 52 MB\r\nI0719 03:50:16.100942 1 logging.cc:49] [MemUsageSnapshot] deserializeCudaEngine begin: CPU 519 MiB, GPU 1514 MiB\r\nI0719 03:50:16.965873 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +495, GPU +212, now: CPU 1025, GPU 1770 (MiB)\r\nI0719 03:50:17.597467 1 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +170, GPU +204, now: CPU 1195, GPU 1974 (MiB)\r\nI0719 03:50:17.599766 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 1195, GPU 1956 (MiB)\r\nI0719 03:50:17.599838 1 logging.cc:49] [MemUsageSnapshot] deserializeCudaEngine end: CPU 1195 MiB, GPU 1956 MiB\r\nI0719 03:50:17.608776 1 logging.cc:49] [MemUsageSnapshot] ExecutionContext creation begin: CPU 1090 MiB, GPU 1956 MiB\r\nI0719 03:50:17.612806 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 1090, GPU 1966 (MiB)\r\nI0719 03:50:17.621031 1 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 1091, GPU 1974 (MiB)\r\nI0719 03:50:17.624827 1 logging.cc:49] [MemUsageSnapshot] ExecutionContext creation end: CPU 1091 MiB, GPU 2124 MiB\r\nI0719 03:50:17.626355 1 tensorrt.cc:1371] Created instance model-a-xx_0_0 on GPU 0 with stream priority 0 and optimization profile default[0];\r\nI0719 03:50:17.649113 1 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: model-c-xx_0 (GPU device 0)\r\nI0719 03:50:23.356970 1 tensorrt.cc:5085] TRITONBACKEND_ModelInstanceInitialize: model-a-xx_0_1 (GPU device 0)\r\nI0719 03:50:23.455711 1 logging.cc:49] Loaded engine size: 52 MB\r\nI0719 03:50:23.455997 1 logging.cc:49] [MemUsageSnapshot] deserializeCudaEngine begin: CPU 2765 MiB, GPU 2591 MiB\r\nI0719 03:50:23.677618 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2774, GPU 2645 (MiB)\r\nI0719 03:50:23.680224 1 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 2775, GPU 2655 (MiB)\r\nI0719 03:50:23.681473 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2775, GPU 2639 (MiB)\r\nI0719 03:50:23.681536 1 logging.cc:49] [MemUsageSnapshot] deserializeCudaEngine end: CPU 2775 MiB, GPU 2639 MiB\r\nI0719 03:50:23.691423 1 logging.cc:49] [MemUsageSnapshot] ExecutionContext creation begin: CPU 2670 MiB, GPU 2639 MiB\r\nI0719 03:50:23.695073 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2670, GPU 2647 (MiB)\r\nI0719 03:50:23.697294 1 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2670, GPU 2655 (MiB)\r\nI0719 03:50:23.699358 1 logging.cc:49] [MemUsageSnapshot] ExecutionContext creation end: CPU 2671 MiB, GPU 2805 MiB\r\nI0719 03:50:23.700253 1 tensorrt.cc:1371] Created instance model-a-xx_0_1 on GPU 0 with stream priority 0 and optimization profile default[0];\r\nI0719 03:50:23.700388 1 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: model-c-xx_0 (GPU device 1)\r\nI0719 03:50:29.382600 1 tensorrt.cc:5085] TRITONBACKEND_ModelInstanceInitialize: model-b-xx_0_2 (GPU device 0)\r\nI0719 03:50:29.482204 1 logging.cc:49] Loaded engine size: 52 MB\r\nI0719 03:50:29.482825 1 logging.cc:49] [MemUsageSnapshot] deserializeCudaEngine begin: CPU 3302 MiB, GPU 2829 MiB\r\nI0719 03:50:29.709631 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 3311, GPU 2881 (MiB)\r\nI0719 03:50:29.712615 1 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 3312, GPU 2891 (MiB)\r\nI0719 03:50:29.713924 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 3311, GPU 2875 (MiB)\r\nI0719 03:50:29.713994 1 logging.cc:49] [MemUsageSnapshot] deserializeCudaEngine end: CPU 3311 MiB, GPU 2875 MiB\r\nI0719 03:50:29.724880 1 logging.cc:49] [MemUsageSnapshot] ExecutionContext creation begin: CPU 3207 MiB, GPU 2875 MiB\r\nI0719 03:50:29.728500 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 3207, GPU 2883 (MiB)\r\nI0719 03:50:29.730766 1 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 3207, GPU 2891 (MiB)\r\nI0719 03:50:29.732809 1 logging.cc:49] [MemUsageSnapshot] ExecutionContext creation end: CPU 3208 MiB, GPU 3041 MiB\r\nI0719 03:50:29.733690 1 tensorrt.cc:1371] Created instance model-a-xx_0_2 on GPU 0 with stream priority 0 and optimization profile default[0];\r\nI0719 03:50:29.733756 1 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: model-c-xx_0 (GPU device 2)\r\nI0719 03:50:35.628325 1 tensorrt.cc:5085] TRITONBACKEND_ModelInstanceInitialize: model-a-xx_0_3 (GPU device 0)\r\nI0719 03:50:35.743472 1 logging.cc:49] Loaded engine size: 52 MB\r\nI0719 03:50:35.743877 1 logging.cc:49] [MemUsageSnapshot] deserializeCudaEngine begin: CPU 3839 MiB, GPU 3065 MiB\r\nI0719 03:50:35.974599 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 3848, GPU 3117 (MiB)\r\nI0719 03:50:35.977574 1 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 3849, GPU 3127 (MiB)\r\nI0719 03:50:35.978851 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 3848, GPU 3111 (MiB)\r\nI0719 03:50:35.978922 1 logging.cc:49] [MemUsageSnapshot] deserializeCudaEngine end: CPU 3848 MiB, GPU 3111 MiB\r\nI0719 03:50:35.990891 1 logging.cc:49] [MemUsageSnapshot] ExecutionContext creation begin: CPU 3744 MiB, GPU 3111 MiB\r\nI0719 03:50:36.017107 1 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 3744, GPU 3119 (MiB)\r\nI0719 03:50:36.023439 1 logging.cc:49] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 3744, GPU 3127 (MiB)\r\nI0719 03:50:36.050573 1 logging.cc:49] [MemUsageSnapshot] ExecutionContext creation end: CPU 3745 MiB, GPU 3277 MiB\r\nI0719 03:50:36.073977 1 tensorrt.cc:1371] Created instance model-a-xx_0_3 on GPU 0 with stream priority 0 and optimization profile default[0];\r\nI0719 03:50:36.074107 1 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: model-a-xx_0 (GPU device 3)\r\nI0719 03:50:36.101280 1 model_repository_manager.cc:1183] successfully loaded 'model-b-xx' version 2\r\nI0719 03:50:41.760865 1 python.cc:1875] TRITONBACKEND_ModelInstanceInitialize: model-c-xx_0 (GPU device 4)\r\nI0719 03:50:47.252304 1 model_repository_manager.cc:1183] successfully loaded 'model-c-xx' version 1\r\nI0719 03:50:47.252609 1 model_repository_manager.cc:1022] loading: model-a-xx:1\r\nI0719 03:50:47.353226 1 model_repository_manager.cc:1183] successfully loaded 'model-a-xx' version 1\r\nI0719 03:50:47.353330 1 server.cc:522]\r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0719 03:50:47.353406 1 server.cc:549]\r\n+-------------+-----------------------------------------------------------------+--------+\r\n| Backend     | Path                                                            | Config |\r\n+-------------+-----------------------------------------------------------------+--------+\r\n| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so         | {}     |\r\n| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {}     |\r\n| openvino    | /opt/tritonserver/backends/openvino/libtriton_openvino.so       | {}     |\r\n| tensorrt    | /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so       | {}     |\r\n| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so | {}     |\r\n| python      | /opt/tritonserver/backends/python/libtriton_python.so           | {}     |\r\n+-------------+-----------------------------------------------------------------+--------+\r\n\r\nI0719 03:50:47.353438 1 server.cc:592]\r\n+----------------+---------+--------+\r\n| Model          | Version | Status |\r\n+----------------+---------+--------+\r\n| model-a-xx    | 1       | READY  |\r\n| model-b-xx | 2       | READY  |\r\n| model-c-xx     | 1       | READY  |\r\n+----------------+---------+--------+\r\n\r\nI0719 03:50:47.353581 1 tritonserver.cc:1920]\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                                                                  |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                                                                 |\r\n| server_version                   | 2.15.0                                                                                                                                                                                 |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\r\n| model_repository_path[0]         | /model-repository                                                                                                                                                                      |\r\n| model_control_mode               | MODE_EXPLICIT                                                                                                                                                                          |\r\n| startup_models_0                 | model-a-xx                                                                                                                                                                            |\r\n| strict_model_config              | 1                                                                                                                                                                                      |\r\n| rate_limit                       | OFF                                                                                                                                                                                    |\r\n| pinned_memory_pool_byte_size     | 1073741824                                                                                                                                                                             |\r\n| cuda_memory_pool_byte_size{0}    | 1073741824                                                                                                                                                                             |\r\n| cuda_memory_pool_byte_size{1}    | 67108864                                                                                                                                                                               |\r\n| cuda_memory_pool_byte_size{2}    | 67108864                                                                                                                                                                               |\r\n| cuda_memory_pool_byte_size{3}    | 67108864                                                                                                                                                                               |\r\n| cuda_memory_pool_byte_size{4}    | 67108864                                                                                                                                                                               |\r\n| response_cache_byte_size         | 0                                                                                                                                                                                      |\r\n| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\r\n| strict_readiness                 | 1                                                                                                                                                                                      |\r\n| exit_timeout                     | 30                                                                                                                                                                                     |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0719 03:50:47.355174 1 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0719 03:50:47.355530 1 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI0719 03:50:47.397234 1 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection timed out)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n[ERROR] evhtp.c:3119     ODDITY, resuming when not paused?!? :: (errno: Connection reset by peer)\r\n\r\n```", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4929/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4925", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4925/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4925/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4925/events", "html_url": "https://github.com/triton-inference-server/server/issues/4925", "id": 1386195866, "node_id": "I_kwDOCQnI4s5Sn6ua", "number": 4925, "title": "Occasional segfault in dynamic batch scheduler using ONNX runtime", "user": {"login": "OvervCW", "id": 60606100, "node_id": "MDQ6VXNlcjYwNjA2MTAw", "avatar_url": "https://avatars.githubusercontent.com/u/60606100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OvervCW", "html_url": "https://github.com/OvervCW", "followers_url": "https://api.github.com/users/OvervCW/followers", "following_url": "https://api.github.com/users/OvervCW/following{/other_user}", "gists_url": "https://api.github.com/users/OvervCW/gists{/gist_id}", "starred_url": "https://api.github.com/users/OvervCW/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OvervCW/subscriptions", "organizations_url": "https://api.github.com/users/OvervCW/orgs", "repos_url": "https://api.github.com/users/OvervCW/repos", "events_url": "https://api.github.com/users/OvervCW/events{/privacy}", "received_events_url": "https://api.github.com/users/OvervCW/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-09-26T14:35:32Z", "updated_at": "2022-10-07T15:51:53Z", "closed_at": "2022-10-07T15:51:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWe are repeatedly loading ONNX models and performing inference on them. Sometimes the first inference request on a newly loaded model will crash Triton with a segmentation fault.\r\n\r\nIt is reproducible in both release and debug builds and results in the following stack trace:\r\n\r\n```\r\nThread 23 \"tritonserver\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fff9affd000 (LWP 2125)]\r\nstd::__uniq_ptr_impl<std::mutex, std::default_delete<std::mutex> >::_M_ptr (this=0x70) at /usr/include/c++/9/bits/unique_ptr.h:154\r\n154\t      pointer    _M_ptr() const { return std::get<0>(_M_t); }\r\n(gdb) bt\r\n#0  std::__uniq_ptr_impl<std::mutex, std::default_delete<std::mutex> >::_M_ptr (this=0x70) at /usr/include/c++/9/bits/unique_ptr.h:154\r\n#1  0x00007ffff739472e in std::unique_ptr<std::mutex, std::default_delete<std::mutex> >::get (this=0x70) at /usr/include/c++/9/bits/unique_ptr.h:361\r\n#2  0x00007ffff7393eec in triton::core::Payload::GetExecMutex (this=0x0) at /tmp/tritonbuild/tritonserver/build/_deps/repo-core-src/src/payload.h:57\r\n#3  0x00007ffff73901aa in triton::core::DynamicBatchScheduler::Enqueue (this=0x7fff6c07f080, request=std::unique_ptr<triton::core::InferenceRequest> = {...}) at /tmp/tritonbuild/tritonserver/build/_deps/repo-core-src/src/dynamic_batch_scheduler.cc:252\r\n#4  0x00007ffff73b74de in triton::core::Model::Enqueue (this=0x7fff6c0a24e0, request=std::unique_ptr<triton::core::InferenceRequest> = {...}) at /tmp/tritonbuild/tritonserver/build/_deps/repo-core-src/src/model.h:97\r\n#5  0x00007ffff73ae11e in triton::core::InferenceRequest::Run (request=std::unique_ptr<triton::core::InferenceRequest> = {...}) at /tmp/tritonbuild/tritonserver/build/_deps/repo-core-src/src/infer_request.cc:245\r\n#6  0x00007ffff7502cce in triton::core::InferenceServer::InferAsync (this=0x5555565bdec0, request=std::unique_ptr<triton::core::InferenceRequest> = {...}) at /tmp/tritonbuild/tritonserver/build/_deps/repo-core-src/src/server.cc:507\r\n#7  0x00007ffff751d0e7 in TRITONSERVER_ServerInferAsync (server=0x5555565bdec0, inference_request=0x7fff8c41cd30, trace=0x0) at /tmp/tritonbuild/tritonserver/build/_deps/repo-core-src/src/tritonserver.cc:2853\r\n#8  0x000055555564d140 in triton::server::(anonymous namespace)::ModelInferHandler::Process (this=0x555556b35fc0, state=0x7fff8c036500, rpc_ok=true) at /workspace/src/grpc_server.cc:3706\r\n#9  0x000055555565ce14 in triton::server::(anonymous namespace)::InferHandler<inference::GRPCInferenceService::WithAsyncMethod_ServerLive<inference::GRPCInferenceService::WithAsyncMethod_ServerReady<inference::GRPCInferenceService::WithAsyncMethod_ModelReady<inference::GRPCInferenceService::WithAsyncMethod_ServerMetadata<inference::GRPCInferenceService::WithAsyncMethod_ModelMetadata<inference::GRPCInferenceService::WithAsyncMethod_ModelInfer<inference::GRPCInferenceService::WithAsyncMethod_ModelStreamInfer<inference::GRPCInferenceService::WithAsyncMethod_ModelConfig<inference::GRPCInferenceService::WithAsyncMethod_ModelStatistics<inference::GRPCInferenceService::WithAsyncMethod_RepositoryIndex<inference::GRPCInferenceService::WithAsyncMethod_RepositoryModelLoad<inference::GRPCInferenceService::WithAsyncMethod_RepositoryModelUnload<inference::GRPCInferenceService::WithAsyncMethod_SystemSharedMemoryStatus<inference::GRPCInferenceService::WithAsyncMethod_SystemSharedMemoryRegister<inference::GRPCInferenceService::WithAsyncMethod_SystemSharedMemoryUnregister<inference::GRPCInferenceService::WithAsyncMethod_CudaSharedMemoryStatus<inference::GRPCInferenceService::WithAsyncMethod_CudaSharedMemoryRegister<inference::GRPCInferenceService::WithAsyncMethod_CudaSharedMemoryUnregister<inference::GRPCInferenceService::WithAsyncMethod_TraceSetting<inference::GRPCInferenceService::Service> > > > > > > > > > > > > > > > > > >, grpc::ServerAsyncResponseWriter<inference::ModelInferResponse>, inference::ModelInferRequest, inference::ModelInferResponse>::<lambda()>::operator()(void) const (this=0x555556b35fc0) at /workspace/src/grpc_server.cc:2503\r\n...\r\n```\r\n\r\n**Triton Information**\r\nv2.25.0\r\n\r\n**Are you using the Triton container or did you build it yourself?**\r\n\r\nThe problem is reproducible with both `nvcr.io/nvidia/tritonserver:22.08-py3` and an image we built ourselves from the v2.25.0 branch using the following command:\r\n\r\n```shell\r\npython build.py --build-type=Debug --backend onnxruntime --enable-logging --enable-stats --enable-metrics --enable-gpu-metrics --enable-tracing --enable-nvtx --enable-gpu --endpoint grpc\r\n```\r\n\r\n**To Reproduce**\r\nRepeatedly load/unload an ONNX model and send an inference request.\r\n\r\nBatching is enabled on all of our models using the `--backend-config=onnxruntime,default-max-batch-size=128` commandline flag (even the ones that technically aren't really meant to be used for batching). With that said, this happens even when sending only a single inference request at a time (with no batch), so I don't think the batch scheduler is actually creating any batches in our scenario.\r\n\r\nI think dynamic batching is enabled because autocomplete seems to do it by default using the aforementioned commandline flag.\r\n\r\n**Expected behavior**\r\nNo crash.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4925/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4925/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4924", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4924/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4924/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4924/events", "html_url": "https://github.com/triton-inference-server/server/issues/4924", "id": 1385504328, "node_id": "I_kwDOCQnI4s5SlR5I", "number": 4924, "title": "[Tensorflow Backend] | [Segfault, intermittent] | Unloading model on GPU results in a segfault i.e. tritonserver crashes", "user": {"login": "nskool", "id": 10671803, "node_id": "MDQ6VXNlcjEwNjcxODAz", "avatar_url": "https://avatars.githubusercontent.com/u/10671803?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nskool", "html_url": "https://github.com/nskool", "followers_url": "https://api.github.com/users/nskool/followers", "following_url": "https://api.github.com/users/nskool/following{/other_user}", "gists_url": "https://api.github.com/users/nskool/gists{/gist_id}", "starred_url": "https://api.github.com/users/nskool/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nskool/subscriptions", "organizations_url": "https://api.github.com/users/nskool/orgs", "repos_url": "https://api.github.com/users/nskool/repos", "events_url": "https://api.github.com/users/nskool/events{/privacy}", "received_events_url": "https://api.github.com/users/nskool/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-09-26T06:33:08Z", "updated_at": "2022-10-28T23:52:04Z", "closed_at": "2022-10-28T23:52:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Description**\r\n1. A TF model  on GPU, with platform `tensorflow_savedmodel` and config.pbtxt as below can be loaded successfully. However, when 'unloaded', the model sometimes segfaults. \r\n2. In successful cases, the control reaches dynamic_batch_scheduler, line 424:\r\n```\r\nI0926 05:03:17.456410 1 dynamic_batch_scheduler.cc:424] Stopping dynamic-batcher thread for llSh...\r\nI0926 05:03:17.456567 1 model_lifecycle.cc:578] successfully unloaded '7891' version 1\r\n```\r\n3. In segfault case, the control terminates before that\r\n```\r\nI0926 05:49:43.401011 1 backend_model_instance.cc:765] Starting backend thread for model_0 at nice 0 on device 3...\r\nI0926 05:49:43.401346 1 model_lifecycle.cc:693] successfully loaded 'model' version 1\r\nI0926 05:49:43.401352 1 dynamic_batch_scheduler.cc:281] Starting dynamic-batcher thread for model at nice 0...\r\nI0926 05:49:45.850504 1 http_server.cc:3369] HTTP request: 2 /v2/repository/models/model/unload\r\nI0926 05:49:45.850761 1 backend_model_instance.cc:788] Stopping backend thread for model_0...\r\nI0926 05:49:45.850836 1 tensorflow.cc:2729] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0926 05:49:45.850943 1 backend_model_instance.cc:788] Stopping backend thread for model_0...\r\nI0926 05:49:45.851018 1 tensorflow.cc:2729] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0926 05:49:45.851094 1 backend_model_instance.cc:788] Stopping backend thread for model_0...\r\nI0926 05:49:45.851168 1 tensorflow.cc:2729] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0926 05:49:45.851220 1 backend_model_instance.cc:788] Stopping backend thread for model_0...\r\nI0926 05:49:45.851287 1 tensorflow.cc:2729] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0926 05:49:45.851331 1 tensorflow.cc:2668] TRITONBACKEND_ModelFinalize: delete model state\r\nSignal (11) received.\r\n 0# 0x00005599374ADD79 in tritonserver\r\n 1# 0x00007FCE8F740090 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# 0x00007FCE8F8889D3 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 3# std::basic_streambuf<char, std::char_traits<char> >::xsputn(char const*, long) in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 4# std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long) in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 5# 0x00007FCE901F6578 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n 6# 0x00007FCE8FB31DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 7# 0x00007FCE905DE609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n 8# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n```\r\n\r\n```\r\nname: \"model\"\r\nplatform: \"tensorflow_savedmodel\"\r\nmax_batch_size: 64\r\ninput {\r\n    name: \"input_1\"\r\n    data_type: TYPE_FP32\r\n    format: FORMAT_NHWC\r\n    dims: [224, 224, 3]\r\n}\r\noutput {\r\n    name: \"probs\"\r\n    data_type: TYPE_FP32\r\n    dims: 1000\r\n}\r\ndefault_model_filename: \"model\"\r\ninstance_group [\r\n  {\r\n    count: 1\r\n    kind: KIND_GPU\r\n  }\r\n]\r\n```\r\n\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 22.08 (can be reproduced with 22.07 as well)\r\n\r\nAre you using the Triton container or did you build it yourself? Used the triton container, as well as did a custom build with just tensorflow backend\r\n\r\n**To Reproduce**\r\n1. Download the model [link](https://torchserve.s3.amazonaws.com/tar_gz_files/tritonserver/model.tar.gz) and untar it to any location e.g. `/opt/ml/models/7891`, directory structure is shared below:\r\n```\r\n(venv) ubuntu@ip-172-31-44-91:/opt/ml/models/7891$ tree\r\n.\r\n\u2514\u2500\u2500 model\r\n    \u251c\u2500\u2500 1\r\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 model\r\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 assets\r\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 saved_model.pb\r\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 variables\r\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 variables.data-00000-of-00002\r\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 variables.data-00001-of-00002\r\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 variables.index\r\n    \u2514\u2500\u2500 config.pbtxt\r\n\r\n5 directories, 5 files\r\n```\r\n2. Run triton server as (change the `tritonserver:latest` image to 22.07 or 22.08 image as necessary):\r\n```\r\n docker run --gpus all -v /opt/ml/models:/opt/ml/models --rm -p8000:8000 -p8001:8001 -p8002:8002 -p8080:8080 tritonserver:latest tritonserver --allow-sagemaker=false --allow-grpc=false --allow-http=true --allow-metrics=false --model-control-mode=explicit --model-repository=/opt/ml/models/7891 --sagemaker-port=8080 --log-verbose true \r\n```\r\n3. Load the model using REST API: `curl -w \"%{http_code}\" -X POST http://0.0.0.0:8000/v2/repository/models/model/load`. Note that model name is `model`. This returns 200\r\n4. Unload the model as: `curl -w \"%{http_code}\" -X POST http://0.0.0.0:8000/v2/repository/models/model/unload`. This *may* succeed, however, if you load-unload a few times, you may hit the segfault issue. Note that even during the segfault, the http response code is 200. However, the tritonserver crashes.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\n1. Unload should be successful at all times. \r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4924/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4915", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4915/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4915/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4915/events", "html_url": "https://github.com/triton-inference-server/server/issues/4915", "id": 1383345317, "node_id": "I_kwDOCQnI4s5SdCyl", "number": 4915, "title": "Triton failed to open the cudaIpcHandle upon prediction request when launched in container under WSL2", "user": {"login": "BorisPolonsky", "id": 12964401, "node_id": "MDQ6VXNlcjEyOTY0NDAx", "avatar_url": "https://avatars.githubusercontent.com/u/12964401?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BorisPolonsky", "html_url": "https://github.com/BorisPolonsky", "followers_url": "https://api.github.com/users/BorisPolonsky/followers", "following_url": "https://api.github.com/users/BorisPolonsky/following{/other_user}", "gists_url": "https://api.github.com/users/BorisPolonsky/gists{/gist_id}", "starred_url": "https://api.github.com/users/BorisPolonsky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BorisPolonsky/subscriptions", "organizations_url": "https://api.github.com/users/BorisPolonsky/orgs", "repos_url": "https://api.github.com/users/BorisPolonsky/repos", "events_url": "https://api.github.com/users/BorisPolonsky/events{/privacy}", "received_events_url": "https://api.github.com/users/BorisPolonsky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-09-23T06:20:54Z", "updated_at": "2022-10-19T02:45:41Z", "closed_at": "2022-10-19T02:45:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nTriton failed to open cudaIpcHandle while trying to serve model exported from official NVIDIA/Merlin [notebook showcase](https://github.com/NVIDIA-Merlin/systems/blob/v0.5.0/examples/Serving-An-XGboost-Model-With-Merlin-Systems.ipynb), where user will train and export an ensembled model, serve it with triton, make prediction via gRPC calls.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nTriton 22.03.0, the one that ships with `nvcr.io/nvidia/merlin/merlin-tensorflow:22.07`\r\nAre you using the Triton container or did you build it yourself?\r\nContainer. Launched under WSL2\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n- Follow [this comment](https://github.com/triton-inference-server/server/issues/4915#issuecomment-1258910783) in this issue, which does the following things\r\n  - Export model refering to this [issue](https://github.com/NVIDIA-Merlin/systems/issues/205), or alternatively, download [here](https://drive.google.com/drive/folders/15Slql2i8jncYB6H1Gxkr1q4jPPvxG9Xk?usp=sharing).\r\n  - Serve it in triton `docker run -it --name merlin -v /home:/home -e LANG=C.UTF-8 -v /home:/home --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -p 6006:6006 nvcr.io/nvidia/merlin/merlin-tensorflow:22.07 bash`\r\n  - Send predcition request with gRPC calls. Use the code block below \"Let's now package the information up as inputs and send it to Triton for inference.\" in the [notebook](https://github.com/NVIDIA-Merlin/systems/blob/v0.5.0/examples/Serving-An-XGboost-Model-With-Merlin-Systems.ipynb) mentioned above.\r\n\r\nAnd triton failed to process the request for `1_predictforest`\r\n```\r\n0923 05:19:56.350606 2129 pb_stub.cc:776] Failed to process the request(s) for model '1_predictforest', message: TypeError: __init__(): incompatible constructor arguments. The following argument types are supported:\r\n    1. c_python_backend_utils.InferenceResponse(output_tensors: List[c_python_backend_utils.Tensor], error: c_python_backend_utils.TritonError = None)\r\n\r\nInvoked with: kwargs: tensors=[], error=\"<class 'c_python_backend_utils.TritonModelException'>, Failed to open the cudaIpcHandle. error: unknown error, [<FrameSummary file /home/polonsky/Documents/merlin-systems-0.5.0/examples/ensemble/1_predictforest/1/model.py, line 105 in execute>, <FrameSummary file /usr/local/lib/python3.8/dist-packages/merlin/systems/dag/op_runner.py, line 38 in execute>, <FrameSummary file /usr/local/lib/python3.8/dist-packages/merlin/systems/dag/ops/fil.py, line 160 in transform>]\"\r\n\r\nAt:\r\n  /home/polonsky/Documents/merlin-systems-0.5.0/examples/ensemble/1_predictforest/1/model.py(122): execute\r\n```\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nAn ensenmbled XGBoost model\r\n```\r\nname: \"ensemble_model\"\r\nplatform: \"ensemble\"\r\ninput {\r\n  name: \"movieId\"\r\n  data_type: TYPE_INT32\r\n  dims: -1\r\n  dims: -1\r\n}\r\ninput {\r\n  name: \"userId\"\r\n  data_type: TYPE_INT32\r\n  dims: -1\r\n  dims: -1\r\n}\r\ninput {\r\n  name: \"genres\"\r\n  data_type: TYPE_INT32\r\n  dims: -1\r\n  dims: -1\r\n}\r\ninput {\r\n  name: \"TE_movieId_rating\"\r\n  data_type: TYPE_FP64\r\n  dims: -1\r\n  dims: -1\r\n}\r\ninput {\r\n  name: \"userId_count\"\r\n  data_type: TYPE_FP32\r\n  dims: -1\r\n  dims: -1\r\n}\r\ninput {\r\n  name: \"gender\"\r\n  data_type: TYPE_INT32\r\n  dims: -1\r\n  dims: -1\r\n}\r\ninput {\r\n  name: \"zip_code\"\r\n  data_type: TYPE_INT32\r\n  dims: -1\r\n  dims: -1\r\n}\r\ninput {\r\n  name: \"age\"\r\n  data_type: TYPE_INT32\r\n  dims: -1\r\n  dims: -1\r\n}\r\noutput {\r\n  name: \"output__0\"\r\n  data_type: TYPE_FP32\r\n  dims: -1\r\n  dims: -1\r\n}\r\nensemble_scheduling {\r\n  step {\r\n    model_name: \"0_transformworkflow\"\r\n    model_version: -1\r\n    input_map {\r\n      key: \"TE_movieId_rating\"\r\n      value: \"TE_movieId_rating\"\r\n    }\r\n    input_map {\r\n      key: \"age\"\r\n      value: \"age\"\r\n    }\r\n    input_map {\r\n      key: \"gender\"\r\n      value: \"gender\"\r\n    }\r\n    input_map {\r\n      key: \"genres\"\r\n      value: \"genres\"\r\n    }\r\n    input_map {\r\n      key: \"movieId\"\r\n      value: \"movieId\"\r\n    }\r\n    input_map {\r\n      key: \"userId\"\r\n      value: \"userId\"\r\n    }\r\n    input_map {\r\n      key: \"userId_count\"\r\n      value: \"userId_count\"\r\n    }\r\n    input_map {\r\n      key: \"zip_code\"\r\n      value: \"zip_code\"\r\n    }\r\n    output_map {\r\n      key: \"TE_movieId_rating\"\r\n      value: \"TE_movieId_rating_0\"\r\n    }\r\n    output_map {\r\n      key: \"age\"\r\n      value: \"age_0\"\r\n    }\r\n    output_map {\r\n      key: \"gender\"\r\n      value: \"gender_0\"\r\n    }\r\n    output_map {\r\n      key: \"genres\"\r\n      value: \"genres_0\"\r\n    }\r\n    output_map {\r\n      key: \"movieId\"\r\n      value: \"movieId_0\"\r\n    }\r\n    output_map {\r\n      key: \"userId\"\r\n      value: \"userId_0\"\r\n    }\r\n    output_map {\r\n      key: \"userId_count\"\r\n      value: \"userId_count_0\"\r\n    }\r\n    output_map {\r\n      key: \"zip_code\"\r\n      value: \"zip_code_0\"\r\n    }\r\n  }\r\n  step {\r\n    model_name: \"1_predictforest\"\r\n    model_version: -1\r\n    input_map {\r\n      key: \"TE_movieId_rating\"\r\n      value: \"TE_movieId_rating_0\"\r\n    }\r\n    input_map {\r\n      key: \"age\"\r\n      value: \"age_0\"\r\n    }\r\n    input_map {\r\n      key: \"gender\"\r\n      value: \"gender_0\"\r\n    }\r\n    input_map {\r\n      key: \"genres\"\r\n      value: \"genres_0\"\r\n    }\r\n    input_map {\r\n      key: \"movieId\"\r\n      value: \"movieId_0\"\r\n    }\r\n    input_map {\r\n      key: \"userId\"\r\n      value: \"userId_0\"\r\n    }\r\n    input_map {\r\n      key: \"userId_count\"\r\n      value: \"userId_count_0\"\r\n    }\r\n    input_map {\r\n      key: \"zip_code\"\r\n      value: \"zip_code_0\"\r\n    }\r\n    output_map {\r\n      key: \"output__0\"\r\n      value: \"output__0\"\r\n    }\r\n  }\r\n}\r\n```\r\nAnd here's what `1_predictforest/1/model.py` look like:\r\n```\r\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\r\n#\r\n# Redistribution and use in source and binary forms, with or without\r\n# modification, are permitted provided that the following conditions\r\n# are met:\r\n#  * Redistributions of source code must retain the above copyright\r\n#    notice, this list of conditions and the following disclaimer.\r\n#  * Redistributions in binary form must reproduce the above copyright\r\n#    notice, this list of conditions and the following disclaimer in the\r\n#    documentation and/or other materials provided with the distribution.\r\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\r\n#    contributors may be used to endorse or promote products derived\r\n#    from this software without specific prior written permission.\r\n#\r\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\r\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\r\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\r\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\r\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\r\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\r\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\r\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\r\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\r\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\r\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\n\r\nimport json\r\nimport pathlib\r\nimport sys\r\nimport traceback\r\n\r\nimport triton_python_backend_utils as pb_utils\r\n\r\nfrom merlin.systems.dag.op_runner import OperatorRunner\r\nfrom merlin.systems.dag.ops.operator import InferenceDataFrame\r\n\r\n\r\nclass TritonPythonModel:\r\n    \"\"\"Model for Triton Python Backend.\r\n\r\n    Every Python model must have \"TritonPythonModel\" as the class name\r\n    \"\"\"\r\n\r\n    def initialize(self, args):\r\n        \"\"\"Called only once when the model is being loaded. Allowing\r\n        the model to initialize any state associated with this model.\r\n\r\n        Parameters\r\n        ----------\r\n        args : dict\r\n          Both keys and values are strings. The dictionary keys and values are:\r\n          * model_config: A JSON string containing the model configuration\r\n          * model_instance_kind: A string containing model instance kind\r\n          * model_instance_device_id: A string containing model instance device ID\r\n          * model_repository: Model repository path\r\n          * model_version: Model version\r\n          * model_name: Model name\r\n        \"\"\"\r\n        self.model_config = json.loads(args[\"model_config\"])\r\n\r\n        self.runner = OperatorRunner(\r\n            self.model_config,\r\n            # model_repository=_parse_model_repository(args[\"model_repository\"]),\r\n            # model_name=args[\"model_name\"],\r\n            # model_version=args[\"model_version\"],\r\n        )\r\n\r\n    def execute(self, requests):\r\n        \"\"\"Receives a list of pb_utils.InferenceRequest as the only argument. This\r\n        function is called when an inference is requested for this model. Depending on the\r\n        batching configuration (e.g. Dynamic Batching) used, `requests` may contain\r\n        multiple requests. Every Python model, must create one pb_utils.InferenceResponse\r\n        for every pb_utils.InferenceRequest in `requests`. If there is an error, you can\r\n        set the error argument when creating a pb_utils.InferenceResponse.\r\n\r\n        Parameters\r\n        ----------\r\n        requests : list\r\n          A list of pb_utils.InferenceRequest\r\n\r\n        Returns\r\n        -------\r\n        list\r\n          A list of pb_utils.InferenceResponse. The length of this list must\r\n          be the same as `requests`\r\n        \"\"\"\r\n        params = self.model_config[\"parameters\"]\r\n        op_names = json.loads(params[\"operator_names\"][\"string_value\"])\r\n        first_operator_name = op_names[0]\r\n        operator_params = json.loads(params[first_operator_name][\"string_value\"])\r\n        input_column_names = list(json.loads(operator_params[\"input_dict\"]).keys())\r\n\r\n        responses = []\r\n\r\n        for request in requests:\r\n            try:\r\n                # transform the triton tensors to a dict of name:numpy tensor\r\n                input_tensors = {\r\n                    name: pb_utils.get_input_tensor_by_name(request, name).as_numpy()\r\n                    for name in input_column_names\r\n                }\r\n\r\n                inf_df = InferenceDataFrame(input_tensors)\r\n\r\n                raw_tensor_tuples = self.runner.execute(inf_df)\r\n\r\n                output_tensors = []\r\n                for name, data in raw_tensor_tuples:\r\n                    if isinstance(data, pb_utils.Tensor):\r\n                        output_tensors.append(data)\r\n                        continue\r\n                    data = data.get() if hasattr(data, \"get\") else data\r\n                    tensor = pb_utils.Tensor(name, data)\r\n                    output_tensors.append(tensor)\r\n\r\n                responses.append(pb_utils.InferenceResponse(output_tensors))\r\n\r\n            except Exception:  # pylint: disable=broad-except\r\n                exc_type, exc_value, exc_traceback = sys.exc_info()\r\n                tb_string = repr(traceback.extract_tb(exc_traceback))\r\n                responses.append(\r\n                    pb_utils.InferenceResponse(\r\n                        tensors=[], error=f\"{exc_type}, {exc_value}, {tb_string}\"\r\n                    )\r\n                )\r\n\r\n        return responses\r\n\r\n\r\ndef _parse_model_repository(model_repository: str) -> str:\r\n    \"\"\"\r\n    Extract the model repository path from the model_repository value\r\n    passed to the TritonPythonModel initialize method.\r\n    \"\"\"\r\n    # Handle bug in Tritonserver 22.06\r\n    # model_repository argument became path to model.py\r\n    # instead of path to model directory within the model repository\r\n    if model_repository.endswith(\".py\"):\r\n        return str(pathlib.Path(model_repository).parent.parent.parent)\r\n    else:\r\n        return str(pathlib.Path(model_repository).parent)\r\n\r\n```\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\nTriton inference server should return the desired output properly, unless it's something related to WSL2 that leads to the problem.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4915/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4886", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4886/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4886/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4886/events", "html_url": "https://github.com/triton-inference-server/server/issues/4886", "id": 1374119576, "node_id": "I_kwDOCQnI4s5R52aY", "number": 4886, "title": "Dynamic batching but require fixed input size", "user": {"login": "Jinghao14", "id": 113586970, "node_id": "U_kgDOBsUzGg", "avatar_url": "https://avatars.githubusercontent.com/u/113586970?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jinghao14", "html_url": "https://github.com/Jinghao14", "followers_url": "https://api.github.com/users/Jinghao14/followers", "following_url": "https://api.github.com/users/Jinghao14/following{/other_user}", "gists_url": "https://api.github.com/users/Jinghao14/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jinghao14/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jinghao14/subscriptions", "organizations_url": "https://api.github.com/users/Jinghao14/orgs", "repos_url": "https://api.github.com/users/Jinghao14/repos", "events_url": "https://api.github.com/users/Jinghao14/events{/privacy}", "received_events_url": "https://api.github.com/users/Jinghao14/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-09-15T08:14:40Z", "updated_at": "2022-11-29T17:03:48Z", "closed_at": "2022-11-29T17:03:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nTriton model was built by the command:\r\ntrtexec --onnx=model.onnx --minShapes=cummuFeatures:64x252,indepFeatures:64x401 --optShapes=cummuFeatures:128x252,indepFeatures:128x401 --maxShapes=cummuFeatures:300x252,indepFeatures:300x401 --useCudaGraph --workspace=512 --saveEngine=model.plan\r\n\r\nTriton server config:\r\nmax_batch_size : 256\r\ndynamic_batching {\r\n  preferred_batch_size:[256]\r\n  max_queue_delay_microseconds: 150000\r\n}\r\n\r\nBut when i tried perf_analyzer with commond: \r\nperf_analyzer -m search_ads_satsfv1_TW --percentile=99 --concurrency-range 2 -b 16\r\n\r\nIt shows that \r\n<img width=\"1244\" alt=\"image\" src=\"https://user-images.githubusercontent.com/113586970/190351342-bbea1158-8930-4e5a-bdec-805712a30105.png\">\r\n \r\nperf_analyzer -m search_ads_satsfv1_TW --percentile=99 --concurrency-range 16 -b 16 is ok.\r\nBut want to know that if the inputs batch size is dynamic, and i had config the server to dynamic batch, why still require batchsize as 256?\r\n\r\n**Triton Information**\r\n  With docker: nvcr.io/nvidia/tritonserver:22.07-py3\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4886/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4886/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4880", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4880/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4880/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4880/events", "html_url": "https://github.com/triton-inference-server/server/issues/4880", "id": 1372263396, "node_id": "I_kwDOCQnI4s5RyxPk", "number": 4880, "title": "too many open files", "user": {"login": "wdongdongde", "id": 44493076, "node_id": "MDQ6VXNlcjQ0NDkzMDc2", "avatar_url": "https://avatars.githubusercontent.com/u/44493076?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wdongdongde", "html_url": "https://github.com/wdongdongde", "followers_url": "https://api.github.com/users/wdongdongde/followers", "following_url": "https://api.github.com/users/wdongdongde/following{/other_user}", "gists_url": "https://api.github.com/users/wdongdongde/gists{/gist_id}", "starred_url": "https://api.github.com/users/wdongdongde/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wdongdongde/subscriptions", "organizations_url": "https://api.github.com/users/wdongdongde/orgs", "repos_url": "https://api.github.com/users/wdongdongde/repos", "events_url": "https://api.github.com/users/wdongdongde/events{/privacy}", "received_events_url": "https://api.github.com/users/wdongdongde/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-09-14T03:10:56Z", "updated_at": "2022-09-27T14:31:28Z", "closed_at": "2022-09-26T22:19:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nafter running few days, there are so many tcp connection,and finally stop accepting requests.\r\n<img width=\"244\" alt=\"image\" src=\"https://user-images.githubusercontent.com/44493076/190049694-c6a0e7e9-5506-4fdc-a236-e2fcb98f7cd4.png\">\r\n\r\n<img width=\"527\" alt=\"image\" src=\"https://user-images.githubusercontent.com/44493076/190049459-796e0f51-e1e9-494b-a631-85cfc836f64c.png\">\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n\r\n<img width=\"528\" alt=\"image\" src=\"https://user-images.githubusercontent.com/44493076/190049743-68e65e7d-1e53-4222-b314-659d63750d2b.png\">\r\n\r\nthere are three python backend models and one pytorch model\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4880/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4880/timeline", "performed_via_github_app": null, "state_reason": "not_planned"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4865", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4865/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4865/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4865/events", "html_url": "https://github.com/triton-inference-server/server/issues/4865", "id": 1368441222, "node_id": "I_kwDOCQnI4s5RkMGG", "number": 4865, "title": "Python backend 0-size GPU tensors causing \"failed to get cuda pointer device attribute: invalid argument\"", "user": {"login": "darintay", "id": 1653481, "node_id": "MDQ6VXNlcjE2NTM0ODE=", "avatar_url": "https://avatars.githubusercontent.com/u/1653481?v=4", "gravatar_id": "", "url": "https://api.github.com/users/darintay", "html_url": "https://github.com/darintay", "followers_url": "https://api.github.com/users/darintay/followers", "following_url": "https://api.github.com/users/darintay/following{/other_user}", "gists_url": "https://api.github.com/users/darintay/gists{/gist_id}", "starred_url": "https://api.github.com/users/darintay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/darintay/subscriptions", "organizations_url": "https://api.github.com/users/darintay/orgs", "repos_url": "https://api.github.com/users/darintay/repos", "events_url": "https://api.github.com/users/darintay/events{/privacy}", "received_events_url": "https://api.github.com/users/darintay/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-09-10T00:58:40Z", "updated_at": "2022-09-29T00:42:27Z", "closed_at": "2022-09-29T00:42:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nReturning a gpu-backed tensor where one of the dimensions is size 0 causes the inference to fail with the error \"message: failed to get cuda pointer device attribute: invalid argument\".\r\n\r\nThis only occurs if the tensor is on the GPU, and one of its dimensions are 0.\r\n\r\nI've put together a minimal example below.\r\n\r\n**Triton Information**\r\n\r\nWhat version of Triton are you using?\r\n22.08\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nContainer, from NGC.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\n1. Setup model repo\r\n\r\nconfig.pbtxt\r\n```\r\nname: \"test_model\"\r\nbackend: \"python\"\r\n\r\ninput [\r\n  {\r\n    name: \"X\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 4 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"OUTPUT\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1 ]\r\n  }\r\n]\r\n```\r\n\r\nmodel.py\r\n```python\r\nimport torch\r\nimport triton_python_backend_utils as pb_utils\r\nfrom torch.utils.dlpack import to_dlpack, from_dlpack\r\n\r\n\r\nclass TritonPythonModel:\r\n    def initialize(self, args):\r\n        pass\r\n\r\n    def execute(self, requests):\r\n        responses = []\r\n\r\n        for request in requests:\r\n            in_0 = pb_utils.get_input_tensor_by_name(request, \"X\")\r\n\r\n            SHAPE = (0, )\r\n\r\n            # Adding this line avoids a crash.\r\n            # SHAPE = (1, )\r\n\r\n            torch_test = torch.ones(SHAPE, dtype=torch.float32)\r\n            device = torch.device(\"cuda:0\")\r\n            torch_test = torch_test.to(device)\r\n\r\n            # Adding this line avoids a crash.\r\n            # torch_test = torch_test.to(\"cpu\")\r\n\r\n            out_tensor_1 = pb_utils.Tensor.from_dlpack(\"OUTPUT\", to_dlpack(torch_test))\r\n\r\n            inference_response = pb_utils.InferenceResponse(\r\n                output_tensors=[out_tensor_1]\r\n            )\r\n            responses.append(inference_response)\r\n\r\n        return responses\r\n```\r\n\r\n2. Run in the NGC container:\r\n```bash\r\n$ pip install torch\r\n$ tritonserver --model-repository .\r\n$ perf_analyzer -m test_model\r\n...\r\n\r\n0910 00:55:16.004736 357 pb_stub.cc:777] Failed to process the request(s) for model 'test_model', message: failed to get cuda pointer device attribute: invalid argument\r\n```\r\n\r\n\r\n3. Note that uncommenting either of the commented out lines (non-0-dim or cpu placement) causes the model to start working again.\r\n\r\n**Expected behavior**\r\nShould correctly return the tensor output.\r\n\r\nInstead, it errors (unless you place the tensor on CPU, or ensure all dimensions are non-0).\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4865/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4855", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4855/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4855/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4855/events", "html_url": "https://github.com/triton-inference-server/server/issues/4855", "id": 1365893281, "node_id": "I_kwDOCQnI4s5RaeCh", "number": 4855, "title": "strange coredump in libtritonserver.so", "user": {"login": "frankxyy", "id": 6971044, "node_id": "MDQ6VXNlcjY5NzEwNDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/6971044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frankxyy", "html_url": "https://github.com/frankxyy", "followers_url": "https://api.github.com/users/frankxyy/followers", "following_url": "https://api.github.com/users/frankxyy/following{/other_user}", "gists_url": "https://api.github.com/users/frankxyy/gists{/gist_id}", "starred_url": "https://api.github.com/users/frankxyy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frankxyy/subscriptions", "organizations_url": "https://api.github.com/users/frankxyy/orgs", "repos_url": "https://api.github.com/users/frankxyy/repos", "events_url": "https://api.github.com/users/frankxyy/events{/privacy}", "received_events_url": "https://api.github.com/users/frankxyy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2022-09-08T09:30:46Z", "updated_at": "2022-09-30T18:47:14Z", "closed_at": "2022-09-30T18:47:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "I add one line of log in backend_model_instance.cc of triton core project. Like this below:\r\n![image](https://user-images.githubusercontent.com/6971044/189088474-7e8959e9-5d11-4787-9042-45ab3ab0d032.png)\r\n\r\n\r\nThen I build the triton core project and get libtritonserver.so. Then the server with the newly built libtritonserver.so meet coredump error when receiving a request to an ensemble model containing a bls model in the whole pipeline. However, If I request another ensemble model without a bls model\uff0cthe coredump error will not happen. \r\n\r\n\r\nThe coredump file analysis is as below. It seem that it has no relation with the position of code added.\r\n![image](https://user-images.githubusercontent.com/6971044/189087290-21dd557b-3202-4bf3-8215-72abd607fba8.png)\r\n\r\nWhat is the reason?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4855/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4855/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4841", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4841/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4841/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4841/events", "html_url": "https://github.com/triton-inference-server/server/issues/4841", "id": 1359984099, "node_id": "I_kwDOCQnI4s5RD7Xj", "number": 4841, "title": "Under high load, stateful batcher queues small batches", "user": {"login": "jamied157", "id": 16978470, "node_id": "MDQ6VXNlcjE2OTc4NDcw", "avatar_url": "https://avatars.githubusercontent.com/u/16978470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamied157", "html_url": "https://github.com/jamied157", "followers_url": "https://api.github.com/users/jamied157/followers", "following_url": "https://api.github.com/users/jamied157/following{/other_user}", "gists_url": "https://api.github.com/users/jamied157/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamied157/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamied157/subscriptions", "organizations_url": "https://api.github.com/users/jamied157/orgs", "repos_url": "https://api.github.com/users/jamied157/repos", "events_url": "https://api.github.com/users/jamied157/events{/privacy}", "received_events_url": "https://api.github.com/users/jamied157/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-09-02T10:09:04Z", "updated_at": "2022-11-18T10:15:24Z", "closed_at": "2022-10-24T22:41:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI'm using a custom C++ backend to the Triton Server that runs using the stateful batcher with the oldest strategy. When testing this under a high load:\r\n\r\n- num_clients : ~500\r\n- max_batch_size : 64\r\n- max_candidate_sequences: 128\r\n- no queue_delay\r\n\r\nWe initially see quite good behaviour from the batcher (large batches, queued instantly one after the other). Eventually however, the batcher stops queueing requests and then starts queueing very small batches. It does eventually pick up again but performance never really reaches the same as the beginning. \r\n\r\nThis might not help too much but we see this behaviour at the start (with TRITONBACKEND_ModelInstanceExecute running one after the other, this is batch size 64):\r\n![Screenshot 2022-09-02 at 10 33 58](https://user-images.githubusercontent.com/16978470/188110648-42c34045-4a44-4190-a2a6-5123b4bc6df5.png)\r\n\r\nThen as time goes on these stop and when we pick up again, batches are much smaller and have gaps:\r\n![Screenshot 2022-09-02 at 10 35 47](https://user-images.githubusercontent.com/16978470/188110968-059831d9-d03d-4abb-ae54-1e172738dcfa.png)\r\n\r\nIt does eventually go back to batch_size 64 but then quickly drops off once more.\r\n\r\nI'm wary that this behaviour is consistent with clients not spawning quickly enough to saturate the server, however increasing the number of clients to a large degree doesn't seem to help here at all.\r\n\r\nAnother point of interest is that eventually calls to `TRITONBACKEND_RequestRelease` end up being very slow (30ms), could this point at whatever the bottleneck on the server is? (EDIT: interestingly this seems to get worse over time, I can get 1000ms waiting for this call to finish, whereas the quickest requests will take <0.01ms)\r\n\r\nIf you have any tips on trying to understand what the state of the queue on the server is or what is going on in the grpc layer that would be appreciated as I'm that's where my suspicions lie. It could very well be some way we're not using the TRITONBACKEND api correctly but I think I'm following the instructions [here](https://github.com/triton-inference-server/backend/blob/main/README.md#decoupled-responses) correctly\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 22.02\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nWe build triton ourselves however I think we see similar behaviour with the container (but haven't tried this much).\r\n\r\n**To Reproduce**\r\nI can't share the code we are running but depending on where you think the issue is I can try and simplify so I can share (although this might be some work). \r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nI've mentioned some of this above but to be clear:\r\n\r\n- stateful batcher, oldest strategy\r\n- num_clients: ~500\r\n- custom c++ backend\r\n- GRPC bidirectional streaming mode\r\n- decoupled model\r\n\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4841/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4841/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4804", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4804/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4804/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4804/events", "html_url": "https://github.com/triton-inference-server/server/issues/4804", "id": 1348156638, "node_id": "I_kwDOCQnI4s5QWzze", "number": 4804, "title": "Failed to set cuda graph shape when I set max_batch_size==0", "user": {"login": "wangchengdng", "id": 10705707, "node_id": "MDQ6VXNlcjEwNzA1NzA3", "avatar_url": "https://avatars.githubusercontent.com/u/10705707?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangchengdng", "html_url": "https://github.com/wangchengdng", "followers_url": "https://api.github.com/users/wangchengdng/followers", "following_url": "https://api.github.com/users/wangchengdng/following{/other_user}", "gists_url": "https://api.github.com/users/wangchengdng/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangchengdng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangchengdng/subscriptions", "organizations_url": "https://api.github.com/users/wangchengdng/orgs", "repos_url": "https://api.github.com/users/wangchengdng/repos", "events_url": "https://api.github.com/users/wangchengdng/events{/privacy}", "received_events_url": "https://api.github.com/users/wangchengdng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-08-23T15:40:23Z", "updated_at": "2022-09-23T18:27:21Z", "closed_at": "2022-09-23T18:26:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\ncuda graph failed when I set max_batch_size==0\r\n\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n22.04\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nnvcr.io/nvidia/tritonserver:22.04-py3\r\n\r\n**To Reproduce**\r\n***model***\r\nI used pytorch ResNet18 pretrained model \uff0cand converted to onnx model\r\n```\r\nimport torch\r\nfrom torch import nn\r\nimport torchvision\r\nimport argparse\r\nimport torchvision.models as models\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--output_model\", type=str, required=True, help=\"model output path\")\r\n\r\ndef main():\r\n    args = parser.parse_args()\r\n    output_model_path = args.output_model\r\n    model = models.resnet18()\r\n    model = model.to('cuda:0')\r\n    model.eval()\r\n    x = torch.ones(1, 3, 224, 224).to('cuda:0')\r\n    torch.onnx.export(\r\n            model=model,\r\n            args=x,\r\n            f=output_model_path,\r\n            opset_version=11,\r\n            export_params=True,\r\n            do_constant_folding=True,\r\n            input_names = ['INPUT__0'],\r\n            output_names = ['OUTPUT__0'],\r\n            dynamic_axes={'INPUT__0' : {0:'bs'}, 'OUTPUT__0' : {0:'bs'}}\r\n        )\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nthen I converted it to tensorrt plan file\r\n```\r\ntrtexec --onnx=resnet18.onnx --explicitBatch --optShapes=INPUT__0:5x3x224x224 --buildOnly --saveEngine=resnet18.plan --workspace=12288 --device=1\r\n```\r\n***tritonserver config***\r\nmy tritonserver config\r\n```\r\nplatform: \"tensorrt_plan\"\r\nmax_batch_size : 0\r\ninput: [\r\n    {\r\n        name: \"INPUT__0\",\r\n        data_type: TYPE_FP32,\r\n        dims: [5, 3, 224, 224],\r\n    }\r\n],\r\ninstance_group [\r\n  {\r\n    count: 1\r\n    kind: KIND_GPU\r\n    gpus: [0]\r\n  }\r\n]\r\noptimization{\r\n  graph: {\r\n      level : 1\r\n  },\r\n  eager_batching : 1,\r\n  cuda: {\r\n    graphs:1,\r\n    graph_spec: [\r\n      {\r\n        input: {\r\n            key: \"INPUT__0\",\r\n            value: {dim:[5, 3, 224, 224]}\r\n        }\r\n      }\r\n    ],\r\n    busy_wait_events:1,\r\n    output_copy_stream: 1\r\n  }\r\n}\r\n```\r\n***result***\r\nwhen I run tritonserver\uff0cI get following error:\r\n```\r\nI0823 14:33:25.438942 8964 tensorrt.cc:3193] Detected INPUT__0 as execution binding for resnet_0\r\nI0823 14:33:25.438952 8964 tensorrt.cc:3193] Detected OUTPUT__0 as execution binding for resnet_0\r\nE0823 14:33:25.454761 8964 logging.cc:43] 3: [executionContext.cpp::setBindingDimensions::945] Error Code 3: API Usage Error (Parameter check failed at: runtime/api/executionContext.cpp::setBindingDimensions::945, condition: engineDims.nbDims == dimensions.nbDims\r\n)\r\nE0823 14:33:25.454795 8964 tensorrt.cc:5090] Failed to set cuda graph shape for resnet_0trt failed to set binding dimension to [1,5,3,224,224] for binding 0 for resnet_0\r\nI0823 14:33:25.454810 8964 tensorrt.cc:1426] Created instance resnet_0 on GPU 0 with stream priority 0 and optimization profile default[0];\r\nI0823 14:33:25.454952 8964 backend_model_instance.cc:734] Starting backend thread for resnet_0 at nice 0 on device 0...\r\nI0823 14:33:25.455107 8964 model_repository_manager.cc:1352] successfully loaded 'resnet' version 1\r\n```\r\nIt will add batch dimension and making the original dimension 5x3x224x224 become 1x5x224x224.\r\nI found that tensorrt backend increases this dimension when max_batch_size is equal to 0 [https://github.com/triton-inference-server/tensorrt_backend/blob/main/src/tensorrt.cc#L5348](url).\r\nI also tried to set the batch size in the cuda spec to 5, but he will get an error when verifying the configuration, the configuration as follows\r\n```\r\nplatform: \"tensorrt_plan\"\r\nmax_batch_size : 0\r\ninput: [\r\n    {\r\n        name: \"INPUT__0\",\r\n        data_type: TYPE_FP32,\r\n        dims: [5, 3, 224, 224],\r\n    }\r\n],\r\ninstance_group [\r\n  {\r\n    count: 1\r\n    kind: KIND_GPU\r\n    gpus: [0]\r\n  }\r\n]\r\noptimization{\r\n  graph: {\r\n      level : 1\r\n  },\r\n  eager_batching : 1,\r\n  cuda: {\r\n    graphs:1,\r\n    graph_spec: [\r\n      {\r\n        batch_size:5,\r\n        input: {\r\n            key: \"INPUT__0\",\r\n            value: {dim:[3, 224, 224]}\r\n        }\r\n      }\r\n    ],\r\n    busy_wait_events:1,\r\n    output_copy_stream: 1\r\n  }\r\n}\r\n```\r\n**Expected behavior**\r\nWe don't want to use dynamic batcher, so we need to set max_batch_size to 0, and we also need to use cuda graph,how do we need to configure these two features\r\n\r\nthanks", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4804/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4804/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4783", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4783/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4783/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4783/events", "html_url": "https://github.com/triton-inference-server/server/issues/4783", "id": 1343355867, "node_id": "I_kwDOCQnI4s5QEfvb", "number": 4783, "title": "Python backend bug when using model load api to reload model in explicit mode", "user": {"login": "SaratM34", "id": 1553258, "node_id": "MDQ6VXNlcjE1NTMyNTg=", "avatar_url": "https://avatars.githubusercontent.com/u/1553258?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SaratM34", "html_url": "https://github.com/SaratM34", "followers_url": "https://api.github.com/users/SaratM34/followers", "following_url": "https://api.github.com/users/SaratM34/following{/other_user}", "gists_url": "https://api.github.com/users/SaratM34/gists{/gist_id}", "starred_url": "https://api.github.com/users/SaratM34/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SaratM34/subscriptions", "organizations_url": "https://api.github.com/users/SaratM34/orgs", "repos_url": "https://api.github.com/users/SaratM34/repos", "events_url": "https://api.github.com/users/SaratM34/events{/privacy}", "received_events_url": "https://api.github.com/users/SaratM34/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2022-08-18T16:42:07Z", "updated_at": "2023-03-16T22:28:34Z", "closed_at": "2023-03-16T22:28:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI have a found a bug when using the model load api to re-load an already loaded python backend model in explicit mode. To explain the scenario, We have a model which is already being loaded and running by Triton. We have a new version of the model which we would like Triton to load. So when triton is currently running and serving the model, we deleted the same model from model repository  and copied the new version of the model. If we call the load API `/v2/repository/models/<model_name>/load` it will start re-loading the model. The bug here is that triton is not using the conda environment of the new version of the model. it continues to use the conda environment of old model hence resulting is failed model re-load in case there are newer dependencies in the conda environment for newer version. \r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nr22.07\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTriton Container\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nIts easy to reproduce the issue, create two versions of same python backend model. one with an additional python dependency i.e both versions just only differ in conda environments (provided example model.py and requirements.txt below for each version). use triton to load one version of the model and when triton is successfully loaded the model and running, remove the model from model repository and copy the other version which has an additonal dependency and call the model reload api. it will fail saying module not found error(for example, in the below case it will fail to re-load with pandas module not found error).\r\n\r\n**Version 1 - model.py**\r\n```\r\nimport triton_python_backend_utils as pb_utils\r\n \r\nimport numpy as np\r\n \r\n class TritonPythonModel:\r\n    def initialize(self, args):\r\n    \tpass\r\n    def execute(self, requests):\r\n    \tpass\r\n    def finalize(self):\r\n        pass\r\n```\r\n\r\nversion 1 - requirements.txt\r\n```\r\nnumpy\r\n```\r\n\r\n**Version 2 - model.py**\r\n```\r\nimport triton_python_backend_utils as pb_utils\r\n \r\nimport numpy as np\r\nimport pandas as pd\r\n \r\n class TritonPythonModel:\r\n    def initialize(self, args):\r\n    \tpass\r\n    def execute(self, requests):\r\n    \tpass\r\n    def finalize(self):\r\n        pass\r\n```\r\n\r\nversion 2 - requirements.txt\r\n```\r\nnumpy\r\npandas\r\n```\r\n\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\npython backend\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nwhen reloading an model it should reload conda environment too", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4783/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4783/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4778", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4778/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4778/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4778/events", "html_url": "https://github.com/triton-inference-server/server/issues/4778", "id": 1341258604, "node_id": "I_kwDOCQnI4s5P8fts", "number": 4778, "title": "triton pytorch backend malloc coredump ", "user": {"login": "jackzhou121", "id": 13568557, "node_id": "MDQ6VXNlcjEzNTY4NTU3", "avatar_url": "https://avatars.githubusercontent.com/u/13568557?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jackzhou121", "html_url": "https://github.com/jackzhou121", "followers_url": "https://api.github.com/users/jackzhou121/followers", "following_url": "https://api.github.com/users/jackzhou121/following{/other_user}", "gists_url": "https://api.github.com/users/jackzhou121/gists{/gist_id}", "starred_url": "https://api.github.com/users/jackzhou121/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jackzhou121/subscriptions", "organizations_url": "https://api.github.com/users/jackzhou121/orgs", "repos_url": "https://api.github.com/users/jackzhou121/repos", "events_url": "https://api.github.com/users/jackzhou121/events{/privacy}", "received_events_url": "https://api.github.com/users/jackzhou121/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2022-08-17T06:42:31Z", "updated_at": "2022-09-05T09:25:38Z", "closed_at": "2022-09-05T09:25:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n**Description**\r\nI use triton sdk to do torchscript model inference, i use two process with nvidia-mps, and i found sometimes one of two process  failed with coredump, i use gdb to debug the problem, here is the bt info:\r\nProgram terminated with signal SIGSEGV, Segmentation fault.\r\n#0 0x00007f1df8ef823b in malloc () from /usr/lib/x86_64-linux-gnu/libc.so.6\r\n[Current thread is 1 (Thread 0x7f1c817fe000 (LWP 126))]\r\n(gdb) bt\r\n#0 0x00007f1df8ef823b in malloc () from /usr/lib/x86_64-linux-gnu/libc.so.6\r\n#1 0x00007f1df9266b39 in operator new(unsigned long) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#2 0x00007f1df1a21c1c in void std::vector<torch::jit::Value*, std::allocator<torch::jit::Value*> >::emplace_back<torch::jit::Value*>(torch::jit::Value*&&) ()\r\nfrom /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n#3 0x00007f1df1acffee in torch::jit::Node::addOutput() () from /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n#4 0x00007f1df1ad76c5 in torch::jit::Block::cloneFrom(torch::jit::Block*, std::function<torch::jit::Value* (torch::jit::Value*)>) () from /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n#5 0x00007f1df1ad7f84 in torch::jit::Graph::copy() () from /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n#6 0x00007f1df19a8724 in torch::jit::GraphFunction::get_executor() () from /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n#7 0x00007f1df19a579e in torch::jit::GraphFunction::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) () from /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n#8 0x00007f1df19a5c5e in torch::jit::GraphFunction::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&) ()\r\nfrom /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n#9 0x00007f1df19b84bb in torch::jit::Method::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&) const ()\r\nfrom /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n#10 0x00007f1d40126f5d in triton::backend::pytorch::ModelInstanceState::Execute(std::vector<TRITONBACKEND_Response*, std::allocator<TRITONBACKEND_Response*> >*, unsigned int, std::vector<c10::IValue, std::allocator<c10::IValue> >*, std::vector<at::Tensor, std::allocator<at::Tensor> >*) () from /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n#11 0x00007f1d4012d255 in triton::backend::pytorch::ModelInstanceState::ProcessRequests(TRITONBACKEND_Request**, unsigned int) ()\r\nfrom /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n#12 0x00007f1d4012eaa4 in TRITONBACKEND_ModelInstanceExecute () from /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n#13 0x00007f1df80b0faa in nvidia::inferenceserver::TritonModelInstance::Execute(std::vector<TRITONBACKEND_Request*, std::allocator<TRITONBACKEND_Request*> >&) ()\r\nfrom /opt/tritonserver/lib/libtritonserver.so\r\n#14 0x00007f1df80b1857 in nvidia::inferenceserver::TritonModelInstance::Schedule(std::vector<std::unique_ptr<nvidia::inferenceserver::InferenceRequest, std::default_delete<nvidia::inferenceserver::InferenceRequest> >, std::allocator<std::unique_ptr<nvidia::inferenceserver::InferenceRequest, std::default_delete<nvidia::inferenceserver::InferenceRequest> > > >&&, std::function<void ()> const&) () from /opt/tritonserver/lib/libtritonserver.so\r\n#15 0x00007f1df7f5ccc1 in nvidia::inferenceserver::Payload::Execute(bool*) () from /opt/tritonserver/lib/libtritonserver.so\r\n#16 0x00007f1df80ab4f7 in nvidia::inferenceserver::TritonModelInstance::TritonBackendThread::BackendThread(int, int) () from /opt/tritonserver/lib/libtritonserver.so\r\n#17 0x00007f1df9292de4 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#18 0x00007f1df9d0c609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#19 0x00007f1df8f7d163 in clone () from /usr/lib/x86_64-linux-gnu/libc.so.6 \r\n\r\n**Triton Information**\r\ni use ngc triton container 21.11, tritonversion: 2.16, pytorch:1.11.0a0+b6df043\r\nNVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.5\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nyes i use ngc triton container 21.11\r\n\r\n**To Reproduce**\r\nstart nvidia mps and run two process \r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\ninstance_group [\r\n  {\r\n    count: 8\r\n    kind: KIND_GPU\r\n    gpus:[0,1]\r\n  }\r\n]\r\n\r\nparameters: {\r\nkey: \"DISABLE_OPTIMIZED_EXECUTION\"\r\n    value: {\r\n    string_value:\"true\"\r\n    }\r\n}\r\n\r\nparameters: {\r\nkey: \"ENABLE_NVFUSER\"\r\n    value: {\r\n    string_value: \"true\"\r\n    }\r\n}\r\n\r\nparameters: {\r\nkey: \"INFERENCE_MODE\"\r\n    value: {\r\n    string_value: \"true\"\r\n    }\r\n}\r\n\r\ninput [\r\n  {\r\n    name: \"xwithouttone__0\"\r\n    data_type: TYPE_INT64\r\n    format: FORMAT_NONE\r\n    dims: [-1]\r\n  },\r\n  {\r\n    name: \"tone__1\"\r\n    data_type: TYPE_INT64\r\n    format: FORMAT_NONE\r\n    dims: [-1]\r\n  },\r\n  {\r\n    name: \"prosodyx__2\"\r\n    data_type: TYPE_INT64\r\n    format: FORMAT_NONE\r\n    dims: [-1]\r\n  },\r\n  {\r\n    name: \"emotionid__3\"\r\n    data_type: TYPE_INT64\r\n    format: FORMAT_NONE\r\n    dims: [-1]\r\n  },\r\n  {\r\n    name: \"emotionlevel__4\"\r\n    data_type: TYPE_FP32\r\n    format: FORMAT_NONE\r\n    dims: [-1]\r\n  },\r\n  {\r\n    name: \"alpha__5\"\r\n    data_type: TYPE_FP32\r\n    format: FORMAT_NONE\r\n    dims: [-1]\r\n  }\r\n\r\n]\r\noutput [\r\n  {\r\n    name: \"output__0\"\r\n    data_type: TYPE_FP16\r\n    dims: [1,-1,256]\r\n  }\r\n]\r\n\r\ndefault_model_filename: \"20220804_novel_f25_fastspeech_tensorRT_a30.plan\"\r\n\r\noptimization {\r\n    input_pinned_memory {\r\n        enable: true\r\n    },\r\n    output_pinned_memory {\r\n        enable: true\r\n    }\r\n}\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4778/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4769", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4769/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4769/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4769/events", "html_url": "https://github.com/triton-inference-server/server/issues/4769", "id": 1339727914, "node_id": "I_kwDOCQnI4s5P2qAq", "number": 4769, "title": "Core dump when dynamic batch Infer using tensorflow backend", "user": {"login": "zhaotyer", "id": 89376832, "node_id": "MDQ6VXNlcjg5Mzc2ODMy", "avatar_url": "https://avatars.githubusercontent.com/u/89376832?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhaotyer", "html_url": "https://github.com/zhaotyer", "followers_url": "https://api.github.com/users/zhaotyer/followers", "following_url": "https://api.github.com/users/zhaotyer/following{/other_user}", "gists_url": "https://api.github.com/users/zhaotyer/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhaotyer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhaotyer/subscriptions", "organizations_url": "https://api.github.com/users/zhaotyer/orgs", "repos_url": "https://api.github.com/users/zhaotyer/repos", "events_url": "https://api.github.com/users/zhaotyer/events{/privacy}", "received_events_url": "https://api.github.com/users/zhaotyer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2022-08-16T02:27:18Z", "updated_at": "2023-04-10T16:22:00Z", "closed_at": "2023-04-10T16:22:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen I use tritonserever22.02 for dynamic batch inference, the coredump will occasionally appear in the first inference after the model is loaded successfully\r\n![\u5fae\u4fe1\u56fe\u7247_20220816102231](https://user-images.githubusercontent.com/89376832/184784906-60d5b341-aceb-48b1-a030-3e4acc828083.png)\r\n![\u5fae\u4fe1\u56fe\u7247_20220816102242](https://user-images.githubusercontent.com/89376832/184784978-c611239e-362a-489a-8472-5ff081f4ae98.png)\r\n\r\n\r\n**Triton Information**\r\nnvcr.io/nvidia/tritonserver:22.02-py3\r\n\r\nAre you using the Triton container or did you build it yourself?\r\n Triton container \r\n\r\n**To Reproduce**\r\n1.Start docker container\r\n2.Load model\r\n3.Send an inference request with multi images\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nUsing bls, tensorflow backend\r\nyou can obtain model and client from [https://github.com/zhaotyer/sample](url)\r\n\r\n**Expected behavior**\r\nnormal execution, no core dump\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4769/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4769/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4764", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4764/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4764/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4764/events", "html_url": "https://github.com/triton-inference-server/server/issues/4764", "id": 1337914283, "node_id": "I_kwDOCQnI4s5PvvOr", "number": 4764, "title": "Does model should instantly free memory after unload?", "user": {"login": "alxmamaev", "id": 17113191, "node_id": "MDQ6VXNlcjE3MTEzMTkx", "avatar_url": "https://avatars.githubusercontent.com/u/17113191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alxmamaev", "html_url": "https://github.com/alxmamaev", "followers_url": "https://api.github.com/users/alxmamaev/followers", "following_url": "https://api.github.com/users/alxmamaev/following{/other_user}", "gists_url": "https://api.github.com/users/alxmamaev/gists{/gist_id}", "starred_url": "https://api.github.com/users/alxmamaev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alxmamaev/subscriptions", "organizations_url": "https://api.github.com/users/alxmamaev/orgs", "repos_url": "https://api.github.com/users/alxmamaev/repos", "events_url": "https://api.github.com/users/alxmamaev/events{/privacy}", "received_events_url": "https://api.github.com/users/alxmamaev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-08-13T09:31:23Z", "updated_at": "2022-08-31T20:36:43Z", "closed_at": "2022-08-31T20:36:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I tested the model repository protocol, which allows to load and unload models.\r\nI was sequentially called load and unload requests, like:\r\n```\r\nPOST v2/repository/models/${MODEL_NAME}/load\r\nPOST v2/repository/models/${MODEL_NAME}/unload\r\nPOST v2/repository/models/${MODEL_NAME}/load\r\nPOST v2/repository/models/${MODEL_NAME}/unload\r\nPOST v2/repository/models/${MODEL_NAME}/load\r\nPOST v2/repository/models/${MODEL_NAME}/unload\r\nPOST v2/repository/models/${MODEL_NAME}/load\r\nPOST v2/repository/models/${MODEL_NAME}/unload\r\n```\r\nEvery time I loaded and unloaded the same model with the same version. Nothing was changed in the repository at this time.\r\nThe model backend was PyTorch, model is loaded on Host memory (CPU Instance)\r\nI guessed that the total used memory would not change after the first load, but actually, that is not true. After every `unload` triton free a few memory, much smaller than was got on `load`, on each `load` triton takes more and more host memory.\r\nSo in a log way, it may cause Memory Error. Is that undefined behavior? \r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4764/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4764/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4756", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4756/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4756/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4756/events", "html_url": "https://github.com/triton-inference-server/server/issues/4756", "id": 1334754852, "node_id": "I_kwDOCQnI4s5Pjr4k", "number": 4756, "title": "TF_SIGNATURE_DEF is not used when selected on service startup", "user": {"login": "ShamariYoti", "id": 57357716, "node_id": "MDQ6VXNlcjU3MzU3NzE2", "avatar_url": "https://avatars.githubusercontent.com/u/57357716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ShamariYoti", "html_url": "https://github.com/ShamariYoti", "followers_url": "https://api.github.com/users/ShamariYoti/followers", "following_url": "https://api.github.com/users/ShamariYoti/following{/other_user}", "gists_url": "https://api.github.com/users/ShamariYoti/gists{/gist_id}", "starred_url": "https://api.github.com/users/ShamariYoti/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ShamariYoti/subscriptions", "organizations_url": "https://api.github.com/users/ShamariYoti/orgs", "repos_url": "https://api.github.com/users/ShamariYoti/repos", "events_url": "https://api.github.com/users/ShamariYoti/events{/privacy}", "received_events_url": "https://api.github.com/users/ShamariYoti/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2022-08-10T14:53:21Z", "updated_at": "2023-03-24T00:40:27Z", "closed_at": "2023-03-24T00:40:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nContext: Trying to load a tensorflow saved model which has multiple signature definitions.\r\n\r\nProblem: The signature definition selected is not used. This happens randomly, when loading the container multiple times different signature definitions will be used even If `TF_SIGNATURE_DEF` has been set.\r\n\r\nI see the log messages below when this happens.\r\n\r\n```\r\n2022-08-10 14:46:32.003735: W triton/tensorflow_backend_tf.cc:998] unable to find serving signature 'serving_default\r\n2022-08-10 14:46:32.003743: W triton/tensorflow_backend_tf.cc:1000] using signature 'random_sig_def'\r\n```\r\n\r\nFrom looking at the [code](https://github.com/triton-inference-server/tensorflow_backend/blob/4c6166a02832ad1dfec4f34a6e1a518f869d7c1c/src/tensorflow_backend_tf.cc#L991-L1003) it seems like it falls into this section and chooses a signature def at random? But as I set the signature def in the config file id expect that block of code not to be executed?\r\n\r\n**Triton Information**\r\nTriton version: `22.07-py3`\r\n\r\nThis is the triton docker container.\r\n\r\n**To Reproduce**\r\nI load in a model which has multiple signature definitions, this model has three.\r\n\r\nI created a `config.pbtxt` which looks like the following, I played around with this config for a while I added all the relevant inputs and outputs to but the same outcome.\r\n\r\n```\r\nplatform: \"tensorflow_savedmodel\"  \r\nparameters: {\r\nkey: \"TF_SIGNATURE_DEF\"\r\nvalue: {\r\n  string_value: \"sig_def_one\"\r\n  }\r\n}\r\n```\r\n\r\nThe model is a tensorflow saved model which has three signature definitions of which all have different inputs and outputs. (I can't provide the model here). When I run `saved_model_cli show --dir ./model --tag_set serve` I am able to see all sigature definition.\r\n\r\n**Expected behavior**\r\n\r\nI'd expect `TF_SIGNATURE_DEF` to be used when loading the models.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4756/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4756/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4743", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4743/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4743/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4743/events", "html_url": "https://github.com/triton-inference-server/server/issues/4743", "id": 1329376007, "node_id": "I_kwDOCQnI4s5PPKsH", "number": 4743, "title": "Python Backend complains \"triton_python_backend_utils\" has no attribute \"InferenceRequest\"", "user": {"login": "Michael-Jing", "id": 7981324, "node_id": "MDQ6VXNlcjc5ODEzMjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/7981324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Michael-Jing", "html_url": "https://github.com/Michael-Jing", "followers_url": "https://api.github.com/users/Michael-Jing/followers", "following_url": "https://api.github.com/users/Michael-Jing/following{/other_user}", "gists_url": "https://api.github.com/users/Michael-Jing/gists{/gist_id}", "starred_url": "https://api.github.com/users/Michael-Jing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Michael-Jing/subscriptions", "organizations_url": "https://api.github.com/users/Michael-Jing/orgs", "repos_url": "https://api.github.com/users/Michael-Jing/repos", "events_url": "https://api.github.com/users/Michael-Jing/events{/privacy}", "received_events_url": "https://api.github.com/users/Michael-Jing/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-08-05T02:46:03Z", "updated_at": "2022-09-30T22:43:27Z", "closed_at": "2022-09-30T22:43:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm using the python business logic scripting, and a conda packed python environment with python3.8. Both 22.06 and 22.07 version show the following error message \"UNAVAILABLE: Internal: AttributeError: module 'triton_python_backend_utils' has no attribute 'InferenceRequest'\". but it works ok on the third party docker image flyingmachine/tritonserver-w-ort-1.11.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4743/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4711", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4711/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4711/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4711/events", "html_url": "https://github.com/triton-inference-server/server/issues/4711", "id": 1321966621, "node_id": "I_kwDOCQnI4s5Oy5wd", "number": 4711, "title": "[Question] About perf_analyzer request rate", "user": {"login": "SeungsuBaek", "id": 76544552, "node_id": "MDQ6VXNlcjc2NTQ0NTUy", "avatar_url": "https://avatars.githubusercontent.com/u/76544552?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SeungsuBaek", "html_url": "https://github.com/SeungsuBaek", "followers_url": "https://api.github.com/users/SeungsuBaek/followers", "following_url": "https://api.github.com/users/SeungsuBaek/following{/other_user}", "gists_url": "https://api.github.com/users/SeungsuBaek/gists{/gist_id}", "starred_url": "https://api.github.com/users/SeungsuBaek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SeungsuBaek/subscriptions", "organizations_url": "https://api.github.com/users/SeungsuBaek/orgs", "repos_url": "https://api.github.com/users/SeungsuBaek/repos", "events_url": "https://api.github.com/users/SeungsuBaek/events{/privacy}", "received_events_url": "https://api.github.com/users/SeungsuBaek/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-07-29T09:06:26Z", "updated_at": "2023-02-10T03:18:17Z", "closed_at": "2023-02-10T03:18:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi!\r\n\r\nI have a question about perf_analyzer and tritonserver's trace.json.\r\n\r\n---\r\nSERVER\r\n```\r\n$ tritonserver --model-repository=/triton_dev/models --trace-file /triton_dev/experiments/trace/trace.json --trace-level TIMESTAMPS --trace-rate 1\r\n```\r\n---\r\nCLIENT\r\n```\r\n$ perf_analyzer -b 1 -m resnet --request-rate-range 50:50 --sync --request-distribution constant\r\n```\r\n\r\nI wanted to see if the request comes to the server according to the request rate set by the perf_analyzer. So after using these commands, I checked trace.json and drew a graph of how many requests per second arrived.\r\n\r\nBelow graph is the result of that.\r\n( { \"name\": \"**http recv start**\", \"ns\":  123456789} ) // only use this information in trace.json.\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/76544552/181723897-627da891-bc6a-49b2-8ac7-312f2ab3cc85.png)\r\n\r\n\r\nMy question is that **why is the request rate measured higher than the perf_analyzer's value for about 3 seconds?**\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4711/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4711/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4665", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4665/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4665/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4665/events", "html_url": "https://github.com/triton-inference-server/server/issues/4665", "id": 1308845982, "node_id": "I_kwDOCQnI4s5OA2ee", "number": 4665, "title": "Server crashes on loading shared libraries", "user": {"login": "PRIYANKArythem3", "id": 51945165, "node_id": "MDQ6VXNlcjUxOTQ1MTY1", "avatar_url": "https://avatars.githubusercontent.com/u/51945165?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PRIYANKArythem3", "html_url": "https://github.com/PRIYANKArythem3", "followers_url": "https://api.github.com/users/PRIYANKArythem3/followers", "following_url": "https://api.github.com/users/PRIYANKArythem3/following{/other_user}", "gists_url": "https://api.github.com/users/PRIYANKArythem3/gists{/gist_id}", "starred_url": "https://api.github.com/users/PRIYANKArythem3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PRIYANKArythem3/subscriptions", "organizations_url": "https://api.github.com/users/PRIYANKArythem3/orgs", "repos_url": "https://api.github.com/users/PRIYANKArythem3/repos", "events_url": "https://api.github.com/users/PRIYANKArythem3/events{/privacy}", "received_events_url": "https://api.github.com/users/PRIYANKArythem3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 28, "created_at": "2022-07-19T00:59:30Z", "updated_at": "2023-01-28T02:50:58Z", "closed_at": "2022-09-30T22:43:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI am in the development phase of running Deep Learning Model on Triton Inference Server. I am using the LD_PRELOAD trick to load customs ops needed to support inference. But as soon as I do that, the server crashes with the following error:\r\n\r\n```\r\n\r\nSignal (11) received.\r\n 0# 0x000055D4EB0F28A9 in tritonserver\r\n 1# 0x00007F62E3251210 in /lib/x86_64-linux-gnu/libc.so.6\r\n 2# re2::RE2::Match(re2::StringPiece const&, unsigned long, unsigned long, re2::RE2::Anchor, re2::StringPiece*, int) const in /lib/x86_64-linux-gnu/libre2.so.5\r\n 3# re2::RE2::DoMatch(re2::StringPiece const&, re2::RE2::Anchor, unsigned long*, re2::RE2::Arg const* const*, int) const in /lib/x86_64-linux-gnu/libre2.so.5\r\n 4# 0x000055D4EB0F7196 in tritonserver\r\n 5# 0x000055D4EB2BD226 in tritonserver\r\n 6# 0x000055D4EB2C20A8 in tritonserver\r\n 7# 0x000055D4EB2BFE8E in tritonserver\r\n 8# 0x000055D4EB2A6E20 in tritonserver\r\n 9# 0x000055D4EB2AE940 in tritonserver\r\n10# 0x000055D4EB2AF38F in tritonserver\r\n11# 0x000055D4EB2C3CE2 in tritonserver\r\n12# 0x00007F62E3ABD609 in /lib/x86_64-linux-gnu/libpthread.so.0\r\n13# clone in /lib/x86_64-linux-gnu/libc.so.6\r\n\r\n\r\nSegmentation fault\r\n```\r\n\r\n \r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n\r\n\r\nEnvironment\r\nTensorRT Version:\r\nGPU Type: GPU 0: Tesla V100-SXM2-16GB\r\n\r\nTRITON_SERVER_VERSION=2.15.0\r\nNVIDIA_TRITON_SERVER_VERSION=21.10\r\nNSIGHT_SYSTEMS_VERSION=2021.3.2.4\r\nTriton Image: \u201c21.10\u201d\r\nCUDA Version: CUDA_VERSION=11.4.3.001\r\nCUDA_DRIVER_VERSION=470.57.02\r\nCUDNN Version:\r\nOperating System + Version: Distributor ID: Linux 0e2e4678631d 5.10.47-linuxkit #1 SMP Sat Jul 3 21:51:47 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\nPython Version (if applicable): python3.8\r\n\r\nAre you using the Triton container or did you build it yourself?\r\n Triton Container\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior\r\n\r\n```\r\ndocker run -it \\\r\n --rm -p8000:8000 -p8001:8001 -p8002:8002 \\\r\n -v /Users/priyankasaraf/repo/triton/selftest/model_dir/OutNonPyFuncFP32:/models \\\r\n -v /Users/priyankasaraf/repo/triton/selftest/binaries:/triton/lib \\\r\n --entrypoint /bin/bash \\\r\n nvcr.io/nvidia/tritonserver:21.10-py3\r\n```\r\n\r\n```\r\nexport LD_LIBRARY_PATH=/opt/tritonserver/backends/tensorflow2:$LD_LIBRARY_PATH && export LD_PRELOAD=\"/triton/lib/_sentencepiece_tokenizer.so /triton/lib/_normalize_ops.so /triton/lib/_regex_split_ops.so /triton/lib/_wordpiece_tokenizer.so\" && tritonserver --backend-config=tensorflow,version=2 --model-repository=/models \\\r\n--model-control-mode=explicit \\\r\n--log-verbose=5 --log-info=true --log-warning=true --log-error=true \\\r\n--http-port=8000 --grpc-port=8001 \\\r\n--metrics-port=8002\r\n```\r\n.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nThe server must not crash\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4665/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4665/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4636", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4636/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4636/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4636/events", "html_url": "https://github.com/triton-inference-server/server/issues/4636", "id": 1302919727, "node_id": "I_kwDOCQnI4s5NqPov", "number": 4636, "title": "bug: Trace output for `BYTES` has invalid JSON encoding", "user": {"login": "brightsparc", "id": 360368, "node_id": "MDQ6VXNlcjM2MDM2OA==", "avatar_url": "https://avatars.githubusercontent.com/u/360368?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brightsparc", "html_url": "https://github.com/brightsparc", "followers_url": "https://api.github.com/users/brightsparc/followers", "following_url": "https://api.github.com/users/brightsparc/following{/other_user}", "gists_url": "https://api.github.com/users/brightsparc/gists{/gist_id}", "starred_url": "https://api.github.com/users/brightsparc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brightsparc/subscriptions", "organizations_url": "https://api.github.com/users/brightsparc/orgs", "repos_url": "https://api.github.com/users/brightsparc/repos", "events_url": "https://api.github.com/users/brightsparc/events{/privacy}", "received_events_url": "https://api.github.com/users/brightsparc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-07-13T05:23:21Z", "updated_at": "2022-08-01T00:28:10Z", "closed_at": "2022-07-29T23:59:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Description**\r\nThe trace file being written to disc doesn't not contain validate json due to the addition quotes being include in the output of BYTES (string) inputs.  See below the text `\"male\"` is being output with `\"\"make\"\"`.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n* nvcr.io/nvidia/tritonserver:22.05-pyt-python-py3\r\n\r\nAre you using the Triton container or did you build it yourself?\r\n* triton container\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nFollowing is the example input payload that I sent to the model:\r\n\r\n```\r\npayload = {\r\n    \"inputs\": [\r\n        {\"name\": \"Sex\", \"shape\": [1], \"datatype\": \"BYTES\", \"data\": [\"male\"] },\r\n        {\"name\": \"Age\", \"shape\": [1], \"datatype\": \"FP32\", \"data\": [34.5] },\r\n    ]\r\n}\r\n```\r\n\r\nAnd a sample from the trace output.\r\n\r\n```\r\n{\"id\":1,\"activity\":\"TENSOR_QUEUE_INPUT\",\"tensor\":{\"name\":\"Sex\",\"data\":\"\"male\"\",\"shape\":\"1\",\"dtype\":\"BYTES\"},\r\n{\"name\":\"Age\",\"data\":\"34.5\",\"shape\":\"1\",\"dtype\":\"FP32\"}\r\n```\r\n\r\nI am using an ensemble model with the input/outputs as show below:\r\n\r\n```\r\nname: \"titanic_ensemble\"\r\nplatform: \"ensemble\"\r\nmax_batch_size: 0 # Dynamic batch not supported\r\ninput [\r\n  {\r\n    name: \"Pclass\"\r\n    data_type: TYPE_STRING\r\n    dims: [ 1 ]\r\n  },\r\n  {\r\n    name: \"Sex\"\r\n    data_type: TYPE_STRING\r\n    dims: [ 1 ]\r\n  },\r\n  {\r\n    name: \"Age\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1 ]\r\n  },\r\n  {\r\n    name: \"SibSp\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1 ]\r\n  },\r\n  {\r\n    name: \"Parch\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1 ]\r\n  },\r\n  {\r\n    name: \"Fare\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1 ]\r\n  },\r\n  {\r\n    name: \"Embarked\"\r\n    data_type: TYPE_STRING\r\n    dims: [ 1 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"Survived_predictions\"\r\n    data_type: TYPE_STRING\r\n    dims: [ 1 ]\r\n  },\r\n  {\r\n    name: \"Survived_probabilities\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1, 3 ]\r\n  }\r\n]\r\n...\r\n```\r\n\r\n**Expected behavior**\r\nI would expect to be able to parse the json file, but attempting to pipe to JQ fails eg:\r\n\r\n```\r\ncat trace.json | jq\r\n```\r\n\r\nIt seems that additional quotes are [added](https://github.com/triton-inference-server/server/blob/main/src/tracer.cc#L578-L582) for the BYTES type which is in addition to the quote being add a the [start](https://github.com/triton-inference-server/server/blob/main/src/tracer.cc#L451) and [end](https://github.com/triton-inference-server/server/blob/main/src/tracer.cc#L597) of the data value.\r\n\r\nA potential fix could be to not quote all outputs, which would mean the 34.5 value is returned as a floating point in json that matches the input.  Otherwise, quotes in strings should get escaped.\r\n\r\nI notice that each file is an actual json array. I wonder if tracer could be simpler and more scalable to just create a JSON lines output, so that you append to the buffer and can flush without having to start and end the trace file with an array `[ ]`  ?", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4636/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4636/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4566", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4566/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4566/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4566/events", "html_url": "https://github.com/triton-inference-server/server/issues/4566", "id": 1290030427, "node_id": "I_kwDOCQnI4s5M5E1b", "number": 4566, "title": "Triton terminated with Signal (6)", "user": {"login": "erichtho", "id": 30315656, "node_id": "MDQ6VXNlcjMwMzE1NjU2", "avatar_url": "https://avatars.githubusercontent.com/u/30315656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erichtho", "html_url": "https://github.com/erichtho", "followers_url": "https://api.github.com/users/erichtho/followers", "following_url": "https://api.github.com/users/erichtho/following{/other_user}", "gists_url": "https://api.github.com/users/erichtho/gists{/gist_id}", "starred_url": "https://api.github.com/users/erichtho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erichtho/subscriptions", "organizations_url": "https://api.github.com/users/erichtho/orgs", "repos_url": "https://api.github.com/users/erichtho/repos", "events_url": "https://api.github.com/users/erichtho/events{/privacy}", "received_events_url": "https://api.github.com/users/erichtho/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-06-30T11:59:34Z", "updated_at": "2022-10-08T06:54:57Z", "closed_at": "2022-10-08T06:54:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using triton grpc client to infer, triton will exit unexpectedly sometimes.\r\nlike using:\r\n\r\n```\r\nwith tritonclient.grpc.InferenceServerClient('localhost:8001', verbose = False) as client:\r\n            outputs = [\r\n                httpclient.InferRequestedOutput('logits', ),\r\n                httpclient.InferRequestedOutput('embs', )\r\n            ]\r\n\r\n            # data_loader is a torch dataloader with 4 workers\r\n            for sent_count, test_batch in enumerate(data_loader):\r\n                with autocast():\r\n                    processed_signal, processed_signal_length = preprocessor(\r\n                        input_signal = test_batch[0].to(device),\r\n                        length = test_batch[1].to(device)\r\n                    )\r\n                inputs = [\r\n                    httpclient.InferInput(\"audio_signal\", list(processed_signal.shape), \"FP16\"),\r\n                    httpclient.InferInput(\"length\", [1, 1], np_to_triton_dtype(np.int32))\r\n                ]\r\n                inputs[0].set_data_from_numpy(processed_signal.cpu().numpy().astype(np.float16), )\r\n                inputs[1].set_data_from_numpy(processed_signal_length.cpu().numpy().astype(np.int32).reshape(1, 1))\r\n                result = client.infer(model_name = \"tensorrt_emb\",\r\n                                          inputs = inputs,\r\n                                          outputs = outputs)\r\n```\r\n\r\nand tritonserver output\uff1a\r\n\r\nterminate called after throwing an instance of 'nvinfer1::InternalError'\r\n  what():  Assertion mUsedAllocators.find(alloc) != mUsedAllocators.end() && \"Myelin free callback called with invalid MyelinAllocator\" failed.\r\nSignal (6) received.\r\n 0# 0x00005602FC4F21B9 in tritonserver\r\n 1# 0x00007FC98736C0C0 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# gsignal in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 3# abort in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 4# 0x00007FC987725911 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 5# 0x00007FC98773138C in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 6# 0x00007FC987730369 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 7# __gxx_personality_v0 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 8# 0x00007FC98752BBEF in /usr/lib/x86_64-linux-gnu/libgcc_s.so.1\r\n 9# _Unwind_RaiseException in /usr/lib/x86_64-linux-gnu/libgcc_s.so.1\r\n10# __cxa_throw in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n11# nvinfer1::Lobber<nvinfer1::InternalError>::operator()(char const*, char const*, int, int, nvinfer1::ErrorCode, char const*) in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8\r\n12# 0x00007FC9020EECBC in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8\r\n13# 0x00007FC902A7220F in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8\r\n14# 0x00007FC902A2862D in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8\r\n15# 0x00007FC902A7F653 in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8\r\n16# 0x00007FC9020EE715 in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8\r\n17# 0x00007FC901C8BAD0 in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8\r\n18# 0x00007FC9020F41F4 in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8\r\n19# 0x00007FC902913FD8 in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8\r\n20# 0x00007FC90291478C in /usr/lib/x86_64-linux-gnu/libnvinfer.so.8\r\n21# 0x00007FC97A57C6D7 in /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n22# 0x00007FC97A5855FE in /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n23# TRITONBACKEND_ModelInstanceExecute in /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n24# 0x00007FC987C1D73A in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n25# 0x00007FC987C1E0F7 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n26# 0x00007FC987CDB411 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n27# 0x00007FC987C175C7 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n28# 0x00007FC98775DDE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n29# 0x00007FC98896D609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n30# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n\r\ntritonserver version\uff1a22.05-py3(docker image)\r\nusing tensorrt backend. \r\nos: ubuntu 20.04\r\n\r\n**How To Reproduce**\r\nWe use trtexec to transform a onnx model to tensorRT engine(with maxShapes=1x80x12000), then put into triton model repository.\r\nWhen send dozens of request with shape 1x80x11000(like 8000) and other model requests in same time(different grpc client in different process, not multiprocessing, but multiple .py running)\uff0ctriton will exit by chance.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4566/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4566/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4537", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4537/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4537/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4537/events", "html_url": "https://github.com/triton-inference-server/server/issues/4537", "id": 1279844566, "node_id": "I_kwDOCQnI4s5MSODW", "number": 4537, "title": "Server start stuck when loading python model instantiating certain transformers model", "user": {"login": "AJHoeh", "id": 44893009, "node_id": "MDQ6VXNlcjQ0ODkzMDA5", "avatar_url": "https://avatars.githubusercontent.com/u/44893009?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AJHoeh", "html_url": "https://github.com/AJHoeh", "followers_url": "https://api.github.com/users/AJHoeh/followers", "following_url": "https://api.github.com/users/AJHoeh/following{/other_user}", "gists_url": "https://api.github.com/users/AJHoeh/gists{/gist_id}", "starred_url": "https://api.github.com/users/AJHoeh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AJHoeh/subscriptions", "organizations_url": "https://api.github.com/users/AJHoeh/orgs", "repos_url": "https://api.github.com/users/AJHoeh/repos", "events_url": "https://api.github.com/users/AJHoeh/events{/privacy}", "received_events_url": "https://api.github.com/users/AJHoeh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2022-06-22T09:25:50Z", "updated_at": "2023-02-01T12:15:31Z", "closed_at": "2023-01-27T16:43:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen startimg triton server with a python model which instantiates certain hugging face transformers models during `initialize()` the server gets stuck. Namely I tried it with [this checkpoint](https://huggingface.co/asahi417/tner-xlm-roberta-large-multiconer-multi/tree/main). Since it works if I use [this checkpoint](https://huggingface.co/Babelscape/wikineural-multilingual-ner) instead, I guess the Problem could be the model size? I also experienced a similiar Problem when using the aforementioned working checkpoint but also loading other models (onnx). Could this be related to memory/gpu settings of the docker container?\r\n\r\n**Triton Information**\r\nnvcr.io/nvidia/tritonserver:22.04-py3\r\n\r\n**To Reproduce**\r\nenv:\r\n```\r\nconda create --name triton-transformers python=3.8.10\r\nconda activate triton-transformers\r\nconda install -c huggingface transformers==4.14.1 tokenizers==0.10.3\r\nexport PYTHONNOUSERSITE=True\r\ncomda-pack\r\n```\r\n\r\nmodel:\r\n\r\n```\r\n...\r\nfrom transformers import AutoModelForTokenClassification,\r\n\r\nclass TritonPythonModel:\r\n    def initialize(self, args):\r\n        self.checkpoint = r\"/checkpoints/tner-xlm-roberta-large-multiconer-multi/\"\r\n        self.ner_model = AutoModelForTokenClassification.from_pretrained(self.checkpoint)\r\n        print('Initialized...')\r\n\r\n    def execute(self, requests):\r\n        ...\r\n```\r\n\r\nconfig:\r\n```\r\nbackend: \"python\"\r\nmax_batch_size: 8\r\ninput [\r\n  {\r\n    name: \"sequence\"\r\n    data_type: TYPE_STRING\r\n    dims: [ -1, -1 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"entities\"\r\n    data_type: TYPE_STRING\r\n    dims: [ -1, -1 ]\r\n  }\r\n]\r\nparameters: [\r\n  {\r\n    key: \"EXECUTION_ENV_PATH\"\r\n    value: { string_value: \"/envs/triton-transformers.tar.gz\"}\r\n  }\r\n]\r\n```\r\n**Expected behavior**\r\nTriton Server should just start.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4537/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4537/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4515", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4515/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4515/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4515/events", "html_url": "https://github.com/triton-inference-server/server/issues/4515", "id": 1273530561, "node_id": "I_kwDOCQnI4s5L6IjB", "number": 4515, "title": "Relative path for s3 storage", "user": {"login": "Revist", "id": 26026957, "node_id": "MDQ6VXNlcjI2MDI2OTU3", "avatar_url": "https://avatars.githubusercontent.com/u/26026957?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Revist", "html_url": "https://github.com/Revist", "followers_url": "https://api.github.com/users/Revist/followers", "following_url": "https://api.github.com/users/Revist/following{/other_user}", "gists_url": "https://api.github.com/users/Revist/gists{/gist_id}", "starred_url": "https://api.github.com/users/Revist/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Revist/subscriptions", "organizations_url": "https://api.github.com/users/Revist/orgs", "repos_url": "https://api.github.com/users/Revist/repos", "events_url": "https://api.github.com/users/Revist/events{/privacy}", "received_events_url": "https://api.github.com/users/Revist/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2022-06-16T12:58:22Z", "updated_at": "2022-12-16T18:12:11Z", "closed_at": "2022-10-15T00:56:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI have the following model repository structure:\r\n\r\n\r\n```\r\nmodel_repository\r\n\u251c\u2500\u2500 model1\r\n\u2502   \u251c\u2500\u2500 1\r\n\u2502   \u2502   \u251c\u2500\u2500 model.py\r\n\u2502   \u2502   \u2514\u2500\u2500 ...\r\n\u2502   \u2514\u2500\u2500 config.pbtxt\r\n\u251c\u2500\u2500 model2\r\n\u2502   \u251c\u2500\u2500 1\r\n\u2502   \u2502   \u251c\u2500\u2500 model.py\r\n\u2502   \u2502   \u2514\u2500\u2500 ...\r\n\u2502   \u2514\u2500\u2500 config.pbtxt\r\n\u2514\u2500\u2500 my_environment.tar.gz\r\n```\r\n\r\nto use `my_environment.tar.gz` in every `config.pbtxt` I have:\r\n\r\n```\r\nparameters: {\r\n  key: \"EXECUTION_ENV_PATH\",\r\n  value: {string_value: \"$$TRITON_MODEL_DIRECTORY/../my_environment.tar.gz\"}\r\n}\r\n```\r\nThis works when I have model_repository locally, however on s3 I get the error:\r\n\r\n`failed to load 'model' version 1: Internal: Failed to get the canonical path for /tmp/folderWE2Op7/../my_environment.tar.gz.`\r\n\r\n\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n\r\nI use 22.03 docker image.\r\n\r\nIs there an easy way to give relative paths on s3?\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4515/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4515/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4511", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4511/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4511/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4511/events", "html_url": "https://github.com/triton-inference-server/server/issues/4511", "id": 1270184970, "node_id": "I_kwDOCQnI4s5LtXwK", "number": 4511, "title": "perf_analyzer and perf_client don't have 'x' permission after pip install tritonclient(2.22.3)", "user": {"login": "EmmaQiaoCh", "id": 49423675, "node_id": "MDQ6VXNlcjQ5NDIzNjc1", "avatar_url": "https://avatars.githubusercontent.com/u/49423675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EmmaQiaoCh", "html_url": "https://github.com/EmmaQiaoCh", "followers_url": "https://api.github.com/users/EmmaQiaoCh/followers", "following_url": "https://api.github.com/users/EmmaQiaoCh/following{/other_user}", "gists_url": "https://api.github.com/users/EmmaQiaoCh/gists{/gist_id}", "starred_url": "https://api.github.com/users/EmmaQiaoCh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EmmaQiaoCh/subscriptions", "organizations_url": "https://api.github.com/users/EmmaQiaoCh/orgs", "repos_url": "https://api.github.com/users/EmmaQiaoCh/repos", "events_url": "https://api.github.com/users/EmmaQiaoCh/events{/privacy}", "received_events_url": "https://api.github.com/users/EmmaQiaoCh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-06-14T03:00:28Z", "updated_at": "2022-06-20T16:03:20Z", "closed_at": "2022-06-20T16:03:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nAfter pip install tritonclient[all], found /usr/local/bin/perf_analyzer and /usr/local/bin/perf_client don't have execute permission\r\n\r\n**Triton Information**\r\ntritonclient[all] version is 2.22.3\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nI use the triton container, image is nvcr.io/nvidia/tritonserver:22.05-py3\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nPull and launch the triton image, then in the container execute: pip install tritonclient[all]\r\nAfter that, when execute perf_analyzer, meet this error: /usr/local/bin/perf_analyzer: Permission denied\r\nand ls the file permission:\r\n-rw-r--r-- 1 root root 12974760 Jun 14 02:04 perf_analyzer\r\n-rw-r--r-- 1 root root 12974760 Jun 14 02:04 perf_client\r\nBTW, when uninstall tritonclient and re-install it with the version 2.22.0, no this permission issue.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nAfter install tritonclient, the binary should have 'x' permission\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4511/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4509", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4509/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4509/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4509/events", "html_url": "https://github.com/triton-inference-server/server/issues/4509", "id": 1268219472, "node_id": "I_kwDOCQnI4s5Ll35Q", "number": 4509, "title": "triton image classification example client http memory leak", "user": {"login": "IdoGabay111", "id": 89010709, "node_id": "MDQ6VXNlcjg5MDEwNzA5", "avatar_url": "https://avatars.githubusercontent.com/u/89010709?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IdoGabay111", "html_url": "https://github.com/IdoGabay111", "followers_url": "https://api.github.com/users/IdoGabay111/followers", "following_url": "https://api.github.com/users/IdoGabay111/following{/other_user}", "gists_url": "https://api.github.com/users/IdoGabay111/gists{/gist_id}", "starred_url": "https://api.github.com/users/IdoGabay111/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IdoGabay111/subscriptions", "organizations_url": "https://api.github.com/users/IdoGabay111/orgs", "repos_url": "https://api.github.com/users/IdoGabay111/repos", "events_url": "https://api.github.com/users/IdoGabay111/events{/privacy}", "received_events_url": "https://api.github.com/users/IdoGabay111/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-06-11T09:35:05Z", "updated_at": "2022-07-08T19:33:19Z", "closed_at": "2022-07-08T19:33:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nA clear and concise description of what the bug is.\r\nanswer:\r\nI'm using a modified version of the triton client image classification example (the one published on may 4th).\r\nI run the .py file in a docker container with http request to triton which is in another container.\r\nthe memory leak happend about once every 30 seconds and it's about 300Mb of memory that is added all in a span of 5 seconds and than maintain the new size for about 20 seconds and repeat.\r\ni know the memory leak doesn't is not related to other proccesses because when i use grpc i don't get memory leak.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n21.08\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nI'm using the Triton container.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nchanges i did : insert all the main in the example to a function so i can call it from another .py function. i also send PIL image through this function call instead odf getting the image from url in the example.\r\nanother change is that i return the results instead of returning \"PASS\".\r\n\r\ni think the main difference is that i dont open new python process every request with the triton client so i can work in real time.\r\nin this method ican get up to 30 fps vs 4 fps if i use the method in the example that I need to reopen python process every request.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\nframework : pytorch\r\nmodel : resnet50\r\ninput : a PIL image in size 640x480 (i'm sending one image in a batch)\r\noutput : dictionary of the results.\r\n\r\n\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\ntriton cluent works with out open new process every request without memory leak.\r\n\r\n[cups_labels.txt](https://github.com/triton-inference-server/server/files/8871162/cups_labels.txt)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4509/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4505", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4505/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4505/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4505/events", "html_url": "https://github.com/triton-inference-server/server/issues/4505", "id": 1266125580, "node_id": "I_kwDOCQnI4s5Ld4sM", "number": 4505, "title": "Status Message: CUDNN error executing cudnnFindConvolutionForwardAlgorithmEx", "user": {"login": "niqbal996", "id": 28758769, "node_id": "MDQ6VXNlcjI4NzU4NzY5", "avatar_url": "https://avatars.githubusercontent.com/u/28758769?v=4", "gravatar_id": "", "url": "https://api.github.com/users/niqbal996", "html_url": "https://github.com/niqbal996", "followers_url": "https://api.github.com/users/niqbal996/followers", "following_url": "https://api.github.com/users/niqbal996/following{/other_user}", "gists_url": "https://api.github.com/users/niqbal996/gists{/gist_id}", "starred_url": "https://api.github.com/users/niqbal996/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/niqbal996/subscriptions", "organizations_url": "https://api.github.com/users/niqbal996/orgs", "repos_url": "https://api.github.com/users/niqbal996/repos", "events_url": "https://api.github.com/users/niqbal996/events{/privacy}", "received_events_url": "https://api.github.com/users/niqbal996/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2022-06-09T13:22:23Z", "updated_at": "2023-02-03T21:03:29Z", "closed_at": "2022-10-18T01:21:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI am working on a Triton C-API application in combination with ROS1 to do inference with a YOLOv5 custom model on ROS1 image topics. I have a working implementation of the same model with gRPC mode so the model and the config are correct. When I send the normalized image to the triton, it gives me the following error. I tried googling this error but cannot make sense of what exactly is the problem here. Some insights would help a lot. \r\n```bash\r\nI0609 12:54:24.923918 21492 libtorch.cc:1381] TRITONBACKEND_Initialize: pytorch\r\nI0609 12:54:24.923949 21492 libtorch.cc:1391] Triton TRITONBACKEND API version: 1.9\r\nI0609 12:54:24.923953 21492 libtorch.cc:1397] 'pytorch' TRITONBACKEND API version: 1.9\r\n2022-06-09 12:54:29.035423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nI0609 12:54:29.076962 21492 tensorflow.cc:2181] TRITONBACKEND_Initialize: tensorflow\r\nI0609 12:54:29.076984 21492 tensorflow.cc:2191] Triton TRITONBACKEND API version: 1.9\r\nI0609 12:54:29.076989 21492 tensorflow.cc:2197] 'tensorflow' TRITONBACKEND API version: 1.9\r\nI0609 12:54:29.076992 21492 tensorflow.cc:2221] backend configuration:\r\n{}\r\nI0609 12:54:29.186855 21492 onnxruntime.cc:2400] TRITONBACKEND_Initialize: onnxruntime\r\nI0609 12:54:29.186876 21492 onnxruntime.cc:2410] Triton TRITONBACKEND API version: 1.9\r\nI0609 12:54:29.186880 21492 onnxruntime.cc:2416] 'onnxruntime' TRITONBACKEND API version: 1.9\r\nI0609 12:54:29.186884 21492 onnxruntime.cc:2446] backend configuration:\r\n{}\r\nI0609 12:54:29.236464 21492 openvino.cc:1207] TRITONBACKEND_Initialize: openvino\r\nI0609 12:54:29.236483 21492 openvino.cc:1217] Triton TRITONBACKEND API version: 1.9\r\nI0609 12:54:29.236488 21492 openvino.cc:1223] 'openvino' TRITONBACKEND API version: 1.9\r\nI0609 12:54:30.318676 21492 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fbd36000000' with size 268435456\r\nI0609 12:54:30.319079 21492 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\nI0609 12:54:30.319094 21492 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\r\nW0609 12:54:31.182282 21492 server.cc:206] failed to enable peer access for some device pairs\r\nI0609 12:54:31.184446 21492 model_repository_manager.cc:1077] loading: YOLOv5nCOCO:1\r\nI0609 12:54:31.284776 21492 model_repository_manager.cc:1077] loading: YOLOv5nCROP:1\r\nI0609 12:54:31.284788 21492 onnxruntime.cc:2481] TRITONBACKEND_ModelInitialize: YOLOv5nCOCO (version 1)\r\nI0609 12:54:31.285700 21492 onnxruntime.cc:2524] TRITONBACKEND_ModelInstanceInitialize: YOLOv5nCOCO (GPU device 0)\r\nI0609 12:54:31.386580 21492 model_repository_manager.cc:1077] loading: FCOS_detectron:1\r\nI0609 12:54:32.209536 21492 onnxruntime.cc:2481] TRITONBACKEND_ModelInitialize: YOLOv5nCROP (version 1)\r\nI0609 12:54:32.209978 21492 libtorch.cc:1430] TRITONBACKEND_ModelInitialize: FCOS_detectron (version 1)\r\nI0609 12:54:32.210246 21492 libtorch.cc:293] Optimized execution is enabled for model instance 'FCOS_detectron'\r\nI0609 12:54:32.210254 21492 libtorch.cc:311] Inference Mode is disabled for model instance 'FCOS_detectron'\r\nI0609 12:54:32.210258 21492 libtorch.cc:406] NvFuser is not specified for model instance 'FCOS_detectron'\r\nI0609 12:54:32.210272 21492 onnxruntime.cc:2524] TRITONBACKEND_ModelInstanceInitialize: YOLOv5nCOCO (GPU device 1)\r\nI0609 12:54:33.016676 21492 onnxruntime.cc:2524] TRITONBACKEND_ModelInstanceInitialize: YOLOv5nCROP (GPU device 0)\r\nI0609 12:54:33.017054 21492 model_repository_manager.cc:1231] successfully loaded 'YOLOv5nCOCO' version 1\r\nI0609 12:54:33.093639 21492 libtorch.cc:1474] TRITONBACKEND_ModelInstanceInitialize: FCOS_detectron (GPU device 0)\r\nI0609 12:54:33.450545 21492 onnxruntime.cc:2524] TRITONBACKEND_ModelInstanceInitialize: YOLOv5nCROP (GPU device 1)\r\nI0609 12:54:33.503772 21492 libtorch.cc:1474] TRITONBACKEND_ModelInstanceInitialize: FCOS_detectron (GPU device 1)\r\nI0609 12:54:33.504063 21492 model_repository_manager.cc:1231] successfully loaded 'YOLOv5nCROP' version 1\r\nI0609 12:54:33.851533 21492 model_repository_manager.cc:1231] successfully loaded 'FCOS_detectron' version 1\r\nI0609 12:54:33.851605 21492 server.cc:549] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0609 12:54:33.851659 21492 server.cc:576] \r\n+-------------+-------------------------------------------------------------------------+--------+\r\n| Backend     | Path                                                                    | Config |\r\n+-------------+-------------------------------------------------------------------------+--------+\r\n| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so                 | {}     |\r\n| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so         | {}     |\r\n| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so         | {}     |\r\n| openvino    | /opt/tritonserver/backends/openvino_2021_4/libtriton_openvino_2021_4.so | {}     |\r\n+-------------+-------------------------------------------------------------------------+--------+\r\n\r\nI0609 12:54:33.851703 21492 server.cc:619] \r\n+----------------+---------+--------+\r\n| Model          | Version | Status |\r\n+----------------+---------+--------+\r\n| FCOS_detectron | 1       | READY  |\r\n| YOLOv5nCOCO    | 1       | READY  |\r\n| YOLOv5nCROP    | 1       | READY  |\r\n+----------------+---------+--------+\r\n\r\nI0609 12:54:33.892306 21492 metrics.cc:650] Collecting metrics for GPU 0: NVIDIA GeForce RTX 3070\r\nI0609 12:54:33.892335 21492 metrics.cc:650] Collecting metrics for GPU 1: NVIDIA GeForce RTX 3070\r\nI0609 12:54:33.892721 21492 tritonserver.cc:2123] \r\n+----------------------------------+--------------------------------------------------------------------------+\r\n| Option                           | Value                                                                    |\r\n+----------------------------------+--------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                   |\r\n| server_version                   | 2.21.0                                                                   |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_depende |\r\n|                                  | nts) schedule_policy model_configuration system_shared_memory cuda_share |\r\n|                                  | d_memory binary_tensor_data statistics trace                             |\r\n| model_repository_path[0]         | /opt/model_repo/                                                         |\r\n| model_control_mode               | MODE_POLL                                                                |\r\n| strict_model_config              | 1                                                                        |\r\n| rate_limit                       | OFF                                                                      |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                |\r\n| cuda_memory_pool_byte_size{0}    | 67108864                                                                 |\r\n| cuda_memory_pool_byte_size{1}    | 67108864                                                                 |\r\n| response_cache_byte_size         | 0                                                                        |\r\n| min_supported_compute_capability | 7.5                                                                      |\r\n| strict_readiness                 | 1                                                                        |\r\n| exit_timeout                     | 30                                                                       |\r\n+----------------------------------+--------------------------------------------------------------------------+\r\n\r\nServer Health: live 1, ready 1\r\nServer Metadata:\r\n{\"name\":\"triton\",\"version\":\"2.21.0\",\"extensions\":[\"classification\",\"sequence\",\"model_repository\",\"model_repository(unload_dependents)\",\"schedule_policy\",\"model_configuration\",\"system_shared_memory\",\"cuda_shared_memory\",\"binary_tensor_data\",\"statistics\",\"trace\"]}\r\n2022-06-09 12:54:35.981968507 [E:onnxruntime:log, cuda_call.cc:118 CudaCall] CUDNN failure 4: CUDNN_STATUS_INTERNAL_ERROR ; GPU=0 ; hostname=agrigaia-ws3-u ; expr=cudnnFindConvolutionForwardAlgorithmEx( s_.handle, s_.x_tensor, s_.x_data, s_.w_desc, s_.w_data, s_.conv_desc, s_.y_tensor, s_.y_data, 1, &algo_count, &perf, algo_search_workspace.get(), max_ws_size); \r\n2022-06-09 12:54:35.981998167 [E:onnxruntime:, sequential_executor.cc:346 Execute] Non-zero status code returned while running Conv node. Name:'Conv_0' Status Message: CUDNN error executing cudnnFindConvolutionForwardAlgorithmEx( s_.handle, s_.x_tensor, s_.x_data, s_.w_desc, s_.w_data, s_.conv_desc, s_.y_tensor, s_.y_data, 1, &algo_count, &perf, algo_search_workspace.get(), max_ws_size)\r\n2022-06-09 12:54:35.982022932 [E:onnxruntime:log, cuda_call.cc:118 CudaCall] CUDA failure 700: an illegal memory access was encountered ; GPU=0 ; hostname=agrigaia-ws3-u ; expr=cudaEventRecord(current_deferred_release_event, static_cast<cudaStream_t>(GetComputeStream())); \r\nerror: response status: Internal - onnx runtime error 1: Non-zero status code returned while running Conv node. Name:'Conv_0' Status Message: CUDNN error executing cudnnFindConvolutionForwardAlgorithmEx( s_.handle, s_.x_tensor, s_.x_data, s_.w_desc, s_.w_data, s_.conv_desc, s_.y_tensor, s_.y_data, 1, &algo_count, &perf, algo_search_workspace.get(), max_ws_size)\r\n```\r\n**TRITON information**\r\nI am working inside a docker dev container built on Triton-22.04 base image with **ROS1 Noetic** and **Opencv 4.2.0** installed on top of it. \r\n**nvcr.io/nvidia/tritonserver:22.04-py3**\r\nUbuntu 20.04 \r\nRos Noetic \r\nOpenCV 4.2.0\r\nModel config file: \r\n```bash\r\nname: \"YOLOv5nCOCO\"\r\nplatform: \"onnxruntime_onnx\"\r\nmax_batch_size : 0\r\ninput [\r\n  {\r\n    name: \"images\"\r\n    data_type: TYPE_FP32\r\n    format: FORMAT_NCHW\r\n    dims: [ 3, 512, 512 ]\r\n    reshape { shape: [ 1, 3, 512, 512 ] }\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"output\"\r\n    data_type: TYPE_FP32\r\n    dims: [1, 16128, 85]\r\n  }\r\n]\r\n\r\n```\r\n**Steps to reproduce**\r\nI have written my main.cpp file based on your [simple.cc](https://github.com/triton-inference-server/server/blob/main/src/simple.cc) example file. Unfortunately, I cannot share the full project\r\nHere are my preprocessing steps from receiving ros image topic till triton: \r\n```cpp\r\ncv_ptr = cv_bridge::toCvCopy(msg, \"rgb8\");\r\n      \r\n// Store the values of the OpenCV-compatible image into the current_frame variable\r\ncv::Mat current_frame = cv_ptr->image;\r\n\r\n// Preprocessing of the image\r\n// normalize the image and convert to \r\ncurrent_frame.convertTo(current_frame, CV_32F, 1.0/255.0, 0);\r\n//resize the image to model input size \r\ncv::resize(current_frame, current_frame, cv::Size(512, 512), cv::INTER_LINEAR);\r\n// NCHW channels first\r\n// cv::transpose(current_frame, current_frame);\r\n\r\n// Convert Mat to Array/Vector in OpenCV https://stackoverflow.com/a/26685567\r\nstd::vector<float> input_data;\r\nif (current_frame.isContinuous()) {\r\n  input_data.assign(current_frame.data, \r\n    current_frame.data + current_frame.total()*current_frame.channels());\r\n} else {\r\n  for (int i = 0; i < current_frame.rows; ++i) {\r\n    input_data.insert(input_data.end(), current_frame.ptr<float>(i), \r\n    current_frame.ptr<float>(i) + current_frame.cols * current_frame.channels());\r\n  }\r\n}\r\n\r\nauto input = \"images\";\r\nauto output = \"output\";                                                \r\nsize_t input_size = input_data.size() * sizeof(float);                                \r\nconst TRITONSERVER_DataType input_datatype = TRITONSERVER_TYPE_FP32;  \r\nstd::vector<int64_t> input_shape({current_frame.channels(), current_frame.rows, current_frame.cols}); \r\nconst void* input_base = &input_data[0];\r\n\r\n// Push data into Triton format\r\nFAIL_IF_ERR(\r\n    TRITONSERVER_InferenceRequestAddInput(\r\n        irequest, input, input_datatype, &input_shape[0], input_shape.size()),\r\n    \"setting input meta-data for the request\");\r\n\r\nFAIL_IF_ERR(\r\n    TRITONSERVER_InferenceRequestAppendInputData(\r\n        irequest, input, input_base, input_size, requested_memory_type,\r\n        0 /* memory_type_id */),\r\n    \"assigning INPUT data\");\r\nFAIL_IF_ERR(\r\n      TRITONSERVER_InferenceRequestAddRequestedOutput(irequest, output),\r\n      \"requesting output for the request\");\r\n\r\n// Triton connection\r\nauto p = new std::promise<TRITONSERVER_InferenceResponse*>();\r\nstd::future<TRITONSERVER_InferenceResponse*> completed = p->get_future();\r\n\r\nFAIL_IF_ERR(\r\n    TRITONSERVER_InferenceRequestSetResponseCallback(\r\n        irequest, allocator, nullptr /* response_allocator_userp */,\r\n        InferResponseComplete, reinterpret_cast<void*>(p)),\r\n    \"setting response callback\");\r\n\r\n\r\n\r\nFAIL_IF_ERR(TRITONSERVER_ServerInferAsync(server->get(), irequest, nullptr /* trace */),\"running inference\");\r\n\r\nTRITONSERVER_InferenceResponse* completed_response = completed.get();\r\n\r\nFAIL_IF_ERR(TRITONSERVER_InferenceResponseError(completed_response),\"response status\");\r\n//<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Here is the error\r\n```\r\nI tried running the same triton server from its executable and my model repository over a gRPC call to check if the ONNX export is correct. Everything works smoothly via gRPC. Some google links suggested that it might be an issue on RTX 20 series but I also reproduced the same error on RTX 3070. I can narrow it down to something wrong with my input image data but the error handling does not really specify the problem. Some insights would help a lot where to look exactly. \r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4505/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4491", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4491/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4491/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4491/events", "html_url": "https://github.com/triton-inference-server/server/issues/4491", "id": 1264712566, "node_id": "I_kwDOCQnI4s5LYft2", "number": 4491, "title": "Triton server stops while making Async request from multiple threads", "user": {"login": "InfiniteLife", "id": 7304884, "node_id": "MDQ6VXNlcjczMDQ4ODQ=", "avatar_url": "https://avatars.githubusercontent.com/u/7304884?v=4", "gravatar_id": "", "url": "https://api.github.com/users/InfiniteLife", "html_url": "https://github.com/InfiniteLife", "followers_url": "https://api.github.com/users/InfiniteLife/followers", "following_url": "https://api.github.com/users/InfiniteLife/following{/other_user}", "gists_url": "https://api.github.com/users/InfiniteLife/gists{/gist_id}", "starred_url": "https://api.github.com/users/InfiniteLife/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/InfiniteLife/subscriptions", "organizations_url": "https://api.github.com/users/InfiniteLife/orgs", "repos_url": "https://api.github.com/users/InfiniteLife/repos", "events_url": "https://api.github.com/users/InfiniteLife/events{/privacy}", "received_events_url": "https://api.github.com/users/InfiniteLife/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2022-06-08T13:03:20Z", "updated_at": "2022-06-16T08:54:25Z", "closed_at": "2022-06-16T08:53:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "Whenever I make inference request from 2 threads with `AsyncInfer`, after like 16-20 requests I get error from client:\r\n\r\n```\r\nSocket closed\r\n\r\nInference unsuccessful Connection reset by peer\r\ninference  failed with error: Connection reset by peer\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nand Triton server is also closes:\r\n\r\n```\r\nterminate called after throwing an instance of 'c10::ValueError'\r\n  what():  Specified device cuda:0 does not match device of data cuda:-2\r\nException raised from make_tensor at /opt/pytorch/pytorch/build/aten/src/ATen/Functions.cpp:24 (most recent call first):\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6c (0x7f9954f71ecc in /opt/tritonserver/backends/pytorch/libc10.so)\r\nframe #1: <unknown function> + 0xa7474f (0x7f998c74b74f in /opt/tritonserver/backends/pytorch/libtorch_cpu.so)\r\nframe #2: <unknown function> + 0x1c787 (0x7f9995ba5787 in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so)\r\nframe #3: <unknown function> + 0x20386 (0x7f9995ba9386 in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so)\r\nframe #4: TRITONBACKEND_ModelInstanceExecute + 0x38a (0x7f9995bab4aa in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so)\r\nframe #5: <unknown function> + 0x31207a (0x7f9997aed07a in /opt/tritonserver/bin/../lib/libtritonserver.so)\r\nframe #6: <unknown function> + 0x312797 (0x7f9997aed797 in /opt/tritonserver/bin/../lib/libtritonserver.so)\r\nframe #7: <unknown function> + 0x1a5221 (0x7f9997980221 in /opt/tritonserver/bin/../lib/libtritonserver.so)\r\nframe #8: <unknown function> + 0x30c607 (0x7f9997ae7607 in /opt/tritonserver/bin/../lib/libtritonserver.so)\r\nframe #9: <unknown function> + 0xd6de4 (0x7f9997331de4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\r\nframe #10: <unknown function> + 0x8609 (0x7f99977ae609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #11: clone + 0x43 (0x7f999701c163 in /usr/lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nSignal (6) received.\r\n 0# 0x000055DC36B10549 in tritonserver\r\n 1# 0x00007F9996F400C0 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# gsignal in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 3# abort in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 4# 0x00007F99972F9911 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 5# 0x00007F999730538C in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 6# 0x00007F99973053F7 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 7# 0x00007F99973056A9 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 8# 0x00007F998C74B78F in /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n 9# 0x00007F9995BA5787 in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n10# 0x00007F9995BA9386 in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n11# TRITONBACKEND_ModelInstanceExecute in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n12# 0x00007F9997AED07A in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n13# 0x00007F9997AED797 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n14# 0x00007F9997980221 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n15# 0x00007F9997AE7607 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n16# 0x00007F9997331DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n17# 0x00007F99977AE609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n18# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n\r\nSignal (11) received.\r\n 0# 0x000055DC36B10549 in tritonserver\r\n 1# 0x00007F9996F400C0 in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 2# abort in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n 3# 0x00007F99972F9911 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 4# 0x00007F999730538C in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 5# 0x00007F99973053F7 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 6# 0x00007F99973056A9 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 7# 0x00007F998C74B78F in /opt/tritonserver/backends/pytorch/libtorch_cpu.so\r\n 8# 0x00007F9995BA5787 in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n 9# 0x00007F9995BA9386 in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n10# TRITONBACKEND_ModelInstanceExecute in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\n11# 0x00007F9997AED07A in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n12# 0x00007F9997AED797 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n13# 0x00007F9997980221 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n14# 0x00007F9997AE7607 in /opt/tritonserver/bin/../lib/libtritonserver.so\r\n15# 0x00007F9997331DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n16# 0x00007F99977AE609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n17# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n```\r\n\r\nIt works fine if I make inference request from one thread.\r\nModel that is supposed to make inference is torchscript with `max_batch_size 128`, placed on 2 GPUs. ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4491/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4456", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4456/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4456/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4456/events", "html_url": "https://github.com/triton-inference-server/server/issues/4456", "id": 1258492133, "node_id": "I_kwDOCQnI4s5LAxDl", "number": 4456, "title": "Cannot load model from GCS when LD_PRELOAD env var is set", "user": {"login": "lross68", "id": 53309531, "node_id": "MDQ6VXNlcjUzMzA5NTMx", "avatar_url": "https://avatars.githubusercontent.com/u/53309531?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lross68", "html_url": "https://github.com/lross68", "followers_url": "https://api.github.com/users/lross68/followers", "following_url": "https://api.github.com/users/lross68/following{/other_user}", "gists_url": "https://api.github.com/users/lross68/gists{/gist_id}", "starred_url": "https://api.github.com/users/lross68/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lross68/subscriptions", "organizations_url": "https://api.github.com/users/lross68/orgs", "repos_url": "https://api.github.com/users/lross68/repos", "events_url": "https://api.github.com/users/lross68/events{/privacy}", "received_events_url": "https://api.github.com/users/lross68/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2022-06-02T17:39:32Z", "updated_at": "2022-07-05T22:25:41Z", "closed_at": "2022-07-05T22:25:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI am trying to deploy the [universal sentence encoder](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3) using the method described in [this issue comment](https://github.com/triton-inference-server/server/issues/3604#issuecomment-982125998), which requires the `LD_PRELOAD` trick. When the environment variable `LD_PRELOAD` is set, triton fails to load the model from the GCS bucket:\r\n```\r\n=============================\r\n== Triton Inference Server ==\r\n=============================\r\n\r\nNVIDIA Release 22.05 (build 38317651)\r\nTriton Server Version 2.22.0\r\n\r\nCopyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n   Use the NVIDIA Container Toolkit to start this container with GPU support; see\r\n   https://docs.nvidia.com/datacenter/cloud-native/ .\r\n\r\nW0602 16:34:04.094388 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\nI0602 16:34:04.094496 1 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\n\r\nI0602 16:49:09.045304 1 tritonserver.cc:2138]\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                                                                        |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                                                                       |\r\n| server_version                   | 2.22.0                                                                                                                                                                                       |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\r\n| model_repository_path[0]         | gs://my_bucket/model_repository                                                                                                                                                          |\r\n| model_control_mode               | MODE_NONE                                                                                                                                                                                    |\r\n| strict_model_config              | 0                                                                                                                                                                                            |\r\n| rate_limit                       | OFF                                                                                                                                                                                          |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\r\n| response_cache_byte_size         | 0                                                                                                                                                                                            |\r\n| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\r\n| strict_readiness                 | 1                                                                                                                                                                                            |\r\n| exit_timeout                     | 30                                                                                                                                                                                           |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0602 16:49:09.045362 1 server.cc:254] No server context available. Exiting immediately.\r\nerror: creating server: Internal - Could not get MetaData for bucket with name my_bucket: Retry policy exhausted in GetBucketMetadata: EasyPerform() - CURL error [35]=SSL connect error [UNAVAILABLE]\r\n```\r\nIf I simply unset the `LD_PRELOAD` environment variable, then triton successfully loads the model from the GCS bucket, but inference requests yield `Op type not registered 'SentencepieceOp' in binary...` as expected and described in [the issue](https://github.com/triton-inference-server/server/issues/3604) for the above issue comment. Furthermore, if I copy the model repository to my local filesystem and set the `LD_PRELOAD` env var then I can load the model and successfully serve requests.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n22.05\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nBasing off the official image. My Dockerfile:\r\n```\r\n# Base off the nvidia base image\r\nARG TRITON_VERSION=22.05-py3\r\nFROM nvcr.io/nvidia/tritonserver:${TRITON_VERSION}\r\n\r\n# Install tf text package\r\nARG TF_TEXT_VERSION=2.8.*\r\nRUN pip install tensorflow-text==${TF_TEXT_VERSION}\r\n\r\n# Update the LD_LIBRARY_PATH environment variable\r\nARG ADD_LD_LIBRARY_PATH=/opt/tritonserver/backends/tensorflow2\r\nENV LD_LIBRARY_PATH=${ADD_LD_LIBRARY_PATH}:${LD_LIBRARY_PATH}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\nDownload the model:\r\n```\r\nmkdir -p model_repository/muse/1/model.savedmodel\r\nwget https://tfhub.dev/google/universal-sentence-encoder-multilingual/3?tf-hub-format=compressed -O model_repository/muse/1/model.savedmodel/data.tar.gz\r\ntar -xvf model_repository/muse/1/model.savedmodel/data.tar.gz - model_repository/muse/1/model.savedmodel/\r\n```\r\n\r\nAdd the config to `model_repository/muse/config.pbtxt`:\r\n```\r\nname: \"muse\"\r\nplatform: \"tensorflow_savedmodel\"\r\nmax_batch_size: 0\r\ninput [\r\n    {\r\n        name: \"inputs\"\r\n        data_type: TYPE_STRING\r\n        dims: [-1]\r\n    }\r\n]\r\noutput [\r\n    {\r\n        name: \"outputs\"\r\n        data_type: TYPE_FP32\r\n        dims: [-1, 512]\r\n    }\r\n]\r\n```\r\n\r\nCopy the model to a GCS bucket:\r\n```\r\ngsutil cp -r model_repository/muse/ <BUCKET_NAME>\r\n```\r\n\r\nRun Triton:\r\n```\r\ndocker run -t -p 8000:8000 --rm  \\\r\n  -v $(pwd):/workspace \\\r\n  --name=tritonserver \\\r\n  -e AIP_MODE=True \\\r\n  -e LD_PRELOAD=/usr/local/lib/python3.8/dist-packages/tensorflow_text/python/ops/_sentencepiece_tokenizer.so \\\r\n  -e GOOGLE_APPLICATION_CREDENTIALS=/workspace/gcs_creds.json \\\r\n  <IMAGE_NAME> \\\r\n  --model-repository gs://my_bucket/model_repository \\\r\n  --strict-model-config=false \\\r\n  --log-verbose=1 \\\r\n  --backend-config=tensorflow,version=2\r\n```\r\nIn my case, I need my google credentials in the current working directory at `gcs_creds.json`.\r\n\r\n**Expected behavior**\r\nShould be able to load the universal sentence encoder from a GCS bucket and correctly serve requests\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4456/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4456/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4405", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4405/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4405/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4405/events", "html_url": "https://github.com/triton-inference-server/server/issues/4405", "id": 1241391661, "node_id": "I_kwDOCQnI4s5J_iIt", "number": 4405, "title": "Shape does not match with data while using kserve http protocol BYTES data type ", "user": {"login": "xyangk", "id": 9495054, "node_id": "MDQ6VXNlcjk0OTUwNTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/9495054?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xyangk", "html_url": "https://github.com/xyangk", "followers_url": "https://api.github.com/users/xyangk/followers", "following_url": "https://api.github.com/users/xyangk/following{/other_user}", "gists_url": "https://api.github.com/users/xyangk/gists{/gist_id}", "starred_url": "https://api.github.com/users/xyangk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xyangk/subscriptions", "organizations_url": "https://api.github.com/users/xyangk/orgs", "repos_url": "https://api.github.com/users/xyangk/repos", "events_url": "https://api.github.com/users/xyangk/events{/privacy}", "received_events_url": "https://api.github.com/users/xyangk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-05-19T08:06:09Z", "updated_at": "2022-07-14T20:59:59Z", "closed_at": "2022-07-14T20:59:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nAs describe at [kserve_api](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#tensor-data), 2-dimensional matrix cat be natural format or one-dimensional, that works fine while datatype is INT32\uff0cbut It's different while datatype is BYTES. My triton model support batching, If I want to send data like 'a' and 'b' with shape [2,1], `['a', 'b']`  and  `[['a', 'b']]` works fine, but `[['a'],['b']]` failed.\r\n\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nTriton container r22.04\r\n\r\n**To Reproduce**\r\nmodel config.pbtxt :\r\n```\r\nname: \"tokenizer\"\r\nbackend: \"python\"\r\nmax_batch_size: 64\r\ninput [\r\n{\r\n\tname: \"inputs\"\r\n\tdata_type: TYPE_STRING\r\n\tdims: [ 1 ]\r\n}\r\n]\r\noutput [\r\n{\r\n\tname: \"input_ids\"\r\n\tdata_type: TYPE_INT32\r\n\tdims: [ 100 ]\r\n}\r\n]\r\n```\r\nThese data format works:\r\nflattened one-dimensional :\r\n```\r\ndata = {\r\n  \"id\" : \"42\",\r\n  \"inputs\" : [\r\n    {\r\n      \"name\" : \"inputs\",\r\n      \"shape\" : [ 2,1 ],\r\n      \"datatype\" : \"BYTES\",\r\n      \"data\" : [\"a\",\"b\"]\r\n    }\r\n  ],\r\n  \"outputs\" : [\r\n    {\r\n      \"name\" : \"input_ids\"\r\n    }\r\n  ]\r\n}\r\n\r\nresp = requests.post(\"http://127.0.0.1:8000/v2/models/tokenizer/infer\", json=data)\r\nresp.json()\r\n\r\n# result\r\n{\"id\":\"42\",\r\n\"model_name\":\"tokenizer\",\r\n\"model_version\":\"1\",\r\n\"outputs\":[{\r\n    \"name\":\"input_ids\",\r\n    \"datatype\":\"INT32\",\r\n    \"shape\":[2,100],\r\n    \"data\":[2217,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8306,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\r\n    }]\r\n}\r\n```\r\nmulti-dimensional  but data true shape is [1,2]:\r\n```\r\ndata = {\r\n  \"id\" : \"42\",\r\n  \"inputs\" : [\r\n    {\r\n      \"name\" : \"inputs\",\r\n      \"shape\" : [ 2,1 ],\r\n      \"datatype\" : \"BYTES\",\r\n      \"data\" : [[\"a\",\"b\"]]\r\n    }\r\n  ],\r\n  \"outputs\" : [\r\n    {\r\n      \"name\" : \"input_ids\"\r\n    }\r\n  ]\r\n}\r\n\r\nresp = requests.post(\"http://127.0.0.1:8000/v2/models/tokenizer/infer\", json=data)\r\nresp.json()\r\n\r\n# result\r\n{'id': '42','model_name': \r\n'tokenizer',\r\n'model_version': '1',\r\n'outputs': [{\r\n    'name': 'input_ids',  \r\n    'datatype': 'INT32',  \r\n    'shape': [2, 100],  \r\n    'data': [2217,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8306,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\r\n    }]\r\n}\r\n```\r\nThis does not wok:\r\nmulti-dimensional  match data true shape :\r\n```\r\ndata = {\r\n  \"id\" : \"42\",\r\n  \"inputs\" : [\r\n    {\r\n      \"name\" : \"inputs\",\r\n      \"shape\" : [ 2,1 ],\r\n      \"datatype\" : \"BYTES\",\r\n      \"data\" : [[\"a\"],[\"b\"]]\r\n    }\r\n  ],\r\n  \"outputs\" : [\r\n    {\r\n      \"name\" : \"input_ids\"\r\n    }\r\n  ]\r\n}\r\n\r\nresp = requests.post(\"http://127.0.0.1:8000/v2/models/tokenizer/infer\", json=data)\r\nresp.json()\r\n\r\n# result\r\n{'error': \"Unable to parse 'data': Shape does not match true shape of 'data' field\"}\r\n```\r\n\r\n\r\n**Expected behavior**\r\nI am a newbie using Trton, the tokenizer serve is going to ensemble with a pytorch encoder, please figure me out if I miss something. \r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4405/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4379", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4379/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4379/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4379/events", "html_url": "https://github.com/triton-inference-server/server/issues/4379", "id": 1237013134, "node_id": "I_kwDOCQnI4s5Ju1KO", "number": 4379, "title": "Response cache is not working well", "user": {"login": "danielwonght", "id": 16788996, "node_id": "MDQ6VXNlcjE2Nzg4OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/16788996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielwonght", "html_url": "https://github.com/danielwonght", "followers_url": "https://api.github.com/users/danielwonght/followers", "following_url": "https://api.github.com/users/danielwonght/following{/other_user}", "gists_url": "https://api.github.com/users/danielwonght/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielwonght/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielwonght/subscriptions", "organizations_url": "https://api.github.com/users/danielwonght/orgs", "repos_url": "https://api.github.com/users/danielwonght/repos", "events_url": "https://api.github.com/users/danielwonght/events{/privacy}", "received_events_url": "https://api.github.com/users/danielwonght/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-05-16T11:27:55Z", "updated_at": "2022-08-03T21:44:38Z", "closed_at": "2022-08-03T21:44:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nA clear and concise description of what the bug is.\r\nI turned on the response cache by setting the config as _response_cache {enable: True}_. When the request input is repeatedly the same, everything goes alright and the Inference/second can reach 25000++ while the concurrency is set to 3 (by using **Performance Analyze**).[1] But, when the input is randomly generated for each input, the performance of the Triton server is getting worse along the time from 600 inference/second down to 100 inference/second.[2] It's like the server spends increasingly more time on dealing with LRU cache stuff.\r\n[offical instruction](https://github.com/triton-inference-server/server/blob/main/docs/response_cache.md) shows that _\"Only input tensors located in CPU memory will be hashable for accessing the cache\"_. My model is deployed using GPU. According to [1], I think, the cache is working, but not in the right way, proven by [2].\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nv2.17.0 (with container version 21.12)\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTriton container.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n1. Download a whatever BERT pre-trained model.\r\n2. Convert the model into onnx format by using _torch.onnx.export_.\r\n3. Deploy by the Triton server.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n> name: \"bert\"\r\n> platform: \"onnxruntime_onnx\"\r\n> max_batch_size : 8\r\n> input [\r\n>   {\r\n>     name: \"input_ids\"\r\n>     data_type: TYPE_INT64\r\n>     dims: [ -1 ]\r\n>   },\r\n>   {\r\n>     name: \"token_type_ids\"\r\n>     data_type: TYPE_INT64\r\n>     dims: [ -1 ]\r\n>   },\r\n>   {\r\n>     name: \"attention_mask\"\r\n>     data_type: TYPE_INT64\r\n>     dims: [ -1 ]\r\n>   }\r\n> ]\r\n> output [\r\n>   {\r\n>     name: \"pooler\"\r\n>     data_type: TYPE_FP32\r\n>     dims: [-1]\r\n>   }\r\n> ]\r\n> dynamic_batching {}\r\n> instance_group [\r\n>     {\r\n>         count: 1\r\n>         kind: KIND_GPU\r\n>         gpus: [ 0 ]\r\n>     }\r\n> ]\r\n> response_cache {\r\n>   enable: True\r\n> }\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nIdeally, the **Response Cache** may affect the model performance subtly, when the input is varying all the time. ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4379/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4379/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4346", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4346/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4346/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4346/events", "html_url": "https://github.com/triton-inference-server/server/issues/4346", "id": 1227721782, "node_id": "I_kwDOCQnI4s5JLYw2", "number": 4346, "title": "Loading ONNX model fails because of insufficient CUDA driver version", "user": {"login": "janjagusch", "id": 25852654, "node_id": "MDQ6VXNlcjI1ODUyNjU0", "avatar_url": "https://avatars.githubusercontent.com/u/25852654?v=4", "gravatar_id": "", "url": "https://api.github.com/users/janjagusch", "html_url": "https://github.com/janjagusch", "followers_url": "https://api.github.com/users/janjagusch/followers", "following_url": "https://api.github.com/users/janjagusch/following{/other_user}", "gists_url": "https://api.github.com/users/janjagusch/gists{/gist_id}", "starred_url": "https://api.github.com/users/janjagusch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/janjagusch/subscriptions", "organizations_url": "https://api.github.com/users/janjagusch/orgs", "repos_url": "https://api.github.com/users/janjagusch/repos", "events_url": "https://api.github.com/users/janjagusch/events{/privacy}", "received_events_url": "https://api.github.com/users/janjagusch/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-05-06T11:06:16Z", "updated_at": "2022-05-11T23:50:24Z", "closed_at": "2022-05-11T23:49:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nI'm trying to load the [densenet_onnx](https://github.com/triton-inference-server/server/tree/main/docs/examples/model_repository/densenet_onnx) example into Trition v22.04.\r\n\r\nUpon startup, I get the following error message:\r\n\r\n```\r\n# Truncated for readability, full traceback below ...\r\nI0506 10:50:30.170242 1 server.cc:619]\r\n+---------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Model         | Version | Status                                                                                                                                            |\r\n+---------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| densenet_onnx | 1       | UNAVAILABLE: Internal: onnx runtime error 1: /workspace/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:122 bool onnxruntime::CudaCall(E |\r\n|               |         | RRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /workspace/onnxruntime/onnxruntime/core/prov |\r\n|               |         | iders/cuda/cuda_call.cc:116 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool T |\r\n|               |         | HRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32539 ; hostname=3d35a7f61dd1 ; expr=cudaSetDevic |\r\n|               |         | e(info_.device_id);                                                                                                                               |\r\n|               |         |                                                                                                                                                   |\r\n+---------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------+\r\n```\r\n\r\nI don't want to run my model on the GPU and hence believe that I don't need CUDA drivers in the first place. Is it possible to use an ONNX runtime version that only uses the CPU and hence doesn't call CUDA?\r\n\r\n**Triton Information**\r\n\r\nI'm using `nvcr.io/nvidia/tritonserver:22.04-py3`.\r\n\r\n**To Reproduce**\r\n\r\nIn an empty directory:\r\n\r\n```\r\nmkdir -p model_repository/densenet_onnx/1\r\nwget -O model_repository/densenet_onnx/1/model.onnx \\\r\n     https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx\r\n\r\ndocker run \\\r\n    --interactive \\\r\n    --tty \\\r\n    --shm-size 1500000000 \\\r\n    --rm -p8000:8000 -p8001:8001 -p8002:8002 -v $(pwd)/model_repository:/models \\\r\n    nvcr.io/nvidia/tritonserver:22.04-py3 tritonserver \\\r\n        --model-repository=/models \\\r\n        --strict-model-config=false\r\n```\r\n\r\n<details>\r\n  <summary>Full traceback</summary>\r\n  \r\n  ```\r\n=============================\r\n== Triton Inference Server ==\r\n=============================\r\n\r\nNVIDIA Release 22.04 (build 36821869)\r\nTriton Server Version 2.21.0\r\n\r\nCopyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n   Use the NVIDIA Container Toolkit to start this container with GPU support; see\r\n   https://docs.nvidia.com/datacenter/cloud-native/ .\r\n\r\nWARNING: [Torch-TensorRT] - Unable to read CUDA capable devices. Return status: 35\r\nI0506 10:59:05.291016 1 libtorch.cc:1381] TRITONBACKEND_Initialize: pytorch\r\nI0506 10:59:05.291274 1 libtorch.cc:1391] Triton TRITONBACKEND API version: 1.9\r\nI0506 10:59:05.291290 1 libtorch.cc:1397] 'pytorch' TRITONBACKEND API version: 1.9\r\n2022-05-06 10:59:05.457743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nI0506 10:59:05.501305 1 tensorflow.cc:2181] TRITONBACKEND_Initialize: tensorflow\r\nI0506 10:59:05.501395 1 tensorflow.cc:2191] Triton TRITONBACKEND API version: 1.9\r\nI0506 10:59:05.501417 1 tensorflow.cc:2197] 'tensorflow' TRITONBACKEND API version: 1.9\r\nI0506 10:59:05.501436 1 tensorflow.cc:2221] backend configuration:\r\n{}\r\nI0506 10:59:05.503125 1 onnxruntime.cc:2400] TRITONBACKEND_Initialize: onnxruntime\r\nI0506 10:59:05.503202 1 onnxruntime.cc:2410] Triton TRITONBACKEND API version: 1.9\r\nI0506 10:59:05.503231 1 onnxruntime.cc:2416] 'onnxruntime' TRITONBACKEND API version: 1.9\r\nI0506 10:59:05.503252 1 onnxruntime.cc:2446] backend configuration:\r\n{}\r\nI0506 10:59:05.518030 1 openvino.cc:1207] TRITONBACKEND_Initialize: openvino\r\nI0506 10:59:05.518106 1 openvino.cc:1217] Triton TRITONBACKEND API version: 1.9\r\nI0506 10:59:05.518125 1 openvino.cc:1223] 'openvino' TRITONBACKEND API version: 1.9\r\nW0506 10:59:05.518162 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\nI0506 10:59:05.518201 1 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\nI0506 10:59:05.529323 1 model_repository_manager.cc:1077] loading: densenet_onnx:1\r\nI0506 10:59:05.637889 1 onnxruntime.cc:2481] TRITONBACKEND_ModelInitialize: densenet_onnx (version 1)\r\nI0506 10:59:05.932762 1 onnxruntime.cc:2504] TRITONBACKEND_ModelFinalize: delete model state\r\nE0506 10:59:05.932885 1 model_repository_manager.cc:1234] failed to load 'densenet_onnx' version 1: Internal: onnx runtime error 1: /workspace/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:122 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /workspace/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:116 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32553 ; hostname=8518a28ba867 ; expr=cudaSetDevice(info_.device_id);\r\n\r\n\r\nI0506 10:59:05.933914 1 server.cc:549]\r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0506 10:59:05.933966 1 server.cc:576]\r\n+-------------+-------------------------------------------------------------------------+--------+\r\n| Backend     | Path                                                                    | Config |\r\n+-------------+-------------------------------------------------------------------------+--------+\r\n| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so                 | {}     |\r\n| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so         | {}     |\r\n| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so         | {}     |\r\n| openvino    | /opt/tritonserver/backends/openvino_2021_4/libtriton_openvino_2021_4.so | {}     |\r\n+-------------+-------------------------------------------------------------------------+--------+\r\n\r\nI0506 10:59:05.934006 1 server.cc:619]\r\n+---------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Model         | Version | Status                                                                                                                                            |\r\n+---------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| densenet_onnx | 1       | UNAVAILABLE: Internal: onnx runtime error 1: /workspace/onnxruntime/onnxruntime/core/providers/cuda/cuda_call.cc:122 bool onnxruntime::CudaCall(E |\r\n|               |         | RRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /workspace/onnxruntime/onnxruntime/core/prov |\r\n|               |         | iders/cuda/cuda_call.cc:116 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool T |\r\n|               |         | HRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=32553 ; hostname=8518a28ba867 ; expr=cudaSetDevic |\r\n|               |         | e(info_.device_id);                                                                                                                               |\r\n|               |         |                                                                                                                                                   |\r\n+---------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0506 10:59:05.934159 1 tritonserver.cc:2123]\r\n+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                      |\r\n+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                     |\r\n| server_version                   | 2.21.0                                                                                                                                     |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda |\r\n|                                  | _shared_memory binary_tensor_data statistics trace                                                                                         |\r\n| model_repository_path[0]         | /models                                                                                                                                    |\r\n| model_control_mode               | MODE_NONE                                                                                                                                  |\r\n| strict_model_config              | 0                                                                                                                                          |\r\n| rate_limit                       | OFF                                                                                                                                        |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                  |\r\n| response_cache_byte_size         | 0                                                                                                                                          |\r\n| min_supported_compute_capability | 6.0                                                                                                                                        |\r\n| strict_readiness                 | 1                                                                                                                                          |\r\n| exit_timeout                     | 30                                                                                                                                         |\r\n+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0506 10:59:05.934192 1 server.cc:250] Waiting for in-flight requests to complete.\r\nI0506 10:59:05.934201 1 server.cc:266] Timeout 30: Found 0 model versions that have in-flight inferences\r\nI0506 10:59:05.934208 1 server.cc:281] All models are stopped, unloading models\r\nI0506 10:59:05.934218 1 server.cc:288] Timeout 30: Found 0 live models and 0 in-flight non-inference requests\r\nerror: creating server: Internal - failed to load all models\r\n  ```\r\n</details>\r\n\r\nI'm running this on MacBook Pro with macOS 12.2, 2,3 GHz Quad-Core Intel Core i7, and Intel Iris Plus Graphics 1536 MB.\r\n\r\n**Expected behavior**\r\n\r\nI expect Triton to load the ONN model.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4346/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4331", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4331/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4331/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4331/events", "html_url": "https://github.com/triton-inference-server/server/issues/4331", "id": 1225314568, "node_id": "I_kwDOCQnI4s5JCNEI", "number": 4331, "title": "Python backend stuck at TRITONBACKEND_ModelInstanceInitialize", "user": {"login": "huangyz0918", "id": 15646062, "node_id": "MDQ6VXNlcjE1NjQ2MDYy", "avatar_url": "https://avatars.githubusercontent.com/u/15646062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/huangyz0918", "html_url": "https://github.com/huangyz0918", "followers_url": "https://api.github.com/users/huangyz0918/followers", "following_url": "https://api.github.com/users/huangyz0918/following{/other_user}", "gists_url": "https://api.github.com/users/huangyz0918/gists{/gist_id}", "starred_url": "https://api.github.com/users/huangyz0918/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/huangyz0918/subscriptions", "organizations_url": "https://api.github.com/users/huangyz0918/orgs", "repos_url": "https://api.github.com/users/huangyz0918/repos", "events_url": "https://api.github.com/users/huangyz0918/events{/privacy}", "received_events_url": "https://api.github.com/users/huangyz0918/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2022-05-04T12:39:31Z", "updated_at": "2023-01-27T16:42:04Z", "closed_at": "2023-01-27T16:42:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI wanna start the python backend following the example. But the container stucks at \r\n\r\n```bash\r\n=============================\r\n== Triton Inference Server ==\r\n=============================\r\n\r\nNVIDIA Release 22.04 (build 36821869)\r\nTriton Server Version 2.21.0\r\n\r\nCopyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n   Use the NVIDIA Container Toolkit to start this container with GPU support; see\r\n   https://docs.nvidia.com/datacenter/cloud-native/ .\r\n\r\nWARNING: [Torch-TensorRT] - Unable to read CUDA capable devices. Return status: 35\r\nI0504 12:25:52.440894 1 libtorch.cc:1381] TRITONBACKEND_Initialize: pytorch\r\nI0504 12:25:52.441090 1 libtorch.cc:1391] Triton TRITONBACKEND API version: 1.9\r\nI0504 12:25:52.441100 1 libtorch.cc:1397] 'pytorch' TRITONBACKEND API version: 1.9\r\nW0504 12:25:52.441171 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\nI0504 12:25:52.441209 1 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\nI0504 12:25:52.442350 1 model_repository_manager.cc:1077] loading: resnet:1\r\nI0504 12:25:52.547089 1 python.cc:1769] Using Python execution env /models/resnet/../my-pytorch.tar.gz\r\nI0504 12:25:52.547228 1 python.cc:2054] TRITONBACKEND_ModelInstanceInitialize: resnet_0 (CPU device 0)\r\n```\r\nMy machine dose not have GPU. The confg \r\n\r\n```\r\nname: \"resnet\"\r\nbackend: \"python\"\r\n\r\ninput [\r\n  {\r\n    name: \"INPUT0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, 3, 224, 224 ]\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"OUTPUT0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, 1000 ]\r\n  }\r\n]\r\n\r\ninstance_group [{\r\n  count: 1\r\n  kind: KIND_CPU\r\n}]\r\n\r\nparameters: {\r\n  key: \"EXECUTION_ENV_PATH\",\r\n  value: {string_value: \"$$TRITON_MODEL_DIRECTORY/../my-pytorch.tar.gz\"}\r\n}\r\n\r\n```\r\n**Triton Information**\r\nnvcr.io/nvidia/tritonserver:22.04-pyt-python-py3\r\n\r\n**Are you using the Triton container or did you build it yourself?**\r\ndocker container\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4331/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4330", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4330/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4330/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4330/events", "html_url": "https://github.com/triton-inference-server/server/issues/4330", "id": 1225187799, "node_id": "I_kwDOCQnI4s5JBuHX", "number": 4330, "title": "Model warmup fails, yet load is reported successful", "user": {"login": "whateverforever", "id": 19725374, "node_id": "MDQ6VXNlcjE5NzI1Mzc0", "avatar_url": "https://avatars.githubusercontent.com/u/19725374?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whateverforever", "html_url": "https://github.com/whateverforever", "followers_url": "https://api.github.com/users/whateverforever/followers", "following_url": "https://api.github.com/users/whateverforever/following{/other_user}", "gists_url": "https://api.github.com/users/whateverforever/gists{/gist_id}", "starred_url": "https://api.github.com/users/whateverforever/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whateverforever/subscriptions", "organizations_url": "https://api.github.com/users/whateverforever/orgs", "repos_url": "https://api.github.com/users/whateverforever/repos", "events_url": "https://api.github.com/users/whateverforever/events{/privacy}", "received_events_url": "https://api.github.com/users/whateverforever/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-05-04T10:29:42Z", "updated_at": "2022-05-19T18:19:21Z", "closed_at": "2022-05-19T18:19:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen loading a model that has warmup configured, but the warmup fails (e.g. because not enough GPU memory is left), the loading is still reported as successful to the client. \r\n\r\n**Triton Information**\r\n21.09\r\n\r\n**Are you using the Triton container or did you build it yourself?**\r\nContainer\r\n\r\n**To Reproduce**\r\nConfigure a model with warmup, reserve enough GPU mem (via `cudaMalloc` for example) so that the warmup inference will fail. Load the model.\r\n\r\nFramework is tensorflow 2. Model is an object detector that takes as input 1x1280x800x3 fp32 and returns bounding boxes and classification scores. Automatic mixed precision is enabled.\r\n\r\n```\r\nI0504 09:59:19.325324 142435 tensorflow.cc:1564] TRITONBACKEND_ModelExecute: Running detector with 1 requests\r\nI0504 09:59:19.329262 142435 tensorflow.cc:1816] TRITONBACKEND_ModelExecute: input 'input' is GPU tensor: false\r\n2022-05-04 09:59:20.079468: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2102] Running auto_mixed_precision_cuda graph optimizer\r\n2022-05-04 09:59:20.157482: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1112] Automatic Mixed Precision Grappler Pass Summary:\r\n\r\nTotal processable nodes: 1560\r\nRecognized nodes available for conversion: 1079\r\nTotal nodes converted: 385\r\nTotal FP16 Cast ops used (excluding Const and Variable casts): 1\r\nAllowlisted nodes converted: 111\r\nDenylisted nodes blocking conversion: 0\r\nNodes blocked from conversion by denylisted nodes: 0\r\n\r\nFor more information regarding mixed precision training, including how to make automatic mixed precision aware of a custom op type, please see the documentation available here:\r\nhttps://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tfamp\r\n\r\n\r\nI0504 09:59:21.946031 142435 http_server.cc:120] HTTP request: 0 /metrics\r\n2022-05-04 09:59:22.061873: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.95G (2092515328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2022-05-04 09:59:22.062316: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.75G (1883263744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2022-05-04 09:59:22.062783: I tensorflow/stream_executor/cuda/cuda_driver.cc:732] failed to allocate 1.58G (1694937344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2022-05-04 09:59:22.498970: E tensorflow/stream_executor/cuda/cuda_dnn.cc:386] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2022-05-04 09:59:22.502246: E tensorflow/stream_executor/cuda/cuda_dnn.cc:386] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2022-05-04 09:59:22.505484: E tensorflow/stream_executor/cuda/cuda_dnn.cc:386] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2022-05-04 09:59:22.517159: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2022-05-04 09:59:22.517410: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at conv_ops.cc:1332 : Not found: No algorithm worked!\r\nE0504 09:59:22.517893 142435 triton_model_instance.cc:86] warmup error: Internal - 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node subnet/conv1_conv/Conv2D}}]]\r\n         [[output/_59]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node subnet/conv1_conv/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\nI0504 09:59:22.517968 142435 model_repository_manager.cc:1183] successfully loaded 'detector' version 1\r\n```\r\n\r\n**Expected behavior**\r\nI would expect the loading to fail, because not everything that makes up the loading succeeded.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4330/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4330/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4310", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4310/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4310/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4310/events", "html_url": "https://github.com/triton-inference-server/server/issues/4310", "id": 1221043851, "node_id": "I_kwDOCQnI4s5Ix6aL", "number": 4310, "title": "image_client.py example is broken for multiple classification outputs", "user": {"login": "rgov", "id": 108767, "node_id": "MDQ6VXNlcjEwODc2Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/108767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgov", "html_url": "https://github.com/rgov", "followers_url": "https://api.github.com/users/rgov/followers", "following_url": "https://api.github.com/users/rgov/following{/other_user}", "gists_url": "https://api.github.com/users/rgov/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgov/subscriptions", "organizations_url": "https://api.github.com/users/rgov/orgs", "repos_url": "https://api.github.com/users/rgov/repos", "events_url": "https://api.github.com/users/rgov/events{/privacy}", "received_events_url": "https://api.github.com/users/rgov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-04-29T15:07:07Z", "updated_at": "2022-05-04T17:55:59Z", "closed_at": "2022-05-04T17:55:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am running Triton via the NGC container version 22.04 (which I take to be 2.21.0) and the Python `tritonclient` module version 2.21.0 on Python 3.9.12. I am using the `densenet_onnx` example model from the 2.20.0 tag of this repository.\r\n\r\nWhen I run the command in the Quick Start tutorial, I get an error:\r\n\r\n```\r\n$ python image_client.py -m densenet_onnx -c 3 -s INCEPTION ./mug.jpg\r\nRequest 1, batch size 1\r\nTraceback (most recent call last):\r\n  File \"image_client.py\", line 475, in <module>\r\n    postprocess(response, output_name, FLAGS.batch_size, max_batch_size > 0)\r\n  File \"image_client.py\", line 191, in postprocess\r\n    raise Exception(\"expected {} results, got {}\".format(\r\nException: expected 1 results, got 3\r\n```\r\n\r\nThis is affected by the `-c 3` argument; if I pass `-c 1` then the error goes away.\r\n\r\nThe assertion checks for one output per input, but in this case for the `densenet_onnx` classification model there are multiple outputs (the top N classifications).\r\n\r\nRemoving the assertion works fine:\r\n\r\n```\r\nRequest 1, batch size 1\r\n    13.915170 (504) = COFFEE MUG\r\n    12.018229 (968) = CUP\r\n    9.839116 (967) = ESPRESSO\r\nPASS\r\n```", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4310/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4263", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4263/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4263/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4263/events", "html_url": "https://github.com/triton-inference-server/server/issues/4263", "id": 1212033176, "node_id": "I_kwDOCQnI4s5IPiiY", "number": 4263, "title": "compose.py failure: module 'build' has no attribute 'get_container_versions'", "user": {"login": "jayvdb", "id": 15092, "node_id": "MDQ6VXNlcjE1MDky", "avatar_url": "https://avatars.githubusercontent.com/u/15092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jayvdb", "html_url": "https://github.com/jayvdb", "followers_url": "https://api.github.com/users/jayvdb/followers", "following_url": "https://api.github.com/users/jayvdb/following{/other_user}", "gists_url": "https://api.github.com/users/jayvdb/gists{/gist_id}", "starred_url": "https://api.github.com/users/jayvdb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jayvdb/subscriptions", "organizations_url": "https://api.github.com/users/jayvdb/orgs", "repos_url": "https://api.github.com/users/jayvdb/repos", "events_url": "https://api.github.com/users/jayvdb/events{/privacy}", "received_events_url": "https://api.github.com/users/jayvdb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-04-22T08:48:44Z", "updated_at": "2022-04-22T21:28:04Z", "closed_at": "2022-04-22T21:28:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\n```py\r\nTraceback (most recent call last):\r\n  File \"/path/to/triton-server-repo/compose.py\", line 388, in <module>\r\n    get_container_version_if_not_specified()\r\n  File \"/path/to/triton-server-repo/compose.py\", line 162, in get_container_version_if_not_specified\r\n    _, FLAGS.container_version = build.get_container_versions(\r\nAttributeError: module 'build' has no attribute 'get_container_versions'\r\n```\r\n\r\n`get_container_versions` was removed in de061a4 (PR #4174) , but it is still used by `compose.py` on branch `main`, c.f. https://github.com/triton-inference-server/server/blob/db3ca6a/compose.py#L162\r\n\r\n**To Reproduce**\r\nRun `python3 compose.py --dry-run` without any other arguments.\r\n\r\n**Expected behavior**\r\nNo exceptions for use without any args.  If this mode is no longer supported, a nice error message would be great.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4263/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4215", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4215/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4215/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4215/events", "html_url": "https://github.com/triton-inference-server/server/issues/4215", "id": 1203629207, "node_id": "I_kwDOCQnI4s5HveyX", "number": 4215, "title": "Python Model with BLS: failed to get cuda pointer device attribute", "user": {"login": "naor2013", "id": 5496913, "node_id": "MDQ6VXNlcjU0OTY5MTM=", "avatar_url": "https://avatars.githubusercontent.com/u/5496913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/naor2013", "html_url": "https://github.com/naor2013", "followers_url": "https://api.github.com/users/naor2013/followers", "following_url": "https://api.github.com/users/naor2013/following{/other_user}", "gists_url": "https://api.github.com/users/naor2013/gists{/gist_id}", "starred_url": "https://api.github.com/users/naor2013/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/naor2013/subscriptions", "organizations_url": "https://api.github.com/users/naor2013/orgs", "repos_url": "https://api.github.com/users/naor2013/repos", "events_url": "https://api.github.com/users/naor2013/events{/privacy}", "received_events_url": "https://api.github.com/users/naor2013/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2022-04-13T18:13:18Z", "updated_at": "2022-06-24T14:50:11Z", "closed_at": "2022-06-24T14:50:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nA clear and concise description of what the bug is.\r\nI'm using a Python model with BLS to call a TorchScript model that runs on a gpu.\r\nI can see that when calling the Python model, it sends the request to the TorchScript model and succeeds based on the \"add response output\" log that happens after the model call.\r\nThen I get a \"failed to get cuda pointer device attribute: initialization error\".\r\nI tried running the Python model with kind_gpu and kind_cpu, with and without the \u201c FORCE_CPU_ONLY_INPUT_TENSORS\u201d parameter.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nI\u2019m using 22.02. Is there any chance that this bug relates to the many pull requests to the Python backend in the last month, specifically [this one?](https://github.com/triton-inference-server/python_backend/pull/126) \r\n\r\nAre you using the Triton container or did you build it yourself?\r\nUsing the Triton container.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4215/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4215/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4199", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4199/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4199/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4199/events", "html_url": "https://github.com/triton-inference-server/server/issues/4199", "id": 1199170567, "node_id": "I_kwDOCQnI4s5HeeQH", "number": 4199, "title": "tritonserver ensemble SEGV in nvidia::inferenceserver::RateLimiter::EnqueuePayload ", "user": {"login": "dfisk", "id": 60303, "node_id": "MDQ6VXNlcjYwMzAz", "avatar_url": "https://avatars.githubusercontent.com/u/60303?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dfisk", "html_url": "https://github.com/dfisk", "followers_url": "https://api.github.com/users/dfisk/followers", "following_url": "https://api.github.com/users/dfisk/following{/other_user}", "gists_url": "https://api.github.com/users/dfisk/gists{/gist_id}", "starred_url": "https://api.github.com/users/dfisk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dfisk/subscriptions", "organizations_url": "https://api.github.com/users/dfisk/orgs", "repos_url": "https://api.github.com/users/dfisk/repos", "events_url": "https://api.github.com/users/dfisk/events{/privacy}", "received_events_url": "https://api.github.com/users/dfisk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-04-10T20:56:12Z", "updated_at": "2022-05-02T16:18:37Z", "closed_at": "2022-05-02T16:18:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Greetings, \r\n\r\nThis issue is wrt.\r\n\r\nNVIDIA Release 22.03 (build 33743047)\r\nTriton Server Version 2.20.0\r\n\r\nI have developed an ensemble example from scratch that combines XGBoost(1), Tensorflow(1), and Scikit-Learn(2) framework models, along with multiplexer(1) and combiner(1) python models using the tensorflow2, fil and python backends.\r\n \r\nThe server initializes properly inference to each of the (6) models succeed. However, the ensemble model that uses all 6 of these models exposes a SEGV in the server:\r\n \r\n```\r\nI0409 20:15:58.652373 1141 rate_limiter.cc:135] Should not print this\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n\r\nThread 93 \"tritonserver\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7f9564ffd000 (LWP 1303)]\r\n__GI___pthread_mutex_lock (mutex=0x38) at ../nptl/pthread_mutex_lock.c:67\r\n67      ../nptl/pthread_mutex_lock.c: No such file or directory.\r\n\r\n(gdb) bt\r\n#0  __GI___pthread_mutex_lock (mutex=0x38) at ../nptl/pthread_mutex_lock.c:67\r\n#1  0x00007f975a4ce900 in nvidia::inferenceserver::RateLimiter::EnqueuePayload(nvidia::inferenceserver::TritonModel const*, std::shared_ptr<nvidia::inferenceserver::Payload>) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#2  0x00007f975a3cc0ab in nvidia::inferenceserver::DynamicBatchScheduler::Enqueue(std::unique_ptr<nvidia::inferenceserver::InferenceRequest, std::default_delete<nvidia::inferenceserver::InferenceRequest> >&) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#3  0x00007f975a416001 in nvidia::inferenceserver::InferenceRequest::Run(std::unique_ptr<nvidia::inferenceserver::InferenceRequest, std::default_delete<nvidia::inferenceserver::InferenceRequest> >&) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#4  0x00007f975a4bb2d8 in nvidia::inferenceserver::InferenceServer::InferAsync(std::unique_ptr<nvidia::inferenceserver::InferenceRequest, std::default_delete<nvidia::inferenceserver::InferenceRequest> >&) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#5  0x00007f975a4e31bd in TRITONSERVER_ServerInferAsync () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#6  0x0000562c5cf32644 in nvidia::inferenceserver::HTTPAPIServer::HandleInfer(evhtp_request*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) ()\r\n#7  0x0000562c5cf33185 in nvidia::inferenceserver::HTTPAPIServer::Handle(evhtp_request*) ()\r\n#8  0x0000562c5d118dc5 in htp.request_parse_fini_ ()\r\n#9  0x0000562c5d11dae6 in htparser_run ()\r\n#10 0x0000562c5d11b9fe in htp.connection_readcb_ ()\r\n#11 0x0000562c5d1029b0 in bufferevent_run_deferred_callbacks_locked ()\r\n#12 0x0000562c5d10a4d0 in event_process_active_single_queue.isra ()\r\n#13 0x0000562c5d10af1f in event_base_loop ()\r\n#14 0x0000562c5d11f852 in _evthr_loop ()\r\n#15 0x00007f975a2a9609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#16 0x00007f9759b17163 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n\r\n```\r\n\r\nThe model directory is available from this public GCS bucket for reproduction:\r\n`gs://dfisk-model-test/triton-ensemble-example`\r\n\r\nI am also attaching the client test scripts. \r\n\r\nTo reproduce: pull the tritonserver image from [NGC 22.03](nvcr.io/nvidia/tritonserver:22.03-py3) and run the container built from the [fil_backend](https://github.com/triton-inference-server/fil_backend) with:\r\n```\r\n$ docker run \\\r\n  --gpus=all \\\r\n  --rm \\\r\n  -p 8000:8000 \\\r\n  -p 8001:8001 \\\r\n  -p 8002:8002 \\\r\n   --shm-size=8G \\\r\n   --ulimit memlock=-1 \\\r\n  triton_fil \\\r\n  tritonserver \\\r\n  --model-repository=gs://dfisk-model-test/triton-ensemble-example\r\n```\r\n\r\nAfter editing the attached scripts to point to your server, try each of:\r\n`combine_01.py  mux.py  sci_1.py  sci_2.py  tf_01.py  xgb_01.py  ensemble_01.py`\r\n[triton-ensemble-example-test-scripts.zip](https://github.com/triton-inference-server/server/files/8459884/triton-ensemble-example-test-scripts.zip)\r\n\r\nIn my testing all but ensemble work correct with the subject SEGV exposed by ensemble_01.py\r\n\r\nI am in the process of building a debug tritonserver with this version to possibly assist with the fix. I am also looking at adding some rate limiter configuration to ensure the sub-system is initialized; I am otherwise not using the rate limiter in this configuration but the crash appears to be in that sub-system. \r\n\r\nThanks for your assistance!\r\n ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4199/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4199/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4172", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4172/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4172/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4172/events", "html_url": "https://github.com/triton-inference-server/server/issues/4172", "id": 1194960802, "node_id": "I_kwDOCQnI4s5HOaei", "number": 4172, "title": "Jetson PyTorch wheel broken link", "user": {"login": "1-800-BAD-CODE", "id": 50530592, "node_id": "MDQ6VXNlcjUwNTMwNTky", "avatar_url": "https://avatars.githubusercontent.com/u/50530592?v=4", "gravatar_id": "", "url": "https://api.github.com/users/1-800-BAD-CODE", "html_url": "https://github.com/1-800-BAD-CODE", "followers_url": "https://api.github.com/users/1-800-BAD-CODE/followers", "following_url": "https://api.github.com/users/1-800-BAD-CODE/following{/other_user}", "gists_url": "https://api.github.com/users/1-800-BAD-CODE/gists{/gist_id}", "starred_url": "https://api.github.com/users/1-800-BAD-CODE/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/1-800-BAD-CODE/subscriptions", "organizations_url": "https://api.github.com/users/1-800-BAD-CODE/orgs", "repos_url": "https://api.github.com/users/1-800-BAD-CODE/repos", "events_url": "https://api.github.com/users/1-800-BAD-CODE/events{/privacy}", "received_events_url": "https://api.github.com/users/1-800-BAD-CODE/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2022-04-06T18:09:59Z", "updated_at": "2022-04-07T17:13:14Z", "closed_at": "2022-04-07T16:10:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nFollowing the Jetson [documentation](https://github.com/triton-inference-server/server/blob/r22.02/docs/jetson.md) for 2.19.0, attempting to install the PyTorch wheel leads to a broken link.\r\n\r\n**Triton Information**\r\n2.19.0\r\n\r\n**To Reproduce**\r\n\r\n\r\nLink for 2.19 (broken):\r\n```\r\nwget https://developer.download.nvidia.com/compute/redist/jp/v461/pytorch/torch-1.11.0a0+17540c5-cp36-cp36m-linux_aarch64.whl\r\n```\r\n\r\nLink for 2.20 (works):\r\n```\r\nwget https://developer.download.nvidia.com/compute/redist/jp/v50/pytorch/torch-1.12.0a0+2c916ef.nv22.3-cp38-cp38-linux_aarch64.whl\r\n```\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4172/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4153", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4153/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4153/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4153/events", "html_url": "https://github.com/triton-inference-server/server/issues/4153", "id": 1191066641, "node_id": "I_kwDOCQnI4s5G_jwR", "number": 4153, "title": "Previous success build of full tritonserver failed for most recent release branches (r21.12...r22.03)", "user": {"login": "yaoyunda", "id": 72677351, "node_id": "MDQ6VXNlcjcyNjc3MzUx", "avatar_url": "https://avatars.githubusercontent.com/u/72677351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaoyunda", "html_url": "https://github.com/yaoyunda", "followers_url": "https://api.github.com/users/yaoyunda/followers", "following_url": "https://api.github.com/users/yaoyunda/following{/other_user}", "gists_url": "https://api.github.com/users/yaoyunda/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaoyunda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaoyunda/subscriptions", "organizations_url": "https://api.github.com/users/yaoyunda/orgs", "repos_url": "https://api.github.com/users/yaoyunda/repos", "events_url": "https://api.github.com/users/yaoyunda/events{/privacy}", "received_events_url": "https://api.github.com/users/yaoyunda/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-04-03T22:32:18Z", "updated_at": "2022-05-02T22:25:33Z", "closed_at": "2022-04-18T16:13:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nFollowing the build doc, using build.py \r\nthe \"tritonserver\" image is built successfully for r21.12, r22.02, r22.03 a week ago \r\nhowever, it is now all failed.\r\nThe main branch build is still ok.\r\n\r\n**Triton Information**\r\nr21.12, r22.02, r22.03\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nbuild it myself\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nrun:\r\n ./build.py --cmake-dir=./build --build-dir=/mnt/citritonbuild --enable-logging --enable-stats --enable-tracing --enable-metrics --enable-gpu-metrics --enable-gpu --filesystem=gcs --filesystem=azure_storage --filesystem=s3 --endpoint=http --endpoint=grpc --repo-tag=common:r22.03 --repo-tag=core:r22.03 --repo-tag=backend:r22.03 --repo-tag=thirdparty:r22.03 --backend=ensemble --backend=tensorrt:r22.03 --backend=identity:r22.03 --backend=repeat:r22.03 --backend=square:r22.03 --backend=onnxruntime:r22.03 --backend=pytorch:r22.03 --backend=tensorflow1:r22.03 --backend=tensorflow2:r22.03 --backend=openvino:r22.03 --backend=python:r22.03 --backend=dali:r22.03 --backend=fil:r22.03 --repoagent=checksum:r22.03\r\n\r\nget: \r\nthe end of log:\r\n2022-04-03T20:59:00.4602576Z Building Triton Inference Server\r\n2022-04-03T20:59:00.4602869Z component \"common\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:00.4603180Z component \"core\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:00.4603497Z component \"backend\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:00.4603807Z component \"thirdparty\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:00.4604108Z error: make install failed\r\n2022-04-03T20:59:01.3195657Z platform linux\r\n2022-04-03T20:59:01.3196099Z machine x86_64\r\n2022-04-03T20:59:01.3196334Z version 2.20.0\r\n2022-04-03T20:59:01.3197284Z default repo-tag: r22.03\r\n2022-04-03T20:59:01.3197570Z container version 22.03\r\n2022-04-03T20:59:01.3197858Z upstream container version 22.03\r\n2022-04-03T20:59:01.3198200Z backend \"ensemble\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3198513Z backend \"tensorrt\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3198829Z backend \"identity\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3199160Z backend \"repeat\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3199813Z backend \"square\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3200134Z backend \"onnxruntime\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3200441Z backend \"pytorch\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3200766Z backend \"tensorflow1\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3201248Z backend \"tensorflow2\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3201559Z backend \"openvino\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3201870Z backend \"python\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3202163Z backend \"dali\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3202463Z backend \"fil\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3202774Z repoagent \"checksum\" at tag/branch \"r22.03\"\r\n2022-04-03T20:59:01.3203093Z error: **### docker run tritonserver_builder failed**\r\n2022-04-03T20:59:01.3512032Z ##[error]Bash exited with code '1'.\r\n2022-04-03T20:59:01.3527685Z ##[section]Finishing: Build Triton Server Image\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nthe build should be successful\r\n[tritonserver_build_r22.03_log_success.txt](https://github.com\r\n[tritonserver_build_r22.03_log_fail.txt](https://github.com/triton-inference-server/server/files/8405638/tritonserver_build_r22.03_log_fail.txt)\r\n/triton-inference-server/server/files/8405637/tritonserver_build_r22.03_log_success.txt)\r\n.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4153/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4150", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4150/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4150/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4150/events", "html_url": "https://github.com/triton-inference-server/server/issues/4150", "id": 1190945067, "node_id": "I_kwDOCQnI4s5G_GEr", "number": 4150, "title": "Optional input for python backend", "user": {"login": "donggrame", "id": 75006897, "node_id": "MDQ6VXNlcjc1MDA2ODk3", "avatar_url": "https://avatars.githubusercontent.com/u/75006897?v=4", "gravatar_id": "", "url": "https://api.github.com/users/donggrame", "html_url": "https://github.com/donggrame", "followers_url": "https://api.github.com/users/donggrame/followers", "following_url": "https://api.github.com/users/donggrame/following{/other_user}", "gists_url": "https://api.github.com/users/donggrame/gists{/gist_id}", "starred_url": "https://api.github.com/users/donggrame/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/donggrame/subscriptions", "organizations_url": "https://api.github.com/users/donggrame/orgs", "repos_url": "https://api.github.com/users/donggrame/repos", "events_url": "https://api.github.com/users/donggrame/events{/privacy}", "received_events_url": "https://api.github.com/users/donggrame/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2022-04-03T14:20:22Z", "updated_at": "2022-09-29T22:47:55Z", "closed_at": "2022-04-08T04:32:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nAccording to [this comment](https://github.com/triton-inference-server/server/issues/3419#issuecomment-992973441), I thought python backend supports `optional` argument in `input` since `22.01`.\r\nI can confirm that identity backend works with `optional` but not python backend.\r\nAm I missing something? I couldn't find any documents about it.\r\n\r\n**Triton Information**\r\nnvcr.io/nvidia/tritonserver:22.03-pyt-python-py3\r\n\r\n**To Reproduce**\r\n\r\n- config.pbtxt\r\n```console\r\n$ cat /models/test/config.pbtxt\r\n\r\nbackend: \"python\"\r\n\r\nmax_batch_size: 0\r\n\r\ninput [\r\n  {\r\n    name: \"INPUT\"\r\n    data_type: TYPE_INT32\r\n    dims: [ 1 ]\r\n    optional: true\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"OUTPUT\"\r\n    data_type: TYPE_INT32\r\n    dims: [ 1 ]\r\n  }\r\n]\r\n```\r\n\r\n- /models/test/1/model.py\r\n```python\r\nimport json\r\nimport numpy as np\r\n\r\nimport triton_python_backend_utils as pb_utils\r\n\r\nclass TritonPythonModel:\r\n\r\n    def initialize(self, args):\r\n        self.model_config = json.loads(args['model_config'])\r\n        self.model_instance_kind = args['model_instance_kind']\r\n        self.model_instance_device_id  = args['model_instance_device_id']\r\n\r\n\r\n    def execute(self, requests):\r\n        responses = []\r\n        for request in requests:\r\n            in_0 = pb_utils.get_input_tensor_by_name(request, \"INPUT\")\r\n            if in_0 is None:\r\n                in_0 = np.array([1])\r\n            else:\r\n                in_0 = in_0.as_numpy()\r\n\r\n            out = pb_utils.Tensor(\"OUTPUT\", in_0)\r\n\r\n            inference_response = pb_utils.InferenceResponse(\r\n                output_tensors=[out])\r\n            responses.append(inference_response)\r\n\r\n        return responses\r\n```\r\n\r\n```console\r\n$ docker run -it --shm-size=1g --ulimit memlock=-1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v /models:/models nvcr.io/nvidia/tritonserver:22.03-pyt-python-py3 tritonserver --model-repository /models --log-verbose 99\r\n\r\n=============================\r\n== Triton Inference Server ==\r\n=============================\r\n\r\nNVIDIA Release 22.03 (build 33743047)\r\nTriton Server Version 2.20.0\r\n\r\nCopyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n   Use the NVIDIA Container Toolkit to start this container with GPU support; see\r\n   https://docs.nvidia.com/datacenter/cloud-native/ .\r\n\r\nI0403 14:07:22.516259 1 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\nWARNING: [Torch-TensorRT] - Unable to read CUDA capable devices. Return status: 35\r\nI0403 14:07:22.919495 1 libtorch.cc:1309] TRITONBACKEND_Initialize: pytorch\r\nI0403 14:07:22.919516 1 libtorch.cc:1319] Triton TRITONBACKEND API version: 1.8\r\nI0403 14:07:22.919523 1 libtorch.cc:1325] 'pytorch' TRITONBACKEND API version: 1.8\r\nW0403 14:07:22.919660 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\nI0403 14:07:22.919685 1 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\nI0403 14:07:22.922665 1 model_config_utils.cc:657] Server side auto-completed config: name: \"test\"\r\ninput {\r\n  name: \"INPUT\"\r\n  data_type: TYPE_INT32\r\n  dims: 1\r\n  optional: true\r\n}\r\noutput {\r\n  name: \"OUTPUT\"\r\n  data_type: TYPE_INT32\r\n  dims: 1\r\n}\r\nbackend: \"python\"\r\n\r\nI0403 14:07:22.922816 1 model_repository_manager.cc:703] AsyncLoad() 'test'\r\nI0403 14:07:22.923020 1 model_repository_manager.cc:942] TriggerNextAction() 'test' version 1: 1\r\nI0403 14:07:22.923042 1 model_repository_manager.cc:978] Load() 'test' version 1\r\nI0403 14:07:22.923086 1 model_repository_manager.cc:997] loading: test:1\r\nI0403 14:07:23.023602 1 model_repository_manager.cc:1055] CreateModel() 'test' version 1\r\nI0403 14:07:23.023831 1 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\r\nI0403 14:07:23.027695 1 python.cc:1679] 'python' TRITONBACKEND API version: 1.8\r\nI0403 14:07:23.027726 1 python.cc:1701] backend configuration:\r\n{}\r\nI0403 14:07:23.027744 1 python.cc:1811] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\r\nI0403 14:07:23.028108 1 python.cc:1859] TRITONBACKEND_ModelInitialize: test (version 1)\r\nI0403 14:07:23.030466 1 model_config_utils.cc:1608] ModelConfig 64-bit fields:\r\nI0403 14:07:23.030494 1 model_config_utils.cc:1610]     ModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\r\nI0403 14:07:23.030506 1 model_config_utils.cc:1610]     ModelConfig::dynamic_batching::max_queue_delay_microseconds\r\nI0403 14:07:23.030517 1 model_config_utils.cc:1610]     ModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\r\nI0403 14:07:23.030528 1 model_config_utils.cc:1610]     ModelConfig::ensemble_scheduling::step::model_version\r\nI0403 14:07:23.030542 1 model_config_utils.cc:1610]     ModelConfig::input::dims\r\nI0403 14:07:23.030579 1 model_config_utils.cc:1610]     ModelConfig::input::reshape::shape\r\nI0403 14:07:23.030622 1 model_config_utils.cc:1610]     ModelConfig::instance_group::secondary_devices::device_id\r\nI0403 14:07:23.030634 1 model_config_utils.cc:1610]     ModelConfig::model_warmup::inputs::value::dims\r\nI0403 14:07:23.030644 1 model_config_utils.cc:1610]     ModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\r\nI0403 14:07:23.030655 1 model_config_utils.cc:1610]     ModelConfig::optimization::cuda::graph_spec::input::value::dim\r\nI0403 14:07:23.030687 1 model_config_utils.cc:1610]     ModelConfig::output::dims\r\nI0403 14:07:23.030697 1 model_config_utils.cc:1610]     ModelConfig::output::reshape::shape\r\nI0403 14:07:23.030708 1 model_config_utils.cc:1610]     ModelConfig::sequence_batching::direct::max_queue_delay_microseconds\r\nI0403 14:07:23.030729 1 model_config_utils.cc:1610]     ModelConfig::sequence_batching::max_sequence_idle_microseconds\r\nI0403 14:07:23.030741 1 model_config_utils.cc:1610]     ModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\r\nI0403 14:07:23.030769 1 model_config_utils.cc:1610]     ModelConfig::sequence_batching::state::dims\r\nI0403 14:07:23.030797 1 model_config_utils.cc:1610]     ModelConfig::sequence_batching::state::initial_state::dims\r\nI0403 14:07:23.030807 1 model_config_utils.cc:1610]     ModelConfig::version_policy::specific::versions\r\nI0403 14:07:23.031250 1 python.cc:1882] TRITONBACKEND_ModelFinalize: delete model state\r\nI0403 14:07:23.031276 1 triton_backend_manager.cc:101] unloading backend 'python'\r\nI0403 14:07:23.031296 1 python.cc:1839] TRITONBACKEND_Finalize: Start\r\nI0403 14:07:23.031465 1 python.cc:1844] TRITONBACKEND_Finalize: End\r\nE0403 14:07:23.033782 1 model_repository_manager.cc:1155] failed to load 'test' version 1: Invalid argument: 'optional' is set to true for input 'INPUT' while the backend model doesn't support optional input\r\nI0403 14:07:23.033809 1 model_repository_manager.cc:942] TriggerNextAction() 'test' version 1: 0\r\nI0403 14:07:23.033823 1 model_repository_manager.cc:956] no next action, trigger OnComplete()\r\nI0403 14:07:23.033967 1 model_repository_manager.cc:549] VersionStates() 'test'\r\nI0403 14:07:23.034055 1 model_repository_manager.cc:549] VersionStates() 'test'\r\nI0403 14:07:23.034137 1 server.cc:524]\r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0403 14:07:23.034209 1 server.cc:551]\r\n+---------+---------------------------------------------------------+--------+\r\n| Backend | Path                                                    | Config |\r\n+---------+---------------------------------------------------------+--------+\r\n| pytorch | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so | {}     |\r\n+---------+---------------------------------------------------------+--------+\r\n\r\nI0403 14:07:23.034232 1 model_repository_manager.cc:525] ModelStates()\r\nI0403 14:07:23.034357 1 server.cc:594]\r\n+-------+---------+-----------------------------------------------------------------------------------------------------------------------------------+\r\n| Model | Version | Status                                                                                                                            |\r\n+-------+---------+-----------------------------------------------------------------------------------------------------------------------------------+\r\n| test  | 1       | UNAVAILABLE: Invalid argument: 'optional' is set to true for input 'INPUT' while the backend model doesn't support optional input |\r\n+-------+---------+-----------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0403 14:07:23.034742 1 tritonserver.cc:1962]\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                                                                        |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                                                                       |\r\n| server_version                   | 2.20.0                                                                                                                                                                                       |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\r\n| model_repository_path[0]         | /models                                                                                                                                                                                      |\r\n| model_control_mode               | MODE_NONE                                                                                                                                                                                    |\r\n| strict_model_config              | 1                                                                                                                                                                                            |\r\n| rate_limit                       | OFF                                                                                                                                                                                          |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\r\n| response_cache_byte_size         | 0                                                                                                                                                                                            |\r\n| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\r\n| strict_readiness                 | 1                                                                                                                                                                                            |\r\n| exit_timeout                     | 30                                                                                                                                                                                           |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0403 14:07:23.034831 1 server.cc:252] Waiting for in-flight requests to complete.\r\nI0403 14:07:23.034844 1 model_repository_manager.cc:648] AsyncUnload() 'test'\r\nI0403 14:07:23.034861 1 model_repository_manager.cc:942] TriggerNextAction() 'test' version 1: 2\r\nI0403 14:07:23.034874 1 model_repository_manager.cc:1022] Unload() 'test' version 1\r\nI0403 14:07:23.034893 1 model_repository_manager.cc:489] LiveModelStates()\r\nI0403 14:07:23.034905 1 server.cc:267] Timeout 30: Found 0 live models and 0 in-flight non-inference requests\r\nI0403 14:07:23.034921 1 triton_backend_manager.cc:101] unloading backend 'pytorch'\r\nerror: creating server: Internal - failed to load all models\r\n\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4150/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4133", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4133/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4133/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4133/events", "html_url": "https://github.com/triton-inference-server/server/issues/4133", "id": 1186703370, "node_id": "I_kwDOCQnI4s5Gu6gK", "number": 4133, "title": "Dynamic Batching not creating batches correctly and incorrect inference results", "user": {"login": "omrifried", "id": 60021598, "node_id": "MDQ6VXNlcjYwMDIxNTk4", "avatar_url": "https://avatars.githubusercontent.com/u/60021598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/omrifried", "html_url": "https://github.com/omrifried", "followers_url": "https://api.github.com/users/omrifried/followers", "following_url": "https://api.github.com/users/omrifried/following{/other_user}", "gists_url": "https://api.github.com/users/omrifried/gists{/gist_id}", "starred_url": "https://api.github.com/users/omrifried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/omrifried/subscriptions", "organizations_url": "https://api.github.com/users/omrifried/orgs", "repos_url": "https://api.github.com/users/omrifried/repos", "events_url": "https://api.github.com/users/omrifried/events{/privacy}", "received_events_url": "https://api.github.com/users/omrifried/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "rmccorm4", "id": 21284872, "node_id": "MDQ6VXNlcjIxMjg0ODcy", "avatar_url": "https://avatars.githubusercontent.com/u/21284872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmccorm4", "html_url": "https://github.com/rmccorm4", "followers_url": "https://api.github.com/users/rmccorm4/followers", "following_url": "https://api.github.com/users/rmccorm4/following{/other_user}", "gists_url": "https://api.github.com/users/rmccorm4/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmccorm4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmccorm4/subscriptions", "organizations_url": "https://api.github.com/users/rmccorm4/orgs", "repos_url": "https://api.github.com/users/rmccorm4/repos", "events_url": "https://api.github.com/users/rmccorm4/events{/privacy}", "received_events_url": "https://api.github.com/users/rmccorm4/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmccorm4", "id": 21284872, "node_id": "MDQ6VXNlcjIxMjg0ODcy", "avatar_url": "https://avatars.githubusercontent.com/u/21284872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmccorm4", "html_url": "https://github.com/rmccorm4", "followers_url": "https://api.github.com/users/rmccorm4/followers", "following_url": "https://api.github.com/users/rmccorm4/following{/other_user}", "gists_url": "https://api.github.com/users/rmccorm4/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmccorm4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmccorm4/subscriptions", "organizations_url": "https://api.github.com/users/rmccorm4/orgs", "repos_url": "https://api.github.com/users/rmccorm4/repos", "events_url": "https://api.github.com/users/rmccorm4/events{/privacy}", "received_events_url": "https://api.github.com/users/rmccorm4/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 28, "created_at": "2022-03-30T16:24:34Z", "updated_at": "2022-06-24T21:44:43Z", "closed_at": "2022-06-21T19:16:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI am deploying a triton server to GKE via the [gke-marketplace-app](https://github.com/triton-inference-server/server/tree/main/deploy/gke-marketplace-app) documentation. When I try to use dynamic batching, the requests are not batched and it is only sent with a batch size of 1. Additionally, the inference only results in one detection when it should be multiple.\r\n\r\n**Triton Information**\r\nThe version is 2.17 as this is what the marketplace feature deploys.\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nDeployed via gcp marketplace\r\n\r\n**To Reproduce**\r\nI create the inference server with the following config:\r\n```\r\nname: \"sample\"\r\nplatform: \"pytorch_libtorch\"\r\nmax_batch_size : 16\r\ninput [\r\n  {\r\n    name: \"INPUT__0\"\r\n    data_type: TYPE_UINT8\r\n    format: FORMAT_NCHW\r\n    dims: [ 3, 512, 512 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"OUTPUT__0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, 4 ]\r\n  },\r\n {\r\n    name: \"OUTPUT__1\"\r\n    data_type: TYPE_INT64\r\n    dims: [ -1 ]\r\n   label_filename: \"sample.txt\"\r\n  },\r\n  {\r\n    name: \"OUTPUT__2\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1 ]\r\n  }\r\n]\r\n dynamic_batching {\r\n    max_queue_delay_microseconds: 50000\r\n}\r\n```\r\n\r\nI am calling inference as follows:\r\n```\r\nmodel = \"sample\"\r\nclient = httpclient.InferenceServerClient( url = url )\r\n\r\ninput_1 = httpclient.InferInput(name = \"INPUT__0\", shape = list(data.shape), datatype = \"UINT8\")\r\ninput_2 = httpclient.InferInput(name = \"INPUT__0\", shape = list(data.shape), datatype = \"UINT8\")\r\n\r\ninput_1.set_data_from_numpy(data, binary_data = True)\r\ninput_2.set_data_from_numpy(data, binary_data = True)\r\n\r\noutput_00 = httpclient.InferRequestedOutput(name = \"OUTPUT__0\", binary_data = False)\r\noutput_01 = httpclient.InferRequestedOutput(name = \"OUTPUT__1\", binary_data = False)\r\noutput_02 = httpclient.InferRequestedOutput(name = \"OUTPUT__2\", binary_data = False)\r\n\r\noutput_10 = httpclient.InferRequestedOutput(name = \"OUTPUT__0\", binary_data = False)\r\noutput_11 = httpclient.InferRequestedOutput(name = \"OUTPUT__1\", binary_data = False)\r\noutput_12 = httpclient.InferRequestedOutput(name = \"OUTPUT__2\", binary_data = False)\r\n\r\n# Is this correct? I tried using reshape in the config, but it did not work. Without this I get errors about data shape.\r\ninput_1.set_shape([1, 3, 512, 512]\r\ninput_2.set_shape([1, 3, 512, 512]\r\n\r\nresponse_1 = client.async_infer(model_name = model, inputs = [input_1], outputs = [output_00, output_01, output_02])\r\nresponse_2 = client.async_infer(model_name = model, inputs = [input_2], outputs = [output_10, output_11, output_12])\r\n```\r\n  \r\n**Expected behavior**\r\nWith the above code, when I run `print(response_1.get_result().get_response())` I am only seeing one detection, but I know that the model detects multiple objects during direct inference on local:\r\n```\r\n{... [{'name': 'OUTPUT__0', 'datatype': 'FP32', 'shape': [1, 4], 'data': [x_min, y_min, x_max, y_max]}, ...}\r\n```\r\nAdditionally, when I run `print(client.get_inference_statistics())` I am seeing only a batch size of 1 when I expect 2 in this case:\r\n```\r\n{ ... 'batch_stats': [{'batch_size': 1, 'compute_input' : {'count': 2 ...}}] ... }\r\n```", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4133/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4113", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4113/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4113/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4113/events", "html_url": "https://github.com/triton-inference-server/server/issues/4113", "id": 1180725889, "node_id": "I_kwDOCQnI4s5GYHKB", "number": 4113, "title": "Endless wait when loading Python backend model on Jetson - v2.19.0", "user": {"login": "KarelvdVrie", "id": 17055112, "node_id": "MDQ6VXNlcjE3MDU1MTEy", "avatar_url": "https://avatars.githubusercontent.com/u/17055112?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KarelvdVrie", "html_url": "https://github.com/KarelvdVrie", "followers_url": "https://api.github.com/users/KarelvdVrie/followers", "following_url": "https://api.github.com/users/KarelvdVrie/following{/other_user}", "gists_url": "https://api.github.com/users/KarelvdVrie/gists{/gist_id}", "starred_url": "https://api.github.com/users/KarelvdVrie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KarelvdVrie/subscriptions", "organizations_url": "https://api.github.com/users/KarelvdVrie/orgs", "repos_url": "https://api.github.com/users/KarelvdVrie/repos", "events_url": "https://api.github.com/users/KarelvdVrie/events{/privacy}", "received_events_url": "https://api.github.com/users/KarelvdVrie/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2022-03-25T12:16:41Z", "updated_at": "2022-04-06T15:17:25Z", "closed_at": "2022-04-06T15:17:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nUsing the TRITON 2.19.0 server build into an image docker for the Jetson TX2 with the `add_sub` Python model makes the server hang forever when loading the model. The TensorRT, ONNX, and Tensorflow backends do load their models.\r\nThe TRITON does not give an error but the following is the output log:\r\n```\r\ntriton-server  | I0325 11:25:44.774643 1 shared_library.cc:108] OpenLibraryHandle: /Workdir/TRITON/backends/pytorch/libtriton_pytorch.so\r\ntriton-server  | I0325 11:25:44.777248 1 triton_backend_manager.cc:101] unloading backend 'pytorch'\r\ntriton-server  | I0325 11:25:44.895887 1 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x101070000' with size 268435456\r\ntriton-server  | I0325 11:25:44.896137 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\ntriton-server  | I0325 11:25:44.906950 1 model_config_utils.cc:667] Server side auto-completed config: name: \"add_sub\"\r\ntriton-server  | input {\r\ntriton-server  |   name: \"INPUT0\"\r\ntriton-server  |   data_type: TYPE_FP32\r\ntriton-server  |   dims: 4\r\ntriton-server  | }\r\ntriton-server  | input {\r\ntriton-server  |   name: \"INPUT1\"\r\ntriton-server  |   data_type: TYPE_FP32\r\ntriton-server  |   dims: 4\r\ntriton-server  | }\r\ntriton-server  | output {\r\ntriton-server  |   name: \"OUTPUT0\"\r\ntriton-server  |   data_type: TYPE_FP32\r\ntriton-server  |   dims: 4\r\ntriton-server  | }\r\ntriton-server  | output {\r\ntriton-server  |   name: \"OUTPUT1\"\r\ntriton-server  |   data_type: TYPE_FP32\r\ntriton-server  |   dims: 4\r\ntriton-server  | }\r\ntriton-server  | instance_group {\r\ntriton-server  |   kind: KIND_CPU\r\ntriton-server  | }\r\ntriton-server  | backend: \"python\"\r\ntriton-server  | \r\ntriton-server  | I0325 11:25:44.907182 1 model_repository_manager.cc:700] AsyncLoad() 'add_sub'\r\ntriton-server  | I0325 11:25:44.907307 1 model_repository_manager.cc:939] TriggerNextAction() 'add_sub' version 1: 1\r\ntriton-server  | I0325 11:25:44.907370 1 model_repository_manager.cc:975] Load() 'add_sub' version 1\r\ntriton-server  | I0325 11:25:44.907384 1 model_repository_manager.cc:994] loading: add_sub:1\r\ntriton-server  | I0325 11:25:45.008071 1 model_repository_manager.cc:1052] CreateModel() 'add_sub' version 1\r\ntriton-server  | I0325 11:25:45.008675 1 shared_library.cc:108] OpenLibraryHandle: /Workdir/TRITON/backends/python/libtriton_python.so\r\ntriton-server  | I0325 11:25:45.019121 1 python.cc:1684] 'python' TRITONBACKEND API version: 1.8\r\ntriton-server  | I0325 11:25:45.019217 1 python.cc:1703] backend configuration:\r\ntriton-server  | {}\r\ntriton-server  | I0325 11:25:45.019271 1 python.cc:1819] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\r\ntriton-server  | I0325 11:25:45.022821 1 python.cc:1859] TRITONBACKEND_ModelInitialize: add_sub (version 1)\r\ntriton-server  | I0325 11:25:45.026011 1 model_config_utils.cc:1616] ModelConfig 64-bit fields:\r\ntriton-server  | I0325 11:25:45.026120 1 model_config_utils.cc:1618]    ModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\r\ntriton-server  | I0325 11:25:45.026151 1 model_config_utils.cc:1618]    ModelConfig::dynamic_batching::max_queue_delay_microseconds\r\ntriton-server  | I0325 11:25:45.026168 1 model_config_utils.cc:1618]    ModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\r\ntriton-server  | I0325 11:25:45.026193 1 model_config_utils.cc:1618]    ModelConfig::ensemble_scheduling::step::model_version\r\ntriton-server  | I0325 11:25:45.026211 1 model_config_utils.cc:1618]    ModelConfig::input::dims\r\ntriton-server  | I0325 11:25:45.026227 1 model_config_utils.cc:1618]    ModelConfig::input::reshape::shape\r\ntriton-server  | I0325 11:25:45.026243 1 model_config_utils.cc:1618]    ModelConfig::instance_group::secondary_devices::device_id\r\ntriton-server  | I0325 11:25:45.026265 1 model_config_utils.cc:1618]    ModelConfig::model_warmup::inputs::value::dims\r\ntriton-server  | I0325 11:25:45.026281 1 model_config_utils.cc:1618]    ModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\r\ntriton-server  | I0325 11:25:45.026296 1 model_config_utils.cc:1618]    ModelConfig::optimization::cuda::graph_spec::input::value::dim\r\ntriton-server  | I0325 11:25:45.026311 1 model_config_utils.cc:1618]    ModelConfig::output::dims\r\ntriton-server  | I0325 11:25:45.026334 1 model_config_utils.cc:1618]    ModelConfig::output::reshape::shape\r\ntriton-server  | I0325 11:25:45.026352 1 model_config_utils.cc:1618]    ModelConfig::sequence_batching::direct::max_queue_delay_microseconds\r\ntriton-server  | I0325 11:25:45.026368 1 model_config_utils.cc:1618]    ModelConfig::sequence_batching::max_sequence_idle_microseconds\r\ntriton-server  | I0325 11:25:45.026383 1 model_config_utils.cc:1618]    ModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\r\ntriton-server  | I0325 11:25:45.026398 1 model_config_utils.cc:1618]    ModelConfig::sequence_batching::state::dims\r\ntriton-server  | I0325 11:25:45.026413 1 model_config_utils.cc:1618]    ModelConfig::sequence_batching::state::initial_state::dims\r\ntriton-server  | I0325 11:25:45.026429 1 model_config_utils.cc:1618]    ModelConfig::version_policy::specific::versions\r\ntriton-server  | I0325 11:25:45.027675 1 python.cc:1908] TRITONBACKEND_ModelInstanceInitialize: add_sub_0 (CPU device 0)\r\ntriton-server  | I0325 11:25:45.027749 1 backend_model_instance.cc:72] Creating instance add_sub_0 on CPU using artifact ''\r\ntriton-server  | I0325 11:25:45.053005 9 python.cc:1209] Starting Python backend stub:  exec /Workdir/TRITON/backends/python/triton_python_backend_stub /Workdir/Models/add_sub/1/model.py triton_python_backend_shm_region_1 67108864 67108864 1 /Workdir/TRITON/backends/python 64 add_sub_0\r\n```\r\nThere does not seem to be any system load during the time it is waiting, no growth of memory usage either.\r\n\r\n**Triton Information**\r\nI build a container for the 2.19.0 TRITON server using the prebuild server tar using the following Dockerfile:\r\n\r\n```\r\nFROM nvcr.io/nvidia/nvidia-l4t-base:r32.7.1\r\n\r\nENV JETPACK_VERSION=4.6.1\r\nENV TRITON_VERSION=2.19.0\r\nENV TRITON_PATH=/Workdir/TRITON/\r\n\r\nENV DEBIAN_FRONTEND=noninteractive\r\n\r\nRUN apt-get update              \\\r\n        && apt-get install -y --no-install-recommends \\\r\n        libb64-0d               \\\r\n        libre2-4                \\\r\n        libssl1.1               \\\r\n        rapidjson-dev           \\\r\n        libopenblas-dev         \\\r\n        libarchive-dev          \\\r\n        zlib1g                  \\\r\n        python3                 \\\r\n        python3-dev             \\\r\n        python3-pip\r\n\r\nRUN python3 -m pip install -U pip setuptools\r\nRUN python3 -m pip install numpy\r\n\r\nWORKDIR ${TRITON_PATH}/\r\n\r\nRUN wget https://github.com/triton-inference-server/server/releases/download/v$TRITON_VERSION/tritonserver$TRITON_VERSION-jetpack$JETPACK_VERSION.tgz \\\r\n    && tar -xvf tritonserver$TRITON_VERSION-jetpack$JETPACK_VERSION.tgz \\\r\n    && ln -s ${TRITON_PATH}/bin/tritonserver /usr/bin/tritonserver \\\r\n    && rm tritonserver${TRITON_VERSION}-jetpack$JETPACK_VERSION.tgz\r\n\r\nWORKDIR /Workdir/\r\n```\r\n\r\n\r\n**To Reproduce**\r\nUsing the above Dockerfile to build the image with the following docker-compose file to run the container:\r\n```\r\n  triton-server:\r\n    image: triton-inference-server:2.19.0-aarch64-jetpack4.6\r\n    container_name: triton-server\r\n    ports:\r\n      - 8000:8000\r\n      - 8001:8001\r\n      - 8002:8002\r\n    runtime: nvidia\r\n    volumes:\r\n      - /etc/timezone:/etc/timezone:ro\r\n      - ./Models:/Workdir/Models\r\n    environment:\r\n      - \"NVIDIA_VISIBLE_DEVICES=0\"\r\n      - \"NVIDIA_DRIVER_CAPABILITIES=compute,graphics,utility,video\"\r\n    command: tritonserver --backend-directory=/Workdir/TRITON/backends --model-store=/Workdir/Models --strict-model-config=false --exit-on-error=true --log-verbose=true --model-control-mode=Explicit --load-model \"add_sub\"\r\n```\r\nThe model + configuration can be found in the [examples directory of the Python backend](https://github.com/triton-inference-server/python_backend/tree/main/examples/add_sub). No modifications have been made to these files.\r\n\r\n**Expected behavior**\r\nI expect the model to load the same as it does on an AMD64 platform. I have not tested previous builds of the server because the standard packages did not include the Python backend yet. I hope this is enough information, if I can help please let me know.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4113/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4113/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4089", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4089/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4089/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4089/events", "html_url": "https://github.com/triton-inference-server/server/issues/4089", "id": 1176866596, "node_id": "I_kwDOCQnI4s5GJY8k", "number": 4089, "title": "Input to the script for publishing models to mlflow is overly particular with inputs", "user": {"login": "rmkraus", "id": 4956442, "node_id": "MDQ6VXNlcjQ5NTY0NDI=", "avatar_url": "https://avatars.githubusercontent.com/u/4956442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmkraus", "html_url": "https://github.com/rmkraus", "followers_url": "https://api.github.com/users/rmkraus/followers", "following_url": "https://api.github.com/users/rmkraus/following{/other_user}", "gists_url": "https://api.github.com/users/rmkraus/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmkraus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmkraus/subscriptions", "organizations_url": "https://api.github.com/users/rmkraus/orgs", "repos_url": "https://api.github.com/users/rmkraus/repos", "events_url": "https://api.github.com/users/rmkraus/events{/privacy}", "received_events_url": "https://api.github.com/users/rmkraus/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-03-22T14:24:17Z", "updated_at": "2022-04-22T15:58:55Z", "closed_at": "2022-04-22T15:58:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory /common/models/abp-nvsmi-xgb/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/click/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/click/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/click/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/click/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"/mlflow/triton-inference-server/server/deploy/mlflow-triton-plugin/scripts/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/mlflow/models/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"/mlflow/triton-inference-server/server/deploy/mlflow-triton-plugin/scripts/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"/opt/conda/envs/mlflow/lib/python3.8/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"/opt/conda/envs/mlflow/lib/python3.8/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"/opt/conda/envs/mlflow/lib/python3.8/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '/tmp/tmpdg2r5f0_/model/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory /common/models/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4089/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4068", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4068/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4068/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4068/events", "html_url": "https://github.com/triton-inference-server/server/issues/4068", "id": 1172288105, "node_id": "I_kwDOCQnI4s5F37Jp", "number": 4068, "title": "Perf_Analyzer always throwing 'std::length_error'", "user": {"login": "arnabdas", "id": 1061389, "node_id": "MDQ6VXNlcjEwNjEzODk=", "avatar_url": "https://avatars.githubusercontent.com/u/1061389?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arnabdas", "html_url": "https://github.com/arnabdas", "followers_url": "https://api.github.com/users/arnabdas/followers", "following_url": "https://api.github.com/users/arnabdas/following{/other_user}", "gists_url": "https://api.github.com/users/arnabdas/gists{/gist_id}", "starred_url": "https://api.github.com/users/arnabdas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arnabdas/subscriptions", "organizations_url": "https://api.github.com/users/arnabdas/orgs", "repos_url": "https://api.github.com/users/arnabdas/repos", "events_url": "https://api.github.com/users/arnabdas/events{/privacy}", "received_events_url": "https://api.github.com/users/arnabdas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2022-03-17T11:59:48Z", "updated_at": "2022-03-31T17:18:26Z", "closed_at": "2022-03-31T17:18:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWe are trying `perf_analyzer` to profile our custom model for TTS with Triton. We are trying with various combinations of the options like the following.\r\n\r\n``` bash\r\nperf_client -m nemo_model1 -u $host:$port -i gRPC --concurrency-range 1:4:2\r\nperf_client -m nemo_model1 -u \"$host:$port\" -i gRPC -v -v --input-data=\"random\" --string-length 61\r\n```\r\nBut every time we got the following error:\r\n\r\n``` text\r\nname: \"nemo_model1\"\r\nversions: \"1\"\r\nplatform: \"python\"\r\ninputs {\r\n  name: \"input__0\"\r\n  datatype: \"BYTES\"\r\n  shape: -1\r\n  shape: -1\r\n}\r\noutputs {\r\n  name: \"output__0\"\r\n  datatype: \"INT16\"\r\n  shape: -1\r\n  shape: -1\r\n}\r\n\r\nconfig {\r\n  name: \"nemo_model1\"\r\n  version_policy {\r\n    latest {\r\n      num_versions: 1\r\n    }\r\n  }\r\n  max_batch_size: 128\r\n  input {\r\n    name: \"input__0\"\r\n    data_type: TYPE_STRING\r\n    dims: -1\r\n  }\r\n  output {\r\n    name: \"output__0\"\r\n    data_type: TYPE_INT16\r\n    dims: -1\r\n  }\r\n  instance_group {\r\n    name: \"nemo_model1_0\"\r\n    count: 1\r\n    gpus: 0\r\n    kind: KIND_GPU\r\n  }\r\n  dynamic_batching {\r\n    preferred_batch_size: 128\r\n  }\r\n  optimization {\r\n    input_pinned_memory {\r\n      enable: true\r\n    }\r\n    output_pinned_memory {\r\n      enable: true\r\n    }\r\n  }\r\n  backend: \"python\"\r\n}\r\n\r\nterminate called after throwing an instance of 'std::length_error'\r\n  what():  vector::_M_default_append\r\nAborted (core dumped)\r\n```\r\n\r\nWhat are we doing wrong in invoking the command.\r\n\r\n**Triton Information**\r\nWe are currently using `v2.8.0`.\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nWe have used the container \r\n\r\n``` bash\r\nFROM nvcr.io/nvidia/tritonserver:21.03-py3\r\n\r\nADD models /models\r\n\r\n# Setup Python\r\nRUN apt update\r\nRUN apt upgrade -y\r\nRUN apt install -y software-properties-common libfreetype6-dev libsndfile-dev\r\nRUN add-apt-repository ppa:deadsnakes/ppa\r\nRUN apt install -y python3-dev python3-venv\r\nRUN /usr/bin/python3 -m pip install --upgrade pip\r\n\r\n# Install NeMo\r\nRUN pip install torch==1.10.1\r\nADD NeMo NeMo/\r\nADD nemo.patch NeMo/\r\nWORKDIR NeMo\r\nRUN patch -p1 < nemo.patch\r\nRUN pip install .[tts]\r\nWORKDIR ../\r\nENV PYTHONPATH=NeMo\r\n# Copy hardcoded models\r\nRUN mkdir models\r\nCOPY default_model/imda-only.txt default_model/imda-en_g2p.pt models/\r\nRUN apt-get clean autoclean && apt-get autoremove --yes && rm -rf /var/lib/{apt,dpkg,cache,log}/\r\n\r\nCMD [\"/opt/tritonserver/bin/tritonserver\", \"--model-repository=/models\"]\r\n```\r\n\r\nFollowing is our model configuration file:\r\n\r\n``` text\r\nname: \"nemo_model1\"\r\nbackend: \"python\"\r\nmax_batch_size: 128\r\ninput {\r\n    name: \"input__0\"\r\n    data_type: TYPE_STRING\r\n    dims: [-1]\r\n  }\r\noutput [\r\n  {\r\n    name: \"output__0\"\r\n    data_type: TYPE_INT16\r\n    dims: [-1]\r\n  }\r\n]\r\ndynamic_batching {}\r\ninstance_group {\r\n      count: 1\r\n      kind: KIND_GPU\r\n  }\r\n```\r\n\r\nHow to overcome the error with `perf_analyzer`. We are invoking `perf_analyzer` on a Ubuntu 20.04 node. We have installed the required dependencies. However for the following two packages, there were no matching candidate, so we installed the available packages for them.\r\n\r\n``` bash\r\nlibopencv-dev=3.2.0+dfsg-4ubuntu0.1 \\\r\nlibopencv-core-dev=3.2.0+dfsg-4ubuntu0.1 \\\r\n```", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4068/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4068/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4051", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4051/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4051/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4051/events", "html_url": "https://github.com/triton-inference-server/server/issues/4051", "id": 1167823883, "node_id": "I_kwDOCQnI4s5Fm5QL", "number": 4051, "title": "[Question]Can TRITON POLL MODE hot-update the ensemble model?", "user": {"login": "FlyTOmeLight", "id": 41530068, "node_id": "MDQ6VXNlcjQxNTMwMDY4", "avatar_url": "https://avatars.githubusercontent.com/u/41530068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FlyTOmeLight", "html_url": "https://github.com/FlyTOmeLight", "followers_url": "https://api.github.com/users/FlyTOmeLight/followers", "following_url": "https://api.github.com/users/FlyTOmeLight/following{/other_user}", "gists_url": "https://api.github.com/users/FlyTOmeLight/gists{/gist_id}", "starred_url": "https://api.github.com/users/FlyTOmeLight/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FlyTOmeLight/subscriptions", "organizations_url": "https://api.github.com/users/FlyTOmeLight/orgs", "repos_url": "https://api.github.com/users/FlyTOmeLight/repos", "events_url": "https://api.github.com/users/FlyTOmeLight/events{/privacy}", "received_events_url": "https://api.github.com/users/FlyTOmeLight/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-03-14T03:00:53Z", "updated_at": "2022-05-31T17:54:09Z", "closed_at": "2022-05-31T17:54:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "In POLL MODE, when I updated the subfolder corresponding to ensemble, the following error was reported:\r\n<img width=\"957\" alt=\"image\" src=\"https://user-images.githubusercontent.com/41530068/158097243-369dee67-2e67-4b75-9414-ef0dfac325b3.png\">\r\n\r\nAnother scenario, in POLL MODE, when I update the corresponding subfolder of a submodel in the orchestration model, the following error is reported:\r\n<img width=\"958\" alt=\"image\" src=\"https://user-images.githubusercontent.com/41530068/158097446-980b24f7-2d7e-4ef7-82c1-a4ccb7d4d98d.png\">\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4051/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4045", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4045/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4045/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4045/events", "html_url": "https://github.com/triton-inference-server/server/issues/4045", "id": 1166692514, "node_id": "I_kwDOCQnI4s5FilCi", "number": 4045, "title": "Auto-Setting upstream container version in build.py for no container build", "user": {"login": "jishminor", "id": 16505634, "node_id": "MDQ6VXNlcjE2NTA1NjM0", "avatar_url": "https://avatars.githubusercontent.com/u/16505634?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jishminor", "html_url": "https://github.com/jishminor", "followers_url": "https://api.github.com/users/jishminor/followers", "following_url": "https://api.github.com/users/jishminor/following{/other_user}", "gists_url": "https://api.github.com/users/jishminor/gists{/gist_id}", "starred_url": "https://api.github.com/users/jishminor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jishminor/subscriptions", "organizations_url": "https://api.github.com/users/jishminor/orgs", "repos_url": "https://api.github.com/users/jishminor/repos", "events_url": "https://api.github.com/users/jishminor/events{/privacy}", "received_events_url": "https://api.github.com/users/jishminor/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-03-11T17:46:35Z", "updated_at": "2022-04-13T15:33:29Z", "closed_at": "2022-04-13T15:33:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Description**\r\nWhen running a no container build using build.py, upstream image is not set resulting in failed docker image pulls for the desired backends.\r\n\r\n**Triton Information**\r\nr22.02\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nBuilding myself using no container build\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nRunning on an x86 ubuntu 20.04 machine. Command being run to build is:\r\n```bash\r\n./build.py --cmake-dir=$HOME/code/server/build --build-dir=/tmp/citritonbuild --no-container-build --enable-logging --enable-stats --enable-tracing --enable-metrics --endpoint=http --endpoint=grpc --backend=pytorch --backend=tensorflow2 --backend=tensorflow1 --extra-backend-cmake-arg=tensorflow2:TRITON_TENSORFLOW_INSTALL_EXTRA_DEPS=ON\r\n```\r\n\r\n**Expected behavior**\r\nIn `build.py` `FLAGS.upstream_container_version` only gets set based on the version map if building with the container build. Line reference [here](https://github.com/triton-inference-server/server/blob/f6ad9caf5ebe5c7a8b93f176df9927af42d588c5/build.py#L1737). Is there a reason this is not also done for the no container build?\r\n\r\nIf I build with passing the flag `--upstream-container-version=22.02`, the build completes fine.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4045/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4045/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4026", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4026/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4026/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4026/events", "html_url": "https://github.com/triton-inference-server/server/issues/4026", "id": 1163601564, "node_id": "I_kwDOCQnI4s5FWyac", "number": 4026, "title": "Ensemble model using BLS: stub unhealthy", "user": {"login": "WadoodAbdul", "id": 52132963, "node_id": "MDQ6VXNlcjUyMTMyOTYz", "avatar_url": "https://avatars.githubusercontent.com/u/52132963?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WadoodAbdul", "html_url": "https://github.com/WadoodAbdul", "followers_url": "https://api.github.com/users/WadoodAbdul/followers", "following_url": "https://api.github.com/users/WadoodAbdul/following{/other_user}", "gists_url": "https://api.github.com/users/WadoodAbdul/gists{/gist_id}", "starred_url": "https://api.github.com/users/WadoodAbdul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WadoodAbdul/subscriptions", "organizations_url": "https://api.github.com/users/WadoodAbdul/orgs", "repos_url": "https://api.github.com/users/WadoodAbdul/repos", "events_url": "https://api.github.com/users/WadoodAbdul/events{/privacy}", "received_events_url": "https://api.github.com/users/WadoodAbdul/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2022-03-09T07:51:01Z", "updated_at": "2023-03-17T15:40:30Z", "closed_at": "2022-04-22T15:59:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI am trying to run tokenizer+encoder for a model in triton. This is implemented using triton BLS and adding in the encoder inference call in model.py. However, this is the error message upon inferencing the ensemble model through triton python client,\r\n`Stub process is unhealthy and it will be restarted.`\r\n\r\n**Triton Information**\r\n22.01\r\n\r\nCommand to run docker triton:\r\n```\r\nsudo docker run -it --rm --gpus all -p9000:8000 -p9001:8001 -p9002:8002 --shm-size 4g \\\r\n-v $PWD/triton_mbart:/models [nvcr.io/nvidia/tritonserver:22.01-py3](http://nvcr.io/nvidia/tritonserver:22.01-py3) \\\r\nbash -c \"pip install transformers torch==1.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html && \\\r\ntritonserver --model-repository=/models\"\r\n```\r\n\r\n**To Reproduce**\r\nThe model repo consists of two models:\r\n- encoder (mbart_onnx_encoder)\r\n- tokenizer+encoder (mbart_tokenizer_encoder)\r\n\r\n##### Encoder\r\nconfig.pbtxt:\r\n```\r\nname: \"mbart_onnx_encoder\"\r\nplatform: \"onnxruntime_onnx\"\r\nmax_batch_size : 0\r\ndefault_model_filename: \"mbart_encoder.onnx\"\r\n\r\ninput [\r\n  {\r\n    name: \"input\"\r\n    data_type: TYPE_INT64\r\n    dims: [ -1, -1 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"output\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, -1, 1024 ]\r\n  }\r\n]\r\n\r\ninstance_group [\r\n    {\r\n      count: 1\r\n      kind: KIND_CPU\r\n    }\r\n]\r\n\r\n```\r\n\r\n##### Tokenizer+Encoder\r\nconfig.pbtxt:\r\n```\r\name: \"mbart_tokenizer_encoder\"\r\nmax_batch_size: 0\r\nbackend: \"python\"\r\n\r\ninput [\r\n{\r\n    name: \"TEXT\"\r\n    data_type: TYPE_STRING\r\n    dims: [ -1 ]\r\n}\r\n]\r\n\r\noutput [\r\n{\r\n    name: \"output\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, -1, 1024 ]\r\n}\r\n]\r\n\r\ninstance_group [\r\n    {\r\n      count: 1\r\n      kind: KIND_GPU\r\n    }\r\n]\r\nparameters: {\r\n  key: \"FORCE_CPU_ONLY_INPUT_TENSORS\"\r\n  value: {\r\n    string_value:\"no\"\r\n  }\r\n}\r\n\r\n```\r\n\r\nmodel.py\r\n```\r\nimport os\r\nfrom typing import Dict, List\r\n\r\nimport torch\r\nimport numpy as np\r\n\r\n\r\ntry:\r\n    # noinspection PyUnresolvedReferences\r\n    import triton_python_backend_utils as pb_utils\r\nexcept ImportError:\r\n    pass  # triton_python_backend_utils exists only inside Triton Python backend.\r\n\r\nfrom transformers import AutoTokenizer, PreTrainedTokenizer, TensorType\r\n\r\n\r\nclass TritonPythonModel:\r\n    tokenizer: PreTrainedTokenizer\r\n\r\n    def initialize(self, args: Dict[str, str]) -> None:\r\n        \"\"\"\r\n        Initialize the tokenization process\r\n        :param args: arguments from Triton config file\r\n        \"\"\"\r\n        # more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc\r\n        path: str = os.path.join(args[\"model_repository\"], args[\"model_version\"])\r\n        self.tokenizer = AutoTokenizer.from_pretrained(path)\r\n        encoder_model = args[\"model_name\"].replace(\"_tokenizer_encoder\", \"_onnx_encoder\")\r\n\r\n        def encoder_inference(input_ids):\r\n            inputs = input_ids\r\n            inference_request = pb_utils.InferenceRequest(\r\n                model_name=encoder_model, requested_output_names=[\"output\"], inputs=inputs\r\n            )\r\n            inference_response = inference_request.exec()\r\n            return [pb_utils.get_output_tensor_by_name(inference_response, \"output\")]\r\n\r\n        self.encoder_inference = encoder_inference\r\n\r\n    def execute(self, requests) -> \"List[List[pb_utils.Tensor]]\":\r\n        \"\"\"\r\n        Parse and tokenize each request\r\n        :param requests: 1 or more requests received by Triton server.\r\n        :return: text as input tensors\r\n        \"\"\"\r\n        responses = []\r\n        # for loop for batch requests (disabled in our case)\r\n        for request in requests:\r\n            # binary data typed back to string\r\n            query = [t.decode(\"UTF-8\") for t in pb_utils.get_input_tensor_by_name(request, \"TEXT\").as_numpy().tolist()]\r\n            tokens: Dict[str, np.ndarray] = self.tokenizer(text=query, return_tensors=TensorType.NUMPY, return_attention_mask=False)\r\n            # tensorrt uses int32 as input type, ort uses int64\r\n            tokens = {k: v.astype(np.int64) for k, v in tokens.items()}\r\n            # communicate the tokenization results to Triton server\r\n            outputs = list()\r\n            for input_name in self.tokenizer.model_input_names:\r\n                tensor_input = pb_utils.Tensor(input_name, tokens[input_name])\r\n                outputs.append(tensor_input)\r\n                break # onnx encoder is not expecting attention masks\r\n            tokenizer_response = pb_utils.InferenceResponse(output_tensors=outputs)\r\n            input_ids = [pb_utils.Tensor.from_dlpack(\"input_ids\", pb_utils.get_output_tensor_by_name(tokenizer_response, \"input_ids\").to_dlpack())]\r\n            encoder_outputs = self.encoder_inference(input_ids)\r\n            inference_response = pb_utils.InferenceResponse(output_tensors=encoder_outputs)\r\n\r\n            responses.append(inference_response)\r\n\r\n        return responses\r\n```\r\n**Expected behavior**\r\n\r\nI have run separate inferencing for encoder and tokenizer using triton python client successfully. However, this issue is faced when calling the ensemble model,\r\n\r\n```\r\ntritonclient.utils.InferenceServerException: Failed to process the request(s) for model instance 'mbart_tokenizer_encoder_0', message: Stub process is not healthy.\r\n```\r\n\r\nNote: I have tried increasing the shm-size and have used 4g.\r\n\r\nMy end goal is to use encoder-decoder model with beam search in triton. It would be great if you could share share any suggestions/resources for this.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4026/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4026/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4017", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4017/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4017/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4017/events", "html_url": "https://github.com/triton-inference-server/server/issues/4017", "id": 1160972560, "node_id": "I_kwDOCQnI4s5FMwkQ", "number": 4017, "title": "tritonserver exited with coredump when using cuda graph optimization", "user": {"login": "wangchengdng", "id": 10705707, "node_id": "MDQ6VXNlcjEwNzA1NzA3", "avatar_url": "https://avatars.githubusercontent.com/u/10705707?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangchengdng", "html_url": "https://github.com/wangchengdng", "followers_url": "https://api.github.com/users/wangchengdng/followers", "following_url": "https://api.github.com/users/wangchengdng/following{/other_user}", "gists_url": "https://api.github.com/users/wangchengdng/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangchengdng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangchengdng/subscriptions", "organizations_url": "https://api.github.com/users/wangchengdng/orgs", "repos_url": "https://api.github.com/users/wangchengdng/repos", "events_url": "https://api.github.com/users/wangchengdng/events{/privacy}", "received_events_url": "https://api.github.com/users/wangchengdng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-03-07T06:40:28Z", "updated_at": "2022-04-09T07:02:20Z", "closed_at": "2022-04-09T07:02:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWe have some TensorRT models that use cuda graph optimization. Our online models need to be updated regularly, but we found that the server will exit when the model is updated, and a coredump will be generated, and the server works fine without using cuda graph.\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n22.02\r\nAre you using the Triton container or did you build it yourself?\r\nUsing the container.\r\n```\r\ndocker pull nvcr.io/nvidia/tritonserver:22.02-py3\r\n```\r\n**To Reproduce**\r\nStep 1:\r\nPrepare the model, the model directory structure is as follows\r\n```\r\nmodel_ple_v3_time_noclip\r\n\u251c\u2500\u2500 1\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 model.plan\r\n\u251c\u2500\u2500 config.pbtxt\r\n\u2514\u2500\u2500 mv.sh\r\n```\r\nThe model configuration is as follows\r\n```\r\nplatform: \"tensorrt_plan\"\r\n\r\ninstance_group [\r\n  {\r\n    count: 2\r\n    kind: KIND_GPU\r\n    gpus: [0]\r\n  }\r\n]\r\n\r\noptimization{\r\n   graph: {\r\n       level : 1\r\n   },\r\n   eager_batching : 1,\r\n   cuda: {\r\n       graphs: 1,\r\n       graph_spec: [\r\n    {\r\n        batch_size: 167\r\n    }\r\n    ]\r\n       busy_wait_events:1,\r\n       output_copy_stream: 1\r\n   }\r\n}\r\n```\r\nStep 2:\r\nStart the server with the following command\r\n```\r\n./bin/tritonserver --grpc-port=8899  --model-repository=model_zoo  --model-control-mode=explicit  --strict-model-config=false --log-verbose=false\r\n```\r\nStep 3:\r\nUse a script to update the model every 5 seconds\r\n```\r\nfor ((i=1;i<=100000;i++));\r\ndo\r\n    src=$(($i))\r\n    dst=$(($i+1))\r\n    cp -rf  $src $dst\r\n    curl 127.0.0.1:8000/v2/repository/models/model_ple_v3_time_noclip/load -X POST\r\n    sleep 5\r\ndone\r\n```\r\nStep 4:\r\nUse perfclient to request the server\r\n```\r\n./perf_client -m model_ple_v3_time_noclip  -p3000 -t5 -b167 -v -u 127.0.0.1:8000\r\n```\r\n\r\nWhen the model is updated, the following error will be reported\r\n```\r\nE0307 03:51:14.842791 3164 logging.cc:43] 1: [cudaDriverHelpers.cpp::cudaEventRecordWithExternalEvent::135] Error Code 1: Cuda Driver (operation failed due to a previous error during capture)\r\nE0307 03:51:14.842840 3164 logging.cc:43] 1: [checkMacros.cpp::catchCudaError::272] Error Code 1: Cuda Runtime (operation failed due to a previous error during capture)\r\nE0307 03:51:14.842854 3164 tensorrt.cc:4996] unable to record CUDA graph for model_ple_v3_time_noclip_0_0\r\nE0307 03:51:14.842979 3164 tensorrt.cc:5004] unable to finish CUDA graph for model_ple_v3_time_noclip_0_0: operation failed due to a previous error during capture\r\nE0307 03:51:14.843961 3164 logging.cc:43] 1: [cudaDriverHelpers.cpp::cudaEventRecordWithExternalEvent::135] Error Code 1: Cuda Driver (operation failed due to a previous error during capture)\r\nE0307 03:51:14.843984 3164 logging.cc:43] 1: [checkMacros.cpp::catchCudaError::272] Error Code 1: Cuda Runtime (operation failed due to a previous error during capture)\r\nE0307 03:51:14.843998 3164 tensorrt.cc:4996] unable to record CUDA graph for model_ple_v3_time_noclip_0_0\r\nE0307 03:51:14.844072 3164 tensorrt.cc:5004] unable to finish CUDA graph for model_ple_v3_time_noclip_0_0: operation failed due to a previous error during capture\r\nI0307 03:51:14.844085 3164 tensorrt.cc:1409] Created instance model_ple_v3_time_noclip_0_0 on GPU 0 with stream priority 0 and optimization profile default[0];\r\nSignal (11) received.\r\n```\r\nThe server will generate the following coredump\r\n```\r\n#0  0x00007f39578af2ba in nvidia::inferenceserver::PinnedMemoryManager::AllocInternal(void**, unsigned long, TRITONSERVER_memorytype_enum*, bool, nvidia::inferenceserver::PinnedMemoryManager::PinnedMemory*) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#1  0x00007f39578b05e1 in nvidia::inferenceserver::PinnedMemoryManager::Alloc(void**, unsigned long, TRITONSERVER_memorytype_enum*, bool) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#2  0x00007f39579f858d in TRITONBACKEND_MemoryManagerAllocate () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#3  0x00007f37f053ac2f in triton::backend::BackendOutputResponder::FlushPendingPinned(char const*, TRITONSERVER_memorytype_enum, long) () from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n#4  0x00007f37f053c274 in triton::backend::BackendOutputResponder::ProcessTensor(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, TRITONSERVER_datatype_enum, std::vector<long, std::allocator<long> >&, char const*, TRITONSERVER_memorytype_enum, long) () from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n#5  0x00007f37f04d124f in triton::backend::tensorrt::ModelInstanceState::Run(TRITONBACKEND_Request**, unsigned int, unsigned long) () from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n#6  0x00007f37f04db0ce in triton::backend::tensorrt::ModelInstanceState::ProcessRequests(TRITONBACKEND_Request**, unsigned int) () from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n#7  0x00007f37f04dc156 in TRITONBACKEND_ModelInstanceExecute () from /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so\r\n#8  0x00007f3957a0bf9a in nvidia::inferenceserver::TritonModelInstance::Execute(std::vector<TRITONBACKEND_Request*, std::allocator<TRITONBACKEND_Request*> >&) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#9  0x00007f3957a0c6b7 in nvidia::inferenceserver::TritonModelInstance::Schedule(std::vector<std::unique_ptr<nvidia::inferenceserver::InferenceRequest, std::default_delete<nvidia::inferenceserver::InferenceRequest> >, std::allocator<std::unique_ptr<nvidia::inferenceserver::InferenceRequest, std::default_delete<nvidia::inferenceserver::InferenceRequest> > > >&&, std::function<void ()> const&) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#10 0x00007f39578a41a1 in nvidia::inferenceserver::Payload::Execute(bool*) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#11 0x00007f3957a06527 in nvidia::inferenceserver::TritonModelInstance::TritonBackendThread::BackendThread(int, int) () from /opt/tritonserver/bin/../lib/libtritonserver.so\r\n#12 0x00007f3957258de4 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6\r\n#13 0x00007f39576d5609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#14 0x00007f3956f43163 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n```\r\n**Expected behavior**\r\nWe hope to use cuda graph to optimize inference speed\r\nWe hope that the model can be updated normally when using cuda graph\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4017/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4002", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/4002/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/4002/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/4002/events", "html_url": "https://github.com/triton-inference-server/server/issues/4002", "id": 1157296760, "node_id": "I_kwDOCQnI4s5E-vJ4", "number": 4002, "title": "(triton-third-party) Azure-storage-cpplite dependency will not build with tag 0.3.0", "user": {"login": "alexraymond", "id": 31191280, "node_id": "MDQ6VXNlcjMxMTkxMjgw", "avatar_url": "https://avatars.githubusercontent.com/u/31191280?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexraymond", "html_url": "https://github.com/alexraymond", "followers_url": "https://api.github.com/users/alexraymond/followers", "following_url": "https://api.github.com/users/alexraymond/following{/other_user}", "gists_url": "https://api.github.com/users/alexraymond/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexraymond/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexraymond/subscriptions", "organizations_url": "https://api.github.com/users/alexraymond/orgs", "repos_url": "https://api.github.com/users/alexraymond/repos", "events_url": "https://api.github.com/users/alexraymond/events{/privacy}", "received_events_url": "https://api.github.com/users/alexraymond/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "jbkyang-nvi", "id": 80359429, "node_id": "MDQ6VXNlcjgwMzU5NDI5", "avatar_url": "https://avatars.githubusercontent.com/u/80359429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbkyang-nvi", "html_url": "https://github.com/jbkyang-nvi", "followers_url": "https://api.github.com/users/jbkyang-nvi/followers", "following_url": "https://api.github.com/users/jbkyang-nvi/following{/other_user}", "gists_url": "https://api.github.com/users/jbkyang-nvi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbkyang-nvi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbkyang-nvi/subscriptions", "organizations_url": "https://api.github.com/users/jbkyang-nvi/orgs", "repos_url": "https://api.github.com/users/jbkyang-nvi/repos", "events_url": "https://api.github.com/users/jbkyang-nvi/events{/privacy}", "received_events_url": "https://api.github.com/users/jbkyang-nvi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jbkyang-nvi", "id": 80359429, "node_id": "MDQ6VXNlcjgwMzU5NDI5", "avatar_url": "https://avatars.githubusercontent.com/u/80359429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbkyang-nvi", "html_url": "https://github.com/jbkyang-nvi", "followers_url": "https://api.github.com/users/jbkyang-nvi/followers", "following_url": "https://api.github.com/users/jbkyang-nvi/following{/other_user}", "gists_url": "https://api.github.com/users/jbkyang-nvi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbkyang-nvi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbkyang-nvi/subscriptions", "organizations_url": "https://api.github.com/users/jbkyang-nvi/orgs", "repos_url": "https://api.github.com/users/jbkyang-nvi/repos", "events_url": "https://api.github.com/users/jbkyang-nvi/events{/privacy}", "received_events_url": "https://api.github.com/users/jbkyang-nvi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2022-03-02T14:49:50Z", "updated_at": "2022-03-15T14:51:29Z", "closed_at": "2022-03-15T14:51:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen building Triton (client) from source with Clang 11, the build will fail at the triton-third-party step because of an issue with azure-storage-cpplite.\r\n\r\n`azure-storage-cpplite/src/azure-storage-cpplite/src/base64.cpp:108:28: error: no member named 'runtime_error' in namespace 'std'`\r\n\r\nThis is caused by a missing `#include <stdexcept>` in their repository. It has been fixed in [this commit](https://github.com/Azure/azure-storage-cpplite/commit/a3a9c21de3fa9932dcfceb2b6617757a511c75c3). Unfortunately, this is not covered by the tag `0.3.0` used in the `third-party` subrepo.\r\n\r\nOne way of circumventing the issue in my case was to update the `GIT_TAG` of azure-storage-cpplite dependency to `origin/master`. \r\n\r\n**Triton Information**\r\nBuilding triton-inference-client from SHA f0f2b89b3b8418a0a79a8dd60329a8b99489268b.\r\n\r\n**To Reproduce**\r\nBuild triton-inference-client from source with Clang 11 + Ubuntu 20.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/4002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/4002/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3997", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3997/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3997/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3997/events", "html_url": "https://github.com/triton-inference-server/server/issues/3997", "id": 1155012358, "node_id": "I_kwDOCQnI4s5E2BcG", "number": 3997, "title": "Memory not released ", "user": {"login": "TannedCung", "id": 54882167, "node_id": "MDQ6VXNlcjU0ODgyMTY3", "avatar_url": "https://avatars.githubusercontent.com/u/54882167?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TannedCung", "html_url": "https://github.com/TannedCung", "followers_url": "https://api.github.com/users/TannedCung/followers", "following_url": "https://api.github.com/users/TannedCung/following{/other_user}", "gists_url": "https://api.github.com/users/TannedCung/gists{/gist_id}", "starred_url": "https://api.github.com/users/TannedCung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TannedCung/subscriptions", "organizations_url": "https://api.github.com/users/TannedCung/orgs", "repos_url": "https://api.github.com/users/TannedCung/repos", "events_url": "https://api.github.com/users/TannedCung/events{/privacy}", "received_events_url": "https://api.github.com/users/TannedCung/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2022-03-01T08:15:57Z", "updated_at": "2023-02-09T04:03:56Z", "closed_at": "2022-04-25T16:54:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\ni do a stress inference into tritonserver. When i increase the load rate, tritonserver increases the memory consumed as well (to a certain level if i stop rising load rate). However, after i stop sending requests to triton, the memory is not released. With the small rate of request this doesn't happen.\r\n\r\n**Triton Information**\r\nImage: `nvcr.io/nvidia/tritonserver:22.01-py3` and `nvcr.io/nvidia/tritonserver:21.04-py3` without GPU\r\nCPU: i7 10400F (i limit triton to use 4 cpus of this)\r\nMemory: 32GB  (limit to 5GB)\r\nModel: OpenVINO [emotions.zip](https://github.com/triton-inference-server/server/files/8159891/emotions.zip)\r\n\r\n\r\n\r\nAre you using the Triton container or did you build it yourself: image from docker hub\r\n\r\n**To Reproduce**\r\nHere are the command in docker-compose\r\n```\r\nface_triton:\r\n    image: nvcr.io/nvidia/tritonserver:22.01-py3\r\n    ports:\r\n      - 8000:8000\r\n      - 8001:8001\r\n      - 8002:8002\r\n    ulimits:\r\n      rtprio: 95\r\n      memlock: -1\r\n\r\n    command: [ tritonserver, --model-repository, /models, --log-verbose, \"1\" ]\r\n    volumes:\r\n      - /models:/models\r\n    environment:\r\n      - NVIDIA_VISIBLE_DEVICES=None\r\n    networks:\r\n      - backend\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: '4'\r\n          memory: 5G\r\n```\r\n\r\n**Expected behavior**\r\nTriton released the RAM after a while\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3997/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3997/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3978", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3978/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3978/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3978/events", "html_url": "https://github.com/triton-inference-server/server/issues/3978", "id": 1148799788, "node_id": "I_kwDOCQnI4s5EeUss", "number": 3978, "title": "Triton cannot inference `tf.math.l2_normalize` correctly from ngc 21.06 ~ ngc 22.03 ( triton 2.20.0)", "user": {"login": "kimdwkimdw", "id": 386070, "node_id": "MDQ6VXNlcjM4NjA3MA==", "avatar_url": "https://avatars.githubusercontent.com/u/386070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kimdwkimdw", "html_url": "https://github.com/kimdwkimdw", "followers_url": "https://api.github.com/users/kimdwkimdw/followers", "following_url": "https://api.github.com/users/kimdwkimdw/following{/other_user}", "gists_url": "https://api.github.com/users/kimdwkimdw/gists{/gist_id}", "starred_url": "https://api.github.com/users/kimdwkimdw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kimdwkimdw/subscriptions", "organizations_url": "https://api.github.com/users/kimdwkimdw/orgs", "repos_url": "https://api.github.com/users/kimdwkimdw/repos", "events_url": "https://api.github.com/users/kimdwkimdw/events{/privacy}", "received_events_url": "https://api.github.com/users/kimdwkimdw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2022-02-24T02:52:36Z", "updated_at": "2022-08-04T17:05:11Z", "closed_at": "2022-08-04T17:05:10Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Description**\r\n\r\nI've converted layers from tensorflow to TensorRT.\r\nThis model's result is valid until ngc 21.05.\r\nFrom 21.06 to 22.01, TensorRT models containing layers converted from `tf.math.l2_normalize` cause wrong result in Triton inference server.\r\n\r\nExpected values are in range between 0.0~1.0, but it generates wrong values from `1e+2` to `1e+5` like exploding.\r\n\r\nI've get correct result without Triton inference with script(https://gist.github.com/kimdwkimdw/2188b4b9ff9f9bf2f7e8dbd999769796)\r\n\r\nreference: https://github.com/NVIDIA/TensorRT/issues/1707\r\n\r\n**Triton Information**\r\n\r\nNGC tags until 21.05-py: valid result\r\nNGC tags from 21.06-py to 22.01-py3: wrong result\r\n\r\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\nAny network from tensorflow including `tf.math.l2_normalize` operation converting from ONNX OPSET from 11 to 13. \r\nNext, convert onnx model to TensortRT model with tensorrt with 21.06 to 22.01(https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorrt)\r\n\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\nExpected values are in range between 0.0~1.0, but it generates wrong values from `1e+2` to `1e+5` like exploding.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3978/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3970", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3970/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3970/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3970/events", "html_url": "https://github.com/triton-inference-server/server/issues/3970", "id": 1147676724, "node_id": "I_kwDOCQnI4s5EaCg0", "number": 3970, "title": "Cannot get CUDA device count, GPU metrics will not be available on multi-gpus", "user": {"login": "shimoshida", "id": 8169618, "node_id": "MDQ6VXNlcjgxNjk2MTg=", "avatar_url": "https://avatars.githubusercontent.com/u/8169618?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shimoshida", "html_url": "https://github.com/shimoshida", "followers_url": "https://api.github.com/users/shimoshida/followers", "following_url": "https://api.github.com/users/shimoshida/following{/other_user}", "gists_url": "https://api.github.com/users/shimoshida/gists{/gist_id}", "starred_url": "https://api.github.com/users/shimoshida/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shimoshida/subscriptions", "organizations_url": "https://api.github.com/users/shimoshida/orgs", "repos_url": "https://api.github.com/users/shimoshida/repos", "events_url": "https://api.github.com/users/shimoshida/events{/privacy}", "received_events_url": "https://api.github.com/users/shimoshida/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-02-23T06:29:17Z", "updated_at": "2022-05-19T00:51:01Z", "closed_at": "2022-03-30T15:56:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI want to deploy Triton server via Azure Kubernetes Service. \r\nMy target node is `ND96asr v4` which is equipped with 8 A100 GPU. \r\nWhen running Triton server **without** loading any models, the following sentences are displayed. \r\n\r\n```\r\nroot@fastertransformer-7dd47c77bb-46gpb:/workspace# mpirun -n 1 --allow-run-as-root tritonserver --model-repository=/workspace\r\nW0221 16:43:52.559411 1908 metrics.cc:274] Cannot get CUDA device count, GPU metrics will not be available\r\nI0221 16:43:52.791832 1908 libtorch.cc:998] TRITONBACKEND_Initialize: pytorch\r\nI0221 16:43:52.791877 1908 libtorch.cc:1008] Triton TRITONBACKEND API version: 1.4\r\n```\r\n(\u203b /workspace is empty dir)\r\nAmong them, \r\n> Cannot get CUDA device count, GPU metrics will not be available\r\n\r\nis trouble with loading model. \r\nI assume that the problem is caused by docker image because at startup\r\n```\r\n=============================\r\n== Triton Inference Server ==\r\n=============================\r\n\r\nNVIDIA Release 21.07 (build 24810355)\r\n\r\nCopyright (c) 2018-2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\nERROR: No supported GPU(s) detected to run this container\r\n```\r\n\r\n> ERROR: No supported GPU(s) detected to run this container\r\n\r\nis obtained. However, I can execute `nvidia-smi` as\r\n<details>\r\n\r\n```\r\nroot@fastertransformer-749fc45c48-hdjhq:/workspace# nvidia-smi\r\nMon Feb 21 20:22:29 2022\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA A100-SXM...  Off  | 00000001:00:00.0 Off |                    0 |\r\n| N/A   41C    P0    49W / 400W |      0MiB / 40536MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA A100-SXM...  Off  | 00000002:00:00.0 Off |                    0 |\r\n| N/A   40C    P0    54W / 400W |      0MiB / 40536MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  NVIDIA A100-SXM...  Off  | 00000003:00:00.0 Off |                    0 |\r\n| N/A   40C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  NVIDIA A100-SXM...  Off  | 00000004:00:00.0 Off |                    0 |\r\n| N/A   41C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  NVIDIA A100-SXM...  Off  | 0000000B:00:00.0 Off |                    0 |\r\n| N/A   41C    P0    57W / 400W |      0MiB / 40536MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  NVIDIA A100-SXM...  Off  | 0000000C:00:00.0 Off |                    0 |\r\n| N/A   39C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  NVIDIA A100-SXM...  Off  | 0000000D:00:00.0 Off |                    0 |\r\n| N/A   40C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  NVIDIA A100-SXM...  Off  | 0000000E:00:00.0 Off |                    0 |\r\n| N/A   41C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n</details>\r\n\r\nHow to fix that?\r\nFor comparison, I also try to deploy to machine equipped with single T4, \r\nand the startup succeeds. \r\n\r\n```\r\nroot@fastertransformer-cc8dbdf6-vbp44:/workspace# nvidia-smi\r\nTue Feb 22 01:39:26 2022\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            Off  | 00000001:00:00.0 Off |                  Off |\r\n| N/A   32C    P8     9W /  70W |      0MiB / 16127MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\nroot@fastertransformer-cc8dbdf6-vbp44:/workspace# mpirun -n 1 --allow-run-as-root tritonserver --model-repository=/workspace\r\nI0221 16:40:48.387855 61 metrics.cc:290] Collecting metrics for GPU 0: Tesla T4\r\nI0221 16:40:48.615749 61 libtorch.cc:998] TRITONBACKEND_Initialize: pytorch\r\n```\r\n\r\nTherefore, I assume the settings for multi-gpus are wrong, but I do not what is wrong...\r\n\r\n**Triton Information**\r\n- docker image: `nvcr.io/nvidia/tritonserver:21.07-py3`\r\n- nvidia driver: `470.57.02`\r\n- CUDA: `11.4`\r\n- K8S: `1.22.4`\r\n- Node Image: `AKSUbuntu-1804gen2containerd-2022.02.01`\r\n- Node Size: `Standard_ND96asr_v4`\r\n\r\n**To Reproduce**\r\nRun Triton server `nvcr.io/nvidia/tritonserver:21.07-py3`\r\n on `ND96asr v4` node via AKS.\r\n\r\n**Expected behavior**\r\nLike the case of using single T4 machine, Triton server can collect metrics\r\n```\r\nroot@fastertransformer-cc8dbdf6-vbp44:/workspace# mpirun -n 1 --allow-run-as-root tritonserver --model-repository=/workspace\r\nI0221 16:40:48.387855 61 metrics.cc:290] Collecting metrics for GPU 0: Tesla T4\r\nI0221 16:40:48.615749 61 libtorch.cc:998] TRITONBACKEND_Initialize: pytorch\r\nI0221 16:40:48.615782 61 libtorch.cc:1008] Triton TRITONBACKEND API version: 1.4\r\nI0221 16:40:48.615786 61 libtorch.cc:1014] 'pytorch' TRITONBACKEND API version: 1.4\r\n...\r\n```\r\n(\u203b /workspace is empty dir)", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3970/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3970/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3962", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3962/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3962/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3962/events", "html_url": "https://github.com/triton-inference-server/server/issues/3962", "id": 1142686189, "node_id": "I_kwDOCQnI4s5EHAHt", "number": 3962, "title": "Build trtion server image with container failed", "user": {"login": "SuoSiFire", "id": 90120870, "node_id": "MDQ6VXNlcjkwMTIwODcw", "avatar_url": "https://avatars.githubusercontent.com/u/90120870?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SuoSiFire", "html_url": "https://github.com/SuoSiFire", "followers_url": "https://api.github.com/users/SuoSiFire/followers", "following_url": "https://api.github.com/users/SuoSiFire/following{/other_user}", "gists_url": "https://api.github.com/users/SuoSiFire/gists{/gist_id}", "starred_url": "https://api.github.com/users/SuoSiFire/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SuoSiFire/subscriptions", "organizations_url": "https://api.github.com/users/SuoSiFire/orgs", "repos_url": "https://api.github.com/users/SuoSiFire/repos", "events_url": "https://api.github.com/users/SuoSiFire/events{/privacy}", "received_events_url": "https://api.github.com/users/SuoSiFire/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-02-18T09:10:23Z", "updated_at": "2022-02-23T19:19:56Z", "closed_at": "2022-02-23T19:19:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nA clear and concise description of what the bug is.\r\n\r\nBuild trtion server image with container failed ,error msg : \r\n\r\ndocker run -it -d --name tensorflow_backend_deps gitlab-master.nvidia.com:5005/dl/dgx/tensorflow:master-py3-base\r\nUnable to find image 'gitlab-master.nvidia.com:5005/dl/dgx/tensorflow:master-py3-base' locally\r\ndocker: Error response from daemon: Get https://gitlab-master.nvidia.com:5005/v2/: dial tcp: lookup gitlab-master.nvidia.com: no such host\r\n\r\nGuess  gitlab-master.nvidia.com is an intranet address \r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n\r\nrelease 2.18.0\r\n\r\nAre you using the Triton container or did you build it yourself?\r\n\r\nbuild it myself.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\n./build.py --cmake-dir=./build --build-dir=/tmp/citritonbuild --enable-logging --enable-stats --enable-tracing --enable-metrics --endpoint=http --endpoint=grpc --repo-tag=common:r22.01 --repo-tag=core:r22.01 --repo-tag=backend:r22.01 --repo-tag=thirdparty:r22.01 --backend=ensemble  --backend=onnxruntime:r22.01 --backend=tensorflow2:r22.01 --backend=openvino:r22.01 --backend=python:r22.01 --repoagent=checksum:r22.01\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\nbuild success.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3962/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3962/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3948", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3948/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3948/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3948/events", "html_url": "https://github.com/triton-inference-server/server/issues/3948", "id": 1140603023, "node_id": "I_kwDOCQnI4s5D_DiP", "number": 3948, "title": "[client c++] build script uses system libcurl instead of own third-party/curl", "user": {"login": "HarHarLinks", "id": 2803622, "node_id": "MDQ6VXNlcjI4MDM2MjI=", "avatar_url": "https://avatars.githubusercontent.com/u/2803622?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HarHarLinks", "html_url": "https://github.com/HarHarLinks", "followers_url": "https://api.github.com/users/HarHarLinks/followers", "following_url": "https://api.github.com/users/HarHarLinks/following{/other_user}", "gists_url": "https://api.github.com/users/HarHarLinks/gists{/gist_id}", "starred_url": "https://api.github.com/users/HarHarLinks/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HarHarLinks/subscriptions", "organizations_url": "https://api.github.com/users/HarHarLinks/orgs", "repos_url": "https://api.github.com/users/HarHarLinks/repos", "events_url": "https://api.github.com/users/HarHarLinks/events{/privacy}", "received_events_url": "https://api.github.com/users/HarHarLinks/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2022-02-16T21:27:35Z", "updated_at": "2022-03-03T11:20:19Z", "closed_at": "2022-03-02T21:34:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nbuild script uses system libcurl instead of own third-party/curl\r\nthe former is too old, leading to this error:\r\n```\r\nsrc/c++/library/http_client.cc:1514:26: error: \u2018CURLOPT_UPLOAD_BUFFERSIZE\u2019 was not declared in this scope; did you mean \u2018CURLOPT_BUFFERSIZE\u2019?\r\n 1514 |   curl_easy_setopt(curl, CURLOPT_UPLOAD_BUFFERSIZE, buffer_byte_size);\r\n```\r\n\r\n`third-party/curl/lib64/libcurl.a` and alongside it headers with the referenced `CURLOPT_UPLOAD_BUFFERSIZE` are present, but not used.\r\n\r\nrelated: #3922\r\n\r\n**Triton Information**\r\nclients r22.01\r\n\r\n**To Reproduce**\r\nPlatform: CentOS 7.9.2009, cmake 3.21.1, GNU Make 3.82, icc (ICC) 2021.5.0 20211109\r\n\r\n```\r\ngit clone -b r22.01 https://github.com/triton-inference-server/client\r\n\r\nmkdir -p client/build && cd client/build\r\n\r\ncmake -DCMAKE_INSTALL_PREFIX=`pwd`/install -DTRITON_ENABLE_CC_HTTP=ON -DTRITON_ENABLE_CC_GRPC=ON -DTRITON_ENABLE_PERF_ANALYZER=ON -DTRITON_ENABLE_GPU=ON -DTRITON_ENABLE_EXAMPLES=ON -DTRITON_ENABLE_TESTS=ON -DTRITON_COMMON_REPO_TAG=r22.01 -DTRITON_THIRD_PARTY_REPO_TAG=r22.01 -DTRITON_CORE_REPO_TAG=r22.01 -DTRITON_BACKEND_REPO_TAG=r22.01 ..\r\n\r\nmake cc-clients\r\n```\r\n\r\nIt will pull and build lots of third party libraries, including curl. Then when it starts to build the triton clients `Found CURL: /usr/lib64/libcurl.so (found version \"7.29.0\")` which is the (outdated in terms of triton requirements) system curl.\r\n\r\n**Expected behavior**\r\nIt builds\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3948/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3948/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3940", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3940/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3940/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3940/events", "html_url": "https://github.com/triton-inference-server/server/issues/3940", "id": 1139423926, "node_id": "I_kwDOCQnI4s5D6jq2", "number": 3940, "title": "Docs for building custom TF backend are obsolete", "user": {"login": "lminer", "id": 5126549, "node_id": "MDQ6VXNlcjUxMjY1NDk=", "avatar_url": "https://avatars.githubusercontent.com/u/5126549?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lminer", "html_url": "https://github.com/lminer", "followers_url": "https://api.github.com/users/lminer/followers", "following_url": "https://api.github.com/users/lminer/following{/other_user}", "gists_url": "https://api.github.com/users/lminer/gists{/gist_id}", "starred_url": "https://api.github.com/users/lminer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lminer/subscriptions", "organizations_url": "https://api.github.com/users/lminer/orgs", "repos_url": "https://api.github.com/users/lminer/repos", "events_url": "https://api.github.com/users/lminer/events{/privacy}", "received_events_url": "https://api.github.com/users/lminer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-02-16T01:45:04Z", "updated_at": "2022-03-10T23:28:23Z", "closed_at": "2022-03-10T23:28:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "If I run `nvcr.io/nvidia/tensorflow:22.01-tf2-py3`\r\n\r\nAnd then run the following command\r\n\r\n```\r\n./nvbuild.sh --triton\r\n```\r\n\r\nI get:\r\n\r\n```\r\nUNKNOWN OPTION --triton\r\nRun ./nvbuild.sh -h for help\r\n```", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3940/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3940/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3929", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3929/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3929/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3929/events", "html_url": "https://github.com/triton-inference-server/server/issues/3929", "id": 1136995895, "node_id": "I_kwDOCQnI4s5DxS43", "number": 3929, "title": "Same output for every batches when using shared memory", "user": {"login": "remib-proovstation", "id": 58515638, "node_id": "MDQ6VXNlcjU4NTE1NjM4", "avatar_url": "https://avatars.githubusercontent.com/u/58515638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/remib-proovstation", "html_url": "https://github.com/remib-proovstation", "followers_url": "https://api.github.com/users/remib-proovstation/followers", "following_url": "https://api.github.com/users/remib-proovstation/following{/other_user}", "gists_url": "https://api.github.com/users/remib-proovstation/gists{/gist_id}", "starred_url": "https://api.github.com/users/remib-proovstation/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/remib-proovstation/subscriptions", "organizations_url": "https://api.github.com/users/remib-proovstation/orgs", "repos_url": "https://api.github.com/users/remib-proovstation/repos", "events_url": "https://api.github.com/users/remib-proovstation/events{/privacy}", "received_events_url": "https://api.github.com/users/remib-proovstation/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-02-14T09:11:22Z", "updated_at": "2022-03-04T10:14:59Z", "closed_at": "2022-03-04T10:14:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen using shared memory in python client with a object detection model in onnx backend, when extracting the results from the inference output on the shared memory, I obtain number_batches times the same result (the one from the last request). \u200b\r\n\r\n**Triton Information**\r\nTriton 21.10 from docker\r\nPython client from `pip install tritonclient[all]`\r\n\r\n_Are you using the Triton container or did you build it yourself?_\r\nUsing Triton from a custom container from base triton.\r\n\r\n**To Reproduce**\r\n\r\nThe client/server interaction works fine when I don\u2019t use shared memory (using simple http protocol), and I\u2019m able to send 15 batches of images to the server, and then collect 15 different results from the server.\r\n\r\nThe problem arises when I try to switch to shared memory : the model receives correctly the 15 different batches, but when I collect the inference responses, with:\r\n```\r\nfor response in responses:\r\n    infer_outputs = []\r\n    # for each output of the model\r\n    for output in model_outputs:\r\n        result = response.get_output(output_name)\r\n        \r\n        # this will always return the same result, the one from the last request\r\n        output_data = shm.get_contents_as_numpy(\r\n            output_triton_shm_handle,\r\n            result.datatype,\r\n            result.shape)\r\n```\r\n I have 15 times the same result, the one from the last request processed by the model.\r\n\r\n\r\n\r\nSimplified version of my client code:\r\n\r\n```import tritonclient.utils.shared_memory as shm\r\n\r\n# for each input, and each output,  create and register a shared memory region using :\r\n\r\nshm_handle = shm.create_shared_memory_region(\r\n        triton_shm_name,\r\n        triton_shm_key,\r\n        byte_size\r\n    )\r\n\r\nclient.register_system_shared_memory(\r\n        triton_shm_name,\r\n        triton_shm_key,\r\n        byte_size,\r\n    )\r\n# we then store the handles for each inputs/outputs\r\n\r\n...\r\n\r\nasync_requests = []\r\n# iterate on the image batches from a pytorch dataloader\r\nfor batch in dataloader:\r\n    # do some preprocessing\r\n\r\n    ...\r\n\r\n    # convert torch Tensor to numpy\r\n    np_batch = batch.numpy()\r\n\r\n    # Put input data values into shared memory\r\n    shm.set_shared_memory_region(input_triton_shm_handle, [np_batch])\r\n\r\n    # definition of this function is below\r\n    inputs, outputs = request_generator(np_batch)\r\n    # send the request(s) asynchronously, \r\n    async_requests.append(\r\n            client.async_infer(\r\n                model_name=model_name,\r\n                inputs=inputs,\r\n                request_id=str(sent_count),\r\n                model_version=model_version,\r\n                outputs=outputs,\r\n            )\r\n    )\r\n\r\n# wait for the server to complete request, then get inference responses\r\nresponses = []\r\nfor async_request in async_requests:\r\n    responses.append(async_request.get_result(timeout=20))\r\n\r\nfor response in responses:\r\n    infer_outputs = []\r\n    # for each output of the model\r\n    for output in model_outputs:\r\n        result = response.get_output(output_name)\r\n        \r\n        # this will always return the same result, the one from the last request\r\n        output_data = shm.get_contents_as_numpy(\r\n            output_triton_shm_handle,\r\n            result.datatype,\r\n            result.shape)\r\n            \r\n        \r\n        infer_outputs.append(output_data)\r\n        \r\n\r\ndef request_generator(self, batched_image_data):\r\n    # Creates infer requested output object for each input using :\r\n    inputs.append(httpclient.InferInput(input_name, batched_image_data.shape, input_datatype))\r\n    inputs[-1].set_shared_memory(\r\n        triton_shm_name,\r\n        byte_size,\r\n    )\r\n    # Creates infer requested output object for each output using :\r\n    outputs.append(httpclient.InferRequestedOutput(output_name, binary_data=True))\r\n    outputs[-1].set_shared_memory(\r\n        triton_shm_name,\r\n        byte_size,\r\n    )\r\n    return inputs, outputs\r\n```\r\nThe configuration of the model:\r\n\r\n```name: \"standarddamages_ps\"\r\n  backend: \"onnxruntime\"\r\n  max_batch_size: 16\r\n  input [\r\n    {\r\n      name: \"image_tensor\"\r\n      data_type: TYPE_FP32\r\n      format: FORMAT_NCHW\r\n      dims: [ 1, 512, 512]\r\n    }\r\n  ]\r\n  output [\r\n    {\r\n      name: \"boxes\"\r\n      data_type: TYPE_FP32\r\n      dims: [ 1000, 4 ]\r\n    }\r\n  ]\r\n  output [\r\n    {\r\n      name: \"classes\"\r\n      data_type: TYPE_FP32\r\n      dims: [ 1000 ]\r\n    }\r\n  ]\r\n  output [\r\n      {\r\n        name: \"scores\"\r\n        data_type: TYPE_FP32\r\n        dims: [ 1000 ]\r\n      }\r\n    ]\r\n```\r\n\r\n**Expected behavior**\r\n\r\nI should obtain a different result for each batch, as it is the case when not using shared memory", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3929/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3908", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3908/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3908/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3908/events", "html_url": "https://github.com/triton-inference-server/server/issues/3908", "id": 1128805895, "node_id": "I_kwDOCQnI4s5DSDYH", "number": 3908, "title": "Failed to build perf_analyzer on macOS", "user": {"login": "lpfhs", "id": 97305712, "node_id": "U_kgDOBczEcA", "avatar_url": "https://avatars.githubusercontent.com/u/97305712?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lpfhs", "html_url": "https://github.com/lpfhs", "followers_url": "https://api.github.com/users/lpfhs/followers", "following_url": "https://api.github.com/users/lpfhs/following{/other_user}", "gists_url": "https://api.github.com/users/lpfhs/gists{/gist_id}", "starred_url": "https://api.github.com/users/lpfhs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lpfhs/subscriptions", "organizations_url": "https://api.github.com/users/lpfhs/orgs", "repos_url": "https://api.github.com/users/lpfhs/repos", "events_url": "https://api.github.com/users/lpfhs/events{/privacy}", "received_events_url": "https://api.github.com/users/lpfhs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1079803573, "node_id": "MDU6TGFiZWwxMDc5ODAzNTcz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-02-09T17:07:01Z", "updated_at": "2022-04-05T19:44:29Z", "closed_at": "2022-04-05T19:44:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nBuilding perf_analyzer from source fails on macOS (Big Sur 11.6.2):\r\n```\r\n[ 85%] Performing build step for 'cc-clients'\r\n[  0%] Building CXX object _deps/repo-core-build/CMakeFiles/triton-core-serverstub.dir/src/tritonserver_stub.cc.o\r\n[  0%] Linking CXX shared library libtritonserver_stub.dylib\r\nld: unknown option: -soname=libtritonserver.so\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake[6]: *** [_deps/repo-core-build/libtritonserver_stub.dylib] Error 1\r\nmake[5]: *** [_deps/repo-core-build/CMakeFiles/triton-core-serverstub.dir/all] Error 2\r\nmake[4]: *** [all] Error 2\r\nmake[3]: *** [cc-clients/src/cc-clients-stamp/cc-clients-build] Error 2\r\nmake[2]: *** [CMakeFiles/cc-clients.dir/all] Error 2\r\nmake[1]: *** [CMakeFiles/cc-clients.dir/rule] Error 2\r\nmake: *** [cc-clients] Error 2\r\n```\r\n\r\n**To Reproduce**\r\n```\r\n  brew install cmake openssl@3 rapidjson opencv   # install dependencies\r\n  git clone -b r22.01 https://github.com/triton-inference-server/client.git\r\n  cd client\r\n  mkdir -p build\r\n  cd build\r\n  BRANCH=$(git rev-parse --abbrev-ref HEAD)\r\n  cmake -DCMAKE_INSTALL_PREFIX=$(pwd)/install \\\r\n    -DTRITON_ENABLE_CC_HTTP=ON \\\r\n    -DTRITON_ENABLE_CC_GRPC=ON \\\r\n    -DTRITON_ENABLE_PERF_ANALYZER=ON \\\r\n    -DTRITON_ENABLE_PYTHON_HTTP=OFF \\\r\n    -DTRITON_ENABLE_PYTHON_GRPC=OFF \\\r\n    -DTRITON_ENABLE_JAVA_HTTP=OFF \\\r\n    -DTRITON_ENABLE_GPU=OFF \\\r\n    -DTRITON_ENABLE_EXAMPLES=OFF \\\r\n    -DTRITON_ENABLE_TESTS=OFF \\\r\n    -DTRITON_COMMON_REPO_TAG=$BRANCH \\\r\n    -DTRITON_THIRD_PARTY_REPO_TAG=$BRANCH \\\r\n    -DTRITON_CORE_REPO_TAG=$BRANCH \\\r\n    -DTRITON_BACKEND_REPO_TAG=$BRANCH \\\r\n    ..\r\n  PKG_CONFIG_PATH=\"/usr/local/opt/openssl@3/lib/pkgconfig\" make cc-clients\r\n```\r\n\r\n**Expected behavior**\r\nSuccessfully build a perf_analyzer binary that runs on macOS.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3908/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3866", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3866/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3866/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3866/events", "html_url": "https://github.com/triton-inference-server/server/issues/3866", "id": 1116995966, "node_id": "I_kwDOCQnI4s5ClAF-", "number": 3866, "title": "Why the third party of grpc-new not appear to contain CMakeLists.txt", "user": {"login": "SimonLliu", "id": 31786027, "node_id": "MDQ6VXNlcjMxNzg2MDI3", "avatar_url": "https://avatars.githubusercontent.com/u/31786027?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SimonLliu", "html_url": "https://github.com/SimonLliu", "followers_url": "https://api.github.com/users/SimonLliu/followers", "following_url": "https://api.github.com/users/SimonLliu/following{/other_user}", "gists_url": "https://api.github.com/users/SimonLliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/SimonLliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SimonLliu/subscriptions", "organizations_url": "https://api.github.com/users/SimonLliu/orgs", "repos_url": "https://api.github.com/users/SimonLliu/repos", "events_url": "https://api.github.com/users/SimonLliu/events{/privacy}", "received_events_url": "https://api.github.com/users/SimonLliu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-01-28T04:42:04Z", "updated_at": "2022-03-25T18:19:00Z", "closed_at": "2022-03-25T18:19:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nThanks for your good work! When i want to build triton-server with docker (https://github.com/triton-inference-server/server/blob/main/docs/build.md#ubuntu-docker), i run the default command. And i find abseil, cares, protobuf have the errors (does not appear to contain CMakeLists.txt).\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nI use the main branch of the triton-server.\r\nAre you using the Triton container or did you build it yourself?\r\nI want to build it myself.\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nI run the command.\r\n`./build.py --cmake-dir=/home/XXX/triton-exp/server/build --build-dir=/tmp/citritonbuild --enable-logging --enable-stats --enable-tracing --enable-metrics --enable-gpu-metrics --enable-gpu --filesystem=gcs --filesystem=azure_storage --filesystem=s3 --endpoint=http --endpoint=grpc --repo-tag=common:main --repo-tag=core:main --repo-tag=backend:main --repo-tag=thirdparty:main --backend=ensemble --backend=identity:main --backend=repeat:main --backend=square:main --backend=onnxruntime:main --backend=pytorch:main --repoagent=checksum:main`\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Bug information**\r\nThere are too many logs so i just list some of them.\r\n`...\r\n[ 52%] Performing configure step for 'absl'\r\ncd /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/absl/src/absl-build && /usr/bin/cmake \"-GUnix Makefiles\" -C/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/absl/tmp/absl-cache-Release.cmake /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo-new/src/grpc/third_party/abseil-cpp\r\n-- Performing Test OPT-Wdouble-promotion - Success\r\n-- Performing Test OPT-Wno-system-headers\r\nloading initial cache file /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/absl/tmp/absl-cache-Release.cmake\r\nRe-run cmake no build system arguments\r\nCMake Error: The source directory \"/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo-new/src/grpc/third_party/abseil-cpp\" **does not appear to contain CMakeLists.txt.**\r\nSpecify --help for usage, or press the help button on the CMake GUI.\r\nmake[3]: *** [_deps/repo-third-party-build/CMakeFiles/absl.dir/build.make:94: _deps/repo-third-party-build/absl/src/absl-stamp/absl-configure] Error 1\r\nmake[3]: Leaving directory '/tmp/tritonbuild/tritonserver/build'\r\nmake[2]: *** [CMakeFiles/Makefile2:379: _deps/repo-third-party-build/CMakeFiles/absl.dir/all] Error 2\r\nmake[2]: *** Waiting for unfinished jobs....\r\n[ 53%] No patch step for 'c-ares'\r\ncd /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build && /usr/bin/cmake -E echo_append\r\nmake[6]: Leaving directory '/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build'\r\n[ 53%] No patch step for 'protobuf'\r\ncd /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build && /usr/bin/cmake -E echo_append\r\ncd /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build && /usr/bin/cmake -E touch /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/c-ares/src/c-ares-stamp/c-ares-patch\r\n[100%] Built target core\r\nmake[5]: Leaving directory '/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build'\r\n/usr/bin/cmake -E cmake_progress_start /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build/CMakeFiles 0\r\nmake[4]: Leaving directory '/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build'\r\ncd /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build && /usr/bin/cmake -E touch /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-stamp/prometheus-cpp-build\r\ncd /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build && /usr/bin/cmake -E touch /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/protobuf/src/protobuf-stamp/protobuf-patch\r\n-- Looking for sys/timerfd.h - found\r\n-- Looking for errno.h\r\n[ 55%] Performing configure step for 'c-ares'\r\ncd /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/c-ares/src/c-ares-build && /usr/bin/cmake \"-GUnix Makefiles\" -C/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/c-ares/tmp/c-ares-cache-Release.cmake /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/third_party/cares/cares\r\n[ 56%] Performing install step for 'prometheus-cpp'\r\ncd /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build && make install\r\n[ 57%] Performing configure step for 'protobuf'\r\ncd /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/protobuf/src/protobuf-build && /usr/bin/cmake \"-GUnix Makefiles\" -C/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/protobuf/tmp/protobuf-cache-Release.cmake /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/third_party/protobuf/cmake\r\nloading initial cache file /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/c-ares/tmp/c-ares-cache-Release.cmake\r\nRe-run cmake no build system arguments\r\nCMake Error: The source directory \"/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/third_party/cares/cares\" **does not appear to contain CMakeLists.txt.**\r\nSpecify --help for usage, or press the help button on the CMake GUI.\r\nmake[3]: *** [_deps/repo-third-party-build/CMakeFiles/c-ares.dir/build.make:94: _deps/repo-third-party-build/c-ares/src/c-ares-stamp/c-ares-configure] Error 1\r\nmake[3]: Leaving directory '/tmp/tritonbuild/tritonserver/build'\r\nmake[2]: *** [CMakeFiles/Makefile2:516: _deps/repo-third-party-build/CMakeFiles/c-ares.dir/all] Error 2\r\nmake[4]: Entering directory '/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build'\r\n/usr/bin/cmake -S/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp -B/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build --check-build-system CMakeFiles/Makefile.cmake 0\r\n/usr/bin/cmake -E cmake_progress_start /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build/CMakeFiles /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build//CMakeFiles/progress.marks\r\nloading initial cache file /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/protobuf/tmp/protobuf-cache-Release.cmake\r\nRe-run cmake no build system arguments\r\nCMake Error: The source directory \"/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/grpc-repo/src/grpc/third_party/protobuf/cmake\" **does not appear to contain CMakeLists.txt.**\r\nSpecify --help for usage, or press the help button on the CMake GUI.\r\nmake[3]: *** [_deps/repo-third-party-build/CMakeFiles/protobuf.dir/build.make:94: _deps/repo-third-party-build/protobuf/src/protobuf-stamp/protobuf-configure] Error 1\r\nmake[3]: Leaving directory '/tmp/tritonbuild/tritonserver/build'\r\nmake[2]: *** [CMakeFiles/Makefile2:353: _deps/repo-third-party-build/CMakeFiles/protobuf.dir/all] Error 2\r\nmake  -f CMakeFiles/Makefile2 all\r\nmake[5]: Entering directory '/tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-build/prometheus-cpp/src/prometheus-cpp-build'\r\nmake  -f core/CMakeFiles/core.dir/build.make core/CMakeFiles/core.dir/depend\r\n-- Performing Test OPT-Wno-system-headers - Success\r\n-- Performing Test OPT-Wno-pedantic-ms-format\r\n...\r\nbegin\r\nplatform linux\r\nmachine x86_64\r\nversion 2.19.0dev\r\ndefault repo-tag: main\r\ncontainer version 22.02dev\r\nupstream container version 21.12\r\nbackend \"ensemble\" at tag/branch \"main\"\r\nbackend \"identity\" at tag/branch \"main\"\r\nbackend \"repeat\" at tag/branch \"main\"\r\nbackend \"square\" at tag/branch \"main\"\r\nbackend \"onnxruntime\" at tag/branch \"main\"\r\nbackend \"pytorch\" at tag/branch \"main\"\r\nrepoagent \"checksum\" at tag/branch \"main\"\r\nerror: docker run tritonserver_builder failed\r\nbegin\r\n`\r\n**Expected behavior**\r\n\r\nThanks!\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3866/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3866/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3852", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3852/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3852/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3852/events", "html_url": "https://github.com/triton-inference-server/server/issues/3852", "id": 1115023639, "node_id": "I_kwDOCQnI4s5CdekX", "number": 3852, "title": "modification of the dimension check in EvaluateTensorRTContext", "user": {"login": "ziyuanjiang", "id": 8045357, "node_id": "MDQ6VXNlcjgwNDUzNTc=", "avatar_url": "https://avatars.githubusercontent.com/u/8045357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ziyuanjiang", "html_url": "https://github.com/ziyuanjiang", "followers_url": "https://api.github.com/users/ziyuanjiang/followers", "following_url": "https://api.github.com/users/ziyuanjiang/following{/other_user}", "gists_url": "https://api.github.com/users/ziyuanjiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ziyuanjiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ziyuanjiang/subscriptions", "organizations_url": "https://api.github.com/users/ziyuanjiang/orgs", "repos_url": "https://api.github.com/users/ziyuanjiang/repos", "events_url": "https://api.github.com/users/ziyuanjiang/events{/privacy}", "received_events_url": "https://api.github.com/users/ziyuanjiang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-01-26T13:31:43Z", "updated_at": "2022-03-16T00:53:07Z", "closed_at": "2022-03-16T00:53:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using the tensorRT backend for my model. \r\nMy model has multiple inputs whose first dimensions are dynamic and should all be the batch size.\r\nWhen I create the tensorRT model plan. Somehow, I encountered errors when setting the first dimension of the inputs in one profile with different min, opt, max values. \uff08The same min, opt, max value for all inputs. Just the values for min, opt, max are different.\uff09\r\nI am still trying to understand that. But I managed to create a model plan with several profiles, with each of the profiles set all the min, opt, max to a same batch size. So each of the profiles corresponds to a batch size ranging from 1 to the max batch size.\r\n\r\nThen I loaded the model plan to triton server, and noticed that the backend always selects the first profile even though the first profile cannot handle a batch size larger than one.\r\n\r\nAfter a short study of the backend, I found that this dimension validation check blocks the backend from choosing the right profile.\r\n\r\nhttps://github.com/triton-inference-server/tensorrt_backend/blob/5b5ede3bb0e60fd1872d6992d3017c0d9e578ae5/src/tensorrt.cc#L2869\r\n\r\nThis is a problem for me because when getting the input_shape_vec from TRITONBACKEND_InputProperties,\r\nthe first dimension is one, neglecting the actual batch size. \r\nAnd even though the manhattan distance of the corresponding profile should be much smaller in the afterward calculation, in my case only the first profile can pass this validation. And then the first profile failed to handle the actual batch size while inferencing.\r\n\r\nI wonder whether you can only check the non-batch dimensions there or you mean that all profiles should be able to handle all the batch sizes and I abuse the ability of profiles.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3852/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3852/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3808", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3808/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3808/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3808/events", "html_url": "https://github.com/triton-inference-server/server/issues/3808", "id": 1103328271, "node_id": "I_kwDOCQnI4s5Bw3QP", "number": 3808, "title": "Triton HTTP python client library call `get_result()` on `InferAsyncRequest` object in different thread results in `greenlet .error`", "user": {"login": "yichong96", "id": 32008333, "node_id": "MDQ6VXNlcjMyMDA4MzMz", "avatar_url": "https://avatars.githubusercontent.com/u/32008333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yichong96", "html_url": "https://github.com/yichong96", "followers_url": "https://api.github.com/users/yichong96/followers", "following_url": "https://api.github.com/users/yichong96/following{/other_user}", "gists_url": "https://api.github.com/users/yichong96/gists{/gist_id}", "starred_url": "https://api.github.com/users/yichong96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yichong96/subscriptions", "organizations_url": "https://api.github.com/users/yichong96/orgs", "repos_url": "https://api.github.com/users/yichong96/repos", "events_url": "https://api.github.com/users/yichong96/events{/privacy}", "received_events_url": "https://api.github.com/users/yichong96/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1079803578, "node_id": "MDU6TGFiZWwxMDc5ODAzNTc4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-01-14T09:22:02Z", "updated_at": "2022-01-21T01:39:19Z", "closed_at": "2022-01-20T23:57:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying out a producer-consumer pattern in python whereby the producer in the main thread sends asynchronous requests to Triton and puts the request into a `queue.Queue()` while the consumer thread consumes the result from this queue. \r\n\r\nWhile the other thread consumes from this queue, I am met with an error when I poll `InferAsyncRequest` object from the queue and call `get_result()` on it . The error shows `greenlet.error: cannot switch to a different thread`. \r\n\r\nI have managed to achieve such a pattern using gRPC bi-directional streaming but am curious whether it is possible to achieve such a pattern using the HTTP client library. \r\n\r\nAm also curious as to whether there are any benefits of using HTTP REST client library over gRPC since gRPC seems to be more optimised with its use of `protocol buffers` and `HTTP/2`. Especially so when the client batches data over to the server because the client using gRPC can send the inference requests over a single connection. ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3808/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3802", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3802/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3802/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3802/events", "html_url": "https://github.com/triton-inference-server/server/issues/3802", "id": 1101237677, "node_id": "I_kwDOCQnI4s5Bo42t", "number": 3802, "title": "Model load call is throwing error only on the first call `POST /v2/repository/models/{MODEL}/load`", "user": {"login": "KshitizLohia", "id": 11562259, "node_id": "MDQ6VXNlcjExNTYyMjU5", "avatar_url": "https://avatars.githubusercontent.com/u/11562259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KshitizLohia", "html_url": "https://github.com/KshitizLohia", "followers_url": "https://api.github.com/users/KshitizLohia/followers", "following_url": "https://api.github.com/users/KshitizLohia/following{/other_user}", "gists_url": "https://api.github.com/users/KshitizLohia/gists{/gist_id}", "starred_url": "https://api.github.com/users/KshitizLohia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KshitizLohia/subscriptions", "organizations_url": "https://api.github.com/users/KshitizLohia/orgs", "repos_url": "https://api.github.com/users/KshitizLohia/repos", "events_url": "https://api.github.com/users/KshitizLohia/events{/privacy}", "received_events_url": "https://api.github.com/users/KshitizLohia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2022-01-13T05:50:31Z", "updated_at": "2022-03-10T18:33:47Z", "closed_at": "2022-03-10T18:33:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nModel load should always throw error if there is an issue in loading the model\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n`nvcr.io/nvidia/tritonserver:21.10-py3`\r\n\r\nAre you using the Triton container or did you build it yourself?\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n1. Bring up the triton server with model https://github.com/guyroyse/redisai-iris/blob/main/iris.onnx\r\n```\r\ntritonserver --strict-model-config=false  --model-repository=/models --model-control-mode=explicit\r\n```\r\n2. Run the model load command twice. We will get a 200 status call on the second call and error on the first call.\r\n```\r\ncurl --location --request POST 'localhost:8000/v2/repository/models/{MODEL}/load'\r\ncurl --location --request POST 'localhost:8000/v2/repository/models/{MODEL}/load'\r\n```\r\nFirst Try:\r\n<img width=\"1042\" alt=\"firsttry\" src=\"https://user-images.githubusercontent.com/11562259/149273868-a0f73df1-36e7-4719-b1ac-df93ff9c5826.png\">\r\nSecond Try:\r\n<img width=\"1058\" alt=\"secondtry\" src=\"https://user-images.githubusercontent.com/11562259/149273881-7260ebe4-1243-439f-b60b-8375e6175607.png\">\r\n\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n1. https://github.com/guyroyse/redisai-iris/blob/main/iris.onnx\r\n2. No config.pbtxt is required as config will be auto generated.\r\n\r\n**Expected behavior**\r\n`POST /v2/repository/models/{MODEL}/load` should always throw error in case of unsuccessful load. Else, the user will be under the impression that model has loaded on the second try.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3802/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3802/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3800", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3800/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3800/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3800/events", "html_url": "https://github.com/triton-inference-server/server/issues/3800", "id": 1101179693, "node_id": "I_kwDOCQnI4s5Boqst", "number": 3800, "title": "`is_server_live()` python GRPC client got no response intermittently, while c++ is OK", "user": {"login": "chunyulin", "id": 7530990, "node_id": "MDQ6VXNlcjc1MzA5OTA=", "avatar_url": "https://avatars.githubusercontent.com/u/7530990?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chunyulin", "html_url": "https://github.com/chunyulin", "followers_url": "https://api.github.com/users/chunyulin/followers", "following_url": "https://api.github.com/users/chunyulin/following{/other_user}", "gists_url": "https://api.github.com/users/chunyulin/gists{/gist_id}", "starred_url": "https://api.github.com/users/chunyulin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chunyulin/subscriptions", "organizations_url": "https://api.github.com/users/chunyulin/orgs", "repos_url": "https://api.github.com/users/chunyulin/repos", "events_url": "https://api.github.com/users/chunyulin/events{/privacy}", "received_events_url": "https://api.github.com/users/chunyulin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-01-13T04:20:28Z", "updated_at": "2023-03-13T22:39:17Z", "closed_at": "2023-03-13T22:33:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nFor GRPC clients, `is_server_live()` python API got no response intermittently **for some client mainchines** while c++ `IsServerLive()` is OK. No problem for HTTP clients at all. I wonder is python's `is_server_live()` implemented differently with c++'s such that they behaved differently in certain networking condition?\r\n\r\n**Triton Information**\r\n- Server: 21.08-py3 container with only `simple*` example models loaded. Although the issue doesn't depend on models.\r\n- Client: Tested in 21.08-py3-sdk container and built c++ client with  `cmake` and only `TRITON_ENABLE_CC_GRPC` on and `make cc-clients`. The pip-installed python client without container is also checked and behaved the same.\r\n\r\n**To Reproduce**\r\n- For python grpc client on some machines, there is a high probability the following script halt when checking server aliveness: \r\n  ```\r\n  for i in {0..10}; do python check_triton.py; done\r\n  ```\r\n- C++ clients were built by `g++ check_triton.cc -lgrpcclient` and it always return true immediately. \r\n- Here are [check_triton.py](https://gist.github.com/chunyulin/83bf74e703a77b84f3e79fbbd15ea0a0) and [check_triton.cc](https://gist.github.com/chunyulin/1ddcec32bf70465f44ce15aafe0a5c6a) in which the hard-coded server address should be accessible now. \r\n\r\n**Expected behavior**\r\nThe python client code should return immediately as c++ `IsServerLive()` does.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3800/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3787", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3787/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3787/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3787/events", "html_url": "https://github.com/triton-inference-server/server/issues/3787", "id": 1098945211, "node_id": "I_kwDOCQnI4s5BgJK7", "number": 3787, "title": "Semantic segmentation model serving", "user": {"login": "sangkyuleeKOR", "id": 44499503, "node_id": "MDQ6VXNlcjQ0NDk5NTAz", "avatar_url": "https://avatars.githubusercontent.com/u/44499503?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sangkyuleeKOR", "html_url": "https://github.com/sangkyuleeKOR", "followers_url": "https://api.github.com/users/sangkyuleeKOR/followers", "following_url": "https://api.github.com/users/sangkyuleeKOR/following{/other_user}", "gists_url": "https://api.github.com/users/sangkyuleeKOR/gists{/gist_id}", "starred_url": "https://api.github.com/users/sangkyuleeKOR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sangkyuleeKOR/subscriptions", "organizations_url": "https://api.github.com/users/sangkyuleeKOR/orgs", "repos_url": "https://api.github.com/users/sangkyuleeKOR/repos", "events_url": "https://api.github.com/users/sangkyuleeKOR/events{/privacy}", "received_events_url": "https://api.github.com/users/sangkyuleeKOR/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1079803578, "node_id": "MDU6TGFiZWwxMDc5ODAzNTc4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-01-11T10:09:14Z", "updated_at": "2022-01-21T18:07:23Z", "closed_at": "2022-01-21T18:07:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello guys I'm trying to serve semantic segmentation model using triton.\r\n\r\nI'm testing sample image (1,3,320,320) and I want to get return segmenation result shape (1,320,320)\r\n\r\nI converted pytorch model to onnx and checked it has been converted successfully.\r\n\r\nhear is my config.pbtxt\r\n```\r\nname: \"dot_onnx\"\r\nplatform: \"onnxruntime_onnx\"\r\nmax_batch_size : 0\r\ninput [\r\n  {\r\n    name: \"INPUT_0\"\r\n    data_type: TYPE_FP32\r\n    format: FORMAT_NCHW\r\n    dims: [ 3, 320, 320 ]\r\n    reshape: {shape: [ 1,3,320,320 ]}\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"OUTPUT_0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1, 320, 320 ]\r\n    reshape: {shape: [ 1,1,320,320 ]}\r\n    label_filename: \"dot_labels.txt\"\r\n  }\r\n]\r\n```\r\n\r\nWhen I send a request to triton, I get result back successfully but, output looks weird for me.\r\n![image](https://user-images.githubusercontent.com/44499503/148922550-cdace9ad-66ca-47d2-b3ce-7198b1840c28.png)\r\n\r\nand here is my tritonclient request\r\n```\r\ndef postprocess(results, output_name, batch_size, batching):\r\n    \"\"\"\r\n    Post-process results to show classifications.\r\n    \"\"\"\r\n    print(results.get_response())\r\n    print(output_name)\r\n    output_array = results.as_numpy(output_name)\r\n    if len(output_array) != batch_size:\r\n        raise Exception(\"expected {} results, got {}\".format(\r\n            batch_size, len(output_array)))\r\n\r\n    # Include special handling for non-batching models\r\n    print(output_array)\r\n    for results in output_array:\r\n        if not batching:\r\n            results = [results]\r\n        for result in results:\r\n            if output_array.dtype.type == np.object_:\r\n                cls = \"\".join(chr(x) for x in result).split(':')\r\n            else:\r\n                cls = result.split(':')\r\n            \r\n            print(\"    {} ({}) = {}\".format(cls[0], cls[1], cls[2]))\r\n```\r\nI know this client example is for image classification, but I should get proper response like numpy array ( 320,320 blahblah) at least. Please help me what I'm doing wrong. Thanks !!", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3787/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3787/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3779", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3779/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3779/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3779/events", "html_url": "https://github.com/triton-inference-server/server/issues/3779", "id": 1096753668, "node_id": "I_kwDOCQnI4s5BXyIE", "number": 3779, "title": "python_backend consuming too much CPU without any incoming request", "user": {"login": "lpfhs", "id": 97305712, "node_id": "U_kgDOBczEcA", "avatar_url": "https://avatars.githubusercontent.com/u/97305712?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lpfhs", "html_url": "https://github.com/lpfhs", "followers_url": "https://api.github.com/users/lpfhs/followers", "following_url": "https://api.github.com/users/lpfhs/following{/other_user}", "gists_url": "https://api.github.com/users/lpfhs/gists{/gist_id}", "starred_url": "https://api.github.com/users/lpfhs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lpfhs/subscriptions", "organizations_url": "https://api.github.com/users/lpfhs/orgs", "repos_url": "https://api.github.com/users/lpfhs/repos", "events_url": "https://api.github.com/users/lpfhs/events{/privacy}", "received_events_url": "https://api.github.com/users/lpfhs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-01-07T23:20:53Z", "updated_at": "2022-01-11T16:24:39Z", "closed_at": "2022-01-11T16:23:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhile running tritonserver within docker,  the `triton_python_backend_stub` process is using about 40% CPU when there is no incoming requests.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? **21.12**\r\nAre you using the Triton container or did you build it yourself? **container**\r\n\r\n**To Reproduce**\r\nStart the `add_sub` model by following the [quick-start](https://github.com/triton-inference-server/python_backend#quick-start) documentation. Then enter the container (`docker exec -it container-name /bin/bash`) and observe CPU usage using `top` command.\r\n\r\nAttach to the `triton_python_backend_stub` process and print a stack trace:\r\n```\r\nroot@20bba308ec83:/opt/tritonserver# gdb -p <pid>\r\n...\r\n(gdb) bt\r\n#0  0x00007f7837b5a618 in futex_abstimed_wait_cancelable (private=128, abstime=0x7fffa8f521f0, clockid=0, expected=0, futex_word=0x7f78317fc0b8)\r\n    at ../sysdeps/nptl/futex-internal.h:320\r\n#1  do_futex_wait (sem=sem@entry=0x7f78317fc0b8, abstime=abstime@entry=0x7fffa8f521f0, clockid=0) at sem_waitcommon.c:112\r\n#2  0x00007f7837b5a743 in __new_sem_wait_slow (sem=0x7f78317fc0b8, abstime=0x7fffa8f521f0, clockid=0) at sem_waitcommon.c:184\r\n#3  0x000055c13dca69b2 in boost::interprocess::ipcdetail::semaphore_timed_wait(sem_t*, boost::posix_time::ptime const&) ()\r\n#4  0x000055c13dca4e59 in triton::backend::python::MessageQueue::Pop(int const&, bool&) ()\r\n#5  0x000055c13dc8ab4a in triton::backend::python::Stub::PopMessage() ()\r\n#6  0x000055c13dc8dc08 in triton::backend::python::Stub::RunCommand() ()\r\n#7  0x000055c13dc79e80 in main ()\r\n```\r\n\r\nstrace shows it keeps calling `futex` and `gettimeofday`:\r\n```\r\nroot@20bba308ec83:/opt/tritonserver# strace -p <pid>\r\nstrace: Process 45 attached\r\nrestart_syscall(<... resuming interrupted read ...>) = -1 ETIMEDOUT (Connection timed out)\r\ngettimeofday({tv_sec=1641597120, tv_usec=997403}, NULL) = 0\r\nfutex(0x7f78317fc0b8, FUTEX_WAIT_BITSET|FUTEX_CLOCK_REALTIME, 0, {tv_sec=1641597121, tv_nsec=997403000}, FUTEX_BITSET_MATCH_ANY) = -1 ETIMEDOUT (Connection timed out)\r\ngettimeofday({tv_sec=1641597121, tv_usec=998610}, NULL) = 0\r\nfutex(0x7f78317fc0b8, FUTEX_WAIT_BITSET|FUTEX_CLOCK_REALTIME, 0, {tv_sec=1641597122, tv_nsec=998610000}, FUTEX_BITSET_MATCH_ANY) = -1 ETIMEDOUT (Connection timed out)\r\ngettimeofday({tv_sec=1641597123, tv_usec=762}, NULL) = 0\r\n...\r\n```\r\n\r\nI've been able to reproduce this on both macOS and Linux as well as tritonserver version 21.05.\r\n\r\n**Expected behavior**\r\nThe `triton_python_backend_stub` process should be using nearly 0% CPU while being idle, waiting for requests.\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3779/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3777", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3777/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3777/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3777/events", "html_url": "https://github.com/triton-inference-server/server/issues/3777", "id": 1096309366, "node_id": "I_kwDOCQnI4s5BWFp2", "number": 3777, "title": "free() invalid pointer", "user": {"login": "Slyne", "id": 6286804, "node_id": "MDQ6VXNlcjYyODY4MDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/6286804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Slyne", "html_url": "https://github.com/Slyne", "followers_url": "https://api.github.com/users/Slyne/followers", "following_url": "https://api.github.com/users/Slyne/following{/other_user}", "gists_url": "https://api.github.com/users/Slyne/gists{/gist_id}", "starred_url": "https://api.github.com/users/Slyne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Slyne/subscriptions", "organizations_url": "https://api.github.com/users/Slyne/orgs", "repos_url": "https://api.github.com/users/Slyne/repos", "events_url": "https://api.github.com/users/Slyne/events{/privacy}", "received_events_url": "https://api.github.com/users/Slyne/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 22, "created_at": "2022-01-07T13:17:23Z", "updated_at": "2022-02-23T04:14:15Z", "closed_at": "2022-02-02T00:15:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen I shut down triton inference server, there's one line:\r\n![3067267c406779d44c4cda84e61911b](https://user-images.githubusercontent.com/6286804/148547849-2d159723-1798-47ed-a8e4-f921f635ce2c.png)\r\n\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?   21.12\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nHere's the dockerfile:\r\n```\r\nFROM nvcr.io/nvidia/tritonserver:21.12-py3\r\nLABEL maintainer=\"NVIDIA\"\r\nLABEL repository=\"tritonserver\"\r\n\r\nRUN apt-get update && apt-get -y install swig && apt-get -y install python3-dev && apt-get install -y cmake\r\nRUN pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\r\nRUN pip3 install -v kaldifeat\r\n```\r\n\r\nHere's the model.py.\r\n```\r\nimport kaldifeat\r\n\r\nclass TritonPythonModel:\r\n\r\n    def initialize(self, args):\r\n        pass\r\n\r\n    def execute(self, requests):\r\n        pass\r\n\r\n    def finalize(self):\r\n        \"\"\"`finalize` is called only once when the model is being unloaded.\r\n        Implementing `finalize` function is OPTIONAL. This function allows\r\n        the model to perform any necessary clean ups before exit.\r\n        \"\"\"\r\n        print('Cleaning up...')\r\n\r\n```\r\nconfig.pbtxt\r\n```\r\nname: \"model\"\r\nbackend: \"python\"\r\nmax_batch_size: 64\r\n\r\ninput [\r\n  {\r\n    name: \"wav\"\r\n    data_type: TYPE_FP32\r\n    dims: [-1]\r\n  },\r\n  {\r\n    name: \"wav_lens\"\r\n    data_type: TYPE_INT32\r\n    dims: [1]\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"speech\"\r\n    data_type: TYPE_FP16\r\n    dims: [-1, 80]  # 80\r\n  },\r\n  {\r\n    name: \"speech_lengths\"\r\n    data_type: TYPE_INT32\r\n    dims: [1]\r\n  }\r\n]\r\n\r\ndynamic_batching {\r\n    preferred_batch_size: [ 16, 32 ]\r\n  }\r\ninstance_group [\r\n    {\r\n      count: 1\r\n      kind: KIND_GPU\r\n    }\r\n]\r\n```\r\n\r\n**To Reproduce**\r\n1.  Build docker based on the above dockerfile.\r\n2. Run the model_repo  with model.py in it.\r\n3. Shut down triton by 'ctrl-c'\r\n\r\n**Expected behavior**\r\nExpect no such line.\r\n\r\nI test on 2 different machine. Both will give this error? warning?     One will not generate core, and another will generate a core file.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3777/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3777/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3770", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3770/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3770/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3770/events", "html_url": "https://github.com/triton-inference-server/server/issues/3770", "id": 1095279420, "node_id": "I_kwDOCQnI4s5BSKM8", "number": 3770, "title": "Could not invoke stateful service in triton bls", "user": {"login": "baoyu-yuan", "id": 12273456, "node_id": "MDQ6VXNlcjEyMjczNDU2", "avatar_url": "https://avatars.githubusercontent.com/u/12273456?v=4", "gravatar_id": "", "url": "https://api.github.com/users/baoyu-yuan", "html_url": "https://github.com/baoyu-yuan", "followers_url": "https://api.github.com/users/baoyu-yuan/followers", "following_url": "https://api.github.com/users/baoyu-yuan/following{/other_user}", "gists_url": "https://api.github.com/users/baoyu-yuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/baoyu-yuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/baoyu-yuan/subscriptions", "organizations_url": "https://api.github.com/users/baoyu-yuan/orgs", "repos_url": "https://api.github.com/users/baoyu-yuan/repos", "events_url": "https://api.github.com/users/baoyu-yuan/events{/privacy}", "received_events_url": "https://api.github.com/users/baoyu-yuan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2022-01-06T12:34:40Z", "updated_at": "2022-01-24T03:51:54Z", "closed_at": "2022-01-21T18:40:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI used the triton bls to run a pipeline service to invoke serveral sub-services, and I found that even I set the correlation_id in the bls code, the server got an zero correlation id, so I cannot invoke one of the sub-services, which is a stateful service. \r\nBy the way, the bls api \"InferenceRequest\" may not contain the flags for the stateful service? like the start and end flags.\r\n\r\n**Triton Information**\r\nI use triton server and python backend with version r21.11. \r\n\r\nAre you using the Triton container or did you build it yourself?\r\nI use the Triton container with version r21.11.\r\n\r\n**To Reproduce**\r\nUse triton bls to invoke a stateful service.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nCannot invoke the stateful service, and would get an error -- \"tritonclient.utils.InferenceServerException: inference request to model 'xxxxx' must specify a non-zero or non-empty correlation ID\"\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3770/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3770/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3758", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3758/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3758/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3758/events", "html_url": "https://github.com/triton-inference-server/server/issues/3758", "id": 1093390375, "node_id": "I_kwDOCQnI4s5BK9An", "number": 3758, "title": "Memory not being released after triton inference - Python ", "user": {"login": "Tandon-A", "id": 37059580, "node_id": "MDQ6VXNlcjM3MDU5NTgw", "avatar_url": "https://avatars.githubusercontent.com/u/37059580?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tandon-A", "html_url": "https://github.com/Tandon-A", "followers_url": "https://api.github.com/users/Tandon-A/followers", "following_url": "https://api.github.com/users/Tandon-A/following{/other_user}", "gists_url": "https://api.github.com/users/Tandon-A/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tandon-A/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tandon-A/subscriptions", "organizations_url": "https://api.github.com/users/Tandon-A/orgs", "repos_url": "https://api.github.com/users/Tandon-A/repos", "events_url": "https://api.github.com/users/Tandon-A/events{/privacy}", "received_events_url": "https://api.github.com/users/Tandon-A/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}, {"login": "jbkyang-nvi", "id": 80359429, "node_id": "MDQ6VXNlcjgwMzU5NDI5", "avatar_url": "https://avatars.githubusercontent.com/u/80359429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbkyang-nvi", "html_url": "https://github.com/jbkyang-nvi", "followers_url": "https://api.github.com/users/jbkyang-nvi/followers", "following_url": "https://api.github.com/users/jbkyang-nvi/following{/other_user}", "gists_url": "https://api.github.com/users/jbkyang-nvi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbkyang-nvi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbkyang-nvi/subscriptions", "organizations_url": "https://api.github.com/users/jbkyang-nvi/orgs", "repos_url": "https://api.github.com/users/jbkyang-nvi/repos", "events_url": "https://api.github.com/users/jbkyang-nvi/events{/privacy}", "received_events_url": "https://api.github.com/users/jbkyang-nvi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2022-01-04T14:07:41Z", "updated_at": "2022-01-11T00:03:28Z", "closed_at": "2022-01-11T00:03:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, \r\n\r\n**Description**\r\n\r\nI am trying to run a classification model (ONNX format) deployed using the inference server. I see that the memory is not being released in the infer call from the client object and also when the set_data_from_numpy function is called. \r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? -- 21.03\r\nAre you using the Triton container or did you build it yourself?  -- container\r\n\r\n**Details**\r\n\r\nFor profiling the memory, I have used the [memory_profiler library](https://pypi.org/project/memory-profiler/). \r\n\r\nThe code used is below:\r\n```python\r\nimport cv2\r\nimport numpy as np\r\n\r\nfrom tritonclient.utils import *\r\nimport tritonclient.http as httpclient\r\n\r\nfrom memory_profiler import profile\r\n\r\n\r\nTRITON_MODEL_NAME = \"classification_onnx\"\r\n\r\n\r\ndef preprocess(frame):\r\n    frame = frame.transpose((2, 0, 1))\r\n    frame = frame.reshape((1, 3, 224, 224))\r\n    return frame\r\n\r\n\r\n@profile\r\ndef infer(frame, client):\r\n    inputs = []\r\n    inputs.append(httpclient.InferInput(\"input\", frame.shape, np_to_triton_dtype(frame.dtype)))\r\n    inputs[0].set_data_from_numpy(frame)\r\n    \r\n    outputs = []\r\n    outputs.append(httpclient.InferRequestedOutput(\"output\"))    \r\n\r\n    response = client.infer(TRITON_MODEL_NAME, inputs, outputs=outputs)\r\n    output = response.as_numpy(\"output\")\r\n    print (\"model output is \", output)\r\n    inputs = []\r\n    del inputs\r\n    outputs = []\r\n    del outputs\r\n    del response\r\n    del output\r\n\r\n\r\n@profile\r\ndef process(frames, client):\r\n    print ('Starting processing')\r\n    for i, frame in enumerate(frames):\r\n        frame = preprocess(frame)\r\n        infer(frame, client)\r\n    print ('Completed processing')\r\n        \r\n\r\n@profile\r\ndef run():\r\n    ip = \"192.168.1.11:8000\"\r\n    client = httpclient.InferenceServerClient(ip)\r\n    frames = [np.random.rand(224,224,3).astype(np.float32) for _ in range(1)]\r\n    process(frames, client)\r\n    client.close()\r\n    del frames\r\n    del client\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n\r\n\r\n```\r\n\r\n\r\nThe memory profiling output is as follows:\r\n```\r\ntesting_triton_1  | Starting processing\r\ntesting_triton_1  | model output is  [[-1.2733034  1.0134717]]\r\ntesting_triton_1  | Filename: test_main.py\r\ntesting_triton_1  | \r\ntesting_triton_1  | Line #    Mem usage    Increment  Occurrences   Line Contents\r\ntesting_triton_1  | =============================================================\r\ntesting_triton_1  |     19    109.8 MiB    109.8 MiB           1   @profile\r\ntesting_triton_1  |     20                                         def infer(frame, client):\r\ntesting_triton_1  |     21    109.8 MiB      0.0 MiB           1       inputs = []\r\ntesting_triton_1  |     22    109.8 MiB      0.0 MiB           1       inputs.append(httpclient.InferInput(\"input\", frame.shape, np_to_triton_dtype(frame.dtype)))\r\ntesting_triton_1  |     23    110.2 MiB      0.4 MiB           1       inputs[0].set_data_from_numpy(frame)\r\ntesting_triton_1  |     24                                             \r\ntesting_triton_1  |     25    110.2 MiB      0.0 MiB           1       outputs = []\r\ntesting_triton_1  |     26    110.2 MiB      0.0 MiB           1       outputs.append(httpclient.InferRequestedOutput(\"output\"))    \r\ntesting_triton_1  |     27                                         \r\ntesting_triton_1  |     28    112.1 MiB      1.9 MiB           1       response = client.infer(TRITON_MODEL_NAME, inputs, outputs=outputs)\r\ntesting_triton_1  |     29    112.1 MiB      0.0 MiB           1       output = response.as_numpy(\"output\")\r\ntesting_triton_1  |     30    112.1 MiB      0.0 MiB           1       print (\"model output is \", output)\r\ntesting_triton_1  |     31    112.1 MiB      0.0 MiB           1       inputs = []\r\ntesting_triton_1  |     32    112.1 MiB      0.0 MiB           1       del inputs\r\ntesting_triton_1  |     33    112.1 MiB      0.0 MiB           1       outputs = []\r\ntesting_triton_1  |     34    112.1 MiB      0.0 MiB           1       del outputs\r\ntesting_triton_1  |     35    112.1 MiB      0.0 MiB           1       del response\r\ntesting_triton_1  |     36    112.1 MiB      0.0 MiB           1       del output\r\ntesting_triton_1  | \r\ntesting_triton_1  | \r\ntesting_triton_1  | Completed processing\r\ntesting_triton_1  | Filename: test_main.py\r\ntesting_triton_1  | \r\ntesting_triton_1  | Line #    Mem usage    Increment  Occurrences   Line Contents\r\ntesting_triton_1  | =============================================================\r\ntesting_triton_1  |     39    109.8 MiB    109.8 MiB           1   @profile\r\ntesting_triton_1  |     40                                         def process(frames, client):\r\ntesting_triton_1  |     41    109.8 MiB      0.0 MiB           1       print ('Starting processing')\r\ntesting_triton_1  |     42    112.1 MiB      0.0 MiB           2       for i, frame in enumerate(frames):\r\ntesting_triton_1  |     43    109.8 MiB      0.0 MiB           1           frame = preprocess(frame)\r\ntesting_triton_1  |     44    112.1 MiB      2.3 MiB           1           infer(frame, client)\r\ntesting_triton_1  |     45    112.1 MiB      0.0 MiB           1       print ('Completed processing')\r\ntesting_triton_1  | \r\ntesting_triton_1  | \r\ntesting_triton_1  | Filename: test_main.py\r\ntesting_triton_1  | \r\ntesting_triton_1  | Line #    Mem usage    Increment  Occurrences   Line Contents\r\ntesting_triton_1  | =============================================================\r\ntesting_triton_1  |     48    109.1 MiB    109.1 MiB           1   @profile\r\ntesting_triton_1  |     49                                         def run():\r\ntesting_triton_1  |     50    109.1 MiB      0.0 MiB           1       ip = \"192.168.1.11:8000\"\r\ntesting_triton_1  |     51    109.1 MiB      0.0 MiB           1       client = httpclient.InferenceServerClient(ip)\r\ntesting_triton_1  |     52    109.8 MiB      0.8 MiB           4       frames = [np.random.rand(224,224,3).astype(np.float32) for _ in range(1)]\r\ntesting_triton_1  |     53    112.1 MiB      2.3 MiB           1       process(frames, client)\r\ntesting_triton_1  |     54    112.1 MiB      0.0 MiB           1       client.close()\r\ntesting_triton_1  |     55    111.7 MiB     -0.5 MiB           1       del frames\r\ntesting_triton_1  |     56    111.7 MiB      0.0 MiB           1       del client\r\ntesting_triton_1  | \r\ntesting_triton_1  | \r\n\r\n```\r\n\r\nAs seen in the above output, the call to set_data_from_numpy and infer on the client leads to memory increment, which is not being released. \r\n\r\nThe model config.pbtxt is below:\r\n```\r\nplatform: \"onnxruntime_onnx\"\r\nmax_batch_size : 8\r\ninput [\r\n  {\r\n    name: \"input\"\r\n    data_type: TYPE_FP32\r\n    # format: FORMAT_NHWC\r\n    dims: [ 3, 224, 224 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"output\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 2 ]\r\n  }\r\n]\r\n```\r\n\r\n**Expected behavior**\r\nI expected that deleting the objects should have released the memory. Since at present the memory is not being released,  performing the inference using multiple threads (for many frames) is leading to a huge build-up in RAM.  \r\n\r\n\r\nPlease let me know if I missed anything. \r\n\r\nRegards, \r\nAbhishek ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3758/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3758/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3745", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3745/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3745/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3745/events", "html_url": "https://github.com/triton-inference-server/server/issues/3745", "id": 1090437331, "node_id": "I_kwDOCQnI4s5A_sDT", "number": 3745, "title": "got error when output is zero rank in BLS", "user": {"login": "Channingss", "id": 12471701, "node_id": "MDQ6VXNlcjEyNDcxNzAx", "avatar_url": "https://avatars.githubusercontent.com/u/12471701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Channingss", "html_url": "https://github.com/Channingss", "followers_url": "https://api.github.com/users/Channingss/followers", "following_url": "https://api.github.com/users/Channingss/following{/other_user}", "gists_url": "https://api.github.com/users/Channingss/gists{/gist_id}", "starred_url": "https://api.github.com/users/Channingss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Channingss/subscriptions", "organizations_url": "https://api.github.com/users/Channingss/orgs", "repos_url": "https://api.github.com/users/Channingss/repos", "events_url": "https://api.github.com/users/Channingss/events{/privacy}", "received_events_url": "https://api.github.com/users/Channingss/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-12-29T10:56:43Z", "updated_at": "2022-01-06T20:40:24Z", "closed_at": "2022-01-06T20:40:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description** \r\nI  call model to inference in a  BLS logic, when output of this model is zero rank(happened in detection task), i got a core dump in this line:\r\n\r\n```\r\ninference_response = inference_request.exec()\r\n``` \r\n\r\nerror logged by triton server\r\n\r\n```\r\n 2# 0x00007F6EADD6E1F9 in /opt/tritonserver/backends/python/libtriton_python.so\r\n 3# 0x00007F6EADD51CDE in /opt/tritonserver/backends/python/libtriton_python.so\r\n 4# 0x00007F6EADD5249F in /opt/tritonserver/backends/python/libtriton_python.so\r\n 5# 0x00007F6EADD5F36D in /opt/tritonserver/backends/python/libtriton_python.so\r\n 6# 0x00007F6EE343047F in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n 7# 0x00007F6EADD4E505 in /opt/tritonserver/backends/python/libtriton_python.so\r\n 8# 0x00007F6EE2FA9DE4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n 9# 0x00007F6EE3427609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0\r\n10# clone in /usr/lib/x86_64-linux-gnu/libc.so.6\r\n```\r\n\r\n**Triton Information**\r\n21.11\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nyes\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nproject download link: https://1drv.ms/u/s!AlYyqSo4c3BygxWTJnkzR4BuWm-I?e=LD75I4\r\n\r\nrun client_bls1.py  get error.\r\nrun client_bls2.py  run successfully.\r\n\r\n**Expected behavior**\r\nrun client_bls1.py should run successfully.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3745/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3738", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3738/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3738/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3738/events", "html_url": "https://github.com/triton-inference-server/server/issues/3738", "id": 1088774025, "node_id": "I_kwDOCQnI4s5A5V-J", "number": 3738, "title": "Messages of `BUG: soft lockup` and freezing when stopping tritonserver container", "user": {"login": "Raz-Hemo", "id": 21125918, "node_id": "MDQ6VXNlcjIxMTI1OTE4", "avatar_url": "https://avatars.githubusercontent.com/u/21125918?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Raz-Hemo", "html_url": "https://github.com/Raz-Hemo", "followers_url": "https://api.github.com/users/Raz-Hemo/followers", "following_url": "https://api.github.com/users/Raz-Hemo/following{/other_user}", "gists_url": "https://api.github.com/users/Raz-Hemo/gists{/gist_id}", "starred_url": "https://api.github.com/users/Raz-Hemo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Raz-Hemo/subscriptions", "organizations_url": "https://api.github.com/users/Raz-Hemo/orgs", "repos_url": "https://api.github.com/users/Raz-Hemo/repos", "events_url": "https://api.github.com/users/Raz-Hemo/events{/privacy}", "received_events_url": "https://api.github.com/users/Raz-Hemo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-12-26T13:39:44Z", "updated_at": "2022-03-10T23:27:28Z", "closed_at": "2022-03-10T23:27:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen stopping the container for triton-server, we experience freezing for long periods of time (about half a minute of freeze, twice in a row) of the entire machine, accompanied by logs to the shell session that look like:\r\n```\r\nMessage from syslogd@goku1 Dec 26 10:26:29 ...\r\n kernel:[ 3151.617450] watchdog: BUG: soft lockup - CPU#81 stuck for 23s! [tritonserver:73305]\r\n```\r\nThis is the relevant part of the dmesg log if it helps:\r\n```\r\n[Sun Dec 26 12:46:31 2021] i40e 0000:22:00.0 enp34s0f0: tx_timeout: VSI_seid: 390, Q 11, NTC: 0x32, HWB: 0x5e, NTU: 0x5e, TAIL: 0x5e, INT: 0x0\r\n[Sun Dec 26 12:46:31 2021] i40e 0000:22:00.0 enp34s0f0: tx_timeout recovery level 1, hung_queue 11\r\n[Sun Dec 26 12:46:31 2021] watchdog: BUG: soft lockup - CPU#20 stuck for 23s! [tritonserver:77999]\r\n[Sun Dec 26 12:46:31 2021] Modules linked in: xt_comment ip_vs_rr xt_ipvs xt_state ip_vs xt_REDIRECT nfsv3 nfs_acl xt_nat veth vxlan ip6_udp_tunnel udp_tunnel xt_policy iptable_mangle xt_mark xt_u32 xt_tcpudp xt_conntrack xt_MASQUERADE nf_conntrack_netlink nfnetlink xfrm_user xfrm_algo xt_addrtype iptable_filter iptable_nat nf_nat nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 bpfilter br_netfilter bridge stp llc rpcsec_gss_krb5 auth_rpcgss aufs nfsv4 nfs lockd grace fscache overlay bonding nls_iso8859_1 dm_multipath scsi_dh_rdac scsi_dh_emc scsi_dh_alua amd64_edac_mod edac_mce_amd wmi_bmof kvm_amd kvm joydev input_leds ipmi_ssif ccp ipmi_si ipmi_devintf ipmi_msghandler 8250_dw mac_hid nvidia_uvm(OE) sch_fq_codel msr sunrpc ip_tables x_tables autofs4 btrfs zstd_compress raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid0 multipath linear raid1 ses enclosure nvidia_drm(POE) nvidia_modeset(POE) crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel\r\n[Sun Dec 26 12:46:31 2021]  nvidia(POE) crypto_simd cryptd glue_helper mgag200 drm_vram_helper i2c_algo_bit hid_generic ttm drm_kms_helper syscopyarea sysfillrect usbhid sysimgblt mpt3sas hid fb_sys_fops raid_class drm i40e scsi_transport_sas i2c_piix4 wmi\r\n[Sun Dec 26 12:46:31 2021] CPU: 20 PID: 77999 Comm: tritonserver Tainted: P        W  OEL    5.4.0-91-generic #102-Ubuntu\r\n[Sun Dec 26 12:46:31 2021] Hardware name: Cisco Systems Inc UCSC-C245-M6SX/UCSC-C245-M6SX, BIOS C245M6.4.2.1c.0.0806211349 08/06/2021\r\n[Sun Dec 26 12:46:31 2021] RIP: 0010:iommu_unmap_page+0x49/0x100\r\n[Sun Dec 26 12:46:31 2021] Code: 48 89 45 d0 31 c0 48 85 d2 75 02 0f 0b 48 8d 42 ff 48 89 d3 48 85 d0 75 f2 49 89 f7 4c 8d af 94 00 00 00 4c 8d a7 98 00 00 00 <45> 31 f6 eb 18 48 8b 45 c8 48 89 c6 49 01 c6 48 f7 de 49 21 f7 49\r\n[Sun Dec 26 12:46:31 2021] RSP: 0018:ffffb598c062f7e0 EFLAGS: 00000246 ORIG_RAX: ffffffffffffff13\r\n[Sun Dec 26 12:46:31 2021] RAX: 0000000000000fff RBX: 0000000000001000 RCX: 000000000000001e\r\n[Sun Dec 26 12:46:31 2021] RDX: 0000000000001000 RSI: 00008f9a8db8d000 RDI: ffff94ec48f19000\r\n[Sun Dec 26 12:46:31 2021] RBP: ffffb598c062f818 R08: ffffb598c062f7e0 R09: 0000000000000000\r\n[Sun Dec 26 12:46:31 2021] R10: 0000000000000001 R11: 000ffffffffff000 R12: ffff94ec48f19098\r\n[Sun Dec 26 12:46:31 2021] R13: ffff94ec48f19094 R14: 00000000ffffffff R15: 00008f9a8db8d000\r\n[Sun Dec 26 12:46:31 2021] FS:  00007fedf7ffe000(0000) GS:ffff94ec6e100000(0000) knlGS:0000000000000000\r\n[Sun Dec 26 12:46:31 2021] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\r\n[Sun Dec 26 12:46:31 2021] CR2: 0000557dd3364b78 CR3: 0000002fd0b1c006 CR4: 0000000000760ee0\r\n[Sun Dec 26 12:46:31 2021] PKRU: 55555554\r\n[Sun Dec 26 12:46:31 2021] Call Trace:\r\n[Sun Dec 26 12:46:31 2021]  __unmap_single.isra.0+0x63/0x130\r\n[Sun Dec 26 12:46:31 2021]  unmap_sg+0x5f/0x70\r\n[Sun Dec 26 12:46:31 2021]  nv_unmap_dma_map_scatterlist+0x59/0xa0 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  nv_dma_unmap_pages+0xa9/0x100 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  nv_dma_unmap_alloc+0x36/0x60 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  _nv032258rm+0xc1/0x1b0 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv027717rm+0x9b/0xc0 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv010121rm+0x29/0x40 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv028732rm+0x9e/0x230 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv028709rm+0x6e/0x110 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv002282rm+0x9/0x20 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv003698rm+0x1b/0x70 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv014538rm+0x784/0x7f0 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv035204rm+0xac/0xe0 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv036727rm+0xb0/0x140 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv036726rm+0x30f/0x4f0 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv036721rm+0x60/0x70 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv036722rm+0x7b/0xb0 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv035112rm+0x40/0xe0 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? _nv000693rm+0x68/0x80 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? rm_cleanup_file_private+0xea/0x170 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? ext4_read_block_bitmap_nowait+0xff/0x5f0\r\n[Sun Dec 26 12:46:31 2021]  ? nvidia_close+0x149/0x2d0 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? nvidia_frontend_close+0x2f/0x50 [nvidia]\r\n[Sun Dec 26 12:46:31 2021]  ? __fput+0xcc/0x260\r\n[Sun Dec 26 12:46:31 2021]  ? ____fput+0xe/0x10\r\n[Sun Dec 26 12:46:31 2021]  ? task_work_run+0x8f/0xb0\r\n[Sun Dec 26 12:46:31 2021]  ? do_exit+0x36e/0xaf0\r\n[Sun Dec 26 12:46:31 2021]  ? do_group_exit+0x47/0xb0\r\n[Sun Dec 26 12:46:31 2021]  ? get_signal+0x169/0x890\r\n[Sun Dec 26 12:46:31 2021]  ? do_signal+0x34/0x6c0\r\n[Sun Dec 26 12:46:31 2021]  ? __set_cpus_allowed_ptr+0xaf/0x230\r\n[Sun Dec 26 12:46:31 2021]  ? __x64_sys_futex+0x13f/0x170\r\n[Sun Dec 26 12:46:31 2021]  ? exit_to_usermode_loop+0xbf/0x160\r\n[Sun Dec 26 12:46:31 2021]  ? do_syscall_64+0x163/0x190\r\n[Sun Dec 26 12:46:31 2021]  ? entry_SYSCALL_64_after_hwframe+0x44/0xa9\r\n```\r\n\r\n**Triton Information**\r\nImage was built `FROM nvcr.io/nvidia/tritonserver:21.12-py3` with a few extra `COPY` and `ENV` steps.\r\nWe are using the newest 495 drivers with two A10 cards in the system:\r\n`nvidia-driver-495/focal-updates,focal-security,now 495.44-0ubuntu0.20.04.1 amd64 [installed]`\r\n\r\n**To Reproduce**\r\nSimply run `docker run nvcr.io/nvidia/tritonserver:21.12-py3 tritonserver --model-control-mode=explicit --model-repository=s3://deployment:9000/models --pinned-memory-pool-byte-size 8589934592 --cuda-memory-pool-byte-size 0:4294967296`\r\nIt will fail to connect to the repository, freeze for a while, then exit.\r\n\r\n**Expected behavior**\r\nNo message that contains \"BUG: soft lockup\" should appear :)\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3738/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3717", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3717/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3717/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3717/events", "html_url": "https://github.com/triton-inference-server/server/issues/3717", "id": 1084445514, "node_id": "I_kwDOCQnI4s5Ao1NK", "number": 3717, "title": "Segmentation fault when delete server through C API", "user": {"login": "handoku", "id": 16758743, "node_id": "MDQ6VXNlcjE2NzU4NzQz", "avatar_url": "https://avatars.githubusercontent.com/u/16758743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/handoku", "html_url": "https://github.com/handoku", "followers_url": "https://api.github.com/users/handoku/followers", "following_url": "https://api.github.com/users/handoku/following{/other_user}", "gists_url": "https://api.github.com/users/handoku/gists{/gist_id}", "starred_url": "https://api.github.com/users/handoku/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/handoku/subscriptions", "organizations_url": "https://api.github.com/users/handoku/orgs", "repos_url": "https://api.github.com/users/handoku/repos", "events_url": "https://api.github.com/users/handoku/events{/privacy}", "received_events_url": "https://api.github.com/users/handoku/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-12-20T07:19:25Z", "updated_at": "2022-03-17T06:09:44Z", "closed_at": "2022-01-25T00:53:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI write a program following `simple server`.\r\n\r\nAfter unload model, the server encounters segmentation fault when exits. Not happen every time, but with a high probability.\r\n\r\ngdb log:\r\n```\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nCore was generated by `./triton_test -r /root/merged_zoo -t 1 -c 1000 -m my_tensorrt_model'.\r\nProgram terminated with signal SIGSEGV, Segmentation fault.\r\n#0  0x00007fab6f5fbd8c in nvidia::inferenceserver::RateLimiter::ModelContext::ContainsPendingRequests(int) ()\r\n   from /opt/tritonserver/lib/libtritonserver.so\r\n[Current thread is 1 (Thread 0x7fab58dfd000 (LWP 3442))]\r\n(gdb) bt\r\n#0  0x00007fab6f5fbd8c in nvidia::inferenceserver::RateLimiter::ModelContext::ContainsPendingRequests(int) ()\r\n   from /opt/tritonserver/lib/libtritonserver.so\r\n#1  0x00007fab6f5fc368 in nvidia::inferenceserver::RateLimiter::ModelInstanceContext::RequestRemoval() ()\r\n   from /opt/tritonserver/lib/libtritonserver.so\r\n#2  0x00007fab6f5fc3d6 in nvidia::inferenceserver::RateLimiter::ModelInstanceContext::WaitForRemoval() ()\r\n   from /opt/tritonserver/lib/libtritonserver.so\r\n#3  0x00007fab6f603561 in nvidia::inferenceserver::RateLimiter::UnregisterModel(nvidia::inferenceserver::TritonModel const*) ()\r\n   from /opt/tritonserver/lib/libtritonserver.so\r\n#4  0x00007fab6f6f4a03 in nvidia::inferenceserver::TritonModel::~TritonModel() () from /opt/tritonserver/lib/libtritonserver.so\r\n#5  0x00007fab6f6f511d in nvidia::inferenceserver::TritonModel::~TritonModel() [clone .localalias] ()\r\n   from /opt/tritonserver/lib/libtritonserver.so\r\n#6  0x00007fab6f5989b7 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<nvidia::inferenceserver::(anonymous namespace)::BackendDeleter::operator()(nvidia::inferenceserver::InferenceBackend*)::{lambda()#1}> > >::_M_run() ()\r\n   from /opt/tritonserver/lib/libtritonserver.so\r\n#7  0x00007fab6f36ade4 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6\r\n#8  0x00007fab6f47f609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#9  0x00007fab6f1a9293 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n```\r\n\r\n**Triton Information**\r\nngc container `r21.10`\r\n\r\n**To Reproduce**\r\n\r\na tensorrt model with 16 instance count, no ratelimiter config\r\n\r\n\r\n**Expected behavior**\r\nServer exits normally.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3717/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3711", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3711/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3711/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3711/events", "html_url": "https://github.com/triton-inference-server/server/issues/3711", "id": 1083317733, "node_id": "I_kwDOCQnI4s5Akh3l", "number": 3711, "title": "Server returns broken json requests when using TensorRT model config", "user": {"login": "andreabrduque", "id": 55791754, "node_id": "MDQ6VXNlcjU1NzkxNzU0", "avatar_url": "https://avatars.githubusercontent.com/u/55791754?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andreabrduque", "html_url": "https://github.com/andreabrduque", "followers_url": "https://api.github.com/users/andreabrduque/followers", "following_url": "https://api.github.com/users/andreabrduque/following{/other_user}", "gists_url": "https://api.github.com/users/andreabrduque/gists{/gist_id}", "starred_url": "https://api.github.com/users/andreabrduque/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andreabrduque/subscriptions", "organizations_url": "https://api.github.com/users/andreabrduque/orgs", "repos_url": "https://api.github.com/users/andreabrduque/repos", "events_url": "https://api.github.com/users/andreabrduque/events{/privacy}", "received_events_url": "https://api.github.com/users/andreabrduque/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2021-12-17T14:32:13Z", "updated_at": "2022-02-01T19:38:00Z", "closed_at": "2022-02-01T19:38:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI have the following model config\r\n\r\n```\r\nname: \"ner\"\r\nplatform: \"onnxruntime_onnx\"\r\nmax_batch_size: 128\r\ninput [\r\n  {\r\n    name: \"input_ids\"\r\n    data_type: TYPE_INT64\r\n    dims: [ -1 ]\r\n  },\r\n  {\r\n    name: \"token_type_ids\"\r\n    data_type: TYPE_INT64\r\n    dims: [ -1 ]\r\n  },\r\n  {\r\n    name: \"attention_mask\"\r\n    data_type: TYPE_INT64\r\n    dims: [ -1 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"start_logits\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, 5 ]\r\n  },\r\n  {\r\n    name: \"end_logits\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, 5 ]\r\n  }\r\n]\r\n\r\noptimization { execution_accelerators {\r\n  gpu_execution_accelerator : [ {\r\n    name : \"tensorrt\"\r\n    parameters { key: \"precision_mode\" value: \"FP16\" }\r\n    parameters { key: \"max_workspace_size_bytes\" value: \"4073741824\" }\r\n    }]\r\n}}\r\n\r\n````\r\n\r\nIt loads fine. When I make requests to it, it returns me a payload broken in the middle such as \r\n\r\n```\r\n{\r\n    \"id\": \"42\",\r\n    \"model_name\": \"ner\",\r\n    \"model_version\": \"1\",\r\n    \"outputs\": [\r\n        {\r\n            \"name\": \"end_logits\",\r\n            \"datatype\": \"FP32\",\r\n            \"shape\": [\r\n                2,\r\n                8,\r\n                5\r\n            ],\r\n            \"data\": [\r\n\r\n```\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n\r\nUsing triton image nvcr.io/nvidia/tritonserver:21.11-py3\r\n\r\n\r\n**To Reproduce**\r\nCreate a model repository with that config. Make a post request to http://localhost:8000/v2/models/ner/infer such as \r\n````\r\ncurl -d '{\r\n    \"id\" : \"42\",\r\n    \"inputs\": [\r\n        {\r\n            \"name\": \"input_ids\",\r\n            \"shape\": [2, 8],\r\n            \"datatype\":\"INT64\",\r\n            \"data\":  [[1,2,3,4,5,6,7,8],[1,2,3,4,5,6,7,8]]\r\n\r\n        },\r\n        {\r\n            \"name\": \"token_type_ids\",\r\n            \"shape\": [2,8],\r\n            \"datatype\":\"INT64\",\r\n            \"data\":  [[1,2,3,4,5,6,7,8],[1,2,3,4,5,6,7,8]]\r\n        },\r\n            {\r\n            \"name\": \"attention_mask\",\r\n            \"shape\": [2, 8],\r\n            \"datatype\":\"INT64\",\r\n            \"data\":  [[1,2,3,4,5,6,7,8],[1,2,3,4,5,6,7,8]]\r\n        }\r\n\r\n   ],\r\n    \"outputs\" : [\r\n    {\r\n      \"name\" : \"start_logits\"\r\n    },\r\n     {\r\n      \"name\" : \"end_logits\"\r\n    }\r\n  ]\r\n\r\n}'  -H \"Content-Type: application/json\" -X POST http://localhost:8000/v2/models/ner/infer\r\n````\r\n\r\n**Expected behavior**\r\nThe full payload with the correct tensors\r\nThe model without TensorRT optimization returns the full payload, with the output tensors as expected. Example:\r\n\r\n```\r\n{\r\n   \"id\":\"42\",\r\n   \"model_name\":\"ner\",\r\n   \"model_version\":\"1\",\r\n   \"outputs\":[\r\n      {\r\n         \"name\":\"end_logits\",\r\n         \"datatype\":\"FP32\",\r\n         \"shape\":[\r\n            2,\r\n            8,\r\n            5\r\n         ],\r\n         \"data\":[\r\n            -9.151423454284668,\r\n            -9.307648658752442,\r\n             ....\r\n            -9.209794044494629,\r\n            -9.336723327636719,\r\n            -9.302064895629883,\r\n            -10.113032341003418,\r\n            -9.484901428222657\r\n         ]\r\n      },\r\n      {\r\n         \"name\":\"start_logits\",\r\n         \"datatype\":\"FP32\",\r\n         \"shape\":[\r\n            2,\r\n            8,\r\n            5\r\n         ],\r\n         \"data\":[\r\n            -8.951006889343262,\r\n            -9.178339004516602,\r\n            ....\r\n            -9.090873718261719,\r\n            -9.383940696716309,\r\n            -9.084102630615235,\r\n            -9.8538236618042,\r\n            -9.339037895202637\r\n         ]\r\n      }\r\n   ]\r\n}\r\n```", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3711/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3711/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3700", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3700/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3700/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3700/events", "html_url": "https://github.com/triton-inference-server/server/issues/3700", "id": 1079250559, "node_id": "I_kwDOCQnI4s5AVA5_", "number": 3700, "title": "Segmentation fault", "user": {"login": "xtianhb", "id": 20509133, "node_id": "MDQ6VXNlcjIwNTA5MTMz", "avatar_url": "https://avatars.githubusercontent.com/u/20509133?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xtianhb", "html_url": "https://github.com/xtianhb", "followers_url": "https://api.github.com/users/xtianhb/followers", "following_url": "https://api.github.com/users/xtianhb/following{/other_user}", "gists_url": "https://api.github.com/users/xtianhb/gists{/gist_id}", "starred_url": "https://api.github.com/users/xtianhb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xtianhb/subscriptions", "organizations_url": "https://api.github.com/users/xtianhb/orgs", "repos_url": "https://api.github.com/users/xtianhb/repos", "events_url": "https://api.github.com/users/xtianhb/events{/privacy}", "received_events_url": "https://api.github.com/users/xtianhb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2021-12-14T03:01:08Z", "updated_at": "2022-01-21T16:58:00Z", "closed_at": "2022-01-21T16:57:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nGot segfault trying to run a TF-TRT model on an embedded device, Jetson TX2. If necessary I can provide a gdb backtrace.\r\nI don't think is a DeepStream bug, I believe something doesn't goes well with Triton and this NN model, and I can't find what it is.\r\n\r\n**Triton Information**\r\n2.13\r\n\r\nAre you using the Triton container or did you build it yourself?\r\n_deepstream-l4t:6.0-triton_\r\n\r\n**To Reproduce**\r\nDownload model:\r\n[ssd_mobilenet_v1_coco_2018_01_28.tar.gz](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz)\r\nExport with:\r\n[Tensorflow TensorRT Integration](https://github.com/NVIDIA/TensorRT/blob/main/quickstart/IntroNotebooks/2.%20Using%20the%20Tensorflow%20TensorRT%20Integration.ipynb)\r\nConfigure and run a DeepStream/Triton application to use the model\r\nIn the container _deepstream-l4t:6.0-triton_ Make sure to use Triton NvInferServer.\r\ndeepstream-app -c config.txt\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n- Model is exported from TF to TF-TRT with the above resources.\r\n- Input: Image\r\n- Outputs: boxes, scores, classes.\r\n- I include config.pbtxt and log\r\n\r\n[triton.pb.txt](https://github.com/triton-inference-server/server/files/7708302/triton.pb.txt)\r\n[ssdv1fpn_trt.log](https://github.com/triton-inference-server/server/files/7708274/ssdv1fpn_trt.log)\r\n\r\n**Expected behavior**\r\nJust run object detection on images. Not segfault.\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3700/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3700/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3678", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3678/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3678/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3678/events", "html_url": "https://github.com/triton-inference-server/server/issues/3678", "id": 1075475649, "node_id": "I_kwDOCQnI4s5AGnTB", "number": 3678, "title": "Stub process is unhealthy and it will be restarted", "user": {"login": "swapkh91", "id": 5755405, "node_id": "MDQ6VXNlcjU3NTU0MDU=", "avatar_url": "https://avatars.githubusercontent.com/u/5755405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/swapkh91", "html_url": "https://github.com/swapkh91", "followers_url": "https://api.github.com/users/swapkh91/followers", "following_url": "https://api.github.com/users/swapkh91/following{/other_user}", "gists_url": "https://api.github.com/users/swapkh91/gists{/gist_id}", "starred_url": "https://api.github.com/users/swapkh91/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/swapkh91/subscriptions", "organizations_url": "https://api.github.com/users/swapkh91/orgs", "repos_url": "https://api.github.com/users/swapkh91/repos", "events_url": "https://api.github.com/users/swapkh91/events{/privacy}", "received_events_url": "https://api.github.com/users/swapkh91/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 36, "created_at": "2021-12-09T11:41:22Z", "updated_at": "2023-03-17T17:07:51Z", "closed_at": "2022-01-31T21:31:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm getting `Stub process is unhealthy and it will be restarted` repeatedly when calling `infer`, after which the server restarts. I have deployed triton server on GKE with 3 models.\r\n\r\n1st time when I infer `model1` I get this error, 2nd and consequent hits don't give this error. But if I infer `model2` after getting successful result from `model1` then again this error pops up and so on for `model3`.\r\n\r\nlogs:\r\n```\r\nresponses.append(self.triton_client.infer(\r\n      File \"/home/swapnesh/triton/triton_env/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\", line 1086, in infer\r\n        raise_error_grpc(rpc_error)\r\n      File \"/home/swapnesh/triton/triton_env/lib/python3.8/site-packages/tritonclient/grpc/__init__.py\", line 61, in raise_error_grpc\r\n        raise get_error_grpc(rpc_error) from None\r\n    tritonclient.utils.InferenceServerException: [StatusCode.INTERNAL] Failed to process the request(s) for model instance 'damage_0', message: Stub process is not healthy.\r\n```\r\n\r\nI'm loading 3 models, using the python backend and custom triton image (converted detectron models) which I've built using this Dockerfile:\r\n```\r\nFROM nvcr.io/nvidia/tritonserver:21.10-py3\r\n\r\nRUN pip3 install torch==1.9.1 torchvision==0.10.1 torchaudio==0.9.1 && \\\r\n    pip3 install pillow\r\n```\r\n\r\nAlso, while running triton server locally using docker, I had to increase the `shm-size` as it was giving error to increase it from 64MB. On Kubernetes its a little tricky, to use `emptyDir` with `Memory` medium. My yaml looks like this:\r\n```\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  labels:\r\n    app: triton-mms\r\n  name: triton-mms\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: triton-mms\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: triton-mms\r\n    spec:\r\n      containers:\r\n      - image: <custom triton image>\r\n        command: [\"/bin/sh\", \"-c\"]\r\n        args: [\"tritonserver --model-repository=<gcs model repo>\"]\r\n        imagePullPolicy: IfNotPresent\r\n        name: triton-mms\r\n        ports:\r\n        - containerPort: 8000\r\n          name: http-triton\r\n        - containerPort: 8001\r\n          name: grpc-triton\r\n        - containerPort: 8002\r\n          name: metrics-triton\r\n        env:\r\n        - name: GOOGLE_APPLICATION_CREDENTIALS\r\n          value: /secret/gcp-creds.json\r\n        resources:\r\n          limits:\r\n            memory: 5Gi\r\n            nvidia.com/gpu: 1\r\n          requests:\r\n            memory: 5Gi\r\n            nvidia.com/gpu: 1\r\n        volumeMounts:\r\n        - mountPath: /dev/shm\r\n          name: dshm\r\n        - name: vsecret\r\n          mountPath: \"/secret\"\r\n          readOnly: true\r\n      volumes:\r\n      - name: dshm\r\n        emptyDir:\r\n          medium: Memory\r\n          sizeLimit: \"1024Mi\"\r\n      - name: vsecret\r\n        secret:\r\n          secretName: gcpcreds\r\n```\r\n\r\nNever faced this issue before and I'm thinking it might be related to `shared memory` as I've never seen that error too.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3678/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3678/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3647", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3647/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3647/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3647/events", "html_url": "https://github.com/triton-inference-server/server/issues/3647", "id": 1069506829, "node_id": "I_kwDOCQnI4s4_v2EN", "number": 3647, "title": "error: failed to register input shared memory region: failed to register CUDA shared memory region 'input_data1'", "user": {"login": "simon5u", "id": 21052280, "node_id": "MDQ6VXNlcjIxMDUyMjgw", "avatar_url": "https://avatars.githubusercontent.com/u/21052280?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simon5u", "html_url": "https://github.com/simon5u", "followers_url": "https://api.github.com/users/simon5u/followers", "following_url": "https://api.github.com/users/simon5u/following{/other_user}", "gists_url": "https://api.github.com/users/simon5u/gists{/gist_id}", "starred_url": "https://api.github.com/users/simon5u/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simon5u/subscriptions", "organizations_url": "https://api.github.com/users/simon5u/orgs", "repos_url": "https://api.github.com/users/simon5u/repos", "events_url": "https://api.github.com/users/simon5u/events{/privacy}", "received_events_url": "https://api.github.com/users/simon5u/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-12-02T12:43:51Z", "updated_at": "2021-12-22T00:26:42Z", "closed_at": "2021-12-22T00:26:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nOur PC has two GPUs and we use tritonserver 21.02 to host the yolo5 models with cuda shared memory on these two GPUs. \r\nWe run the tritonserver, eveything is working fine and the GPU memory consumption is around 5GB each. Then, we start to run our client code using two processes because we need to support 20 cameras, each process will handle 10 cameras with hardware decoding. We set the CUDA_VISIBLE_DEVICES=0 for the first process, and 1 for the second process. \r\nWe are having the problem \"error: failed to register input shared memory region: failed to register CUDA shared memory region 'input_data1'\" when we run the second process. We dont have any problem on running the first process. We think that the problem was caused by the cuda shared memory. Any advice on this issue?\r\n\r\n**Triton Information**\r\nnvcr.io/nvidia/deepstream:5.1-21.02-triton\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nWe pull the original docker.\r\n\r\n**To Reproduce**\r\nRun the tritonserver to host the yolo5 model with cuda shared memory and using two GPUs.\r\nThen, we run two client processes with CUDA_VISIBLE_DEVICES=0 or 1.\r\nFirst process is running well with bbox output from yolo5.\r\nSecond process will crash and show a log \"error: failed to register input shared memory region: failed to register CUDA shared memory region 'input_data1'\"\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nModel is yolo5 from https://github.com/ultralytics/yolov5/releases. Input is a 2D image and output is an object bbox.\r\n\r\n**Expected behavior**\r\nWe are expecting a way to direct the client process to the triton model hosted in a particular gpu.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3647/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3624", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3624/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3624/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3624/events", "html_url": "https://github.com/triton-inference-server/server/issues/3624", "id": 1064460126, "node_id": "I_kwDOCQnI4s4_cl9e", "number": 3624, "title": "dynamic batching not working properly while requests waiting in queue", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 18, "created_at": "2021-11-26T13:13:53Z", "updated_at": "2022-01-24T16:08:41Z", "closed_at": "2022-01-14T19:11:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n- Model has dynamic_batching on. max_queue_delay_microseconds is not set, thus default to 0. It is a gpu torchscript model.\r\n1. Send 5 requests to a model asynchronously.\r\n2. The model started executing the first request as soon as it arrived\r\n```\r\nI1126 10:15:28.121351 27161 infer_request.cc:547] prepared: [0x0x7ff6380055c0] request id: , model: main, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7ff638015b38] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noverride inputs:\r\ninputs:\r\n[0x0x7ff638015b38] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noriginal requested outputs:\r\nOUTPUT__0\r\nrequested outputs:\r\nOUTPUT__0\r\n...\r\nI1126 10:15:28.121456 27161 libtorch.cc:1347] model main, instance main_0, executing 1 requests\r\n```\r\n3. While the model is running, other 4 requests is reported prepared by infer_request.cc and should be waiting in queue\r\n```\r\nI1126 10:15:28.121460 27161 infer_request.cc:547] prepared: [0x0x7ff638006e70] request id: , model: main, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7ff638007178] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noverride inputs:\r\ninputs:\r\n[0x0x7ff638007178] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noriginal requested outputs:\r\nOUTPUT__0\r\nrequested outputs:\r\nOUTPUT__0\r\nI1126 10:15:28.121517 27161 http_server.cc:2727] HTTP request: 2 /v2/models/main/infer\r\nI1126 10:15:28.121526 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121532 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121545 27161 pinned_memory_manager.cc:161] pinned memory allocation: size 48, addr 0x7ff668000090\r\nI1126 10:15:28.121550 27161 infer_request.cc:547] prepared: [0x0x7ff638007610] request id: , model: main, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7ff638007938] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noverride inputs:\r\ninputs:\r\n[0x0x7ff638007938] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noriginal requested outputs:\r\nOUTPUT__0\r\nrequested outputs:\r\nOUTPUT__0\r\n\r\nI1126 10:15:28.121581 27161 http_server.cc:2727] HTTP request: 2 /v2/models/main/infer\r\nI1126 10:15:28.121589 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121595 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121614 27161 infer_request.cc:547] prepared: [0x0x7ff638007e00] request id: , model: main, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7ff638008108] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noverride inputs:\r\ninputs:\r\n[0x0x7ff638008108] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noriginal requested outputs:\r\nOUTPUT__0\r\nrequested outputs:\r\nOUTPUT__0\r\n\r\nI1126 10:15:28.121643 27161 http_server.cc:2727] HTTP request: 2 /v2/models/main/infer\r\nI1126 10:15:28.121651 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121658 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121676 27161 infer_request.cc:547] prepared: [0x0x7ff638008be0] request id: , model: main, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7ff638008f98] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noverride inputs:\r\ninputs:\r\n[0x0x7ff638008f98] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noriginal requested outputs:\r\nOUTPUT__0\r\nrequested outputs:\r\nOUTPUT__0\r\n```\r\n4. Next model execution happens 2.94s after the last of the 5 requests arrived, which is a plenty of time for the backend to collect it into a batch of 4 and send it to the model for execution. However, that is not what we're seeing here. the next execution only executes 1 request. The model execution after that also executes 1 request. The request after that finally does some batching and the model executes 2 requests. \r\n```\r\nI1126 10:15:31.066632 27161 libtorch.cc:1347] model main, instance main_0, executing 1 requests\r\nI1126 10:15:31.066639 27161 libtorch.cc:686] TRITONBACKEND_ModelExecute: Running main_0 with 1 requests\r\n...\r\nI1126 10:15:31.170789 27161 libtorch.cc:1347] model main, instance main_0, executing 1 requests\r\nI1126 10:15:31.170796 27161 libtorch.cc:686] TRITONBACKEND_ModelExecute: Running main_0 with 1 requests\r\n...\r\nI1126 10:15:31.275147 27161 libtorch.cc:1347] model main, instance main_0, executing 2 requests\r\nI1126 10:15:31.275154 27161 libtorch.cc:686] TRITONBACKEND_ModelExecute: Running main_0 with 2 requests\r\n```\r\nNow, if I change the max_queue_delay_microseconds to something like 10ms, it does seems to form a batch if the next request comes within that timeframe, but not the other requests after that duration even though the model is still executing.\r\nShouldn't all requests waiting in queue, while the model executes, be submitted for the next inference? \r\n\r\n**Triton Information**\r\nWe are using the 21.11 ngc trtis image. It is deployed in a kubernetes cluster and it may have gone through a little bit of modifications but should be pretty much the same. \r\nWe used the V100 gpu, but this issue seems to be happening on a T4 gpu(21.10 trtis in this case) as well. \r\nnvidia-smi command shows me that I'm running this image with nvidia driver 450.102.04 cuda version 11.0\r\nthe torchscript model was scripted from pytorch version 1.10.0+cu102\r\n\r\n**To Reproduce**\r\nI tried to replicate this issue with a simplest model possible.\r\nI used the following script to create a torchscript model.\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass MainModel(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n    \r\n    def forward(\r\n        self,\r\n        inputs,\r\n    ):\r\n        for i in range(10000):\r\n            inputs = inputs * 2\r\n        return inputs\r\ntorch.jit.save(torch.jit.script(MainModel()),'models/main/1/model.pt')\r\n```\r\nThe following config.pbtxt was used\r\n```\r\nname: \"main\"\r\nbackend: \"pytorch\"\r\n\r\ninput [\r\n  {\r\n    name: \"INPUT__0\"\r\n    data_type: TYPE_INT32\r\n    dims: [ -1 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"OUTPUT__0\"\r\n    data_type: TYPE_INT32\r\n    dims: [ -1 ]\r\n  }\r\n]\r\ninstance_group [{\r\n    kind: KIND_GPU\r\n    count: 1\r\n}]\r\ndynamic_batching {\r\n}\r\nmax_batch_size: 10000\r\nparameters: [\r\n    {\r\n        key: \"DISABLE_OPTIMIZED_EXECUTION\"\r\n            value: {\r\n                string_value:\"true\"\r\n            }\r\n    }\r\n]\r\n```\r\n\r\n\r\n**Expected behavior**\r\nafter the first execution with 1 request, the next execution should process 4 requests since they arrived well before the next model execution and the log says they were prepared.\r\n\r\nThis issue has been causing a huge performance drop. Could you give us guidance on debugging this issue? Thank you\r\n\r\n## FULL LOG\r\n```\r\n$ tritonserver --model-repository models --log-verbose 1 --model-control-mode=explicit --load-model main\r\nI1126 10:15:24.232224 27161 metrics.cc:298] Collecting metrics for GPU 0: Tesla V100-SXM2-32GB\r\nI1126 10:15:24.232590 27161 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\r\nI1126 10:15:24.614565 27161 libtorch.cc:1192] TRITONBACKEND_Initialize: pytorch\r\nI1126 10:15:24.614602 27161 libtorch.cc:1202] Triton TRITONBACKEND API version: 1.6\r\nI1126 10:15:24.614612 27161 libtorch.cc:1208] 'pytorch' TRITONBACKEND API version: 1.6\r\nI1126 10:15:24.614661 27161 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so\r\n2021-11-26 19:15:25.059380: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nI1126 10:15:25.113301 27161 tensorflow.cc:2170] TRITONBACKEND_Initialize: tensorflow\r\nI1126 10:15:25.113334 27161 tensorflow.cc:2180] Triton TRITONBACKEND API version: 1.6\r\nI1126 10:15:25.113344 27161 tensorflow.cc:2186] 'tensorflow' TRITONBACKEND API version: 1.6\r\nI1126 10:15:25.113353 27161 tensorflow.cc:2210] backend configuration:\r\n{}\r\nI1126 10:15:25.113398 27161 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so\r\nI1126 10:15:25.116591 27161 onnxruntime.cc:2157] TRITONBACKEND_Initialize: onnxruntime\r\nI1126 10:15:25.116620 27161 onnxruntime.cc:2167] Triton TRITONBACKEND API version: 1.6\r\nI1126 10:15:25.116630 27161 onnxruntime.cc:2173] 'onnxruntime' TRITONBACKEND API version: 1.6\r\nI1126 10:15:25.129781 27161 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/openvino/libtriton_openvino.so\r\nI1126 10:15:25.146186 27161 openvino.cc:1193] TRITONBACKEND_Initialize: openvino\r\nI1126 10:15:25.146208 27161 openvino.cc:1203] Triton TRITONBACKEND API version: 1.6\r\nI1126 10:15:25.146218 27161 openvino.cc:1209] 'openvino' TRITONBACKEND API version: 1.6\r\nI1126 10:15:25.411769 27161 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7ff668000000' with size 268435456\r\nI1126 10:15:25.412908 27161 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\nI1126 10:15:25.414399 27161 backend_factory.h:45] Create TritonBackendFactory\r\nI1126 10:15:25.414418 27161 ensemble_backend_factory.cc:47] Create EnsembleBackendFactory\r\nI1126 10:15:25.417917 27161 model_repository_manager.cc:726] AsyncLoad() 'main'\r\nI1126 10:15:25.418211 27161 model_repository_manager.cc:965] TriggerNextAction() 'main' version 1: 1\r\nI1126 10:15:25.418224 27161 model_repository_manager.cc:1003] Load() 'main' version 1\r\nI1126 10:15:25.418233 27161 model_repository_manager.cc:1022] loading: main:1\r\nI1126 10:15:25.518442 27161 model_repository_manager.cc:1082] CreateInferenceBackend() 'main' version 1\r\nI1126 10:15:25.519135 27161 libtorch.cc:1241] TRITONBACKEND_ModelInitialize: main (version 1)\r\nI1126 10:15:25.520296 27161 model_config_utils.cc:1550] ModelConfig 64-bit fields:\r\nI1126 10:15:25.520312 27161 model_config_utils.cc:1552] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\r\nI1126 10:15:25.520319 27161 model_config_utils.cc:1552] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\r\nI1126 10:15:25.520326 27161 model_config_utils.cc:1552] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\r\nI1126 10:15:25.520332 27161 model_config_utils.cc:1552] \tModelConfig::ensemble_scheduling::step::model_version\r\nI1126 10:15:25.520337 27161 model_config_utils.cc:1552] \tModelConfig::input::dims\r\nI1126 10:15:25.520343 27161 model_config_utils.cc:1552] \tModelConfig::input::reshape::shape\r\nI1126 10:15:25.520349 27161 model_config_utils.cc:1552] \tModelConfig::instance_group::secondary_devices::device_id\r\nI1126 10:15:25.520355 27161 model_config_utils.cc:1552] \tModelConfig::model_warmup::inputs::value::dims\r\nI1126 10:15:25.520361 27161 model_config_utils.cc:1552] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\r\nI1126 10:15:25.520367 27161 model_config_utils.cc:1552] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\r\nI1126 10:15:25.520372 27161 model_config_utils.cc:1552] \tModelConfig::output::dims\r\nI1126 10:15:25.520378 27161 model_config_utils.cc:1552] \tModelConfig::output::reshape::shape\r\nI1126 10:15:25.520384 27161 model_config_utils.cc:1552] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\r\nI1126 10:15:25.520390 27161 model_config_utils.cc:1552] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\r\nI1126 10:15:25.520396 27161 model_config_utils.cc:1552] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\r\nI1126 10:15:25.520402 27161 model_config_utils.cc:1552] \tModelConfig::sequence_batching::state::dims\r\nI1126 10:15:25.520407 27161 model_config_utils.cc:1552] \tModelConfig::version_policy::specific::versions\r\nI1126 10:15:25.520533 27161 libtorch.cc:251] Optimized execution is disabled for model instance 'main'\r\nI1126 10:15:25.520542 27161 libtorch.cc:269] Inference Mode is disabled for model instance 'main'\r\nI1126 10:15:25.520550 27161 libtorch.cc:344] NvFuser is not specified for model instance 'main'\r\nI1126 10:15:25.523385 27161 libtorch.cc:1282] TRITONBACKEND_ModelInstanceInitialize: main_0 (device 0)\r\nI1126 10:15:25.525701 27161 backend_model_instance.cc:105] Creating instance main_0 on GPU 0 (7.0) using artifact 'model.pt'\r\nI1126 10:15:25.546869 27161 triton_model_instance.cc:668] Starting backend thread for main_0 at nice 0 on device 0...\r\nI1126 10:15:25.547031 27161 model_repository_manager.cc:1183] successfully loaded 'main' version 1\r\nI1126 10:15:25.547048 27161 model_repository_manager.cc:965] TriggerNextAction() 'main' version 1: 0\r\nI1126 10:15:25.547055 27161 model_repository_manager.cc:980] no next action, trigger OnComplete()\r\nI1126 10:15:25.547060 27161 dynamic_batch_scheduler.cc:243] Starting dynamic-batcher thread for main at nice 0...\r\nI1126 10:15:25.547079 27161 model_repository_manager.cc:571] VersionStates() 'main'\r\nI1126 10:15:25.547100 27161 model_repository_manager.cc:571] VersionStates() 'main'\r\nI1126 10:15:25.547148 27161 server.cc:522] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI1126 10:15:25.547223 27161 server.cc:549] \r\n+-------------+-----------------------------------------------------------------+--------+\r\n| Backend     | Path                                                            | Config |\r\n+-------------+-----------------------------------------------------------------+--------+\r\n| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so         | {}     |\r\n| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so | {}     |\r\n| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {}     |\r\n| openvino    | /opt/tritonserver/backends/openvino/libtriton_openvino.so       | {}     |\r\n+-------------+-----------------------------------------------------------------+--------+\r\n\r\nI1126 10:15:25.547250 27161 model_repository_manager.cc:547] BackendStates()\r\nI1126 10:15:25.547289 27161 server.cc:592] \r\n+-------+---------+--------+\r\n| Model | Version | Status |\r\n+-------+---------+--------+\r\n| main  | 1       | READY  |\r\n+-------+---------+--------+\r\n\r\nI1126 10:15:25.547424 27161 tritonserver.cc:1920] \r\n+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                      |\r\n+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                     |\r\n| server_version                   | 2.16.0                                                                                                                                     |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda |\r\n|                                  | _shared_memory binary_tensor_data statistics                                                                                               |\r\n| model_repository_path[0]         | models                                                                                                                                     |\r\n| model_control_mode               | MODE_EXPLICIT                                                                                                                              |\r\n| startup_models_0                 | main                                                                                                                                       |\r\n| strict_model_config              | 1                                                                                                                                          |\r\n| rate_limit                       | OFF                                                                                                                                        |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                  |\r\n| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                   |\r\n| response_cache_byte_size         | 0                                                                                                                                          |\r\n| min_supported_compute_capability | 6.0                                                                                                                                        |\r\n| strict_readiness                 | 1                                                                                                                                          |\r\n| exit_timeout                     | 30                                                                                                                                         |\r\n+----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI1126 10:15:25.547593 27161 grpc_server.cc:4071] === GRPC KeepAlive Options ===\r\nI1126 10:15:25.547606 27161 grpc_server.cc:4072] keepalive_time_ms: 7200000\r\nI1126 10:15:25.547615 27161 grpc_server.cc:4074] keepalive_timeout_ms: 20000\r\nI1126 10:15:25.547624 27161 grpc_server.cc:4076] keepalive_permit_without_calls: 0\r\nI1126 10:15:25.547633 27161 grpc_server.cc:4078] http2_max_pings_without_data: 2\r\nI1126 10:15:25.547642 27161 grpc_server.cc:4080] http2_min_recv_ping_interval_without_data_ms: 300000\r\nI1126 10:15:25.547651 27161 grpc_server.cc:4083] http2_max_ping_strikes: 2\r\nI1126 10:15:25.547659 27161 grpc_server.cc:4085] ==============================\r\nI1126 10:15:25.548640 27161 grpc_server.cc:225] Ready for RPC 'ServerLive', 0\r\nI1126 10:15:25.548666 27161 grpc_server.cc:225] Ready for RPC 'ServerReady', 0\r\nI1126 10:15:25.548677 27161 grpc_server.cc:225] Ready for RPC 'ModelReady', 0\r\nI1126 10:15:25.548685 27161 grpc_server.cc:225] Ready for RPC 'ServerMetadata', 0\r\nI1126 10:15:25.548694 27161 grpc_server.cc:225] Ready for RPC 'ModelMetadata', 0\r\nI1126 10:15:25.548704 27161 grpc_server.cc:225] Ready for RPC 'ModelConfig', 0\r\nI1126 10:15:25.548711 27161 grpc_server.cc:225] Ready for RPC 'ModelStatistics', 0\r\nI1126 10:15:25.548721 27161 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryStatus', 0\r\nI1126 10:15:25.548731 27161 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryRegister', 0\r\nI1126 10:15:25.548741 27161 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryUnregister', 0\r\nI1126 10:15:25.548749 27161 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryStatus', 0\r\nI1126 10:15:25.548758 27161 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryRegister', 0\r\nI1126 10:15:25.548768 27161 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryUnregister', 0\r\nI1126 10:15:25.548777 27161 grpc_server.cc:225] Ready for RPC 'RepositoryIndex', 0\r\nI1126 10:15:25.548785 27161 grpc_server.cc:225] Ready for RPC 'RepositoryModelLoad', 0\r\nI1126 10:15:25.548796 27161 grpc_server.cc:225] Ready for RPC 'RepositoryModelUnload', 0\r\nI1126 10:15:25.548812 27161 grpc_server.cc:416] Thread started for CommonHandler\r\nI1126 10:15:25.548942 27161 grpc_server.cc:3150] New request handler for ModelInferHandler, 1\r\nI1126 10:15:25.548969 27161 grpc_server.cc:2202] Thread started for ModelInferHandler\r\nI1126 10:15:25.549084 27161 grpc_server.cc:3503] New request handler for ModelStreamInferHandler, 3\r\nI1126 10:15:25.549112 27161 grpc_server.cc:2202] Thread started for ModelStreamInferHandler\r\nI1126 10:15:25.549125 27161 grpc_server.cc:4117] Started GRPCInferenceService at 0.0.0.0:8001\r\nI1126 10:15:25.549415 27161 http_server.cc:2815] Started HTTPService at 0.0.0.0:8000\r\nI1126 10:15:25.590519 27161 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\r\nI1126 10:15:28.121240 27161 http_server.cc:2727] HTTP request: 2 /v2/models/main/infer\r\nI1126 10:15:28.121289 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121301 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121351 27161 infer_request.cc:547] prepared: [0x0x7ff6380055c0] request id: , model: main, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7ff638015b38] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noverride inputs:\r\ninputs:\r\n[0x0x7ff638015b38] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noriginal requested outputs:\r\nOUTPUT__0\r\nrequested outputs:\r\nOUTPUT__0\r\n\r\nI1126 10:15:28.121413 27161 http_server.cc:2727] HTTP request: 2 /v2/models/main/infer\r\nI1126 10:15:28.121423 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121430 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121456 27161 libtorch.cc:1347] model main, instance main_0, executing 1 requests\r\nI1126 10:15:28.121460 27161 infer_request.cc:547] prepared: [0x0x7ff638006e70] request id: , model: main, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7ff638007178] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noverride inputs:\r\ninputs:\r\n[0x0x7ff638007178] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noriginal requested outputs:\r\nOUTPUT__0\r\nrequested outputs:\r\nOUTPUT__0\r\n\r\nI1126 10:15:28.121480 27161 libtorch.cc:686] TRITONBACKEND_ModelExecute: Running main_0 with 1 requests\r\nI1126 10:15:28.121517 27161 http_server.cc:2727] HTTP request: 2 /v2/models/main/infer\r\nI1126 10:15:28.121526 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121532 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121545 27161 pinned_memory_manager.cc:161] pinned memory allocation: size 48, addr 0x7ff668000090\r\nI1126 10:15:28.121550 27161 infer_request.cc:547] prepared: [0x0x7ff638007610] request id: , model: main, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7ff638007938] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noverride inputs:\r\ninputs:\r\n[0x0x7ff638007938] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noriginal requested outputs:\r\nOUTPUT__0\r\nrequested outputs:\r\nOUTPUT__0\r\n\r\nI1126 10:15:28.121581 27161 http_server.cc:2727] HTTP request: 2 /v2/models/main/infer\r\nI1126 10:15:28.121589 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121595 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121614 27161 infer_request.cc:547] prepared: [0x0x7ff638007e00] request id: , model: main, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7ff638008108] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noverride inputs:\r\ninputs:\r\n[0x0x7ff638008108] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noriginal requested outputs:\r\nOUTPUT__0\r\nrequested outputs:\r\nOUTPUT__0\r\n\r\nI1126 10:15:28.121643 27161 http_server.cc:2727] HTTP request: 2 /v2/models/main/infer\r\nI1126 10:15:28.121651 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121658 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version -1\r\nI1126 10:15:28.121676 27161 infer_request.cc:547] prepared: [0x0x7ff638008be0] request id: , model: main, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7ff638008f98] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noverride inputs:\r\ninputs:\r\n[0x0x7ff638008f98] input: INPUT__0, type: INT32, original shape: [1,12], batch + shape: [1,12], shape: [12]\r\noriginal requested outputs:\r\nOUTPUT__0\r\nrequested outputs:\r\nOUTPUT__0\r\n\r\nI1126 10:15:31.066263 27161 infer_response.cc:165] add response output: output: OUTPUT__0, type: INT32, shape: [1,12]\r\nI1126 10:15:31.066316 27161 http_server.cc:1051] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU\r\nI1126 10:15:31.066330 27161 http_server.cc:1071] HTTP using buffer for: 'OUTPUT__0', size: 48, addr: 0x7ff555af3bd0\r\nI1126 10:15:31.066346 27161 pinned_memory_manager.cc:161] pinned memory allocation: size 48, addr 0x7ff6680000d0\r\nI1126 10:15:31.066393 27161 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7ff6680000d0\r\nI1126 10:15:31.066565 27161 http_server.cc:1086] HTTP release: size 48, addr 0x7ff555af3bd0\r\nI1126 10:15:31.066603 27161 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7ff668000090\r\nI1126 10:15:31.066632 27161 libtorch.cc:1347] model main, instance main_0, executing 1 requests\r\nI1126 10:15:31.066639 27161 libtorch.cc:686] TRITONBACKEND_ModelExecute: Running main_0 with 1 requests\r\nI1126 10:15:31.066661 27161 pinned_memory_manager.cc:161] pinned memory allocation: size 48, addr 0x7ff668000090\r\nI1126 10:15:31.170613 27161 infer_response.cc:165] add response output: output: OUTPUT__0, type: INT32, shape: [1,12]\r\nI1126 10:15:31.170637 27161 http_server.cc:1051] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU\r\nI1126 10:15:31.170647 27161 http_server.cc:1071] HTTP using buffer for: 'OUTPUT__0', size: 48, addr: 0x7ff555af3bd0\r\nI1126 10:15:31.170656 27161 pinned_memory_manager.cc:161] pinned memory allocation: size 48, addr 0x7ff6680000d0\r\nI1126 10:15:31.170684 27161 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7ff6680000d0\r\nI1126 10:15:31.170752 27161 http_server.cc:1086] HTTP release: size 48, addr 0x7ff555af3bd0\r\nI1126 10:15:31.170770 27161 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7ff668000090\r\nI1126 10:15:31.170789 27161 libtorch.cc:1347] model main, instance main_0, executing 1 requests\r\nI1126 10:15:31.170796 27161 libtorch.cc:686] TRITONBACKEND_ModelExecute: Running main_0 with 1 requests\r\nI1126 10:15:31.170809 27161 pinned_memory_manager.cc:161] pinned memory allocation: size 48, addr 0x7ff668000090\r\nI1126 10:15:31.274920 27161 infer_response.cc:165] add response output: output: OUTPUT__0, type: INT32, shape: [1,12]\r\nI1126 10:15:31.274940 27161 http_server.cc:1051] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU\r\nI1126 10:15:31.275019 27161 http_server.cc:1071] HTTP using buffer for: 'OUTPUT__0', size: 48, addr: 0x7ff555af3bd0\r\nI1126 10:15:31.275027 27161 pinned_memory_manager.cc:161] pinned memory allocation: size 48, addr 0x7ff6680000d0\r\nI1126 10:15:31.275052 27161 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7ff6680000d0\r\nI1126 10:15:31.275115 27161 http_server.cc:1086] HTTP release: size 48, addr 0x7ff555af3bd0\r\nI1126 10:15:31.275131 27161 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7ff668000090\r\nI1126 10:15:31.275147 27161 libtorch.cc:1347] model main, instance main_0, executing 2 requests\r\nI1126 10:15:31.275154 27161 libtorch.cc:686] TRITONBACKEND_ModelExecute: Running main_0 with 2 requests\r\nI1126 10:15:31.275167 27161 pinned_memory_manager.cc:161] pinned memory allocation: size 96, addr 0x7ff668000090\r\nI1126 10:15:31.377584 27161 infer_response.cc:165] add response output: output: OUTPUT__0, type: INT32, shape: [1,12]\r\nI1126 10:15:31.377604 27161 http_server.cc:1051] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU\r\nI1126 10:15:31.377612 27161 http_server.cc:1071] HTTP using buffer for: 'OUTPUT__0', size: 48, addr: 0x7ff555af3bd0\r\nI1126 10:15:31.377620 27161 infer_response.cc:165] add response output: output: OUTPUT__0, type: INT32, shape: [1,12]\r\nI1126 10:15:31.377627 27161 http_server.cc:1051] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU\r\nI1126 10:15:31.377633 27161 http_server.cc:1071] HTTP using buffer for: 'OUTPUT__0', size: 48, addr: 0x7ff658022b40\r\nI1126 10:15:31.377641 27161 pinned_memory_manager.cc:161] pinned memory allocation: size 96, addr 0x7ff668000100\r\nI1126 10:15:31.377665 27161 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7ff668000100\r\nI1126 10:15:31.377729 27161 http_server.cc:1086] HTTP release: size 48, addr 0x7ff555af3bd0\r\nI1126 10:15:31.377762 27161 http_server.cc:1086] HTTP release: size 48, addr 0x7ff658022b40\r\nI1126 10:15:31.377779 27161 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7ff668000090\r\nI1126 10:15:33.111422 27161 http_server.cc:2727] HTTP request: 0 /v2/models/main/stats\r\nI1126 10:15:33.111460 27161 model_repository_manager.cc:571] VersionStates() 'main'\r\nI1126 10:15:33.111476 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version 1\r\nI1126 10:15:33.761620 27161 http_server.cc:2727] HTTP request: 0 /v2/models/main/stats\r\nI1126 10:15:33.761654 27161 model_repository_manager.cc:571] VersionStates() 'main'\r\nI1126 10:15:33.761668 27161 model_repository_manager.cc:615] GetInferenceBackend() 'main' version 1\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3624/reactions", "total_count": 2, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3624/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3593", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3593/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3593/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3593/events", "html_url": "https://github.com/triton-inference-server/server/issues/3593", "id": 1059232949, "node_id": "I_kwDOCQnI4s4_Ipy1", "number": 3593, "title": "Error when converting the automatic config json to config.pbtxt", "user": {"login": "philnguyenresson", "id": 28564827, "node_id": "MDQ6VXNlcjI4NTY0ODI3", "avatar_url": "https://avatars.githubusercontent.com/u/28564827?v=4", "gravatar_id": "", "url": "https://api.github.com/users/philnguyenresson", "html_url": "https://github.com/philnguyenresson", "followers_url": "https://api.github.com/users/philnguyenresson/followers", "following_url": "https://api.github.com/users/philnguyenresson/following{/other_user}", "gists_url": "https://api.github.com/users/philnguyenresson/gists{/gist_id}", "starred_url": "https://api.github.com/users/philnguyenresson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/philnguyenresson/subscriptions", "organizations_url": "https://api.github.com/users/philnguyenresson/orgs", "repos_url": "https://api.github.com/users/philnguyenresson/repos", "events_url": "https://api.github.com/users/philnguyenresson/events{/privacy}", "received_events_url": "https://api.github.com/users/philnguyenresson/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "krishung5", "id": 43719498, "node_id": "MDQ6VXNlcjQzNzE5NDk4", "avatar_url": "https://avatars.githubusercontent.com/u/43719498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krishung5", "html_url": "https://github.com/krishung5", "followers_url": "https://api.github.com/users/krishung5/followers", "following_url": "https://api.github.com/users/krishung5/following{/other_user}", "gists_url": "https://api.github.com/users/krishung5/gists{/gist_id}", "starred_url": "https://api.github.com/users/krishung5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krishung5/subscriptions", "organizations_url": "https://api.github.com/users/krishung5/orgs", "repos_url": "https://api.github.com/users/krishung5/repos", "events_url": "https://api.github.com/users/krishung5/events{/privacy}", "received_events_url": "https://api.github.com/users/krishung5/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "krishung5", "id": 43719498, "node_id": "MDQ6VXNlcjQzNzE5NDk4", "avatar_url": "https://avatars.githubusercontent.com/u/43719498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krishung5", "html_url": "https://github.com/krishung5", "followers_url": "https://api.github.com/users/krishung5/followers", "following_url": "https://api.github.com/users/krishung5/following{/other_user}", "gists_url": "https://api.github.com/users/krishung5/gists{/gist_id}", "starred_url": "https://api.github.com/users/krishung5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krishung5/subscriptions", "organizations_url": "https://api.github.com/users/krishung5/orgs", "repos_url": "https://api.github.com/users/krishung5/repos", "events_url": "https://api.github.com/users/krishung5/events{/privacy}", "received_events_url": "https://api.github.com/users/krishung5/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-11-20T21:26:10Z", "updated_at": "2022-03-29T16:38:38Z", "closed_at": "2022-03-29T16:38:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nTensorRT model loads and automatically creates a config json correctly, however, the conversion to config.pbtxt gives an error, even though the files contain the same settings\r\n\r\n**Triton Information**\r\n\r\nnvcr.io/nvidia/tritonserver:21.10-py3\r\n\r\n**To Reproduce**\r\nWhen I start the triton server without config.pbtxt for the model, it loads fine and I can retrieve the config json, which looks like this:\r\n```\r\n{\r\n  \"name\": \"test\",\r\n  \"platform\": \"tensorrt_plan\",\r\n  \"backend\": \"tensorrt\",\r\n  \"version_policy\": {\r\n    \"latest\": {\r\n      \"num_versions\": 1\r\n    }\r\n  },\r\n  \"max_batch_size\": 1,\r\n  \"input\": [\r\n    {\r\n      \"name\": \"input_tensor:0\",\r\n      \"data_type\": \"TYPE_FP32\",\r\n      \"format\": \"FORMAT_NONE\",\r\n      \"dims\": [\r\n        2048,\r\n        2432,\r\n        3\r\n      ],\r\n      \"is_shape_tensor\": false,\r\n      \"allow_ragged_batch\": false\r\n    }\r\n  ],\r\n  \"output\": [\r\n    {\r\n      \"name\": \"num_detections\",\r\n      \"data_type\": \"TYPE_INT32\",\r\n      \"dims\": [\r\n        1\r\n      ],\r\n      \"reshape\": {\r\n        \"shape\": []\r\n      },\r\n      \"label_filename\": \"\",\r\n      \"is_shape_tensor\": false\r\n    },\r\n    {\r\n      \"name\": \"detection_boxes\",\r\n      \"data_type\": \"TYPE_FP32\",\r\n      \"dims\": [\r\n        100,\r\n        4\r\n      ],\r\n      \"label_filename\": \"\",\r\n      \"is_shape_tensor\": false\r\n    },\r\n    {\r\n      \"name\": \"detection_scores\",\r\n      \"data_type\": \"TYPE_FP32\",\r\n      \"dims\": [\r\n        100\r\n      ],\r\n      \"label_filename\": \"\",\r\n      \"is_shape_tensor\": false\r\n    },\r\n    {\r\n      \"name\": \"detection_classes\",\r\n      \"data_type\": \"TYPE_FP32\",\r\n      \"dims\": [\r\n        100\r\n      ],\r\n      \"label_filename\": \"\",\r\n      \"is_shape_tensor\": false\r\n    }\r\n  ],\r\n  \"batch_input\": [],\r\n  \"batch_output\": [],\r\n  \"optimization\": {\r\n    \"priority\": \"PRIORITY_DEFAULT\",\r\n    \"input_pinned_memory\": {\r\n      \"enable\": true\r\n    },\r\n    \"output_pinned_memory\": {\r\n      \"enable\": true\r\n    },\r\n    \"gather_kernel_buffer_threshold\": 0,\r\n    \"eager_batching\": false\r\n  },\r\n  \"instance_group\": [\r\n    {\r\n      \"name\": \"test\",\r\n      \"kind\": \"KIND_GPU\",\r\n      \"count\": 1,\r\n      \"gpus\": [\r\n        0\r\n      ],\r\n      \"secondary_devices\": [],\r\n      \"profile\": [],\r\n      \"passive\": false,\r\n      \"host_policy\": \"\"\r\n    }\r\n  ],\r\n  \"default_model_filename\": \"model.plan\",\r\n  \"cc_model_filenames\": {},\r\n  \"metric_tags\": {},\r\n  \"parameters\": {},\r\n  \"model_warmup\": []\r\n}\r\n\r\n```\r\nI've converted to the pbtxt format required, and it looks like this : \r\n```\r\nname: \"test\"\r\nplatform: \"tensorrt_plan\"\r\nbackend: \"tensorrt\"\r\nversion_policy: {\r\n  latest: {\r\n    num_versions: 1\r\n  }\r\n}\r\nmax_batch_size : 1\r\ninput [\r\n  {\r\n    name: \"input_tensor:0\"\r\n    data_type: TYPE_FP32\r\n    format: FORMAT_NONE\r\n    dims: [ 2048,2432,3]\r\n    is_shape_tensor: false\r\n    allow_ragged_batch: false\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"num_detections\"\r\n    data_type: TYPE_INT32\r\n    dims: [ 1 ]\r\n    reshape { shape: [] }\r\n    label_filename: \"\"\r\n    is_shape_tensor: false\r\n\r\n  },\r\n  {\r\n    name: \"detection_boxes\"\r\n    data_type: TYPE_FP32\r\n    dims: [   100,4 ]\r\n    label_filename: \"\"\r\n    is_shape_tensor: false\r\n\r\n  },\r\n  {\r\n    name: \"detection_scores\"\r\n    data_type: TYPE_FP32\r\n    dims: [   100 ]\r\n    label_filename: \"\"\r\n    is_shape_tensor: false\r\n\r\n  },\r\n  {\r\n    name: \"detection_classes\"\r\n    data_type: TYPE_FP32\r\n    dims: [   100 ]\r\n    label_filename: \"\"\r\n    is_shape_tensor: false\r\n\r\n  }\r\n]\r\nbatch_input: []\r\nbatch_output: []\r\noptimization: {\r\n  priority: PRIORITY_DEFAULT,\r\n  input_pinned_memory: {\r\n    enable: true\r\n  },\r\n  output_pinned_memory: {\r\n    enable: true\r\n  },\r\n  gather_kernel_buffer_threshold: 0,\r\n  eager_batching: false\r\n}\r\ninstance_group: [\r\n  {\r\n    name: \"test\",\r\n    kind: KIND_GPU,\r\n    count: 1,\r\n    gpus: [\r\n      0\r\n    ],\r\n    secondary_devices: [],\r\n    profile: [],\r\n    passive: false,\r\n    host_policy: \"\"\r\n  }\r\n]\r\ndefault_model_filename: \"model.plan\"\r\ncc_model_filenames: {}\r\nmetric_tags: {}\r\nparameters: {}\r\nmodel_warmup: []\r\n\r\n```\r\nHowever, when I use this config.pbtxt, I get an error when trying to load the triton server: \r\n` test                 | 1       | UNAVAILABLE: Internal: unable to autofill for 'test', model tensor configurations are contradicting each other in terms of whether batching is supported |\r\n`\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3593/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3563", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3563/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3563/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3563/events", "html_url": "https://github.com/triton-inference-server/server/issues/3563", "id": 1051904553, "node_id": "I_kwDOCQnI4s4-ssop", "number": 3563, "title": "triton server is down after inferencing with one request", "user": {"login": "mshmnv", "id": 55867421, "node_id": "MDQ6VXNlcjU1ODY3NDIx", "avatar_url": "https://avatars.githubusercontent.com/u/55867421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mshmnv", "html_url": "https://github.com/mshmnv", "followers_url": "https://api.github.com/users/mshmnv/followers", "following_url": "https://api.github.com/users/mshmnv/following{/other_user}", "gists_url": "https://api.github.com/users/mshmnv/gists{/gist_id}", "starred_url": "https://api.github.com/users/mshmnv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mshmnv/subscriptions", "organizations_url": "https://api.github.com/users/mshmnv/orgs", "repos_url": "https://api.github.com/users/mshmnv/repos", "events_url": "https://api.github.com/users/mshmnv/events{/privacy}", "received_events_url": "https://api.github.com/users/mshmnv/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "szalpal", "id": 7101064, "node_id": "MDQ6VXNlcjcxMDEwNjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/7101064?v=4", "gravatar_id": "", "url": "https://api.github.com/users/szalpal", "html_url": "https://github.com/szalpal", "followers_url": "https://api.github.com/users/szalpal/followers", "following_url": "https://api.github.com/users/szalpal/following{/other_user}", "gists_url": "https://api.github.com/users/szalpal/gists{/gist_id}", "starred_url": "https://api.github.com/users/szalpal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/szalpal/subscriptions", "organizations_url": "https://api.github.com/users/szalpal/orgs", "repos_url": "https://api.github.com/users/szalpal/repos", "events_url": "https://api.github.com/users/szalpal/events{/privacy}", "received_events_url": "https://api.github.com/users/szalpal/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "szalpal", "id": 7101064, "node_id": "MDQ6VXNlcjcxMDEwNjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/7101064?v=4", "gravatar_id": "", "url": "https://api.github.com/users/szalpal", "html_url": "https://github.com/szalpal", "followers_url": "https://api.github.com/users/szalpal/followers", "following_url": "https://api.github.com/users/szalpal/following{/other_user}", "gists_url": "https://api.github.com/users/szalpal/gists{/gist_id}", "starred_url": "https://api.github.com/users/szalpal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/szalpal/subscriptions", "organizations_url": "https://api.github.com/users/szalpal/orgs", "repos_url": "https://api.github.com/users/szalpal/repos", "events_url": "https://api.github.com/users/szalpal/events{/privacy}", "received_events_url": "https://api.github.com/users/szalpal/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2021-11-12T12:10:19Z", "updated_at": "2022-03-25T16:47:22Z", "closed_at": "2022-03-25T16:47:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\ntriton server is down after inferencing with wrong request\r\nexample of this request on golang:\r\n```inferReq := &triton.ModelInferRequest{\r\n   ModelName:    modelName,\r\n   ModelVersion: modelVersion,\r\n   Inputs: []*triton.ModelInferRequest_InferInputTensor{\r\n      {\r\n         Name:     \"INPUT\",\r\n         Datatype: \"UINT8\",\r\n         Shape:    []int64{-1, nBytes},\r\n      },\r\n   },\r\n   Outputs: []*triton.ModelInferRequest_InferRequestedOutputTensor{\r\n      {\r\n         Name: \"OUTPUT\",\r\n      },\r\n   },\r\n   RawInputContents: image,\r\n}\r\n```\r\n\r\nIt looks like it's about -1 in Shape. And I know that it's wrong request but it would be good to have just an error without killing server\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nTRITON_VERSION=2.14.0\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTRITON_CONTAINER_VERSION=21.09\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n1. Start triton server\r\n2. Infer model from client with Shape with -1 value\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nIt would be good to have just an error without killing server after wrong request\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3563/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3563/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3528", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3528/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3528/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3528/events", "html_url": "https://github.com/triton-inference-server/server/issues/3528", "id": 1043081108, "node_id": "I_kwDOCQnI4s4-LCeU", "number": 3528, "title": "Tensorflow Object Detection Prediction", "user": {"login": "ManivannanMurugavel", "id": 20641562, "node_id": "MDQ6VXNlcjIwNjQxNTYy", "avatar_url": "https://avatars.githubusercontent.com/u/20641562?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ManivannanMurugavel", "html_url": "https://github.com/ManivannanMurugavel", "followers_url": "https://api.github.com/users/ManivannanMurugavel/followers", "following_url": "https://api.github.com/users/ManivannanMurugavel/following{/other_user}", "gists_url": "https://api.github.com/users/ManivannanMurugavel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ManivannanMurugavel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ManivannanMurugavel/subscriptions", "organizations_url": "https://api.github.com/users/ManivannanMurugavel/orgs", "repos_url": "https://api.github.com/users/ManivannanMurugavel/repos", "events_url": "https://api.github.com/users/ManivannanMurugavel/events{/privacy}", "received_events_url": "https://api.github.com/users/ManivannanMurugavel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1079803578, "node_id": "MDU6TGFiZWwxMDc5ODAzNTc4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2021-11-03T04:42:14Z", "updated_at": "2021-11-30T22:39:20Z", "closed_at": "2021-11-30T22:39:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Triton Information**\r\n=============================\r\n== Triton Inference Server ==\r\n=============================\r\n\r\nNVIDIA Release 21.09 (build 27443074)\r\n\r\n\r\nI deployed my object detection model into triton server. I am using faster-rcnn-inceptionv2 architecture to build my model in tensorflow.\r\n\r\nQuestion:\r\nCan anyone please tell me how can I predict my object detection model using triton client or any other source.\r\n\r\nI am refering this issue [#217](https://github.com/triton-inference-server/server/issues/217) and I used tritonserver python library but I am getting error mentioned below.\r\n\r\n```\r\nimport tensorrtserver.api as triton\r\nurl = 'localhost:8000'\r\nprotocol = triton.ProtocolType.from_str('HTTP')\r\ntriton.InferContext(url, protocol, 'faster_rcnn_inception_v2')\r\n```\r\n\r\nERROR:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/oph/env/tf_yolo/lib/python3.6/site-packages/tensorrtserver/api/__init__.py\", line 1128, in __init__\r\n    streaming, verbose)))\r\n  File \"/home/ubuntu/oph/env/tf_yolo/lib/python3.6/site-packages/tensorrtserver/api/__init__.py\", line 259, in _raise_if_error\r\n    raise ex\r\ntensorrtserver.api.InferenceServerException: [ 0] status request did not return status\r\n\r\n\r\nPlease tell me the solution....\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3528/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3525", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3525/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3525/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3525/events", "html_url": "https://github.com/triton-inference-server/server/issues/3525", "id": 1042594411, "node_id": "I_kwDOCQnI4s4-JLpr", "number": 3525, "title": "Prometheus not working with Triton", "user": {"login": "vsatpathy", "id": 61777986, "node_id": "MDQ6VXNlcjYxNzc3OTg2", "avatar_url": "https://avatars.githubusercontent.com/u/61777986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vsatpathy", "html_url": "https://github.com/vsatpathy", "followers_url": "https://api.github.com/users/vsatpathy/followers", "following_url": "https://api.github.com/users/vsatpathy/following{/other_user}", "gists_url": "https://api.github.com/users/vsatpathy/gists{/gist_id}", "starred_url": "https://api.github.com/users/vsatpathy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vsatpathy/subscriptions", "organizations_url": "https://api.github.com/users/vsatpathy/orgs", "repos_url": "https://api.github.com/users/vsatpathy/repos", "events_url": "https://api.github.com/users/vsatpathy/events{/privacy}", "received_events_url": "https://api.github.com/users/vsatpathy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-11-02T16:47:22Z", "updated_at": "2022-05-23T16:38:43Z", "closed_at": "2021-12-06T23:38:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am facing the same issue, but I guess the issue for all of us is with the integration with prometheus.\r\nAs when I open the Prometheus UI it always keeps saying that http://localhost:8002/metrics is down\r\n\r\n<img width=\"1676\" alt=\"Screenshot 2021-11-02 at 10 14 32 PM\" src=\"https://user-images.githubusercontent.com/61777986/139908717-90dc4b25-5320-40dc-a1b1-079af92372c1.png\">\r\n\r\nAlthough I do get response from the triton metrics end point as expected - \r\n<img width=\"908\" alt=\"Screenshot 2021-11-02 at 10 16 12 PM\" src=\"https://user-images.githubusercontent.com/61777986/139908972-e7d70dc4-3c37-4c5a-8a61-f27cba23c61c.png\">\r\n\r\nAnd hence when I try to integrate it with Grafana, the system keeps throwing an error.\r\nHas anyone been able to resolve this issue?\r\n\r\n_Originally posted by @vsatpathy in https://github.com/triton-inference-server/server/issues/2091#issuecomment-957935491_", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3525/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3525/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3524", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3524/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3524/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3524/events", "html_url": "https://github.com/triton-inference-server/server/issues/3524", "id": 1042076105, "node_id": "I_kwDOCQnI4s4-HNHJ", "number": 3524, "title": "struct.error: unpack_from requires a buffer of at least 274435 bytes", "user": {"login": "sourabh-burnwal", "id": 28016744, "node_id": "MDQ6VXNlcjI4MDE2NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/28016744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sourabh-burnwal", "html_url": "https://github.com/sourabh-burnwal", "followers_url": "https://api.github.com/users/sourabh-burnwal/followers", "following_url": "https://api.github.com/users/sourabh-burnwal/following{/other_user}", "gists_url": "https://api.github.com/users/sourabh-burnwal/gists{/gist_id}", "starred_url": "https://api.github.com/users/sourabh-burnwal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sourabh-burnwal/subscriptions", "organizations_url": "https://api.github.com/users/sourabh-burnwal/orgs", "repos_url": "https://api.github.com/users/sourabh-burnwal/repos", "events_url": "https://api.github.com/users/sourabh-burnwal/events{/privacy}", "received_events_url": "https://api.github.com/users/sourabh-burnwal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2021-11-02T09:34:23Z", "updated_at": "2021-11-29T19:53:44Z", "closed_at": "2021-11-29T19:53:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm getting this error while decoding a TYPE_STRING output response from the Triton Server.\r\nConfig.pbtxt looks like this:\r\n```\r\nname: \"vehicleRec_postprocess\"\r\nbackend: \"python\"\r\nmax_batch_size: 0\r\ninput [\r\n    {\r\n        name: \"model_output\"\r\n        data_type: TYPE_FP32\r\n        dims: [-1, -1]\r\n    },\r\n    {\r\n        name: \"resized_image_shape\"\r\n        data_type: TYPE_INT64\r\n        dims: [2]\r\n    },\r\n    {\r\n        name: \"original_image_shape\"\r\n        data_type: TYPE_INT64\r\n        dims: [2]\r\n    }\r\n]\r\noutput [\r\n    {\r\n        name: \"final_output\"\r\n        data_type: TYPE_STRING\r\n        dims: [-1,-1,-1]\r\n    }\r\n]\r\n```\r\n\r\nIt's actually is part of an ensemble, but this is the last model of that ensemble.\r\nThe sample output I'm getting is:\r\n```\r\n[array([[[b'925', b'369'],\r\n        [b'1013', b'395'],\r\n        [b'bus', b'bus']],\r\n\r\n       [[b'515', b'542'],\r\n        [b'571', b'592'],\r\n        [b'rickshaw', b'rickshaw']],\r\n\r\n       [[b'458', b'357'],\r\n        [b'508', b'397'],\r\n        [b'bus', b'bus']],\r\n\r\n       [[b'868', b'388'],\r\n        [b'900', b'421'],\r\n        [b'car', b'car']],\r\n\r\n       [[b'1021', b'412'],\r\n        [b'1113', b'461'],\r\n        [b'bus', b'bus']],\r\n\r\n       [[b'593', b'364'],\r\n        [b'639', b'416'],\r\n        [b'van', b'van']],\r\n\r\n       [[b'913', b'324'],\r\n        [b'982', b'364'],\r\n        [b'bus', b'bus']],\r\n\r\n       [[b'794', b'411'],\r\n        [b'833', b'445'],\r\n        [b'car', b'car']],\r\n\r\n       [[b'282', b'460'],\r\n        [b'552', b'641'],\r\n        [b'bus', b'bus']],\r\n\r\n       [[b'934', b'403'],\r\n        [b'1104', b'470'],\r\n        [b'bus', b'bus']],\r\n\r\n       [[b'813', b'589'],\r\n        [b'851', b'663'],\r\n        [b'motorbike', b'motorbike']],\r\n\r\n       [[b'1147', b'399'],\r\n        [b'1280', b'541'],\r\n        [b'bus', b'bus']],\r\n\r\n       [[b'717', b'611'],\r\n        [b'765', b'687'],\r\n        [b'motorbike', b'motorbike']],\r\n\r\n       [[b'1219', b'559'],\r\n        [b'1280', b'658'],\r\n        [b'rickshaw', b'rickshaw']],\r\n\r\n       [[b'591', b'363'],\r\n        [b'640', b'416'],\r\n        [b'bus', b'bus']],\r\n\r\n       [[b'505', b'572'],\r\n        [b'571', b'664'],\r\n        [b'rickshaw', b'rickshaw']],\r\n\r\n       [[b'701', b'459'],\r\n        [b'805', b'611'],\r\n        [b'bus', b'bus']],\r\n\r\n       [[b'592', b'440'],\r\n        [b'700', b'587'],\r\n        [b'bus', b'bus']]], dtype=object)]\r\n\r\n```\r\n\r\nThis output contains bounding boxes and its class label in object type format. I receive around 50-60 outputs perfectly fine, then this error gets thrown\r\n```\r\nFile \"/home/ericedge/anaconda3/envs/skylark/lib/python3.6/site-packages/tritonclient/http/__init__.py\", line 1926, in as_numpy\r\n    self._buffer[start_index:end_index])\r\n  File \"/home/ericedge/anaconda3/envs/skylark/lib/python3.6/site-packages/tritonclient/utils/__init__.py\", line 268, in deserialize_bytes_tensor\r\n    sb = struct.unpack_from(\"<{}s\".format(l), val_buf, offset)[0]\r\nstruct.error: unpack_from requires a buffer of at least 274435 bytes\r\n```\r\n\r\nThis is the code snippet from where I'm getting the output:\r\n```\r\ninferenceOutputs = [response.as_numpy(output_name) for output_name in output_names]\r\nfor i in range(len(inferenceOutputs)):\r\n                    for output in outputs[i]:\r\n                        if output[0][0].decode() != '-1':\r\n                            x1 = int(output[0][0])\r\n                            y1 = int(output[0][1])\r\n                            x2 = int(output[1][0])\r\n                            y2 = int(output[1][1])\r\n                            draw_boxes.add(img, x1, y1, x2, y2, output[2][0].decode())\r\n```\r\nwhere draw_boxes is a function to draw the bounding boxes alongwith its class label. How to solve this error, and why is it coming?", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3524/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3519", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3519/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3519/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3519/events", "html_url": "https://github.com/triton-inference-server/server/issues/3519", "id": 1041006955, "node_id": "I_kwDOCQnI4s4-DIFr", "number": 3519, "title": "Load buit-in OpenVINO failed", "user": {"login": "xiehaoina", "id": 13100010, "node_id": "MDQ6VXNlcjEzMTAwMDEw", "avatar_url": "https://avatars.githubusercontent.com/u/13100010?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiehaoina", "html_url": "https://github.com/xiehaoina", "followers_url": "https://api.github.com/users/xiehaoina/followers", "following_url": "https://api.github.com/users/xiehaoina/following{/other_user}", "gists_url": "https://api.github.com/users/xiehaoina/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiehaoina/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiehaoina/subscriptions", "organizations_url": "https://api.github.com/users/xiehaoina/orgs", "repos_url": "https://api.github.com/users/xiehaoina/repos", "events_url": "https://api.github.com/users/xiehaoina/events{/privacy}", "received_events_url": "https://api.github.com/users/xiehaoina/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-11-01T10:15:37Z", "updated_at": "2021-11-22T23:38:31Z", "closed_at": "2021-11-22T23:38:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\ntrying to run a built-in openVINO model by tritonserver but failed with the error msg:\r\nInternal: openvino error in reading network : stoi  \r\n\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nr21.10\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nyes nvcr.io/nvidia/tritonserver:21.10-py3\r\n\r\n**To Reproduce**\r\n1. model preparation by following \r\nhttps://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/face-detection-retail-0044/README.md\r\n\r\n2. construct local model repo\r\ndev@edge-vpu-002:~$ tree trtis_model_repo/                                                                                                                                                          \r\ntrtis_model_repo/                                                                                                                                                                                         \r\n\u251c\u2500\u2500 face-detection-retail-0004                                                                                                                                                                            \r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1                                                                                                                                                                                                 \r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 model.bin                                                                                                                                                                                     \r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 model.xml                                                                                                                                                                                     \r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 config.pbtxt \r\n\r\n3. launch tritonserver\r\n```\r\nsudo docker run  -p 8000:8000 -p 8001:8001 -p8002:8002 -v `pwd`/trtis_model_repo:/trtis_model_repo nvcr.io/nvidia/tritonserver:21.10-py3 /opt/tritonserver/bin/tritonserver --model-repository /trtis_model_repo\r\n```\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n```\r\nname: \"face-detection-retail-0004\"\r\nbackend: \"openvino\"\r\nmax_batch_size : 1\r\nversion_policy: { specific { versions: [1, 1] }}\r\ninput [\r\n  {\r\n    name: \"data\"\r\n    data_type: TYPE_FP32\r\n    dims: [4, 3, 400, 600 ]\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"detection_out\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1, 1, 800, 7 ]\r\n  }\r\n]\r\n\r\ninstance_group {\r\n  kind: KIND_CPU\r\n}\r\n\r\nparameters: {\r\nkey: \"SKIP_OV_DYNAMIC_BATCHSIZE\"\r\nvalue: {\r\nstring_value:\"YES\"\r\n}\r\n}\r\n```\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nexpected: load model successfully \r\nactual:  two model added by myself failed . model generated by triton qa provided successfully\r\n+------------------------------------------+---------+-----------------------------------------------------------------+                                                                                  \r\n| Model                                    | Version | Status                                                          |                                                                                  \r\n+------------------------------------------+---------+-----------------------------------------------------------------+                                                                                  \r\n| face-detection-retail-0004               | 1       | UNAVAILABLE: Internal: openvino error in reading network : stoi |                                                                                  \r\n| openvino_float32_float32_float32         | 1       | READY                                                           |                                                                                  \r\n| openvino_float32_float32_float32         | 3       | READY                                                           |                                                                                  \r\n| openvino_int32_int32_int32               | 1       | READY                                                           |                                                                                  \r\n| openvino_int32_int32_int32               | 2       | READY                                                           |                                                                                  \r\n| openvino_int32_int32_int32               | 3       | READY                                                           |                                                                                  \r\n| openvino_int8_int8_int8                  | 3       | READY                                                           |                                                                                  \r\n| openvino_nobatch_float32_float32_float32 | 1       | READY                                                           |                                                                                  \r\n| openvino_nobatch_float32_float32_float32 | 3       | READY                                                           |                                                                                  \r\n| openvino_nobatch_float32_int32_int32     | 1       | READY                                                           |                                                                                  \r\n| openvino_nobatch_int32_float32_float32   | 1       | READY                                                           |                                                                                  \r\n| openvino_nobatch_int32_int32_int32       | 1       | READY                                                           |                                                                                  \r\n| openvino_nobatch_int32_int32_int32       | 2       | READY                                                           |                                                                                  \r\n| openvino_nobatch_int32_int32_int32       | 3       | READY                                                           |                                                                                  \r\n| openvino_nobatch_int32_int8_int8         | 1       | READY                                                           |                                                                                  \r\n| openvino_nobatch_int8_int32_int32        | 1       | READY                                                           |                                                                                  \r\n| openvino_nobatch_int8_int8_int8          | 3       | READY                                                           |                                                                                  \r\n| ssd_mobilenet_v2_coco                    | 1       | UNAVAILABLE: Internal: openvino error in reading network : stoi | ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3519/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3498", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3498/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3498/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3498/events", "html_url": "https://github.com/triton-inference-server/server/issues/3498", "id": 1036220952, "node_id": "I_kwDOCQnI4s49w3oY", "number": 3498, "title": "Unable to build CPU only image", "user": {"login": "neozhangthe1", "id": 1272486, "node_id": "MDQ6VXNlcjEyNzI0ODY=", "avatar_url": "https://avatars.githubusercontent.com/u/1272486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neozhangthe1", "html_url": "https://github.com/neozhangthe1", "followers_url": "https://api.github.com/users/neozhangthe1/followers", "following_url": "https://api.github.com/users/neozhangthe1/following{/other_user}", "gists_url": "https://api.github.com/users/neozhangthe1/gists{/gist_id}", "starred_url": "https://api.github.com/users/neozhangthe1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neozhangthe1/subscriptions", "organizations_url": "https://api.github.com/users/neozhangthe1/orgs", "repos_url": "https://api.github.com/users/neozhangthe1/repos", "events_url": "https://api.github.com/users/neozhangthe1/events{/privacy}", "received_events_url": "https://api.github.com/users/neozhangthe1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-10-26T12:11:14Z", "updated_at": "2021-11-30T22:28:58Z", "closed_at": "2021-11-30T22:28:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nBy running the following command\r\npython3 compose.py --backend python --backend onnx --backend torch --repoagent checksum\r\n\r\nWe get:\r\n\r\nversion 2.16.0dev\r\nusing container version 21.09\r\npulling container:nvcr.io/nvidia/tritonserver:21.09-cpu-only-py3\r\nError response from daemon: manifest for nvcr.io/nvidia/tritonserver:21.09-cpu-only-py3 not found: manifest unknown: manifest unknown\r\nerror: docker pull container nvcr.io/nvidia/tritonserver:21.09-cpu-only-py3 failed, None\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n21.09\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nUsing the Triton container\r\n\r\n**To Reproduce**\r\npython3 compose.py --backend python --backend onnx --backend torch --repoagent checksum\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3498/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3495", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3495/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3495/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3495/events", "html_url": "https://github.com/triton-inference-server/server/issues/3495", "id": 1035573186, "node_id": "I_kwDOCQnI4s49uZfC", "number": 3495, "title": "Creating custom python backend environment ", "user": {"login": "arshiamalek", "id": 80602936, "node_id": "MDQ6VXNlcjgwNjAyOTM2", "avatar_url": "https://avatars.githubusercontent.com/u/80602936?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arshiamalek", "html_url": "https://github.com/arshiamalek", "followers_url": "https://api.github.com/users/arshiamalek/followers", "following_url": "https://api.github.com/users/arshiamalek/following{/other_user}", "gists_url": "https://api.github.com/users/arshiamalek/gists{/gist_id}", "starred_url": "https://api.github.com/users/arshiamalek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arshiamalek/subscriptions", "organizations_url": "https://api.github.com/users/arshiamalek/orgs", "repos_url": "https://api.github.com/users/arshiamalek/repos", "events_url": "https://api.github.com/users/arshiamalek/events{/privacy}", "received_events_url": "https://api.github.com/users/arshiamalek/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-10-25T21:05:57Z", "updated_at": "2021-12-17T14:42:31Z", "closed_at": "2021-12-17T14:42:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "After creating a the triton custom environment using the steps listed below we get:\r\n\r\n```\r\ntriton_python_backend_stub: ../nptl/pthread_mutex_lock.c:81: __pthread_mutex_lock: Assertion `mutex->__data.__owner == 0' failed.\r\n```\r\n\r\n**Steps we took to create custom triton environment**\r\n\r\n1. First we installed `conda` and created and environment called python-3-8\r\n2. We then installed numpy as required in the documentation along with our custom packages listed below:\r\n```\r\napt-get update\r\napt-get -y install libgl1\r\napt-get install -yq libgtk2.0-dev\r\npip3 install torch==1.9.1+cpu torchvision==0.10.1+cpu torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\r\npip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.9/index.html\r\npip install opencv-python==4.5.2.52\r\n```\r\n\r\nWe also set the  PYTHONNOUSERSITE environment variables as requested: \r\n`export PYTHONNOUSERSITE=True`\r\n\r\n3. We installed CMAKE and cuda-toolkit-11\r\n\r\n*install cmake and cuda*\r\n```\r\napt-get update\r\napt-get install -y apt-transport-https ca-certificates gnupg software-properties-common wget\r\nwget -O -https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | apt-key add -\r\n\r\napt-add-repository 'debhttps://apt.kitware.com/ubuntu/ bionic main'\r\napt-get update\r\napt-get install -y cmake\r\n##install rapidjson and libarchive\r\napt-get install -y rapidjson-dev libarchive-dev\r\n\r\napt-key adv --fetch-keys  http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\r\n\r\nbash -c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/cuda.list'\r\n\r\napt update\r\napt install cuda-toolkit-11-0\r\n```\r\n\r\n4. We set the CUDA directory on our environment to the correct path:\r\n`cmake -D CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-11.0 ..`\r\n\r\n5. we make the directory and create the custom backend stub\r\n```\r\ngit clone https://github.com/triton-inference-server/python_backend -b r21.09\r\ncd r21.09\r\nmkdir build && cd build\r\ncmake -DTRITON_ENABLE_GPU=ON -DCMAKE_INSTALL_PREFIX:PATH=pwd/install ..\r\nmake triton-python-backend-stub\r\n```\r\n\r\n6. We then uploaded the triton_backend_stub file to the model directory on our GCS bucket.\r\n7. after this we used conda-pack to pack out environment and then uploaded it to our detectron2 model directory on our GCS bucket\r\n\r\n\r\n**Triton Information**\r\nTriton Version 21.09\r\nModel: Detectron2\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3495/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3495/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3485", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3485/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3485/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3485/events", "html_url": "https://github.com/triton-inference-server/server/issues/3485", "id": 1030385908, "node_id": "I_kwDOCQnI4s49anD0", "number": 3485, "title": "Version policy `specific` doesn\u2019t work as expected. It doesn\u2019t respect the model_version parameter during inference.", "user": {"login": "dugarsumit", "id": 12299152, "node_id": "MDQ6VXNlcjEyMjk5MTUy", "avatar_url": "https://avatars.githubusercontent.com/u/12299152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dugarsumit", "html_url": "https://github.com/dugarsumit", "followers_url": "https://api.github.com/users/dugarsumit/followers", "following_url": "https://api.github.com/users/dugarsumit/following{/other_user}", "gists_url": "https://api.github.com/users/dugarsumit/gists{/gist_id}", "starred_url": "https://api.github.com/users/dugarsumit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dugarsumit/subscriptions", "organizations_url": "https://api.github.com/users/dugarsumit/orgs", "repos_url": "https://api.github.com/users/dugarsumit/repos", "events_url": "https://api.github.com/users/dugarsumit/events{/privacy}", "received_events_url": "https://api.github.com/users/dugarsumit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-10-19T14:31:42Z", "updated_at": "2022-01-21T18:13:54Z", "closed_at": "2022-01-21T18:13:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nVersion policy specific doesn\u2019t work as expected. It doesn\u2019t respect the model_version parameter during inference.\r\n\r\n**Triton Information**\r\nTRITON_VERSION=2.14.0\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTRITON_CONTAINER_VERSION=21.09\r\n\r\n**To Reproduce**\r\n1. Use `specific` version policy with python backend models having two versions. In each model file you can add some print lines to indicate which model is being used during the inference.\r\n2. Start the triton server\r\n3. Infer the model using the client by passing the model_version.\r\n4. Repeat step 3 multiple times. You will see randomly being served by one of the model versions irrespective of what model_version was passed in the client.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nRequest should be served by the same model version that was passed in the client.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3485/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3485/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3475", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3475/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3475/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3475/events", "html_url": "https://github.com/triton-inference-server/server/issues/3475", "id": 1027663290, "node_id": "I_kwDOCQnI4s49QOW6", "number": 3475, "title": "Intermittent issue with loading a python backend models (21.08)", "user": {"login": "FamousDirector", "id": 24622589, "node_id": "MDQ6VXNlcjI0NjIyNTg5", "avatar_url": "https://avatars.githubusercontent.com/u/24622589?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FamousDirector", "html_url": "https://github.com/FamousDirector", "followers_url": "https://api.github.com/users/FamousDirector/followers", "following_url": "https://api.github.com/users/FamousDirector/following{/other_user}", "gists_url": "https://api.github.com/users/FamousDirector/gists{/gist_id}", "starred_url": "https://api.github.com/users/FamousDirector/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FamousDirector/subscriptions", "organizations_url": "https://api.github.com/users/FamousDirector/orgs", "repos_url": "https://api.github.com/users/FamousDirector/repos", "events_url": "https://api.github.com/users/FamousDirector/events{/privacy}", "received_events_url": "https://api.github.com/users/FamousDirector/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-10-15T17:46:47Z", "updated_at": "2021-11-30T15:27:40Z", "closed_at": "2021-11-29T19:52:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen starting up the container, the model(s) will attempt to be loaded, but occasionally get the following error:\r\n```\r\nE1015 17:29:25.813655 213 model_repository_manager.cc:1215] failed to load '<model_name>' version 1: Internal: Timed out occurred while waiting for the stub process. Failed to initialize model instance <model_name>_0\r\nE1015 17:29:25.813944 213 model_repository_manager.cc:1404] Invalid argument: ensemble '<ensemble_model_name>' depends on '<model_name>' which has no loaded version\r\nterminate called after throwing an instance of 'boost::interprocess::interprocess_exception'\r\n  what():  No such file or directory\r\n```\r\nafter restarting the container, the issue often does not reappear. But after restarting the computer/server, it will come back again.\r\n\r\n**Triton Information**\r\nr21.08 full container\r\n\r\n**To Reproduce**\r\nIt's a fairly complex environment, it would be impossible to capture everything but here are the highlights.\r\n- run command:\r\n```\r\nLD_PRELOAD=\"<engine_plugins>.so\" /opt/tritonserver/bin/tritonserver --model-repository=\"/model_repository/\" --model-control-mode explicit --pinned-memory-pool-byte-size 4000000000 --exit-on-error 0 --exit-timeout-secs 0\r\n```\r\n- docker compose:\r\n```\r\n  inference-server:\r\n    image: <nvcr.io/nvidia/tritonserver:21.08-py3>\r\n    networks:\r\n      - custom_network\r\n    ulimits:\r\n      memlock: -1\r\n      stack: 4000000000\r\n    volumes:\r\n      - updatevol:/usr/local/eheye/\r\n    shm_size: '4gb'\r\n    ipc: shareable\r\n```\r\nOther notes:\r\nThere are a variety of other backends being used, tensorrt and ONNX. All other models load fine. \r\nThe hosts OS is CentOS7.\r\nCPU/GPU/RAM: Intel Xeon SIlver 4114, Tesla T4, 64GB\r\n\r\n**Expected behavior**\r\nThese models should load repeatably. When I load these models on my personal laptop they always load as expected.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3475/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3474", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3474/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3474/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3474/events", "html_url": "https://github.com/triton-inference-server/server/issues/3474", "id": 1027259322, "node_id": "I_kwDOCQnI4s49Oru6", "number": 3474, "title": "Device Auto Reallocaton not working as expected", "user": {"login": "BorisPolonsky", "id": 12964401, "node_id": "MDQ6VXNlcjEyOTY0NDAx", "avatar_url": "https://avatars.githubusercontent.com/u/12964401?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BorisPolonsky", "html_url": "https://github.com/BorisPolonsky", "followers_url": "https://api.github.com/users/BorisPolonsky/followers", "following_url": "https://api.github.com/users/BorisPolonsky/following{/other_user}", "gists_url": "https://api.github.com/users/BorisPolonsky/gists{/gist_id}", "starred_url": "https://api.github.com/users/BorisPolonsky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BorisPolonsky/subscriptions", "organizations_url": "https://api.github.com/users/BorisPolonsky/orgs", "repos_url": "https://api.github.com/users/BorisPolonsky/repos", "events_url": "https://api.github.com/users/BorisPolonsky/events{/privacy}", "received_events_url": "https://api.github.com/users/BorisPolonsky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2021-10-15T09:29:54Z", "updated_at": "2021-11-03T07:11:23Z", "closed_at": "2021-11-03T06:43:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI have lots of classification model in tensorflow SavedModel format that runs perfectly on GPU version of tensorflow/serving. The label look up defined in graph is CPU-only. And the triton server failed to re allocate these op from GPU to CPU with `--strict-model-config=true --backend-config=tensorflow,allow-soft-placement=true` specified in this specific version \r\nHere's the logs from triton-model-server. Please refer to #3344 as well for test result from other users.\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n`nvcr.io/nvidia/tritonserver:21.08-py3`\r\nAre you using the Triton container or did you build it yourself?\r\nContainer\r\n**To Reproduce**\r\nserver options\r\n`--strict-model-config=true --backend-config=tensorflow,allow-soft-placement=true`\r\nconfig.pbtxt\r\n```\r\nplatform: \"tensorflow_savedmodel\"\r\nbackend: \"tensorflow\"\r\nmax_batch_size: 2\r\ninput: [\r\n  {\r\n  name: \"input_ids\"\r\n  data_type: TYPE_INT64\r\n  dims: [ 512 ]\r\n  allow_ragged_batch: false\r\n  },\r\n  {\r\n  name: \"input_mask\"\r\n  data_type: TYPE_INT64\r\n  dims: [ 512 ]\r\n  reshape: { shape: [ ] }\r\n  },\r\n  {\r\n  name: \"segment_ids\"\r\n  data_type: TYPE_INT64\r\n  dims: [ 512 ]\r\n  reshape: { shape: [ ] }\r\n  }\r\n]\r\noutput [\r\n  {\r\n  name: \"cls_embedding\"\r\n  data_type: TYPE_FP32\r\n  dims: [ 768 ]\r\n  }\r\n]\r\n\r\nbatch_input []\r\nbatch_output []\r\ninstance_group: [\r\n  {\r\n  kind: KIND_MODEL\r\n  }\r\n]\r\n```\r\n\r\nLogs\r\n```\r\n2021-09-11 10:11:47.932340: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU \r\nLookupTableImportV2: CPU \r\nHashTableV2: CPU \r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  index_to_string (HashTableV2) /gpu:0\r\n  index_to_string/table_init (LookupTableImportV2) /gpu:0\r\n  index_to_string_Lookup (LookupTableFindV2) /gpu:0\r\n\r\n2021-09-11 10:11:47.933076: I tensorflow/cc/saved_model/loader.cc:251] Restoring SavedModel bundle.\r\n2021-09-11 10:11:47.975969: I tensorflow/cc/saved_model/loader.cc:200] Running initialization op on SavedModel bundle at path: /home/model-repo/ner/1/model.savedmodel\r\n2021-09-11 10:11:47.993625: I tensorflow/cc/saved_model/loader.cc:379] SavedModel load for tags { serve }; Status: success. Took 4368833 microseconds.\r\n2021-09-11 10:11:47.993684: W triton/tensorflow_backend_tf.cc:986] unable to find serving signature 'serving_default\r\n2021-09-11 10:11:47.993693: W triton/tensorflow_backend_tf.cc:988] using signature 'predict'\r\nI0911 02:11:47.993920 1 model_repository_manager.cc:1212] successfully loaded 'ner' version 1\r\nI0911 02:11:47.994044 1 server.cc:504] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0911 02:11:47.994199 1 server.cc:543] \r\n+-------------+-----------------------------------------------------------------+---------------------------------------------+\r\n| Backend     | Path                                                            | Config                                      |\r\n+-------------+-----------------------------------------------------------------+---------------------------------------------+\r\n| tensorrt    | <built-in>                                                      | {}                                          |\r\n| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so         | {}                                          |\r\n| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so | {\"cmdline\":{\"allow-soft-placement\":\"true\"}} |\r\n| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {}                                          |\r\n| openvino    | /opt/tritonserver/backends/openvino/libtriton_openvino.so       | {}                                          |\r\n+-------------+-----------------------------------------------------------------+---------------------------------------------+\r\n\r\nI0911 02:11:47.994222 1 server.cc:586] \r\n+-------+---------+--------+\r\n| Model | Version | Status |\r\n+-------+---------+--------+\r\n| ner   | 1       | READY  |\r\n+-------+---------+--------+\r\n\r\nI0911 02:11:47.994306 1 tritonserver.cc:1718] \r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                                                                  |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                                                                 |\r\n| server_version                   | 2.13.0                                                                                                                                                                                 |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\r\n| model_repository_path[0]         | /home/model-repo                                                                                                                                                                       |\r\n| model_control_mode               | MODE_NONE                                                                                                                                                                              |\r\n| strict_model_config              | 1                                                                                                                                                                                      |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\r\n| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                               |\r\n| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\r\n| strict_readiness                 | 1                                                                                                                                                                                      |\r\n| exit_timeout                     | 30                                                                                                                                                                                     |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0911 02:11:47.994314 1 server.cc:234] Waiting for in-flight requests to complete.\r\nI0911 02:11:47.994318 1 model_repository_manager.cc:1078] unloading: ner:1\r\nI0911 02:11:47.994357 1 server.cc:249] Timeout 30: Found 1 live models and 0 in-flight non-inference requests\r\nI0911 02:11:47.994527 1 tensorflow.cc:2356] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0911 02:11:47.994569 1 tensorflow.cc:2295] TRITONBACKEND_ModelFinalize: delete model state\r\nI0911 02:11:48.000163 1 model_repository_manager.cc:1195] successfully unloaded 'ner' version 1\r\nW0911 02:11:48.939825 1 metrics.cc:395] Unable to get power limit for GPU 0: Success\r\nW0911 02:11:48.939849 1 metrics.cc:410] Unable to get power usage for GPU 0: Success\r\nI0911 02:11:48.994473 1 server.cc:249] Timeout 29: Found 0 live models and 0 in-flight non-inference requests\r\nerror: creating server: Internal - failed to load all models\r\n```\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nThe model with label look up ops defined should re-allocate CPU-only ops automatically, as test results from other users (#3344) suggest that this feature should works in the previous version\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3474/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3474/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3429", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3429/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3429/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3429/events", "html_url": "https://github.com/triton-inference-server/server/issues/3429", "id": 1015213676, "node_id": "I_kwDOCQnI4s48gu5s", "number": 3429, "title": "Docker fails to register cuda shared memory", "user": {"login": "PauloFavero", "id": 26371434, "node_id": "MDQ6VXNlcjI2MzcxNDM0", "avatar_url": "https://avatars.githubusercontent.com/u/26371434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PauloFavero", "html_url": "https://github.com/PauloFavero", "followers_url": "https://api.github.com/users/PauloFavero/followers", "following_url": "https://api.github.com/users/PauloFavero/following{/other_user}", "gists_url": "https://api.github.com/users/PauloFavero/gists{/gist_id}", "starred_url": "https://api.github.com/users/PauloFavero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PauloFavero/subscriptions", "organizations_url": "https://api.github.com/users/PauloFavero/orgs", "repos_url": "https://api.github.com/users/PauloFavero/repos", "events_url": "https://api.github.com/users/PauloFavero/events{/privacy}", "received_events_url": "https://api.github.com/users/PauloFavero/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 35, "created_at": "2021-10-04T13:54:13Z", "updated_at": "2022-08-23T18:32:15Z", "closed_at": "2022-07-19T13:37:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nThe Triton client is unable to register Cuda shared memory when running the script from the docker command. Although, it works when running the docker in interactive mode.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n\r\nServer: nvcr.io/nvidia/tritonserver:21.03-py3\r\nClient: nvcr.io/nvidia/tritonserver:21.03-py3-sdk\r\n\r\nAre you using the Triton container or did you build it yourself? I am using the Triton container\r\n\r\nDockerfile.client\r\n```\r\nFROM  nvcr.io/nvidia/tritonserver:21.03-py3-sdk\r\n\r\nRUN apt update && apt install -y libb64-dev ffmpeg\r\n```\r\n\r\ndocker-compose.yml\r\n\r\n```yml\r\nversion: '2.3'\r\n\r\nservices:\r\n  triton-server:\r\n    container_name: triton-server\r\n    image: nvcr.io/nvidia/tritonserver:21.03-py3\r\n    privileged: true\r\n    runtime: nvidia\r\n    shm_size: '2gb'\r\n    ports:\r\n      - \"8000:8000\"\r\n      - \"8001:8001\"\r\n      - \"8002:8002\"\r\n    ipc: host\r\n    ulimits:\r\n      stack: 67108864\r\n      memlock: -1\r\n    environment:\r\n      - LD_PRELOAD=/plugins/liblayerplugin.so\r\n      - log-verbose=4\r\n    command: bash -c \"tritonserver --model-repository=/models --strict-model-config=false --grpc-infer-allocation-pool-size=16\"\r\n\r\n  triton-client:\r\n    container_name: triton-client\r\n    build:\r\n      context: .\r\n    network_mode: 'host'\r\n    working_dir: /app/src\r\n    depends_on:\r\n      - triton-server\r\n    environment:\r\n      - log-verbose=4\r\n    privileged: true\r\n    runtime: nvidia\r\n    shm_size: '2gb'\r\n    command: bash -c \"python3 simple_grpc_cudashm_client.py --verbose\"\r\n```\r\n\r\n**To Reproduce**\r\n\r\nBuild the client container and then, run `docker-compose up`.  The triton-client container will execute the script  **simple_grpc_cudashm_client.py** but it will throw the following error:\r\n\r\n```bash\r\nunregister_system_shared_memory, metadata ()\r\ntriton-client    | \r\ntriton-client    | Unregistered all system shared memory regions\r\ntriton-client    | unregister_cuda_shared_memory, metadata ()\r\ntriton-client    | \r\ntriton-client    | Unregistered all cuda shared memory regions\r\ntriton-client    | register_cuda_shared_memory, metadata ()\r\ntriton-client    | name: \"output0_data\"\r\ntriton-client    | raw_handle: \"\\260iu\\001\\000\\000\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\0002\\000\\000\\000\\000\\000 \\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\000\\000\\000\\0001\\000\\000\\000\\000\\000\\000\\000\\242\\000\\320\\301\\216\\000\\000\\\\\\000\\000\\000\\000\"\r\ntriton-client    | byte_size: 3276800\r\ntriton-client    | \r\ntriton-client    | Traceback (most recent call last):\r\ntriton-client    |   File \"simple_grpc_cudashm_client.py\", line 61, in <module>\r\ntriton-client    |     triton_client.register_cuda_shared_memory(\r\ntriton-client    |   File \"/usr/local/lib/python3.8/dist-packages/tritonclient/grpc/__init__.py\", line 906, in register_cuda_shared_memory\r\ntriton-client    |     raise_error_grpc(rpc_error)\r\ntriton-client    |   File \"/usr/local/lib/python3.8/dist-packages/tritonclient/grpc/__init__.py\", line 61, in raise_error_grpc\r\ntriton-client    |     raise get_error_grpc(rpc_error) from None\r\ntriton-client    | tritonclient.utils.InferenceServerException: [StatusCode.INVALID_ARGUMENT] failed to register CUDA shared memory region 'output0_data'\r\n```\r\n\r\nThe curious thing happens when I run the script from inside the container. If you run the container with `docker-compose run triton-client bash` and the from the terminal inside the container you execute `python3  simple_grpc_cudashm_client.py --verbose, the client works as expected without errors. This is the output generated in this case:\r\n\r\n```bash\r\nunregister_system_shared_memory, metadata ()\r\n\r\nUnregistered all system shared memory regions\r\nunregister_cuda_shared_memory, metadata ()\r\n\r\nUnregistered all cuda shared memory regions\r\nregister_cuda_shared_memory, metadata ()\r\nname: \"output0_data\"\r\nraw_handle: \" 3\\225\\001\\000\\000\\000\\000\\t\\000\\000\\000\\000\\000\\000\\000\\000\\0002\\000\\000\\000\\000\\000 \\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\000\\000\\000\\0001\\000\\000\\000\\000\\000\\000\\000\\304\\000\\320\\301\\216\\000\\000\\\\\\000\\000\\000\\000\"\r\nbyte_size: 3276800\r\n\r\nRegistered cuda shared memory with name 'output0_data'\r\nregister_cuda_shared_memory, metadata ()\r\nname: \"input0_data\"\r\nraw_handle: \" 3\\225\\001\\000\\000\\000\\000\\t\\000\\000\\000\\000\\000\\000\\000\\000\\0002\\000\\000\\000\\000\\000 \\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\000\\000\\000\\0002\\000\\000\\000\\000\\000\\000\\000\\304\\000\\320\\301\\220\\000\\000\\\\\\000\\000\\000\\000\"\r\nbyte_size: 3276800\r\n\r\nRegistered Cuda shared memory with name 'input0_data'\r\n```\r\n\r\nIt's important to notice that running with `docker-compose run triton-client python3 simple_grpc_cudashm_client.py --verbose also generates the same error.\r\n\r\n## Attachments\r\n\r\n**Script simple_grpc_cudashm_client.py**\r\n\r\n```python\r\nimport os\r\nimport json\r\nimport sys\r\nimport argparse\r\n\r\nimport numpy as np\r\nimport tritonclient.grpc as grpcclient\r\nfrom tritonclient import utils\r\nimport tritonclient.utils.cuda_shared_memory as cudashm\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('-v',\r\n                        '--verbose',\r\n                        action=\"store_true\",\r\n                        required=False,\r\n                        default=False,\r\n                        help='Enable verbose output')\r\n\r\n     parser.add_argument('-u',\r\n                        '--url',\r\n                        type=str,\r\n                        required=False,\r\n                        default='localhost:8001',\r\n                        help='Inference server URL. Default is localhost:8001.')\r\n\r\n    FLAGS = parser.parse_args()\r\n\r\ntry:\r\n        triton_client = grpcclient.InferenceServerClient(url=FLAGS.url,\r\n                                                         verbose=FLAGS.verbose)\r\n    except Exception as e:\r\n        print(\"channel creation failed: \" + str(e))\r\n        sys.exit(1)\r\n\r\n\r\n    triton_client.unregister_system_shared_memory()\r\n    triton_client.unregister_cuda_shared_memory()\r\n\r\n    model_name = \"test\"\r\n    model_version = \"latest\"\r\n\r\n    input_byte_size = 3276800 # 1600x512x4bytes\r\n    output_byte_size = input_byte_size\r\n\r\n    shm_op0_handle = cudashm.create_shared_memory_region(\r\n        \"output0_data\", output_byte_size, 0)\r\n    \r\n    triton_client.register_cuda_shared_memory(\r\n        \"output0_data\", cudashm.get_raw_handle(shm_op0_handle), 0,\r\n        output_byte_size)\r\n    \r\n    shm_ip0_handle = cudashm.create_shared_memory_region(\r\n        \"input0_data\", input_byte_size, 0)\r\n\r\n    triton_client.register_cuda_shared_memory(\r\n        \"input0_data\", cudashm.get_raw_handle(shm_ip0_handle), 0,\r\n        input_byte_size)\r\n\r\n```\r\n\r\nDoes anyone have an idea why this happens when launching the triton-client with docker-compose up? \r\n\r\nThanks\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3429/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3418", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3418/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3418/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3418/events", "html_url": "https://github.com/triton-inference-server/server/issues/3418", "id": 1011014230, "node_id": "I_kwDOCQnI4s48QtpW", "number": 3418, "title": "Onnxruntime execute failure ", "user": {"login": "ValeryNikiforov", "id": 52481844, "node_id": "MDQ6VXNlcjUyNDgxODQ0", "avatar_url": "https://avatars.githubusercontent.com/u/52481844?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ValeryNikiforov", "html_url": "https://github.com/ValeryNikiforov", "followers_url": "https://api.github.com/users/ValeryNikiforov/followers", "following_url": "https://api.github.com/users/ValeryNikiforov/following{/other_user}", "gists_url": "https://api.github.com/users/ValeryNikiforov/gists{/gist_id}", "starred_url": "https://api.github.com/users/ValeryNikiforov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ValeryNikiforov/subscriptions", "organizations_url": "https://api.github.com/users/ValeryNikiforov/orgs", "repos_url": "https://api.github.com/users/ValeryNikiforov/repos", "events_url": "https://api.github.com/users/ValeryNikiforov/events{/privacy}", "received_events_url": "https://api.github.com/users/ValeryNikiforov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-09-29T14:11:07Z", "updated_at": "2022-06-07T16:59:38Z", "closed_at": "2022-03-10T23:37:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Previously, I used the 21.03 container for serving my onnx model (exported Nvidia Citrinet ASR model). Everything worked fine.\r\nRight now I need to use version 21.07+, but I get this creepy error:\r\n\r\n`onnxruntime execute failure 1: Non-zero status code returned while running FusedConv node. Name:'Conv_35_Add_36_Relu_37' Status Message: CUDNN error executing cudnnAddTensor(Base::CudnnHandle(), &alpha, Base::s_.z_tensor, Base::s_.z_data, &alpha, Base::s_.y_tensor, Base::s_.y_data)`\r\n\r\nJust in case, I reconverted Citrinet to onnx with the latest onnxruntime version (1.9.0, but also tried 1.8.1) and checked everything again, nothing changed.\r\n\r\nI checked some containers:\r\n21.03-py3 - everything works fine, I can get server responses with correct inference output data.\r\n21.06-py3, 21.07-py3, 21.08-py3  - getting error (pasted it above).\r\n\r\nCUDA: 11.4, Driver Version: 470.57.02\r\n\r\nWhat can you advise me to run inference in new versions of the container correctly?", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3418/reactions", "total_count": 2, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 2, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3418/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3386", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3386/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3386/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3386/events", "html_url": "https://github.com/triton-inference-server/server/issues/3386", "id": 1003210654, "node_id": "I_kwDOCQnI4s47y8ee", "number": 3386, "title": "Python backend on CPU is slower when serving a pytorch model", "user": {"login": "SaratM34", "id": 1553258, "node_id": "MDQ6VXNlcjE1NTMyNTg=", "avatar_url": "https://avatars.githubusercontent.com/u/1553258?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SaratM34", "html_url": "https://github.com/SaratM34", "followers_url": "https://api.github.com/users/SaratM34/followers", "following_url": "https://api.github.com/users/SaratM34/following{/other_user}", "gists_url": "https://api.github.com/users/SaratM34/gists{/gist_id}", "starred_url": "https://api.github.com/users/SaratM34/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SaratM34/subscriptions", "organizations_url": "https://api.github.com/users/SaratM34/orgs", "repos_url": "https://api.github.com/users/SaratM34/repos", "events_url": "https://api.github.com/users/SaratM34/events{/privacy}", "received_events_url": "https://api.github.com/users/SaratM34/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 29, "created_at": "2021-09-21T20:19:02Z", "updated_at": "2022-01-27T01:45:38Z", "closed_at": "2022-01-26T21:16:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI have a python model that uses pre-trained roberta model for the inference. I have added this model to Triton to use python backend to serve. We also have the exact same python code/model being served using an fastapi application. Both are running on hardware with same specs. When I compared both the models in terms of performance on CPU, the latency with Triton is very high.  I used pytorch profiler to profile the code to debug what is causing the higher latencies with Triton. Below screenshots shows the outputs of pytorch profiler.\r\n\r\n`Triton-CPU`\r\n\r\n![triton-cpu](https://user-images.githubusercontent.com/1553258/134230756-e0f7f093-096a-4f47-b55c-0f8c15bc71f1.png)\r\n\r\n`FastAPI-CPU`\r\n\r\n![api-cpu](https://user-images.githubusercontent.com/1553258/134231062-111138d0-3e45-43f2-8348-8a6673cb98e2.png)\r\n\r\nBased on the screenshots I can see that particularly the `native_layer_norm` is taking significantly longer with Triton when compared with model running using our fastapi application. `native_layer_norm` is part of the pre-trained roberta model.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nVersion: **21.07**\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nI built the image myself based on r21.07 but I have also tested serving the model using Official Triton Containers-r21.07 and r21.08 the issue still remains the same\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Dependencies:**\r\ntorch==1.6.0\r\ntransformers==3.5.1\r\n\r\nconfig.pbtxt\r\n```\r\nname: \"sample-model\"\r\nbackend: \"python\"\r\nmax_batch_size: 8\r\n\r\ninput [\r\n  {\r\n    name: \"INPUT0\"\r\n    data_type: TYPE_STRING\r\n    dims:  [1]\r\n\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"OUTPUT0\"\r\n    data_type: TYPE_STRING\r\n    dims: [1]\r\n  }\r\n]\r\n\r\nparameters: {\r\n  key: \"EXECUTION_ENV_PATH\",\r\n  value: {string_value: \"<path to execution env>\"}\r\n}\r\n\r\ninstance_group [\r\n  {\r\n    count: 1\r\n    kind: KIND_CPU\r\n  }\r\n]\r\n```\r\n\r\n**Expected behavior**\r\nIdeally the performance should be similar when the same model is being run with Triton \r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3386/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3386/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3320", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3320/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3320/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3320/events", "html_url": "https://github.com/triton-inference-server/server/issues/3320", "id": 987371476, "node_id": "MDU6SXNzdWU5ODczNzE0NzY=", "number": 3320, "title": "Memory leak issue on using load/unload API to dynamic loading TensorRT model", "user": {"login": "discipleofhamilton", "id": 8478501, "node_id": "MDQ6VXNlcjg0Nzg1MDE=", "avatar_url": "https://avatars.githubusercontent.com/u/8478501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/discipleofhamilton", "html_url": "https://github.com/discipleofhamilton", "followers_url": "https://api.github.com/users/discipleofhamilton/followers", "following_url": "https://api.github.com/users/discipleofhamilton/following{/other_user}", "gists_url": "https://api.github.com/users/discipleofhamilton/gists{/gist_id}", "starred_url": "https://api.github.com/users/discipleofhamilton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/discipleofhamilton/subscriptions", "organizations_url": "https://api.github.com/users/discipleofhamilton/orgs", "repos_url": "https://api.github.com/users/discipleofhamilton/repos", "events_url": "https://api.github.com/users/discipleofhamilton/events{/privacy}", "received_events_url": "https://api.github.com/users/discipleofhamilton/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-09-03T03:57:02Z", "updated_at": "2021-12-16T06:25:25Z", "closed_at": "2021-11-24T06:32:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI updated my Triton server version from 21.02 to 21.08 (docker image) to develop a complete service with BLS. But I don't have enough GPU memory to load every models at the initial. I used `load`/`unload` client API which you offered to reach the purpose. Here comes the same problem on version 21.02 and 21.08. \r\n1. `unload` API will not exactly release model memory on all the backends (PytTrch, ONNX, TensorFlow, TensorRT). First, PyTorch and ONNX  backends do not fully free up GPU memory, but wouldn't cause a memory leak. In other words, the maximum GPU memory usage on the same model is fixed, and the GPU memory would reduce unloading the model but still occupy a large amount of memory. Second, the TensorFlow backend would not release memory at all. (p.s. In other issue says it's TensorFlow's bug). \r\n2. Use `unload`/`load` API to dynamic load TensorRT model is not only free up GPU memory incompletely but also cause memory leak when reload the same model. what I expects is the fixed maximum of GPU memory usage when the same model reload just like Pytorch, ONNX. Although the GPU memory usage reduced when unload the TensorRT model, the memory usage will increase when reload it.\r\n\r\n**Triton Information**\r\nTriton server version: 21.02 / 21.08\r\nTriton server image: I used the image that official website [offered](https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver), and I tried to build by myself, and got the problem.\r\n\r\n**To Reproduce**\r\nUse version 21.08 for example.\r\n1. `docker run  --rm  -it --gpus all --name Triton_Server --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8000:8000 -p 8001:8001 -p 8002:8002 -v /home/infor/model-repository:/models nvcr.io/nvidia/tritonserver:21.08-py3 tritonserver --model-control-mode=explicit --model-repository=/models/ --strict-model-config=false --grpc-infer-allocation-pool-size=16 --log-verbose 1`\r\n2. use this script(see Client-Testing-Script below) to test GPU memory on each model, and use nvidia-smi to observe the GPU memory usage. (p.s. the tested TensorRT model was converted from densenet_onnx which is provided from model-repository examples)\r\n\r\n**Describe Models and  Testing Result**\r\n* Initial Triton Inference server\r\n![image](https://user-images.githubusercontent.com/8478501/131944365-e8662805-c586-4449-ab44-6e0b74dbb98d.png)\r\n\r\n1. PyTorch:\r\n    * model: [ArcFace from insightface](https://github.com/deepinsight/insightface)\r\n    * config: see ArcFace-Config below\r\n    * result: the first execution didn't use `unload`, the second one used `unload`.  ![mem_test py - triton_client tirton_pytorch 2021-09-03 11-33-31](https://user-images.githubusercontent.com/8478501/131947700-0766ede2-a9fd-4a9e-a728-c21c777bc653.gif)\r\n    * log: [tritonserver_log_pytorch.log](https://github.com/triton-inference-server/server/files/7103200/tritonserver_log_pytorch.log) \r\n\r\n2. ONNX:\r\n    * model: densenet_onnx\r\n    * without `unload`: ![image](https://user-images.githubusercontent.com/8478501/131945479-61eb9959-383d-4092-97e7-379c4fe91882.png)\r\n    * with `unload`: ![mem_test py - triton_clienttirton_onnx_with_unloading_2021-09-03 11-29-40](https://user-images.githubusercontent.com/8478501/131947129-6eb90d2b-b4e1-48e8-9a48-6413ade1484e.gif)\r\n    * log: [tritonserver_log_onnx.log](https://github.com/triton-inference-server/server/files/7103197/tritonserver_log_onnx.log)\r\n\r\n3. TensorFlow:\r\n    * model: Inception_graphdef\r\n    * without `unload`: ![image](https://user-images.githubusercontent.com/8478501/131944844-f36d0db9-cd4a-47a3-9486-1d4347d71fdc.png)\r\n    * with `unload`: ![image](https://user-images.githubusercontent.com/8478501/131944914-4d9fb366-204d-4379-95cd-e9f29fc6f2b7.png)\r\n    * log: [tritonserver_log_tensorflow.log](https://github.com/triton-inference-server/server/files/7103196/tritonserver_log_tensorflow.log)\r\n\r\n4. TensorRT:\r\n    * model: densenet_trt (converted from densenet_onnx)\r\n    * result: the first execution didn't use `unload`, the second one used `unload`.  ![mem_test py - triton_client tirton_trt_2021-09-03 11-36-45](https://user-images.githubusercontent.com/8478501/131947753-c46f292c-15a9-496a-b199-5cc08ad28dd7.gif)\r\n    * log: [tritonserver_log_tensorrt.log](https://github.com/triton-inference-server/server/files/7103202/tritonserver_log_tensorrt.log)\r\n\r\n**Expected behavior**\r\nMy Final goal is when the client request `unload` model, then model will fully release from the GPU memory. At least, There won't cause memory leak on TensorRT model.\r\n\r\n### Client Testing Script\r\n``` python\r\n#!/usr/bin/env python\r\nimport argparse\r\nimport time\r\nimport sys\r\n\r\nimport tritonclient.grpc as grpcclient\r\nfrom tritonclient.utils import InferenceServerException\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('-v',\r\n                        '--verbose',\r\n                        action=\"store_true\",\r\n                        required=False,\r\n                        default=False,\r\n                        help='Enable verbose output')\r\n    parser.add_argument('-u',\r\n                        '--url',\r\n                        type=str,\r\n                        required=False,\r\n                        default='localhost:8001',\r\n                        help='Inference server URL. Default is localhost:8001.')\r\n    parser.add_argument(\r\n        '-m',\r\n        '--model_name',\r\n        type=str,\r\n        required=False,\r\n        default='preprocess_inception_ensemble',\r\n        help='Name of model. Default is preprocess_inception_ensemble.')\r\n    parser.add_argument('-d',\r\n                        '--close_unload_model',\r\n                        action=\"store_true\",\r\n                        required=False,\r\n                        default=False,\r\n                        help='Iteration number')\r\n    parser.add_argument('-l',\r\n                        '--loop',\r\n                        type=int,\r\n                        required=False,\r\n                        default=10,\r\n                        help='Iteration number')\r\n\r\n    FLAGS = parser.parse_args()\r\n\r\n    for i in range(FLAGS.loop):\r\n\r\n        print(\"iteration: {}\".format(i+1))\r\n        print(\"*\" * 50)\r\n\r\n        try:\r\n            triton_client = grpcclient.InferenceServerClient(url=FLAGS.url,\r\n                                                            verbose=FLAGS.verbose)\r\n        except Exception as e:\r\n            print(\"\\tcontext creation failed: \" + str(e))\r\n            sys.exit(1)\r\n\r\n        model_name = FLAGS.model_name\r\n\r\n        load_start = time.time()\r\n        triton_client.load_model(model_name)\r\n        load_end   = time.time()\r\n        print(\"\\tLoading time: {:.2f}ms\".format((load_end - load_start) * 1000))\r\n        if not triton_client.is_model_ready(model_name):\r\n            print('\\tFAILED : Load Model')\r\n            sys.exit(1)\r\n        else:\r\n            print(\"\\tModel loading pass\")\r\n            # Make sure the model matches our requirements, and get some\r\n            # properties of the model that we need for preprocessing\r\n            try:\r\n                model_metadata = triton_client.get_model_metadata(\r\n                    model_name=FLAGS.model_name, model_version=\"1\")\r\n                model_config = triton_client.get_model_config(\r\n                    model_name=FLAGS.model_name, model_version=\"1\"\r\n                )\r\n                # print(\"model config: {}\".format(model_config))\r\n                # print(\"model metadata: {}\".format(model_metadata))\r\n                print(\"\\tGet config and metadata pass\")\r\n            except InferenceServerException as e:\r\n                print(\"\\tfailed to retrieve the metadata or config: \" + str(e))\r\n                sys.exit(1)\r\n\r\n        if not FLAGS.close_unload_model:\r\n            unload_start = time.time()\r\n            triton_client.unload_model(model_name)\r\n            unload_end   = time.time()\r\n            print(\"\\tUnloading time: {:.2f}ms\".format((unload_end - unload_start) * 1000))\r\n            if triton_client.is_model_ready(model_name):\r\n                print('\\tFAILED : Unload Model')\r\n                sys.exit(1)\r\n            else:\r\n                print(\"\\tModel unloading pass\")\r\n```\r\n\r\n### ArcFace Config\r\n```protobuf\r\nname: \"arcface_r100_torch\"\r\nplatform: \"pytorch_libtorch\"\r\nmax_batch_size : 2\r\ninput [\r\n  {\r\n    name: \"input__0\"\r\n    data_type: TYPE_FP32\r\n    format: FORMAT_NCHW\r\n    dims: [ 3 , 112, 112 ]\r\n\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"output__0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 512 ]\r\n    reshape { shape: [ 1, 512 ] }\r\n  }\r\n]\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3320/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3299", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3299/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3299/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3299/events", "html_url": "https://github.com/triton-inference-server/server/issues/3299", "id": 983411335, "node_id": "MDU6SXNzdWU5ODM0MTEzMzU=", "number": 3299, "title": "Periodic dead of server while using python backend", "user": {"login": "LightToYang", "id": 17929130, "node_id": "MDQ6VXNlcjE3OTI5MTMw", "avatar_url": "https://avatars.githubusercontent.com/u/17929130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LightToYang", "html_url": "https://github.com/LightToYang", "followers_url": "https://api.github.com/users/LightToYang/followers", "following_url": "https://api.github.com/users/LightToYang/following{/other_user}", "gists_url": "https://api.github.com/users/LightToYang/gists{/gist_id}", "starred_url": "https://api.github.com/users/LightToYang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LightToYang/subscriptions", "organizations_url": "https://api.github.com/users/LightToYang/orgs", "repos_url": "https://api.github.com/users/LightToYang/repos", "events_url": "https://api.github.com/users/LightToYang/events{/privacy}", "received_events_url": "https://api.github.com/users/LightToYang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-08-31T03:54:22Z", "updated_at": "2023-02-21T17:51:26Z", "closed_at": "2023-02-21T17:51:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nWith python backend, the triton inference server sometimes dead, no response and no error return to the client.\r\n\r\nUsing ctrl+c to stop server, the following codes display :\r\n```\r\nI0831 03:38:40.204847 59 server.cc:249] Timeout 29: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:41.204980 59 server.cc:249] Timeout 28: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:42.205103 59 server.cc:249] Timeout 27: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:43.205240 59 server.cc:249] Timeout 26: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:44.205336 59 server.cc:249] Timeout 25: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:45.205462 59 server.cc:249] Timeout 24: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:46.205591 59 server.cc:249] Timeout 23: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:47.205722 59 server.cc:249] Timeout 22: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:48.205858 59 server.cc:249] Timeout 21: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:49.205976 59 server.cc:249] Timeout 20: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:50.206065 59 server.cc:249] Timeout 19: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:51.206216 59 server.cc:249] Timeout 18: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:52.206337 59 server.cc:249] Timeout 17: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:53.206455 59 server.cc:249] Timeout 16: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:54.206548 59 server.cc:249] Timeout 15: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:55.206642 59 server.cc:249] Timeout 14: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:56.206791 59 server.cc:249] Timeout 13: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:57.206924 59 server.cc:249] Timeout 12: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:58.207010 59 server.cc:249] Timeout 11: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:38:59.207139 59 server.cc:249] Timeout 10: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:39:00.207267 59 server.cc:249] Timeout 9: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:39:01.207384 59 server.cc:249] Timeout 8: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:39:02.207497 59 server.cc:249] Timeout 7: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:39:03.207627 59 server.cc:249] Timeout 6: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:39:04.207761 59 server.cc:249] Timeout 5: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:39:05.207860 59 server.cc:249] Timeout 4: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:39:06.208009 59 server.cc:249] Timeout 3: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:39:07.208151 59 server.cc:249] Timeout 2: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:39:08.208278 59 server.cc:249] Timeout 1: Found 18 live models and 0 in-flight non-inference requests\r\nI0831 03:39:09.208368 59 server.cc:249] Timeout 0: Found 18 live models and 0 in-flight non-inference requests\r\nE0831 03:39:09.208406 59 main.cc:1513] failed to stop server: Internal - Exit timeout expired. Exiting immediately.\r\n```\r\nCLI is frozen after above codes displayed, \r\nthe only way is to restart the container and lauch the server with \r\n`tritonserver --model-repository=/models`, \r\nafter that the server works well.\r\n\r\n\r\n**Triton Information**\r\n\r\n21.06.\r\n\r\n**To Reproduce**\r\n\r\nIt appears every few days (maybe 5<x<10).\r\n\r\n**Expected behavior**\r\n\r\nNone.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3299/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3299/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3237", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3237/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3237/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3237/events", "html_url": "https://github.com/triton-inference-server/server/issues/3237", "id": 972254984, "node_id": "MDU6SXNzdWU5NzIyNTQ5ODQ=", "number": 3237, "title": "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/folder31uWBi/1/model.py'", "user": {"login": "CPFelix", "id": 32193197, "node_id": "MDQ6VXNlcjMyMTkzMTk3", "avatar_url": "https://avatars.githubusercontent.com/u/32193197?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CPFelix", "html_url": "https://github.com/CPFelix", "followers_url": "https://api.github.com/users/CPFelix/followers", "following_url": "https://api.github.com/users/CPFelix/following{/other_user}", "gists_url": "https://api.github.com/users/CPFelix/gists{/gist_id}", "starred_url": "https://api.github.com/users/CPFelix/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CPFelix/subscriptions", "organizations_url": "https://api.github.com/users/CPFelix/orgs", "repos_url": "https://api.github.com/users/CPFelix/repos", "events_url": "https://api.github.com/users/CPFelix/events{/privacy}", "received_events_url": "https://api.github.com/users/CPFelix/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-08-17T03:01:56Z", "updated_at": "2021-08-18T23:04:45Z", "closed_at": "2021-08-18T23:04:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen I use the Python Backend, becasue there are preprocess.py and postprocess.py, I need to rename the python model name.But after I definit \"default_model_filename\", I get the error:\r\n\"\r\nI0817 02:55:35.150648 1 python.cc:604] TRITONBACKEND_ModelInstanceInitialize: post_detection_0 (CPU device 0)\r\nTraceback (most recent call last):\r\n  File \"/opt/tritonserver/backends/python/startup.py\", line 382, in <module>\r\n    python_host = PythonHost(module_path=FLAGS.model_path)\r\n  File \"/opt/tritonserver/backends/python/startup.py\", line 172, in __init__\r\n    spec.loader.exec_module(module)\r\n  File \"<frozen importlib._bootstrap_external>\", line 779, in exec_module\r\n  File \"<frozen importlib._bootstrap_external>\", line 915, in get_code\r\n  File \"<frozen importlib._bootstrap_external>\", line 972, in get_data\r\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/folder31uWBi/1/model.py'\r\nI0817 02:55:35.595524 1 onnxruntime.cc:1787] TRITONBACKEND_ModelInitialize: densenet_onnx (version 1)\r\nI0817 02:55:35.596162 1 onnxruntime.cc:1830] TRITONBACKEND_ModelInstanceInitialize: densenet_onnx (GPU device 0)\r\nWARNING: Since openmp is enabled in this build, this API cannot be used to configure intra op num threads. Please use the openmp environment variables to control the number of threads.\r\nI0817 02:55:41.053828 1 model_repository_manager.cc:1239] successfully loaded 'densenet_onnx' version 1\r\nE0817 02:55:45.171304 1 model_repository_manager.cc:1242] failed to load 'post_detection' version 1: Internal: failed to connect to all addresses\r\n\"\r\nIt seems the \"default_model_filename\" doesn't work?\r\n**Triton Information**\r\n21.03\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTriton container\r\n**To Reproduce**\r\nNone\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nconfig.pbtxt:\r\n\"\r\nname: \"post_detection\"\r\nbackend: \"python\"\r\n\r\ninput [\r\n  {\r\n    name: \"INPUT\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1,7001,1,1]\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"OUTPUT\"\r\n    data_type: TYPE_FP32\r\n    dims: [-1, 6]\r\n  }\r\n]\r\ninstance_group [ { kind: KIND_CPU }]\r\ndefault_model_filename:\"post_detection.py\"\r\n\"\r\n**Expected behavior**\r\nTriton can load python model whose name is not \"model.py\".\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3237/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3227", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3227/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3227/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3227/events", "html_url": "https://github.com/triton-inference-server/server/issues/3227", "id": 969000743, "node_id": "MDU6SXNzdWU5NjkwMDA3NDM=", "number": 3227, "title": "Load triton_python_backend_stub from S3 model repository", "user": {"login": "c-woods-gm", "id": 88451326, "node_id": "MDQ6VXNlcjg4NDUxMzI2", "avatar_url": "https://avatars.githubusercontent.com/u/88451326?v=4", "gravatar_id": "", "url": "https://api.github.com/users/c-woods-gm", "html_url": "https://github.com/c-woods-gm", "followers_url": "https://api.github.com/users/c-woods-gm/followers", "following_url": "https://api.github.com/users/c-woods-gm/following{/other_user}", "gists_url": "https://api.github.com/users/c-woods-gm/gists{/gist_id}", "starred_url": "https://api.github.com/users/c-woods-gm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/c-woods-gm/subscriptions", "organizations_url": "https://api.github.com/users/c-woods-gm/orgs", "repos_url": "https://api.github.com/users/c-woods-gm/repos", "events_url": "https://api.github.com/users/c-woods-gm/events{/privacy}", "received_events_url": "https://api.github.com/users/c-woods-gm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-08-12T15:01:09Z", "updated_at": "2021-09-07T02:09:52Z", "closed_at": "2021-09-07T02:09:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to load a python model from a model repository based in an S3 bucket. The triton_python_backend_stub is located in the model directory within the repository. When I try to load the model I am getting the following error:\r\n\r\n```\r\nI0812 14:50:59.211734 25 python.cc:918] Starting Python backend stub: export LD_LIBRARY_PATH=/tmp/python_env_pkhKPf/0/lib:$LD_LIBRARY_PATH; source /tmp/python_env_pkhKPf/0/bin/activate && exec /tmp/folderyPJDZi/triton_python_backend_stub /tmp/folderyPJDZi/1/model.py /interpreter_CPU_0 67108864 67108864 1 /opt/tritonserver/backends/python\r\nbash: /tmp/folderyPJDZi/triton_python_backend_stub: Permission denied\r\nbash: line 0: exec: /tmp/folderyPJDZi/triton_python_backend_stub: cannot execute: Permission denied\r\n```\r\nIt looks as though the triton_python_backend_stub has not got executable permissions after being downloaded from the S3 bucket. Is this a bug or something that is not supported?", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3227/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3227/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3219", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3219/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3219/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3219/events", "html_url": "https://github.com/triton-inference-server/server/issues/3219", "id": 965131550, "node_id": "MDU6SXNzdWU5NjUxMzE1NTA=", "number": 3219, "title": "sending an output with wrong datatype (e.g. integers > 256 as datatype uint8) should fail", "user": {"login": "moritzhambach", "id": 33765868, "node_id": "MDQ6VXNlcjMzNzY1ODY4", "avatar_url": "https://avatars.githubusercontent.com/u/33765868?v=4", "gravatar_id": "", "url": "https://api.github.com/users/moritzhambach", "html_url": "https://github.com/moritzhambach", "followers_url": "https://api.github.com/users/moritzhambach/followers", "following_url": "https://api.github.com/users/moritzhambach/following{/other_user}", "gists_url": "https://api.github.com/users/moritzhambach/gists{/gist_id}", "starred_url": "https://api.github.com/users/moritzhambach/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/moritzhambach/subscriptions", "organizations_url": "https://api.github.com/users/moritzhambach/orgs", "repos_url": "https://api.github.com/users/moritzhambach/repos", "events_url": "https://api.github.com/users/moritzhambach/events{/privacy}", "received_events_url": "https://api.github.com/users/moritzhambach/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-08-10T16:46:28Z", "updated_at": "2021-09-08T19:33:37Z", "closed_at": "2021-09-08T19:31:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\naccidentally declared the wrong datatype (TYPE_UINT8) for integers above 256 as output of the triton server (python backend). Instead of raising an error, it sends the values modulo 256. makes it hard to debug...\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nnvcr.io/nvidia/tritonserver:21.06-py3\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nnew one with just some python packages on top \r\n\r\n**To Reproduce**\r\nsend integer 257 as uint8 output\r\n\r\n**Expected behavior**\r\nraise error when declared datatype of input / output does not match input / output\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3219/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3191", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3191/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3191/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3191/events", "html_url": "https://github.com/triton-inference-server/server/issues/3191", "id": 959408080, "node_id": "MDU6SXNzdWU5NTk0MDgwODA=", "number": 3191, "title": "Certain FP16 traced pytorch models with batchnorm no longer work with r21.07", "user": {"login": "jamt9000", "id": 1186841, "node_id": "MDQ6VXNlcjExODY4NDE=", "avatar_url": "https://avatars.githubusercontent.com/u/1186841?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamt9000", "html_url": "https://github.com/jamt9000", "followers_url": "https://api.github.com/users/jamt9000/followers", "following_url": "https://api.github.com/users/jamt9000/following{/other_user}", "gists_url": "https://api.github.com/users/jamt9000/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamt9000/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamt9000/subscriptions", "organizations_url": "https://api.github.com/users/jamt9000/orgs", "repos_url": "https://api.github.com/users/jamt9000/repos", "events_url": "https://api.github.com/users/jamt9000/events{/privacy}", "received_events_url": "https://api.github.com/users/jamt9000/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-08-03T19:09:15Z", "updated_at": "2021-08-24T06:30:50Z", "closed_at": "2021-08-23T22:14:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nCertain pytorch traced models now give errors like\r\n\r\n```\r\nin ensemble 'jasper-ts-trace-ensemble', PyTorch execute failure: MALFORMED INPUT: bad dtype in CompareSelect\r\n```\r\n\r\nIn this case with https://ngc.nvidia.com/catalog/models/nvidia:jasper_pyt_torchscript_fp16_amp/version and [fp16/jasper-ts-trace-ensemble/config.pbtxt](https://github.com/NVIDIA/DeepLearningExamples/blob/d788e8d4968e72c722c5148a50a7d4692f6e7bd3/PyTorch/SpeechRecognition/Jasper/triton/model_repo_configs/fp16/jasper-ts-trace-ensemble/config.pbtxt)\r\n\r\nThis is likely to be caused by this pytorch batchnorm/fusing bug https://github.com/pytorch/pytorch/issues/61382 https://github.com/pytorch/pytorch/issues/61336 but I'm opening an issue here since it's likely to affect triton fp16 inference usecases and affects nvidia's released models. Perhaps it should be included as a known issue at least.\r\n\r\n**Triton Information**\r\n\r\nr21.06 or r21.07 prebuilt\r\n\r\n**To Reproduce**\r\n\r\nFor example the nvidia Jasper model, as in https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/Jasper/triton#inference-pipeline-in-triton-inference-server\r\n\r\n**Expected behavior**\r\n\r\nIt should work as before with no error\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3191/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3160", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3160/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3160/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3160/events", "html_url": "https://github.com/triton-inference-server/server/issues/3160", "id": 954333655, "node_id": "MDU6SXNzdWU5NTQzMzM2NTU=", "number": 3160, "title": "Streaming connections interrupted for extremely long time series", "user": {"login": "alecgunny", "id": 14932242, "node_id": "MDQ6VXNlcjE0OTMyMjQy", "avatar_url": "https://avatars.githubusercontent.com/u/14932242?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alecgunny", "html_url": "https://github.com/alecgunny", "followers_url": "https://api.github.com/users/alecgunny/followers", "following_url": "https://api.github.com/users/alecgunny/following{/other_user}", "gists_url": "https://api.github.com/users/alecgunny/gists{/gist_id}", "starred_url": "https://api.github.com/users/alecgunny/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alecgunny/subscriptions", "organizations_url": "https://api.github.com/users/alecgunny/orgs", "repos_url": "https://api.github.com/users/alecgunny/repos", "events_url": "https://api.github.com/users/alecgunny/events{/privacy}", "received_events_url": "https://api.github.com/users/alecgunny/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2021-07-27T23:15:13Z", "updated_at": "2021-10-23T19:20:19Z", "closed_at": "2021-09-08T20:30:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nUsing streaming inference with Triton tends to lead to broken connections after ~millions of inferences that interrupt the service and aren't easy to reconnect. How many inferences before it happens, and even whether it happens at all, is somewhat inconsistent, though by rough eyeball I see it ~50% of the time. The issues as reported on the client side range, but the most common I've seen in production are\r\n\r\n`[StatusCode.UNAVAILABLE] Connection reset by peer`\r\n`[StatusCode.UNAVAILABLE] Socket closed`\r\n\r\nand additionally in the repro I'm providing\r\n`inference request for sequence 1001 to model 'mlp' must specify the START flag on the first request of the sequence`\r\neven though the same sequence has been going for millions of inferences.\r\n\r\nNo logs or issues get reported by the server whenever this happens.\r\n\r\n**Triton Information**\r\nv2.5.0 container build\r\nUsing more recent versions is difficult because of instabilities in the corresponding versions of TensorRT, but if this is a known issue that's been fixed in more recent versions it's not necessarily out of the question.\r\n\r\n**To Reproduce**\r\nThe model used doesn't necessarily matter, but for repro purposes the following code should suffice to export a model the exhibits these issues (even though it's not truly stateful):\r\n```\r\nimport argparse\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tritonclient.grpc import model_config_pb2 as model_config\r\n\r\n\r\ndef main(\r\n    repo_dir: str,\r\n    model_name: str,\r\n    model_version: int = 1,\r\n    input_dim: int = 1024\r\n):\r\n    # create the repo if it doesn't exist\r\n    output_dir = os.path.join(repo_dir, model_name)\r\n    if not os.path.exists(os.path.join(output_dir, str(model_version))):\r\n        os.makedirs(output_dir)\r\n\r\n    # build a generic linear MLP model\r\n    input = tf.keras.Input(\r\n        name=\"input\", shape=(input_dim,), dtype=\"float32\", batch_size=1\r\n    )\r\n    x = input\r\n    for dim in [256, 64, 1]:\r\n        x = tf.keras.layers.Dense(dim)(x)\r\n    model = tf.keras.Model(inputs=input, outputs=x)\r\n    model.save(os.path.join(output_dir, str(model_version), \"model.savedmodel\"))\r\n\r\n    config = model_config.ModelConfig(\r\n        name=model_name,\r\n        platform=\"tensorflow_savedmodel\",\r\n        input=[\r\n            model_config.ModelInput(\r\n                name=\"input\",\r\n                dims=[1, input_dim],\r\n                data_type=model_config.DataType.TYPE_FP32\r\n            )\r\n        ],\r\n        output=[\r\n            model_config.ModelOutput(\r\n                name=x.name.split(\"/\")[0],\r\n                dims=[1, 1],\r\n                data_type=model_config.DataType.TYPE_FP32\r\n            )\r\n        ],\r\n        sequence_batching=model_config.ModelSequenceBatching(\r\n            max_sequence_idle_microseconds=10000000,\r\n            direct=model_config.ModelSequenceBatching.StrategyDirect(),\r\n        ),\r\n        instance_group=[model_config.ModelInstanceGroup(\r\n            gpus=[0],\r\n            count=4\r\n        )]\r\n    )\r\n    with open(os.path.join(output_dir, \"config.pbtxt\"), \"w\") as f:\r\n        f.write(str(config))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--repo-dir\", type=str, required=True)\r\n    parser.add_argument(\"--model-name\", type=str, required=True)\r\n    parser.add_argument(\"--model-version\", type=int, default=1)\r\n    parser.add_argument(\"--input-dim\", type=int, default=1024)\r\n\r\n    flags = parser.parse_args()\r\n    main(**vars(flags))\r\n```\r\nThen start the server and run the following client code\r\n```\r\nimport argparse\r\nimport time\r\nfrom threading import Event\r\n\r\nimport numpy as np\r\nimport tritonclient.grpc as triton\r\n\r\n\r\nclass Callback:\r\n    def __init__(self, stop_event):\r\n        self.stop_event = stop_event\r\n        self.start_time = time.time()\r\n        self.total_requests = 0\r\n\r\n    def __call__(self, result, error=None):\r\n        if error is not None:\r\n            print(\"Error {} got raised after {} s and {} requests\".format(\r\n                str(error), time.time() - self.start_time, self.total_requests\r\n            ))\r\n            self.stop_event.set()\r\n            raise error\r\n\r\n        self.total_requests += 1\r\n        if self.total_requests % 100000 == 0:\r\n            print(\"Completed {} requests after {} s\".format(\r\n                self.total_requests, time.time() - self.start_time\r\n            ))\r\n\r\n\r\ndef main(\r\n    url: str,\r\n    model_name: str,\r\n    model_version: int = 1,\r\n    request_rate: float = 1000.\r\n):\r\n    client = triton.InferenceServerClient(url)\r\n    model_metadata = client.get_model_metadata(model_name)\r\n\r\n    input = triton.InferInput(\r\n        name=model_metadata.inputs[0].name,\r\n        shape=model_metadata.inputs[0].shape,\r\n        datatype=model_metadata.inputs[0].datatype\r\n    )\r\n\r\n    stop_event = Event()\r\n    with client:\r\n        client.start_stream(callback=Callback(stop_event))\r\n\r\n        last_request_time = time.time()\r\n        sequence_start = True\r\n        while not stop_event.is_set():\r\n            x = np.random.randn(*input.shape()).astype(\"float32\")\r\n            input.set_data_from_numpy(x)\r\n\r\n            # do some throttling to avoid overloading the server\r\n            while (time.time() - last_request_time) < 1 / request_rate - 5e-4:\r\n                time.sleep(1e-6)\r\n\r\n            # make the request\r\n            client.async_stream_infer(\r\n                model_name,\r\n                model_version=str(model_version),\r\n                sequence_id=1001,\r\n                inputs=[input],\r\n                sequence_start=sequence_start\r\n            )\r\n            sequence_start = False\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--url\", type=str, required=True)\r\n    parser.add_argument(\"--model-name\", type=str, required=True)\r\n    parser.add_argument(\"--model-version\", type=int, default=1)\r\n    parser.add_argument(\"--request-rate\", type=float, default=1000)\r\n    flags = parser.parse_args()\r\n\r\n    main(**vars(flags))\r\n```\r\n\r\n**Expected behavior**\r\nIdeally the connection should never break, but at the very least advice on how to catch this issue and quickly reconnect to not interrupt service. The issue is that any attempt to exit the current client context has to wait until all outstanding requests are completed, which could be substantial. There can also be other issues where e.g. an attempted reconnect leads to a `Too many pings` complaint from the server.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3160/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3156", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3156/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3156/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3156/events", "html_url": "https://github.com/triton-inference-server/server/issues/3156", "id": 954061586, "node_id": "MDU6SXNzdWU5NTQwNjE1ODY=", "number": 3156, "title": "Triton python backend build failed (main branch)", "user": {"login": "NonStatic2014", "id": 9442504, "node_id": "MDQ6VXNlcjk0NDI1MDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/9442504?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NonStatic2014", "html_url": "https://github.com/NonStatic2014", "followers_url": "https://api.github.com/users/NonStatic2014/followers", "following_url": "https://api.github.com/users/NonStatic2014/following{/other_user}", "gists_url": "https://api.github.com/users/NonStatic2014/gists{/gist_id}", "starred_url": "https://api.github.com/users/NonStatic2014/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NonStatic2014/subscriptions", "organizations_url": "https://api.github.com/users/NonStatic2014/orgs", "repos_url": "https://api.github.com/users/NonStatic2014/repos", "events_url": "https://api.github.com/users/NonStatic2014/events{/privacy}", "received_events_url": "https://api.github.com/users/NonStatic2014/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2021-07-27T16:34:07Z", "updated_at": "2021-08-12T20:24:05Z", "closed_at": "2021-08-12T20:24:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nThe triton build failed if I try to build without cuda.\r\n\r\n> ./build.py --cmake-dir=$(pwd)/build --build-dir=/tmp/citritonbuild --enable-logging --enable-stats --enable-tracing --enable-metrics --filesystem=azure_storage --endpoint=http --endpoint=grpc --repo-tag=common:main --repo-tag=core:main --repo-tag=backend:main --repo-tag=thirdparty:main --backend=ensemble --backend=identity:main --backend=repeat:main --backend=tensorflow2:main --backend=python:main --repoagent=checksum:main\r\n\r\n```\r\n/tmp/tritonbuild/python/src/python.cc: In member function \u2018TRITONSERVER_Error* triton::backend::python::ModelInstanceState::GetInputTensor(uint32_t, triton::backend::python::Tensor*, TRITONBACKEND_Request*, std::vector<TRITONBACKEND_Response*>&)\u2019:\r\n/tmp/tritonbuild/python/src/python.cc:1429:7: error: \u2018cudaSetDevice\u2019 was not declared in this scope\r\n 1429 |       cudaSetDevice(src_memory_type_id);\r\n      |       ^~~~~~~~~~~~~\r\n/tmp/tritonbuild/python/src/python.cc:1430:7: error: \u2018cudaError_t\u2019 was not declared in this scope\r\n 1430 |       cudaError_t err = cudaIpcGetMemHandle(\r\n      |       ^~~~~~~~~~~\r\n/tmp/tritonbuild/python/src/python.cc:1432:11: error: \u2018err\u2019 was not declared in this scope; did you mean \u2018erf\u2019?\r\n 1432 |       if (err != cudaSuccess) {\r\n      |           ^~~\r\n      |           erf\r\n/tmp/tritonbuild/python/src/python.cc:1432:18: error: \u2018cudaSuccess\u2019 was not declared in this scope\r\n 1432 |       if (err != cudaSuccess) {\r\n      |                  ^~~~~~~~~~~\r\n/tmp/tritonbuild/python/src/python.cc:1435:55: error: \u2018cudaGetErrorName\u2019 was not declared in this scope\r\n 1435 |                                           std::string(cudaGetErrorName(err)))\r\n      |                                                       ^~~~~~~~~~~~~~~~\r\n/tmp/tritonbuild/python/src/python.cc:1439:7: error: \u2018CUdeviceptr\u2019 was not declared in this scope\r\n 1439 |       CUdeviceptr start_address;\r\n      |       ^~~~~~~~~~~\r\n/tmp/tritonbuild/python/src/python.cc:1440:7: error: \u2018CUresult\u2019 was not declared in this scope\r\n 1440 |       CUresult cuda_err = cuPointerGetAttribute(\r\n      |       ^~~~~~~~\r\n/tmp/tritonbuild/python/src/python.cc:1443:11: error: \u2018cuda_err\u2019 was not declared in this scope\r\n 1443 |       if (cuda_err != CUDA_SUCCESS) {\r\n      |           ^~~~~~~~\r\n/tmp/tritonbuild/python/src/python.cc:1443:23: error: \u2018CUDA_SUCCESS\u2019 was not declared in this scope; did you mean \u2018EXIT_SUCCESS\u2019?\r\n 1443 |       if (cuda_err != CUDA_SUCCESS) {\r\n      |                       ^~~~~~~~~~~~\r\n      |                       EXIT_SUCCESS\r\n/tmp/tritonbuild/python/src/python.cc:1445:9: error: \u2018cuGetErrorString\u2019 was not declared in this scope\r\n 1445 |         cuGetErrorString(cuda_err, &error_string);\r\n      |         ^~~~~~~~~~~~~~~~\r\n/tmp/tritonbuild/python/src/python.cc:1453:45: error: \u2018start_address\u2019 was not declared in this scope\r\n 1453 |                     reinterpret_cast<char*>(start_address);\r\n      |                                             ^~~~~~~~~~~~~\r\n/tmp/tritonbuild/python/src/python.cc:1454:7: error: \u2018gpu_tensors_map_\u2019 was not declared in this scope\r\n 1454 |       gpu_tensors_map_.insert(\r\n      |       ^~~~~~~~~~~~~~~~\r\nmake[2]: *** [CMakeFiles/triton-python-backend.dir/build.make:82: CMakeFiles/triton-python-backend.dir/src/python.cc.o] Error 1\r\nmake[2]: Leaving directory '/tmp/tritonbuild/python/build'\r\nmake[1]: *** [CMakeFiles/Makefile2:239: CMakeFiles/triton-python-backend.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[ 97%] Linking CXX executable triton_python_backend_stub\r\n/usr/bin/cmake -E cmake_link_script CMakeFiles/triton-python-backend-stub.dir/link.txt --verbose=0\r\nmake[2]: Leaving directory '/tmp/tritonbuild/python/build'\r\n[ 97%] Built target triton-python-backend-stub\r\nmake[1]: Leaving directory '/tmp/tritonbuild/python/build'\r\nmake: *** [Makefile:149: all] Error 2\r\nversion 2.13.0dev\r\ndefault repo-tag: main\r\nbackend \"ensemble\" at tag/branch \"main\"\r\nbackend \"identity\" at tag/branch \"main\"\r\nbackend \"repeat\" at tag/branch \"main\"\r\nbackend \"tensorflow2\" at tag/branch \"main\"\r\nbackend \"python\" at tag/branch \"main\"\r\nrepoagent \"checksum\" at tag/branch \"main\"\r\nBuilding Triton Inference Server\r\ncomponent \"common\" at tag/branch \"main\"\r\ncomponent \"core\" at tag/branch \"main\"\r\ncomponent \"backend\" at tag/branch \"main\"\r\ncomponent \"thirdparty\" at tag/branch \"main\"\r\nerror: make install failed\r\nerror: docker run tritonserver_builder failed\r\n```\r\n\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nMain branch\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTry to build by myself\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nAs above\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nN/A\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nBuild succeeded.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3156/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3156/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3150", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3150/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3150/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3150/events", "html_url": "https://github.com/triton-inference-server/server/issues/3150", "id": 951952753, "node_id": "MDU6SXNzdWU5NTE5NTI3NTM=", "number": 3150, "title": "Doesn't co-work with MPS?", "user": {"login": "Zhaojp-Frank", "id": 12964049, "node_id": "MDQ6VXNlcjEyOTY0MDQ5", "avatar_url": "https://avatars.githubusercontent.com/u/12964049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zhaojp-Frank", "html_url": "https://github.com/Zhaojp-Frank", "followers_url": "https://api.github.com/users/Zhaojp-Frank/followers", "following_url": "https://api.github.com/users/Zhaojp-Frank/following{/other_user}", "gists_url": "https://api.github.com/users/Zhaojp-Frank/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zhaojp-Frank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zhaojp-Frank/subscriptions", "organizations_url": "https://api.github.com/users/Zhaojp-Frank/orgs", "repos_url": "https://api.github.com/users/Zhaojp-Frank/repos", "events_url": "https://api.github.com/users/Zhaojp-Frank/events{/privacy}", "received_events_url": "https://api.github.com/users/Zhaojp-Frank/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-07-23T23:34:33Z", "updated_at": "2022-04-12T17:05:48Z", "closed_at": "2022-04-12T17:05:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen MPS is enabled, TRTIS failed to start and hang there\r\n\r\n**Triton Information**\r\n2.10.0 built in tritonserver:21.05-py3\r\nT4, NV driver 450.80, Centos8 4.9.151\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nusing tritonserver:21.05-py3\r\n\r\n**To Reproduce**\r\njust run the basic sample as shown in https://github.com/triton-inference-server/server/blob/main/docs/quickstart.md#run-triton\r\n1. start MPS, either exclusive or not\r\nwith CUDA_MPS_PIPE_DIRECTORY=/dev/shm/nvidia-mps CUDA_MPS_LOG_DIRECTORY=/dev/shm/mps-log \r\n\r\n2. start  trtis server  container: tritonserver:21.05-py3 --ipc=host\r\n3. start the server:\r\nCUDA_MPS_PIPE_DIRECTORY=/dev/shm/nvidia-mps CUDA_MPS_LOG_DIRECTORY=/dev/shm/mps-log \\\r\ntritonserver --model-repository=</mnt/trt-server/docs/examples/model_repository/> \\\r\n        --model-control-mode=explicit --load-model=densenet_onnx\r\nthen it hangs there forever, and output logs as below:\r\nI0723 22:56:27.379539 107 libtorch.cc:932] TRITONBACKEND_Initialize: pytorch\r\nI0723 22:56:27.379605 107 libtorch.cc:942] Triton TRITONBACKEND API version: 1.0\r\nI0723 22:56:27.379613 107 libtorch.cc:948] 'pytorch' TRITONBACKEND API version: 1.0\r\n2021-07-23 22:56:27.677843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nI0723 22:56:27.809319 107 tensorflow.cc:2165] TRITONBACKEND_Initialize: tensorflow\r\nI0723 22:56:27.809348 107 tensorflow.cc:2175] Triton TRITONBACKEND API version: 1.0\r\nI0723 22:56:27.809357 107 tensorflow.cc:2181] 'tensorflow' TRITONBACKEND API version: 1.0\r\nI0723 22:56:27.809365 107 tensorflow.cc:2205] backend configuration:\r\n{}\r\nI0723 22:56:27.824193 107 onnxruntime.cc:1828] TRITONBACKEND_Initialize: onnxruntime\r\nI0723 22:56:27.824217 107 onnxruntime.cc:1838] Triton TRITONBACKEND API version: 1.0\r\nI0723 22:56:27.824224 107 onnxruntime.cc:1844] 'onnxruntime' TRITONBACKEND API version: 1.0\r\nI0723 22:56:27.865320 107 openvino.cc:1168] TRITONBACKEND_Initialize: openvino\r\nI0723 22:56:27.865338 107 openvino.cc:1178] Triton TRITONBACKEND API version: 1.0\r\nI0723 22:56:27.865347 107 openvino.cc:1184] 'openvino' TRITONBACKEND API version: 1.0\r\n<<<============hang here for ever ======= >\r\n<<<=== w/o MPS it shall output below msg=========>\r\nI0723 22:56:28.118777 107 pinned_memory_manager.cc:206] Pinned memory pool is created at '0x7f6fea000000' with size 268435456\r\nI0723 22:56:28.119930 107 cuda_memory_manager.cc:103] CUDA memory pool is created on device 0 with size 67108864\r\nI0723 22:56:28.119941 107 cuda_memory_manager.cc:103] CUDA memory pool is created on device 1 with size 67108864\r\nI0723 22:56:28.240427 107 model_repository_manager.cc:1043] loading: densenet_onnx:1\r\n\r\n/dev/shm/mps-log/control.log runs ok\r\n[2021-07-24 07:04:56.373 Control 339403] User did not send valid credentials\r\n[2021-07-24 07:04:56.373 Control 339403] Accepting connection...\r\n[2021-07-24 07:04:56.373 Control 339403] NEW CLIENT 347409 from user 0: Server is not ready, push client to pending list\r\n[2021-07-24 07:04:56.374 Control 339403] Starting new server 347828 for user 0\r\n[2021-07-24 07:04:56.376 Control 339403] Accepting connection...\r\n[2021-07-24 07:04:58.495 Control 339403] NEW SERVER 347828: Ready\r\n\r\n /dev/shm/mps-log/server.log sounds good\r\n[2021-07-24 07:04:58.384 Other 347828] Volta MPS: Creating client context\r\n[2021-07-24 07:04:58.445 Other 347828] Volta MPS: Creating client context\r\n[2021-07-24 07:04:58.495 Other 347828] activeThreadsPercentage set to 100.000000\r\n[2021-07-24 07:04:58.495 Other 347828] MPS Server is started\r\n[2021-07-24 07:04:58.495 Other 347828] Volta MPS Server: Received new client request\r\n[2021-07-24 07:04:58.495 Other 347828] MPS Server: worker created\r\n[2021-07-24 07:04:58.495 Other 347828] Volta MPS: Creating worker thread\r\n\r\n**Expected behavior**\r\nsuppose trtIS shwll cowork smoothly with MPS? and start successfully\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3150/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3150/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3142", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3142/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3142/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3142/events", "html_url": "https://github.com/triton-inference-server/server/issues/3142", "id": 950527879, "node_id": "MDU6SXNzdWU5NTA1Mjc4Nzk=", "number": 3142, "title": "openvino error in loading network (with custom op)", "user": {"login": "DayDayupupupup", "id": 18715426, "node_id": "MDQ6VXNlcjE4NzE1NDI2", "avatar_url": "https://avatars.githubusercontent.com/u/18715426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DayDayupupupup", "html_url": "https://github.com/DayDayupupupup", "followers_url": "https://api.github.com/users/DayDayupupupup/followers", "following_url": "https://api.github.com/users/DayDayupupupup/following{/other_user}", "gists_url": "https://api.github.com/users/DayDayupupupup/gists{/gist_id}", "starred_url": "https://api.github.com/users/DayDayupupupup/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DayDayupupupup/subscriptions", "organizations_url": "https://api.github.com/users/DayDayupupupup/orgs", "repos_url": "https://api.github.com/users/DayDayupupupup/repos", "events_url": "https://api.github.com/users/DayDayupupupup/events{/privacy}", "received_events_url": "https://api.github.com/users/DayDayupupupup/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1079803571, "node_id": "MDU6TGFiZWwxMDc5ODAzNTcx", "url": "https://api.github.com/repos/triton-inference-server/server/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-07-22T10:45:25Z", "updated_at": "2021-08-14T02:42:37Z", "closed_at": "2021-08-14T02:42:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nIn my model, there are some ops that OpenVino does not support. So I looked at the documentation on how OpenVino implements custom layers\uff1a\r\nhttps://docs.openvinotoolkit.org/2021.2/openvino_docs_HOWTO_Custom_Layers_Guide.html\r\nI implemented FFT OP with reference to the above documentation, and I have verified that wnet_20.xml can infer in the OpenVino 2021.2 environment. \r\nBut in Triton, it got an error\uff1a\r\n```python\r\nE0722 09:53:30.635025 529 model_repository_manager.cc:1215] failed to load 'fft' version 1: Internal: openvino error in loading network : Unsupported primitive of type: FFT name: lambda_2/IFFT2D\r\n```\r\n**Triton Information**\r\nTriton version\uff1a21.06\r\n\r\n**To Reproduce**\r\nI rename wnet_20.xml & wnet_20.bin to model.xml & model.bin , and model repository is:\r\n```python\r\nfft/\r\n\u251c\u2500\u2500 1\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 model.bin\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 model.xml\r\n\u2514\u2500\u2500 config.pbtxt\r\n```\r\nconfig.pbtxt:\r\n```python\r\nname: \"fft\"\r\nbackend: \"openvino\"\r\ninput {\r\n  name: \"input_1\"\r\n  data_type: TYPE_FP32\r\n  dims: 1\r\n  dims: 2\r\n  dims: 256\r\n  dims: 256\r\n}\r\noutput {\r\n  name: \"conv2d_44/BiasAdd/Add\"\r\n  data_type: TYPE_FP32\r\n  dims: 1\r\n  dims: 1\r\n  dims: 256\r\n  dims: 256\r\n}\r\ninstance_group {\r\n  count: 1\r\n  kind: KIND_CPU\r\n}\r\nparameters: {\r\nkey: \"CPU_EXTENSION_PATH\"\r\nvalue: {\r\nstring_value:\"/data/libtemplate_extension.so\"\r\n}\r\n}\r\n```\r\nfft op info in model.xml:\r\n```python\r\n<layer id=\"157\" name=\"lambda_2/IFFT2D\" type=\"FFT\" version=\"extension\">\r\n                        <data inverse=\"1\"/>\r\n                        <input>\r\n                                <port id=\"0\">\r\n                                        <dim>1</dim>\r\n                                        <dim>2</dim>\r\n                                        <dim>256</dim>\r\n                                        <dim>256</dim>\r\n                                </port>\r\n                        </input>\r\n                        <output>\r\n                                <port id=\"1\" precision=\"FP32\">\r\n                                        <dim>1</dim>\r\n                                        <dim>2</dim>\r\n                                        <dim>256</dim>\r\n                                        <dim>256</dim>\r\n                                </port>\r\n                        </output>\r\n                </layer>\r\n```\r\nI can confirm that wnet_20.xml works in the openVino 2021.2 environment, proving that libtemplate_extension.so is correct.  And from  the triton\u2019s log\uff0c triton also loaded libtemplate_extension.so \uff1a\r\n```python\r\nI0722 09:53:30.461096 529 openvino.cc:286] CPU (MKLDNN) extensions is loaded/data/libtemplate_extension.so\r\n```\r\nSo why the FFT model can\u2018t be loaded\uff1f", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3142/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3142/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3135", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3135/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3135/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3135/events", "html_url": "https://github.com/triton-inference-server/server/issues/3135", "id": 948839851, "node_id": "MDU6SXNzdWU5NDg4Mzk4NTE=", "number": 3135, "title": "Triton requiring config.pbtxt when loading models from s3 (MinIO)?", "user": {"login": "LMarino1", "id": 43488759, "node_id": "MDQ6VXNlcjQzNDg4NzU5", "avatar_url": "https://avatars.githubusercontent.com/u/43488759?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LMarino1", "html_url": "https://github.com/LMarino1", "followers_url": "https://api.github.com/users/LMarino1/followers", "following_url": "https://api.github.com/users/LMarino1/following{/other_user}", "gists_url": "https://api.github.com/users/LMarino1/gists{/gist_id}", "starred_url": "https://api.github.com/users/LMarino1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LMarino1/subscriptions", "organizations_url": "https://api.github.com/users/LMarino1/orgs", "repos_url": "https://api.github.com/users/LMarino1/repos", "events_url": "https://api.github.com/users/LMarino1/events{/privacy}", "received_events_url": "https://api.github.com/users/LMarino1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2021-07-20T16:32:03Z", "updated_at": "2023-01-31T19:29:55Z", "closed_at": "2021-12-20T20:49:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Using MinIO for S3, it seems starting with release 21.04, Triton is no longer automatically generating config files for models that lack them and instead errors out.\r\n\r\nLogs from attempting to run version 21.06.1:\r\n<pre>\r\n=============================\r\n== Triton Inference Server ==\r\n=============================\r\n\r\nNVIDIA Release 21.06 (build 24449615)\r\n\r\nCopyright (c) 2018-2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n   Use Docker with NVIDIA Container Toolkit to start this container; see\r\n   https://github.com/NVIDIA/nvidia-docker.\r\n\r\nI0720 15:30:54.928850 1 libtorch.cc:987] TRITONBACKEND_Initialize: pytorch\r\nI0720 15:30:54.928886 1 libtorch.cc:997] Triton TRITONBACKEND API version: 1.4\r\nI0720 15:30:54.928892 1 libtorch.cc:1003] 'pytorch' TRITONBACKEND API version: 1.4\r\n2021-07-20 15:30:58.469303: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nI0720 15:30:58.518754 1 tensorflow.cc:2165] TRITONBACKEND_Initialize: tensorflow\r\nI0720 15:30:58.518777 1 tensorflow.cc:2175] Triton TRITONBACKEND API version: 1.4\r\nI0720 15:30:58.518785 1 tensorflow.cc:2181] 'tensorflow' TRITONBACKEND API version: 1.4\r\nI0720 15:30:58.518809 1 tensorflow.cc:2205] backend configuration:\r\n{}\r\nI0720 15:30:58.645847 1 onnxruntime.cc:1969] TRITONBACKEND_Initialize: onnxruntime\r\nI0720 15:30:58.645922 1 onnxruntime.cc:1979] Triton TRITONBACKEND API version: 1.4\r\nI0720 15:30:58.645949 1 onnxruntime.cc:1985] 'onnxruntime' TRITONBACKEND API version: 1.4\r\nI0720 15:30:58.741700 1 openvino.cc:1188] TRITONBACKEND_Initialize: openvino\r\nI0720 15:30:58.741725 1 openvino.cc:1198] Triton TRITONBACKEND API version: 1.4\r\nI0720 15:30:58.741730 1 openvino.cc:1204] 'openvino' TRITONBACKEND API version: 1.4\r\nW0720 15:30:58.743358 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\nI0720 15:30:58.743396 1 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\nE0720 15:30:58.895833 1 model_repository_manager.cc:1919] Poll failed for model directory 'densenet_onnx': Could not get MetaData for object at s3://172.17.0.2:9000/bucket-1/model_repository/densenet_onnx/config.pbtxt due to exception: , error message: No response body. with address : 172.17.0.2\r\nI0720 15:30:59.186653 1 model_repository_manager.cc:1045] loading: tensorflow_test:1\r\nI0720 15:30:59.352603 1 model_repository_manager.cc:1045] loading: simple_identity:1\r\nI0720 15:30:59.516047 1 tensorflow.cc:2265] TRITONBACKEND_ModelInitialize: simple_identity (version 1)\r\n2021-07-20 15:30:59.516998: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /tmp/folderNwTlej/1/model.savedmodel\r\n2021-07-20 15:30:59.518218: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2021-07-20 15:30:59.520020: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2021-07-20 15:30:59.520047: E tensorflow/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-07-20 15:30:59.520075: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\r\n2021-07-20 15:30:59.548411: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2799925000 Hz\r\n2021-07-20 15:30:59.548932: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f701400ddb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-07-20 15:30:59.548958: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-07-20 15:30:59.549823: I tensorflow/cc/saved_model/loader.cc:251] Restoring SavedModel bundle.\r\n2021-07-20 15:30:59.549876: I tensorflow/cc/saved_model/loader.cc:261] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /tmp/folderNwTlej/1/model.savedmodel/variables/variables.index\r\n2021-07-20 15:30:59.549914: I tensorflow/cc/saved_model/loader.cc:379] SavedModel load for tags { serve }; Status: success. Took 32918 microseconds.\r\nI0720 15:30:59.550395 1 tensorflow.cc:2314] TRITONBACKEND_ModelInstanceInitialize: simple_identity (CPU device 0)\r\n2021-07-20 15:30:59.550430: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /tmp/folderNwTlej/1/model.savedmodel\r\n2021-07-20 15:30:59.551550: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2021-07-20 15:30:59.551884: I tensorflow/cc/saved_model/loader.cc:251] Restoring SavedModel bundle.\r\n2021-07-20 15:30:59.551916: I tensorflow/cc/saved_model/loader.cc:261] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /tmp/folderNwTlej/1/model.savedmodel/variables/variables.index\r\n2021-07-20 15:30:59.551932: I tensorflow/cc/saved_model/loader.cc:379] SavedModel load for tags { serve }; Status: success. Took 1504 microseconds.\r\nI0720 15:30:59.552115 1 model_repository_manager.cc:1212] successfully loaded 'simple_identity' version 1\r\nI0720 15:30:59.654538 1 tensorflow.cc:2265] TRITONBACKEND_ModelInitialize: tensorflow_test (version 1)\r\n2021-07-20 15:30:59.655047: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /tmp/folderObrYag/1/model.savedmodel\r\n2021-07-20 15:30:59.722012: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2021-07-20 15:30:59.889045: I tensorflow/cc/saved_model/loader.cc:251] Restoring SavedModel bundle.\r\n2021-07-20 15:30:59.889175: I tensorflow/cc/saved_model/loader.cc:261] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /tmp/folderObrYag/1/model.savedmodel/variables/variables.index\r\n2021-07-20 15:30:59.889236: I tensorflow/cc/saved_model/loader.cc:379] SavedModel load for tags { serve }; Status: success. Took 234193 microseconds.\r\nI0720 15:30:59.938570 1 tensorflow.cc:2314] TRITONBACKEND_ModelInstanceInitialize: tensorflow_test (CPU device 0)\r\n2021-07-20 15:30:59.938616: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /tmp/folderObrYag/1/model.savedmodel\r\n2021-07-20 15:30:59.979315: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2021-07-20 15:31:00.142792: I tensorflow/cc/saved_model/loader.cc:251] Restoring SavedModel bundle.\r\n2021-07-20 15:31:00.142861: I tensorflow/cc/saved_model/loader.cc:261] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /tmp/folderObrYag/1/model.savedmodel/variables/variables.index\r\n2021-07-20 15:31:00.142898: I tensorflow/cc/saved_model/loader.cc:379] SavedModel load for tags { serve }; Status: success. Took 204285 microseconds.\r\nI0720 15:31:00.143226 1 model_repository_manager.cc:1212] successfully loaded 'tensorflow_test' version 1\r\nI0720 15:31:00.143350 1 server.cc:504] \r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0720 15:31:00.143507 1 server.cc:543] \r\n+-------------+-----------------------------------------------------------------+--------+\r\n| Backend     | Path                                                            | Config |\r\n+-------------+-----------------------------------------------------------------+--------+\r\n| tensorrt    | <built-in>                                                      | {}     |\r\n| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so         | {}     |\r\n| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so | {}     |\r\n| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {}     |\r\n| openvino    | /opt/tritonserver/backends/openvino/libtriton_openvino.so       | {}     |\r\n+-------------+-----------------------------------------------------------------+--------+\r\n\r\nI0720 15:31:00.143550 1 server.cc:586] \r\n+-----------------+---------+--------+\r\n| Model           | Version | Status |\r\n+-----------------+---------+--------+\r\n| simple_identity | 1       | READY  |\r\n| tensorflow_test | 1       | READY  |\r\n+-----------------+---------+--------+\r\n\r\nI0720 15:31:00.143664 1 tritonserver.cc:1718] \r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Option                           | Value                                                                                                                                                                                  |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| server_id                        | triton                                                                                                                                                                                 |\r\n| server_version                   | 2.11.0                                                                                                                                                                                 |\r\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\r\n| model_repository_path[0]         | s3://172.17.0.2:9000/bucket-1/model_repository                                                                                                                                   |\r\n| model_control_mode               | MODE_NONE                                                                                                                                                                              |\r\n| strict_model_config              | 0                                                                                                                                                                                      |\r\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\r\n| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\r\n| strict_readiness                 | 1                                                                                                                                                                                      |\r\n| exit_timeout                     | 30                                                                                                                                                                                     |\r\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nI0720 15:31:00.143674 1 server.cc:234] Waiting for in-flight requests to complete.\r\nI0720 15:31:00.143679 1 model_repository_manager.cc:1078] unloading: tensorflow_test:1\r\nI0720 15:31:00.143712 1 model_repository_manager.cc:1078] unloading: simple_identity:1\r\nI0720 15:31:00.143757 1 tensorflow.cc:2352] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0720 15:31:00.143787 1 tensorflow.cc:2291] TRITONBACKEND_ModelFinalize: delete model state\r\nI0720 15:31:00.143790 1 server.cc:249] Timeout 30: Found 2 live models and 0 in-flight non-inference requests\r\nI0720 15:31:00.143914 1 tensorflow.cc:2352] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0720 15:31:00.143933 1 tensorflow.cc:2291] TRITONBACKEND_ModelFinalize: delete model state\r\nI0720 15:31:00.144261 1 model_repository_manager.cc:1195] successfully unloaded 'simple_identity' version 1\r\nI0720 15:31:00.199582 1 model_repository_manager.cc:1195] successfully unloaded 'tensorflow_test' version 1\r\nI0720 15:31:01.143922 1 server.cc:249] Timeout 29: Found 0 live models and 0 in-flight non-inference requests\r\nerror: creating server: Internal - failed to load all models\r\n</pre>\r\n\r\nThis error naturally stands out to me the most:\r\n`E0720 15:30:58.895833 1 model_repository_manager.cc:1919] Poll failed for model directory 'densenet_onnx': Could not get MetaData for object at s3://172.17.0.2:9000/bucket-1/model_repository/densenet_onnx/config.pbtxt due to exception: , error message: No response body. with address : 172.17.0.2`\r\n\r\nAlso worth noting that when loading the models from disk it can generate config without issue.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3135/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3134", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3134/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3134/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3134/events", "html_url": "https://github.com/triton-inference-server/server/issues/3134", "id": 948434756, "node_id": "MDU6SXNzdWU5NDg0MzQ3NTY=", "number": 3134, "title": "dlSym cannot locate method 'CreateExtension'", "user": {"login": "zhaohb", "id": 5791375, "node_id": "MDQ6VXNlcjU3OTEzNzU=", "avatar_url": "https://avatars.githubusercontent.com/u/5791375?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhaohb", "html_url": "https://github.com/zhaohb", "followers_url": "https://api.github.com/users/zhaohb/followers", "following_url": "https://api.github.com/users/zhaohb/following{/other_user}", "gists_url": "https://api.github.com/users/zhaohb/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhaohb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhaohb/subscriptions", "organizations_url": "https://api.github.com/users/zhaohb/orgs", "repos_url": "https://api.github.com/users/zhaohb/repos", "events_url": "https://api.github.com/users/zhaohb/events{/privacy}", "received_events_url": "https://api.github.com/users/zhaohb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 25, "created_at": "2021-07-20T08:43:56Z", "updated_at": "2021-08-04T22:15:40Z", "closed_at": "2021-08-04T22:15:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Triton Information**\r\nWhat version of Triton are you using?  21.06\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nbuild my self\r\n\r\nI have now recompiled the openvino back using openvino 2021.2 and need to load a custom operator that has been compiled to so lib,  but got an error when loading so:\r\n\r\n```shell\r\n+---------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| Model   | Version | Status                                                                                                                                                           |\r\n+---------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n| mylayer | 1       | UNAVAILABLE: Internal: ModelState::Create InferenceEngineException: dlSym cannot locate method 'CreateExtension': /data/custom_op_openvino/build/libt |\r\n|         |         | emplate_extension.so: undefined symbol: CreateExtension                                                                                                          |\r\n+---------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\n```\r\n\r\nhow to solve it?\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3134/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3134/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3126", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3126/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3126/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3126/events", "html_url": "https://github.com/triton-inference-server/server/issues/3126", "id": 946471392, "node_id": "MDU6SXNzdWU5NDY0NzEzOTI=", "number": 3126, "title": "Triton spins on startup running in EGX", "user": {"login": "flawaetz", "id": 1516944, "node_id": "MDQ6VXNlcjE1MTY5NDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1516944?v=4", "gravatar_id": "", "url": "https://api.github.com/users/flawaetz", "html_url": "https://github.com/flawaetz", "followers_url": "https://api.github.com/users/flawaetz/followers", "following_url": "https://api.github.com/users/flawaetz/following{/other_user}", "gists_url": "https://api.github.com/users/flawaetz/gists{/gist_id}", "starred_url": "https://api.github.com/users/flawaetz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/flawaetz/subscriptions", "organizations_url": "https://api.github.com/users/flawaetz/orgs", "repos_url": "https://api.github.com/users/flawaetz/repos", "events_url": "https://api.github.com/users/flawaetz/events{/privacy}", "received_events_url": "https://api.github.com/users/flawaetz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-07-16T17:23:02Z", "updated_at": "2021-08-04T18:55:14Z", "closed_at": "2021-08-04T18:55:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen attempting to launch triton on NVIDIA EGX the model server consumes 100% CPU and never finishes startup.\r\n\r\n**Triton Information**\r\n21.06\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nContainer \r\n\r\n**To Reproduce**\r\nI'm running on an EGX instance with two A100 GPUs on Ubuntu 20.04 with docker-ce 20.10.7.   CUDA 11.2 with driver 460.73.01.  \r\n\r\nAfter noticing my regular model repository was failing to launch I tried with the sample model repository provided by [fetch_model.sh](https://github.com/triton-inference-server/server/tree/main/docs/examples) which also exhibited the same behavior.  This is with or without the model configuration files present.\r\n\r\nContainer launch command:\r\n`docker run -it --entrypoint /bin/bash --runtime nvidia nvcr.io/nvidia/tritonserver:21.06-py3`\r\n\r\nTriton start command:\r\n`tritonserver --log-verbose 1 --model-repository /opt/tritonserver/model_repository`\r\n\r\nAfter the following output:\r\n<img width=\"1166\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1516944/125985016-c259311e-78ae-4db6-89a4-90717f9bbce7.png\">\r\n\r\nthe container just sits at 100% CPU indefinitely with no further output.  I straced the process and it looks like its busy mostly in userspace with some repeating system calls.\r\n\r\n**Expected behavior**\r\nTriton should finish starting.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3126/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3126/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3119", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3119/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3119/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3119/events", "html_url": "https://github.com/triton-inference-server/server/issues/3119", "id": 944896067, "node_id": "MDU6SXNzdWU5NDQ4OTYwNjc=", "number": 3119, "title": "Triton server crashes silently with invalid input data", "user": {"login": "NonStatic2014", "id": 9442504, "node_id": "MDQ6VXNlcjk0NDI1MDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/9442504?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NonStatic2014", "html_url": "https://github.com/NonStatic2014", "followers_url": "https://api.github.com/users/NonStatic2014/followers", "following_url": "https://api.github.com/users/NonStatic2014/following{/other_user}", "gists_url": "https://api.github.com/users/NonStatic2014/gists{/gist_id}", "starred_url": "https://api.github.com/users/NonStatic2014/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NonStatic2014/subscriptions", "organizations_url": "https://api.github.com/users/NonStatic2014/orgs", "repos_url": "https://api.github.com/users/NonStatic2014/repos", "events_url": "https://api.github.com/users/NonStatic2014/events{/privacy}", "received_events_url": "https://api.github.com/users/NonStatic2014/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "krishung5", "id": 43719498, "node_id": "MDQ6VXNlcjQzNzE5NDk4", "avatar_url": "https://avatars.githubusercontent.com/u/43719498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krishung5", "html_url": "https://github.com/krishung5", "followers_url": "https://api.github.com/users/krishung5/followers", "following_url": "https://api.github.com/users/krishung5/following{/other_user}", "gists_url": "https://api.github.com/users/krishung5/gists{/gist_id}", "starred_url": "https://api.github.com/users/krishung5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krishung5/subscriptions", "organizations_url": "https://api.github.com/users/krishung5/orgs", "repos_url": "https://api.github.com/users/krishung5/repos", "events_url": "https://api.github.com/users/krishung5/events{/privacy}", "received_events_url": "https://api.github.com/users/krishung5/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "krishung5", "id": 43719498, "node_id": "MDQ6VXNlcjQzNzE5NDk4", "avatar_url": "https://avatars.githubusercontent.com/u/43719498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krishung5", "html_url": "https://github.com/krishung5", "followers_url": "https://api.github.com/users/krishung5/followers", "following_url": "https://api.github.com/users/krishung5/following{/other_user}", "gists_url": "https://api.github.com/users/krishung5/gists{/gist_id}", "starred_url": "https://api.github.com/users/krishung5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krishung5/subscriptions", "organizations_url": "https://api.github.com/users/krishung5/orgs", "repos_url": "https://api.github.com/users/krishung5/repos", "events_url": "https://api.github.com/users/krishung5/events{/privacy}", "received_events_url": "https://api.github.com/users/krishung5/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-07-15T00:40:02Z", "updated_at": "2021-07-29T18:10:58Z", "closed_at": "2021-07-29T18:10:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nSending an invalid input json to the REST API interface can cause triton server crashes silently\r\n\r\n**Triton Information**\r\n21.05-py3 container pulled from nvidia, on a CPU-only machine\r\n\r\n**To Reproduce**\r\n1. Follow instruction [here](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple) to train the mnist_fashion model\r\n2. create a folder for the trained model and make sure layout is correct\r\n3. Run triton: `docker run --rm -p8000:8000 -p8001:8001 -p8002:8002 -v$(pwd)/model:/model nvcr.io/nvidia/tritonserver:21.05-py3 tritonserver --model-repository=/model --log-verbose=1 --strict-model-config=false`\r\n4. Command to send request: `curl -d @\"<filename>.json\" http://localhost:8000/v2/models/fashion/versions/1/infer -v`\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n* Trained Model: https://ufile.io/dde0s62m\r\n* Good input json: https://ufile.io/inpve8py\r\n* Bad input json (causing crash): https://ufile.io/0kfmjc2p\r\n\r\n**Expected behavior**\r\nError message when receive invalid input.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3119/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3119/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3103", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3103/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3103/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3103/events", "html_url": "https://github.com/triton-inference-server/server/issues/3103", "id": 940977954, "node_id": "MDU6SXNzdWU5NDA5Nzc5NTQ=", "number": 3103, "title": "Multiple copies of perf_analyzer cannot be run in parallel with sequence batching due to sequence id collisions", "user": {"login": "raimondasl", "id": 59739769, "node_id": "MDQ6VXNlcjU5NzM5NzY5", "avatar_url": "https://avatars.githubusercontent.com/u/59739769?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raimondasl", "html_url": "https://github.com/raimondasl", "followers_url": "https://api.github.com/users/raimondasl/followers", "following_url": "https://api.github.com/users/raimondasl/following{/other_user}", "gists_url": "https://api.github.com/users/raimondasl/gists{/gist_id}", "starred_url": "https://api.github.com/users/raimondasl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raimondasl/subscriptions", "organizations_url": "https://api.github.com/users/raimondasl/orgs", "repos_url": "https://api.github.com/users/raimondasl/repos", "events_url": "https://api.github.com/users/raimondasl/events{/privacy}", "received_events_url": "https://api.github.com/users/raimondasl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1079803573, "node_id": "MDU6TGFiZWwxMDc5ODAzNTcz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": {"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-07-09T18:22:09Z", "updated_at": "2021-11-11T19:51:03Z", "closed_at": "2021-11-11T19:50:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried to run two perf_analyzer copies from different machines to test the load on a sequence batching model.\r\nUnfortunately, it seems that perf_analyzer does not randomize sequence ids well and there are sequence id collisions.\r\nOn server:\r\n.....\r\nW0709 18:15:26.215562 584 sequence_batch_scheduler.cc:401] sequence 406 for model 'asr' has a conflict. The previous sequence did not end before this sequence start. Previous sequence will be terminated early.\r\nW0709 18:15:26.216838 584 sequence_batch_scheduler.cc:401] sequence 407 for model 'asr' has a conflict. The previous sequence did not end before this sequence start. Previous sequence will be terminated early.\r\nW0709 18:15:26.218117 584 sequence_batch_scheduler.cc:401] sequence 408 for model 'asr' has a conflict. The previous sequence did not end before this sequence start. Previous sequence will be terminated early.\r\n.....\r\n\r\nUltimately clients also terminate with:\r\n....\r\nThread [246] had error: inference request for sequence 1901 to model 'asr' must specify the START flag on the first request of the sequence\r\nThread [248] had error: inference request for sequence 1998 to model 'asr' must specify the START flag on the first request of the sequence\r\nThread [249] had error: inference request for sequence 1858 to model 'asr' must specify the START flag on the first request of the sequence\r\n.....\r\n\r\nCan the perf_analyzer sequence ids be randomized with a random seed to prevent sequence id collisions from multiple perf_analyzer copies running at the same time? Alternatively, could the perf_analyzer take the seed as an input.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3103/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3103/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3074", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3074/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3074/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3074/events", "html_url": "https://github.com/triton-inference-server/server/issues/3074", "id": 934616979, "node_id": "MDU6SXNzdWU5MzQ2MTY5Nzk=", "number": 3074, "title": "Python backend segfault with detectron2", "user": {"login": "bcvanmeurs", "id": 7422223, "node_id": "MDQ6VXNlcjc0MjIyMjM=", "avatar_url": "https://avatars.githubusercontent.com/u/7422223?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bcvanmeurs", "html_url": "https://github.com/bcvanmeurs", "followers_url": "https://api.github.com/users/bcvanmeurs/followers", "following_url": "https://api.github.com/users/bcvanmeurs/following{/other_user}", "gists_url": "https://api.github.com/users/bcvanmeurs/gists{/gist_id}", "starred_url": "https://api.github.com/users/bcvanmeurs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bcvanmeurs/subscriptions", "organizations_url": "https://api.github.com/users/bcvanmeurs/orgs", "repos_url": "https://api.github.com/users/bcvanmeurs/repos", "events_url": "https://api.github.com/users/bcvanmeurs/events{/privacy}", "received_events_url": "https://api.github.com/users/bcvanmeurs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2021-07-01T09:17:32Z", "updated_at": "2021-09-22T14:05:31Z", "closed_at": "2021-09-22T14:05:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI am trying to get a detectron2 model running in Triton.\r\nIn version 21.06 the server hangs on loading the model and displays the following error:\r\n\r\n```\r\ntriton_python_b[23212]: segfault at 7f51a3e2666d ip 00007f51a29ed7e4 sp 00007ffe0a569868 error 4 in libc-2.31.so[7f51a2884000+178000]\r\n\r\n# addr2line -p -a -f -e /usr/lib/x86_64-linux-gnu/libc-2.31.so 178000\r\n0x0000000000178000: __nss_database_lookup at ??:?\r\n```\r\n\r\nIn version 21.04 this error occured sometimes at inference time, there is no output and the server hangs.\r\n```\r\nI0701 09:00:13.631002 1 infer_request.cc:497] prepared: [0x0x7fb864002d90] request id: 42, model: mag, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 0, priority: 0, timeout (us): 0\r\noriginal inputs:\r\n[0x0x7fb864003248] input: INPUT0, type: UINT8, original shape: [7372,4146,3], batch + shape: [7372,4146,3], shape: [7372,4146,3]\r\noverride inputs:\r\ninputs:\r\n[0x0x7fb864003248] input: INPUT0, type: UINT8, original shape: [7372,4146,3], batch + shape: [7372,4146,3], shape: [7372,4146,3]\r\noriginal requested outputs:\r\nrequested outputs:\r\nOUTPUT0\r\n\r\nI0701 09:00:13.631087 1 python.cc:347] model mag, instance mag_0, executing 1 requests\r\n```\r\ndmesg shows\r\n```\r\ntriton_python_b[30191]: segfault at 7faf0dae966d ip 00007faf0c6b07e4 sp 00007ffe2c701488 error 4 in libc-2.31.so[7faf0c547000+178000]\r\n[ 5125.940257] Code: 7f 07 c5 fe 7f 4f 20 c5 fe 7f 54 17 e0 c5 fe 7f 5c 17 c0 c5 f8 77 c3 48 39 f7 0f 87 ab 00 00 00 0f 84 e5 fe ff ff c5 fe 6f 26 <c5> fe 6f 6c 16 e0 c5 fe 6f 74 16 c0 c5 fe 6f 7c 16 a0 c5 7e 6f 44\r\n```\r\nthis error occurs when using curl on the http endpoint as well as with go in grpc.\r\nIt does not seem to occur when using the python client or the perf_analyzer.\r\nAfter using the python client once, it works fine with curl/grpc.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nTried on 21.04 and 21.06\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nUsing the triton container with custom pip installs.\r\n```\r\nFROM nvcr.io/nvidia/tritonserver:21.04-py3 # or 21.06-py3\r\n\r\nRUN apt-get update && apt-get install -y python3-dev\r\nRUN python3 -m pip install --upgrade pip\r\nRUN python3 -m pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\r\nRUN python3 -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.8/index.html\r\nRUN python3 -m pip install opencv-contrib-python-headless\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\nmodel config:\r\n```\r\nname: \"mag\"\r\nbackend: \"python\"\r\n\r\ninput [\r\n  {\r\n    name: \"INPUT0\"\r\n    data_type: TYPE_UINT8\r\n    dims: [ -1 , -1, 3 ]\r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"OUTPUT0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, 4 ]\r\n  }\r\n]\r\n\r\ninstance_group [\r\n  {\r\n    count: 1\r\n    kind: KIND_GPU\r\n  }\r\n]\r\n```\r\n\r\nmodel.py file: https://gist.github.com/bcvanmeurs/053d4443c9669b74eca742c71eaa4d1d\r\n\r\nI can't share our actual model, but I can create a dummy or supply a pretrained one if necessary.\r\n\r\n**Expected behavior**\r\nA consistent inference result or a more clear error message at server startup (21.06).\r\n\r\nThanks in advance!\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3074/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3045", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3045/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3045/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3045/events", "html_url": "https://github.com/triton-inference-server/server/issues/3045", "id": 929181485, "node_id": "MDU6SXNzdWU5MjkxODE0ODU=", "number": 3045, "title": "Concurrent requests to multiple models cause NaN values in output", "user": {"login": "julienschuermans", "id": 14927054, "node_id": "MDQ6VXNlcjE0OTI3MDU0", "avatar_url": "https://avatars.githubusercontent.com/u/14927054?v=4", "gravatar_id": "", "url": "https://api.github.com/users/julienschuermans", "html_url": "https://github.com/julienschuermans", "followers_url": "https://api.github.com/users/julienschuermans/followers", "following_url": "https://api.github.com/users/julienschuermans/following{/other_user}", "gists_url": "https://api.github.com/users/julienschuermans/gists{/gist_id}", "starred_url": "https://api.github.com/users/julienschuermans/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/julienschuermans/subscriptions", "organizations_url": "https://api.github.com/users/julienschuermans/orgs", "repos_url": "https://api.github.com/users/julienschuermans/repos", "events_url": "https://api.github.com/users/julienschuermans/events{/privacy}", "received_events_url": "https://api.github.com/users/julienschuermans/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}, {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 16, "created_at": "2021-06-24T12:32:25Z", "updated_at": "2021-10-07T23:18:20Z", "closed_at": "2021-10-07T23:18:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI use Triton to host two TRT models: an object detector and a feature extractor. When both models are called to perform inference simultaneously (using the python API: `tritonclient.grpc.InferenceServerClient.infer(...)`) the feature extractor returns a numpy array containing NaN values.\r\n\r\nThis does not happen with multiple concurrent requests to any single model.\r\n\r\n**Triton Information**\r\ndocker image: `nvcr.io/nvidia/tritonserver:20.12-py3`\r\nserver_version: 2.6.0\r\ntritonclient==2.3.0\r\n\r\n**To Reproduce**\r\n\r\n1. load triton with two models: `osnet_x0_25_dyn` and `yolov4_32`\r\n\r\nI'm using docker-compose:\r\n\r\n```\r\nversion: '3.7'\r\n\r\nservices:\r\n\r\n  triton:\r\n    hostname: triton\r\n    image: nvcr.io/nvidia/tritonserver:20.12-py3\r\n    command: tritonserver --model-repository=/models --strict-model-config=false --grpc-infer-allocation-pool-size=4 --pinned-memory-pool-byte-size=64000000 --log-verbose 0\r\n    ports:\r\n      - '8001:8001'\r\n    volumes:\r\n      - '/home/julien/modelrepo/models:/models'\r\n      - '/home/julien/modelrepo/plugins:/plugins'\r\n    environment:\r\n      - 'LD_PRELOAD=/plugins/liblayerplugin.so'\r\n    ulimits:\r\n      stack:\r\n        soft: 67108864\r\n        hard: 67108864\r\n    shm_size: '1gb'\r\n    deploy:\r\n      resources:\r\n        reservations:\r\n          devices:\r\n            - driver: nvidia\r\n              device_ids: [ 1 ]\r\n              capabilities: [ gpu ]\r\n```\r\n\r\n2. run both scripts below at the same time (in two different terminal windows).\r\n\r\n`feature_extractor.py`:\r\n\r\n```python\r\nimport numpy as np\r\nimport tritonclient.grpc as grpcclient\r\nfrom multiprocessing import Pool\r\n\r\ndef send_feature_extractor_request(i):\r\n\r\n    buffer = np.random.rand(16, 3, 256, 128).astype(np.float32)\r\n    triton_client = grpcclient.InferenceServerClient(url='localhost:8001')\r\n    res = []\r\n    inputs = [grpcclient.InferInput('input', buffer.shape, 'FP32')]\r\n    outputs = [grpcclient.InferRequestedOutput('output')]\r\n    inputs[0].set_data_from_numpy(buffer)\r\n    for _ in range(100):\r\n        result = triton_client.infer(\r\n            'osnet_x0_25_dyn', inputs=inputs, outputs=outputs)\r\n        output = result.as_numpy('output')\r\n        res.append(np.isnan(np.sum(output)))\r\n\r\n    return {i: any(res)}\r\n\r\nif __name__ == '__main__':\r\n    N = 2\r\n    with Pool(N) as p:\r\n        print(p.map(send_feature_extractor_request, np.arange(0, N).tolist()))\r\n```\r\n\r\nand `detector.py`:\r\n\r\n```python\r\nimport numpy as np\r\nimport tritonclient.grpc as grpcclient\r\nfrom multiprocessing import Pool\r\n\r\ndef send_detector_request(i):\r\n\r\n    buffer = np.random.rand(8, 3, 512, 512).astype(np.float32)\r\n    triton_client = grpcclient.InferenceServerClient(url='localhost:8001')\r\n    res = []\r\n    inputs = [grpcclient.InferInput('data', buffer.shape, 'FP32')]\r\n    outputs = [grpcclient.InferRequestedOutput('prob')]\r\n    inputs[0].set_data_from_numpy(buffer)\r\n    for _ in range(100):\r\n        result = triton_client.infer(\r\n            'yolov4_32', inputs=inputs, outputs=outputs)\r\n        output = result.as_numpy('prob')\r\n        res.append(np.isnan(np.sum(output)))\r\n\r\n    return {i: any(res)}\r\n\r\nif __name__ == '__main__':\r\n    N = 2\r\n    with Pool(N) as p:\r\n        print(p.map(send_detector_request, np.arange(0, N).tolist()))\r\n```\r\n\r\nYou will see that the `feature_extractor.py` script outputs: `[{0: True}, {1: True}]`. This means both subprocesses have encountered NaN values in their responses. \r\n\r\n**Expected behavior**\r\n\r\nI expect both models to return correct values, even when multiple clients send inference requests simultaneously.\r\nThere should never be a NaN in `output`, which means the scripts should return `[{0: False}, {1: False}]`.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3045/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3045/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3033", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3033/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3033/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3033/events", "html_url": "https://github.com/triton-inference-server/server/issues/3033", "id": 927021083, "node_id": "MDU6SXNzdWU5MjcwMjEwODM=", "number": 3033, "title": "cpu memory increase constantly when serving model with triton-inference-server", "user": {"login": "luckycallor", "id": 11307624, "node_id": "MDQ6VXNlcjExMzA3NjI0", "avatar_url": "https://avatars.githubusercontent.com/u/11307624?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luckycallor", "html_url": "https://github.com/luckycallor", "followers_url": "https://api.github.com/users/luckycallor/followers", "following_url": "https://api.github.com/users/luckycallor/following{/other_user}", "gists_url": "https://api.github.com/users/luckycallor/gists{/gist_id}", "starred_url": "https://api.github.com/users/luckycallor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luckycallor/subscriptions", "organizations_url": "https://api.github.com/users/luckycallor/orgs", "repos_url": "https://api.github.com/users/luckycallor/repos", "events_url": "https://api.github.com/users/luckycallor/events{/privacy}", "received_events_url": "https://api.github.com/users/luckycallor/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2021-06-22T09:20:31Z", "updated_at": "2022-04-01T00:44:51Z", "closed_at": "2021-07-09T22:49:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI am serving tensorflow savedmodel with triton-inference-server. While the server recieves requests, the cpu memory used creases constantly. The increasing speed seems related to the number of requests. If you request the server for long time, like several days, the increase is obvious.\r\n\r\n**Triton Information**\r\nI was using triton 21.03 from https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags \r\nand I tried triton 21.05, It seems having the same problem.\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nI was using the Triton container from https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/tags \r\n\r\n**To Reproduce**\r\njust serve a tensorflow savedmodel with triton inference server, and request the server constantly. After several hours, you can observe the cpu memory increases.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\nI was using the efficientnet-b0 model. https://github.com/qubvel/efficientnet\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nI expect the reason is found and the bug is fixed.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3033/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3029", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3029/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3029/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3029/events", "html_url": "https://github.com/triton-inference-server/server/issues/3029", "id": 926670470, "node_id": "MDU6SXNzdWU5MjY2NzA0NzA=", "number": 3029, "title": "Huge inference speed difference when loading a model from S3", "user": {"login": "ioangatop", "id": 41287808, "node_id": "MDQ6VXNlcjQxMjg3ODA4", "avatar_url": "https://avatars.githubusercontent.com/u/41287808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ioangatop", "html_url": "https://github.com/ioangatop", "followers_url": "https://api.github.com/users/ioangatop/followers", "following_url": "https://api.github.com/users/ioangatop/following{/other_user}", "gists_url": "https://api.github.com/users/ioangatop/gists{/gist_id}", "starred_url": "https://api.github.com/users/ioangatop/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ioangatop/subscriptions", "organizations_url": "https://api.github.com/users/ioangatop/orgs", "repos_url": "https://api.github.com/users/ioangatop/repos", "events_url": "https://api.github.com/users/ioangatop/events{/privacy}", "received_events_url": "https://api.github.com/users/ioangatop/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2021-06-21T22:56:21Z", "updated_at": "2022-02-24T16:37:01Z", "closed_at": "2022-02-24T16:37:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi all! I found a huge difference on RPS (Requests per Second) when loading a model from a MINIO S3 bucket.\r\n\r\nSpecifically, when mounting a model from local path I get `40 RPS` with the following command:\r\n```\r\ndocker run --rm -t -i --gpus \"0\" \\\r\n    -p \"$TRITON_HTTP_PORT:$TRITON_HTTP_PORT\" \\\r\n    -p \"$TRITON_gRPC_PORT:$TRITON_gRPC_PORT\" \\\r\n    -p \"$TRITON_METRICS_PORT:$TRITON_METRICS_PORT\" \\\r\n    -v \"$TRITON_MODEL_DIR:/models\" \\\r\n    -e CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES \\\r\n    $TRITON_DOCKER_IMAGE tritonserver \\\r\n        --http-port \"$TRITON_HTTP_PORT\" \\\r\n        --grpc-port \"$TRITON_gRPC_PORT\" \\\r\n        --metrics-port \"$TRITON_METRICS_PORT\" \\\r\n        --model-repository \"/models\" \\\r\n        --log-verbose \"0\"\r\n```\r\n\r\nBut when I point the `model-repository` flag directly to my MINIO instance I get `4 RPS`, using the following:\r\n```\r\ndocker run --rm -t -i --gpus \"0\" \\\r\n    -p \"$TRITON_HTTP_PORT:$TRITON_HTTP_PORT\" \\\r\n    -p \"$TRITON_gRPC_PORT:$TRITON_gRPC_PORT\" \\\r\n    -p \"$TRITON_METRICS_PORT:$TRITON_METRICS_PORT\" \\\r\n    -e CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES \\\r\n    -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\r\n    -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\r\n    $TRITON_DOCKER_IMAGE tritonserver \\\r\n        --http-port \"$TRITON_HTTP_PORT\" \\\r\n        --grpc-port \"$TRITON_gRPC_PORT\" \\\r\n        --metrics-port \"$TRITON_METRICS_PORT\" \\\r\n        --model-repository \"$TRITON_MINIO_BUCKET\" \\\r\n        --log-verbose \"0\"\r\n```\r\n\r\nCould this possibly be resolved by using a special configuration or its probably an internal bug?\r\n\r\nThanks all!", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3029/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3026", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/3026/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/3026/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/3026/events", "html_url": "https://github.com/triton-inference-server/server/issues/3026", "id": 924632090, "node_id": "MDU6SXNzdWU5MjQ2MzIwOTA=", "number": 3026, "title": "unexpected platform type tensorflow_savedmodel for rul", "user": {"login": "GowthamKudupudi", "id": 57786122, "node_id": "MDQ6VXNlcjU3Nzg2MTIy", "avatar_url": "https://avatars.githubusercontent.com/u/57786122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GowthamKudupudi", "html_url": "https://github.com/GowthamKudupudi", "followers_url": "https://api.github.com/users/GowthamKudupudi/followers", "following_url": "https://api.github.com/users/GowthamKudupudi/following{/other_user}", "gists_url": "https://api.github.com/users/GowthamKudupudi/gists{/gist_id}", "starred_url": "https://api.github.com/users/GowthamKudupudi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GowthamKudupudi/subscriptions", "organizations_url": "https://api.github.com/users/GowthamKudupudi/orgs", "repos_url": "https://api.github.com/users/GowthamKudupudi/repos", "events_url": "https://api.github.com/users/GowthamKudupudi/events{/privacy}", "received_events_url": "https://api.github.com/users/GowthamKudupudi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-06-18T07:37:29Z", "updated_at": "2021-07-15T18:14:03Z", "closed_at": "2021-07-15T18:13:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI have built and installed the `server` and `tensorflow_backend`. Below is the output when I run the `tritonserver`\r\n```\r\n$ tritonserver --model-repository=`pwd`/enabled_models --log-warning 1 --log-error 1\r\nI0618 07:08:13.449667 8390 metrics.cc:228] Collecting metrics for GPU 0: NVIDIA Tesla T4\r\n2021-06-18 07:08:13.639625: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nI0618 07:08:13.682413 8390 tensorflow.cc:2165] TRITONBACKEND_Initialize: tensorflow\r\nI0618 07:08:13.682445 8390 tensorflow.cc:2175] Triton TRITONBACKEND API version: 1.0\r\nI0618 07:08:13.682451 8390 tensorflow.cc:2181] 'tensorflow' TRITONBACKEND API version: 1.0\r\nI0618 07:08:13.682455 8390 tensorflow.cc:2205] backend configuration:\r\n{}\r\nI0618 07:08:13.957906 8390 pinned_memory_manager.cc:206] Pinned memory pool is created at '0x7f2476000000' with size 268435456\r\nI0618 07:08:13.958445 8390 cuda_memory_manager.cc:103] CUDA memory pool is created on device 0 with size 67108864\r\nE0618 07:08:13.962236 8390 model_repository_manager.cc:1916] Poll failed for model directory 'rul': unexpected platform type tensorflow_savedmodel for rul\r\nI0618 07:08:13.962387 8390 server.cc:504]\r\n+------------------+------+\r\n| Repository Agent | Path |\r\n+------------------+------+\r\n+------------------+------+\r\n\r\nI0618 07:08:13.962418 8390 server.cc:543]\r\n+------------+-------------------------------------------------------+--------+\r\n| Backend    | Path                                                  | Config |\r\n+------------+-------------------------------------------------------+--------+\r\n| tensorrt   | <built-in>                                            | {}     |\r\n| tensorflow | /opt/tritonserver/backends/tensorflow1/libtriton_tens | {}     |\r\n|            | orflow1.so                                            |        |\r\n+------------+-------------------------------------------------------+--------+\r\n\r\nI0618 07:08:13.962438 8390 server.cc:586]\r\n+-------+---------+--------+\r\n| Model | Version | Status |\r\n+-------+---------+--------+\r\n+-------+---------+--------+\r\n\r\nI0618 07:08:13.962503 8390 tritonserver.cc:1659]\r\n+----------------------------------+------------------------------------------+\r\n| Option                           | Value                                    |\r\n+----------------------------------+------------------------------------------+\r\n| server_id                        | triton                                   |\r\n| server_version                   | 2.10.0                                   |\r\n| server_extensions                | classification sequence model_repository |\r\n|                                  |  model_repository(unload_dependents) sch |\r\n|                                  | edule_policy model_configuration system_ |\r\n|                                  | shared_memory cuda_shared_memory binary_ |\r\n|                                  | tensor_data statistics                   |\r\n| model_repository_path[0]         | /home/gowtham/server/docs/examples/enabl |\r\n|                                  | ed_models                                |\r\n| model_control_mode               | MODE_NONE                                |\r\n| strict_model_config              | 1                                        |\r\n| pinned_memory_pool_byte_size     | 268435456                                |\r\n| cuda_memory_pool_byte_size{0}    | 67108864                                 |\r\n| min_supported_compute_capability | 6.0                                      |\r\n| strict_readiness                 | 1                                        |\r\n| exit_timeout                     | 30                                       |\r\n+----------------------------------+------------------------------------------+\r\n\r\nI0618 07:08:13.962551 8390 server.cc:234] Waiting for in-flight requests to complete.\r\nI0618 07:08:13.962578 8390 server.cc:249] Timeout 30: Found 0 live models and 0 in-flight non-inference requests\r\nerror: creating server: Internal - failed to load all models\r\n```\r\nmodel load fails with `unexpected platform type tensorflow_savedmodel for rul`\r\n\r\n**Triton Information**\r\nr21.05\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nI build it myself\r\n\r\n**To Reproduce**\r\nbuild the `server` with\r\n`./build.py --no-container-build --target-platform ubuntu --cmake-dir=$(pwd)/build --build-dir=`pwd`/citritonbuild --enable-logging --enable-stats --enable-tracing --enable-metrics --enable-gpu-metrics --enable-gpu --filesystem=gcs --filesystem=azure_storage --filesystem=s3 --endpoint=http --endpoint=grpc --repo-tag=common:r21.05 --repo-tag=core:r21.05 --repo-tag=backend:r21.05 --repo-tag=thirdparty:r21.05 --backend=ensemble --backend=tensorrt --backend=python --backend=identity --backend=repeat --backend=square --repoagent=checksum\r\n`\r\nbuild the `tensorflow_backend` with\r\n`cmake -DCMAKE_INSTALL_PREFIX:PATH=`pwd`/install -DTRITON_TENSORFLOW_VERSION=1 -DTRITON_TENSORFLOW_DOCKER_IMAGE=\"nvcr.io/nvidia/tensorflow:21.05-tf1-py3\" -DTRITON_BACKEND_REPO_TAG=r21.05 -DTRITON_CORE_REPO_TAG=r21.05 -DTRITON_COMMON_REPO_TAG=r21.05 ..\r\n`\r\ncopy install files to `/opt/tritonserver`\r\nadd bin and lib to path and run the tritonserver as mentioned in the description above\r\n\r\n\r\n**Expected behavior**\r\nhand built `tritonserver` should be able to recognize `tensorflow_savedmodel` platform just like the same model is loaded by the pre built container 21.05 without a problem.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/3026/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/3026/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2985", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2985/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2985/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2985/events", "html_url": "https://github.com/triton-inference-server/server/issues/2985", "id": 915759705, "node_id": "MDU6SXNzdWU5MTU3NTk3MDU=", "number": 2985, "title": "Triton (+ model.pt) silently exits", "user": {"login": "scamianbas", "id": 3215795, "node_id": "MDQ6VXNlcjMyMTU3OTU=", "avatar_url": "https://avatars.githubusercontent.com/u/3215795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scamianbas", "html_url": "https://github.com/scamianbas", "followers_url": "https://api.github.com/users/scamianbas/followers", "following_url": "https://api.github.com/users/scamianbas/following{/other_user}", "gists_url": "https://api.github.com/users/scamianbas/gists{/gist_id}", "starred_url": "https://api.github.com/users/scamianbas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scamianbas/subscriptions", "organizations_url": "https://api.github.com/users/scamianbas/orgs", "repos_url": "https://api.github.com/users/scamianbas/repos", "events_url": "https://api.github.com/users/scamianbas/events{/privacy}", "received_events_url": "https://api.github.com/users/scamianbas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2021-06-09T04:26:24Z", "updated_at": "2021-06-29T16:55:24Z", "closed_at": "2021-06-11T17:16:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI modified fasterrcnn_resnet50_fpn in order to make it accept a single image (and not a list of images) and make it return a single tensor (and not a dict of boxes, scores, labels, ....) which is actually the label of the category detected with the best score. Then I built a model.pt TorchScript model out of that works fine on my workstation but fails on Triton.\r\n\r\n**Triton Information**\r\n21.05, Triton container\r\n\r\n**To Reproduce**\r\nmkdir server/docs/examples/model_repository/fasterrcnn_resnet50_fpn_pytorch/\r\nmkdir server/docs/examples/model_repository/fasterrcnn_resnet50_fpn_pytorch/1\r\n\r\nput model.pt (cannot attach, too big but I can put it on some filesharing site if needed) under fasterrcnn_resnet50_fpn_pytorch/1/\r\nput config.pbtxt (see below) under fasterrcnn_resnet50_fpn_pytorch/\r\nname: \"fasterrcnn_resnet50_fpn_pytorch\"\r\nbackend: \"pytorch\"\r\nmax_batch_size : 0\r\ninput [\r\n  {\r\n    name: \"input__0\"\r\n    data_type: TYPE_FP32\r\n    format: FORMAT_NCHW\r\n    dims: [ 3, 224, 224 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"output__0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 1 ]\r\n    label_filename: \"coco_labels.txt\"\r\n  }\r\n]\r\n\r\nput coco_labels.txt (see file attached) under fasterrcnn_resnet50_fpn_pytorch/\r\n\r\nthen\r\n\r\ndocker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v /home/mirko/server/docs/examples/model_repository:/models nvcr.io/nvidia/tritonserver:21.05-py3 tritonserver --model-repository=/models\r\n\r\ndocker run -it --rm --net=host nvcr.io/nvidia/tritonserver:21.05-py3-sdk\r\n(then inside this container)\r\nwget https://github.com/bytedance/triton-inference-server/raw/master/qa/images/car.jpg\r\n/workspace/install/python/image_client.py -m fasterrcnn_resnet50_fpn_pytorch -c 1 -s INCEPTION car.jpg\r\n/workspace# /workspace/install/python/image_client.py -m fasterrcnn_resnet50_fpn_pytorch -c 1 -s INCEPTION car.jpg\r\nTraceback (most recent call last):\r\n  File \"/workspace/install/python/image_client.py\", line 437, in <module>\r\n    triton_client.infer(FLAGS.model_name,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tritonclient/http/__init__.py\", line 1149, in infer\r\n    response = self._post(request_uri=request_uri,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tritonclient/http/__init__.py\", line 304, in _post\r\n    response = self._client_stub.post(request_uri=request_uri,\r\n  File \"/usr/local/lib/python3.8/dist-packages/geventhttpclient/client.py\", line 272, in post\r\n    return self.request(METHOD_POST, request_uri, body=body, headers=headers)\r\n  File \"/usr/local/lib/python3.8/dist-packages/geventhttpclient/client.py\", line 250, in request\r\n    raise e\r\n  File \"/usr/local/lib/python3.8/dist-packages/geventhttpclient/client.py\", line 231, in request\r\n    sock.sendall(_request + body)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gevent/_socketcommon.py\", line 699, in sendall\r\n    return _sendall(self, data_memory, flags)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gevent/_socketcommon.py\", line 409, in _sendall\r\n    timeleft = __send_chunk(socket, chunk, flags, timeleft, end)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gevent/_socketcommon.py\", line 349, in __send_chunk\r\n    data_sent += socket.send(chunk, flags, timeout=timeleft)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gevent/_socketcommon.py\", line 722, in send\r\n    return self._sock.send(data, flags)\r\nBrokenPipeError: [Errno 32] Broken pipe\r\n\r\nat the same time triton server container silently exited:\r\nI0609 04:10:51.442854 1 grpc_server.cc:4062] Started GRPCInferenceService at 0.0.0.0:8001\r\nI0609 04:10:51.454597 1 http_server.cc:2887] Started HTTPService at 0.0.0.0:8000\r\nI0609 04:10:51.500880 1 http_server.cc:2906] Started Metrics Service at 0.0.0.0:8002\r\n$ \r\n\r\n**Expected behavior**\r\nTriton server container should not silently exit and inference should at least return the tensor (3, category number)\r\nHere's how model.pt works on my workstation:\r\n\r\nimport torch\r\nimport torchvision\r\nfrom PIL import Image\r\nim = Image.open(\"car.jpg\")\r\nimt = torchvision.transforms.ToTensor()(im)\r\nm = torch.jit.load('model.pt')\r\nm = m.eval()\r\nx = m(imt)\r\nprint(x)\r\ntensor(3.)\r\n\r\n\r\n\r\n\r\n\r\n[coco_labels.txt](https://github.com/triton-inference-server/server/files/6621048/coco_labels.txt)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2985/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2971", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2971/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2971/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2971/events", "html_url": "https://github.com/triton-inference-server/server/issues/2971", "id": 913694822, "node_id": "MDU6SXNzdWU5MTM2OTQ4MjI=", "number": 2971, "title": "Python backend returning wrong results converting from NHWC to NCHW", "user": {"login": "sorny92", "id": 8601395, "node_id": "MDQ6VXNlcjg2MDEzOTU=", "avatar_url": "https://avatars.githubusercontent.com/u/8601395?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sorny92", "html_url": "https://github.com/sorny92", "followers_url": "https://api.github.com/users/sorny92/followers", "following_url": "https://api.github.com/users/sorny92/following{/other_user}", "gists_url": "https://api.github.com/users/sorny92/gists{/gist_id}", "starred_url": "https://api.github.com/users/sorny92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sorny92/subscriptions", "organizations_url": "https://api.github.com/users/sorny92/orgs", "repos_url": "https://api.github.com/users/sorny92/repos", "events_url": "https://api.github.com/users/sorny92/events{/privacy}", "received_events_url": "https://api.github.com/users/sorny92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2021-06-07T15:47:17Z", "updated_at": "2021-06-16T17:41:42Z", "closed_at": "2021-06-16T17:41:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI've created a model with python backend doing simple preprocessing (substract and transpose)\r\n\r\nModel.pbtxt looks like this:\r\n```python\r\nname: \"face_detector_preprocess\"\r\nbackend: \"python\"\r\n\r\ninput [\r\n  {\r\n    name: \"image\"\r\n    data_type: TYPE_UINT8\r\n    dims: [ -1, 320, 320, 3]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"preprocessed_image\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1, 3, 320, 320 ]\r\n  }\r\n]\r\ninstance_group [{\r\n    count: 1\r\n    kind: KIND_CPU }]\r\n```\r\nThe execute method in the model.py looks like this:\r\n```python\r\n    def execute(self, requests):\r\n        output_dtype = self.output_dtype\r\n        responses = []\r\n        for request in requests:\r\n            input_ = pb_utils.get_input_tensor_by_name(request, \"image\")\r\n            in_img = torch.from_numpy(input_.as_numpy())\r\n            in_img = torch.sub(in_img, torch.Tensor([104, 117, 123]))\r\n            in_img = in_img.permute(0,3,1,2)\r\n            out = in_img.numpy().astype(dtype=np.float32)\r\n            output = pb_utils.Tensor(\"preprocessed_image\", out.astype(output_dtype))\r\n            inference_response = pb_utils.InferenceResponse(output_tensors=[output])\r\n            responses.append(inference_response)\r\n        return responses\r\n```\r\nRunning over an example image I get a result that looks like this:\r\n![image](https://user-images.githubusercontent.com/8601395/121043875-4ffe3580-c7b5-11eb-9747-9d0baf555bb1.png)\r\n\r\nDoing this same step outside of Triton gives me a result as expected:\r\n![image](https://user-images.githubusercontent.com/8601395/121046962-6953b180-c7b6-11eb-9920-d9ab7fa6b762.png)\r\n\r\n\r\nThe problem I find is in the permute step I send my image as NWHC and I want to return them as NCHW so then I can just chain this as an ensemble model (the next model uses NCHW). \r\nIf I comment:\r\n```            in_img = in_img.permute(0,3,1,2)```\r\nThe image is OK but in NWHC order so I don't understand what could be happening because doing the same without triton server works fine.\r\n\r\n**Triton Information**\r\n21.04\r\n\r\nI'm using a Triton container which only adds numpy and torch:\r\n```Dockerfile\r\nFROM nvcr.io/nvidia/tritonserver:21.04-py3\r\n\r\nRUN pip install numpy torch\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2971/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2971/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2968", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2968/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2968/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2968/events", "html_url": "https://github.com/triton-inference-server/server/issues/2968", "id": 913125220, "node_id": "MDU6SXNzdWU5MTMxMjUyMjA=", "number": 2968, "title": "rapidjson.JSONDecodeError Error in Running Python Model", "user": {"login": "chandrameenamohan", "id": 20400184, "node_id": "MDQ6VXNlcjIwNDAwMTg0", "avatar_url": "https://avatars.githubusercontent.com/u/20400184?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chandrameenamohan", "html_url": "https://github.com/chandrameenamohan", "followers_url": "https://api.github.com/users/chandrameenamohan/followers", "following_url": "https://api.github.com/users/chandrameenamohan/following{/other_user}", "gists_url": "https://api.github.com/users/chandrameenamohan/gists{/gist_id}", "starred_url": "https://api.github.com/users/chandrameenamohan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chandrameenamohan/subscriptions", "organizations_url": "https://api.github.com/users/chandrameenamohan/orgs", "repos_url": "https://api.github.com/users/chandrameenamohan/repos", "events_url": "https://api.github.com/users/chandrameenamohan/events{/privacy}", "received_events_url": "https://api.github.com/users/chandrameenamohan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-06-07T05:52:14Z", "updated_at": "2021-06-10T14:08:39Z", "closed_at": "2021-06-10T14:08:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nA clear and concise description of what the bug is.\r\nI have build the triton server from the source. \r\nI have only built the python backend model as my I want to run python model. I have used this python example model: https://github.com/triton-inference-server/python_backend/tree/main/examples/add_sub. \r\nI am getting this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"client.py\", line 31, in <module>\r\n    outputs=outputs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tritonclient/http/__init__.py\", line 1114, in infer\r\n    _raise_if_error(response)\r\n  File \"/usr/local/lib/python3.6/site-packages/tritonclient/http/__init__.py\", line 61, in _raise_if_error\r\n    error = _get_error(response)\r\n  File \"/usr/local/lib/python3.6/site-packages/tritonclient/http/__init__.py\", line 50, in _get_error\r\n    error_response = json.loads(response.read())\r\nrapidjson.JSONDecodeError: Parse error at offset 0: The document is empty.\r\n```\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n21.04\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nYes. I build it myself.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nI did not use ubuntu to build it.  I have build it using Oracle Linux Server 7.9.\r\n` ./build.py --cmake-dir=/home/server/build --build-dir=/tmp/citritonbuild --no-container-build --endpoint=http --endpoint=grpc --repo-tag=common:r21.04 --repo-tag=core:r21.04 --repo-tag=backend:r21.04 --repo-tag=thirdparty:r21.04 --backend=python:r21.04 --enable-logging --enable-stats --enable-tracing`\r\n\r\nI tried to debug by adding logs in httpclient side:\r\n\r\n```\r\nRequest uri v2/models/add_sub/infer\r\nBody  b'{\"id\":\"1\",\"inputs\":[{\"name\":\"INPUT0\",\"shape\":[4],\"datatype\":\"FP32\",\"parameters\":{\"binary_data_size\":16}},{\"name\":\"INPUT1\",\"shape\":[4],\"datatype\":\"FP32\",\"parameters\":{\"binary_data_size\":16}}],\"outputs\":[{\"name\":\"OUTPUT0\",\"parameters\":{\"binary_data\":true}},{\"name\":\"OUTPUT1\",\"parameters\":{\"binary_data\":true}}]}Q\\xf5\\x96>X6Y?\\x1a\\x08I?\\x9e$\\x15?z\\x0b\\x88>R\\xba\\t?\\xb68h?\\xb9]Y>'\r\nPOST /v2/models/add_sub/infer, headers {'Inference-Header-Content-Length': 309}\r\nb'{\"id\":\"1\",\"inputs\":[{\"name\":\"INPUT0\",\"shape\":[4],\"datatype\":\"FP32\",\"parameters\":{\"binary_data_size\":16}},{\"name\":\"INPUT1\",\"shape\":[4],\"datatype\":\"FP32\",\"parameters\":{\"binary_data_size\":16}}],\"outputs\":[{\"name\":\"OUTPUT0\",\"parameters\":{\"binary_data\":true}},{\"name\":\"OUTPUT1\",\"parameters\":{\"binary_data\":true}}]}Q\\xf5\\x96>X6Y?\\x1a\\x08I?\\x9e$\\x15?z\\x0b\\x88>R\\xba\\t?\\xb68h?\\xb9]Y>'\r\nHeaders {'Inference-Header-Content-Length': 309}\r\nI0606 10:07:12.153984 65495 http_server.cc:1229] HTTP request: 2 /v2/models/add_sub/infer\r\n<HTTPSocketPoolResponse status=405 headers={'content-length': '0', 'content-type': 'text/plain'}>\r\n```\r\n \r\nI see that client made the call to server for inference but not sure what happened afterwards. \r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\nPython backend model: https://github.com/triton-inference-server/python_backend/tree/main/examples/add_sub\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2968/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2966", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2966/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2966/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2966/events", "html_url": "https://github.com/triton-inference-server/server/issues/2966", "id": 912318120, "node_id": "MDU6SXNzdWU5MTIzMTgxMjA=", "number": 2966, "title": "Failed to determine modification time of model on Azure Storage when starting up triton server after update to release 21.04 ", "user": {"login": "tony-asus", "id": 56814050, "node_id": "MDQ6VXNlcjU2ODE0MDUw", "avatar_url": "https://avatars.githubusercontent.com/u/56814050?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tony-asus", "html_url": "https://github.com/tony-asus", "followers_url": "https://api.github.com/users/tony-asus/followers", "following_url": "https://api.github.com/users/tony-asus/following{/other_user}", "gists_url": "https://api.github.com/users/tony-asus/gists{/gist_id}", "starred_url": "https://api.github.com/users/tony-asus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tony-asus/subscriptions", "organizations_url": "https://api.github.com/users/tony-asus/orgs", "repos_url": "https://api.github.com/users/tony-asus/repos", "events_url": "https://api.github.com/users/tony-asus/events{/privacy}", "received_events_url": "https://api.github.com/users/tony-asus/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2021-06-05T15:44:37Z", "updated_at": "2021-09-01T01:23:50Z", "closed_at": "2021-07-13T17:17:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nI put triton model repository on my Azure storage account: \"--model-repository=as://container/path/to/model\". And when trito server starts up, it shows some errors on top:\r\n\r\nI0605 15:18:26.273583 1 metrics.cc:228] Collecting metrics for GPU 0: Tesla T4\r\nI0605 15:18:29.035544 1 libtorch.cc:932] TRITONBACKEND_Initialize: pytorch\r\nI0605 15:18:29.035594 1 libtorch.cc:942] Triton TRITONBACKEND API version: 1.0\r\nI0605 15:18:29.035599 1 libtorch.cc:948] 'pytorch' TRITONBACKEND API version: 1.0\r\n2021-06-05 15:18:30.134045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nI0605 15:18:31.265063 1 tensorflow.cc:2165] TRITONBACKEND_Initialize: tensorflow\r\nI0605 15:18:31.265094 1 tensorflow.cc:2175] Triton TRITONBACKEND API version: 1.0\r\nI0605 15:18:31.265098 1 tensorflow.cc:2181] 'tensorflow' TRITONBACKEND API version: 1.0\r\nI0605 15:18:31.265101 1 tensorflow.cc:2205] backend configuration:\r\n{}\r\nI0605 15:18:31.419452 1 onnxruntime.cc:1828] TRITONBACKEND_Initialize: onnxruntime\r\nI0605 15:18:31.419492 1 onnxruntime.cc:1838] Triton TRITONBACKEND API version: 1.0\r\nI0605 15:18:31.419496 1 onnxruntime.cc:1844] 'onnxruntime' TRITONBACKEND API version: 1.0\r\nI0605 15:18:31.681421 1 openvino.cc:1168] TRITONBACKEND_Initialize: openvino\r\nI0605 15:18:31.681451 1 openvino.cc:1178] Triton TRITONBACKEND API version: 1.0\r\nI0605 15:18:31.681456 1 openvino.cc:1184] 'openvino' TRITONBACKEND API version: 1.0\r\nI0605 15:18:31.955195 1 pinned_memory_manager.cc:206] Pinned memory pool is created at '0x7f9700000000' with size 268435456\r\nI0605 15:18:31.960663 1 cuda_memory_manager.cc:103] CUDA memory pool is created on device 0 with size 67108864\r\nE0605 15:18:32.583878 1 model_repository_manager.cc:286] **Failed to determine modification time for 'as://tonytesttritonaccount/tony-model-repository/some_model': Internal: Unable to get blob property for file at as://tonytesttritonaccount/tony-model-repository/some_model**\r\n\r\nLater when I wanted to reload model, it showed this error again, and had 8 secs downtime.\r\n\r\nI0605 04:55:36.372867 1 model_repository_manager.cc:1099] unloading: some_model:1\r\nI0605 04:55:36.373008 1 libtorch.cc:1055] TRITONBACKEND_ModelInstanceFinalize: delete instance state\r\nI0605 04:55:36.382971 1 libtorch.cc:1004] TRITONBACKEND_ModelFinalize: delete model state\r\nI0605 04:55:36.386443 1 model_repository_manager.cc:1223] successfully unloaded 'some_model' version 1\r\nE0605 04:55:44.107722 1 model_repository_manager.cc:298] **Failed to determine modification time for 'as://tonytesttritonaccount/tony-model-repository/some_model': Internal: Unable to get blob property for file at as://tonytesttritonaccount/tony-model-repository/some_model**\r\nI0605 04:55:44.262102 1 model_repository_manager.cc:1066] loading: some_model:1\r\nI0605 04:55:45.522576 1 libtorch.cc:981] TRITONBACKEND_ModelInitialize: some_model (version 1)\r\nW0605 04:55:45.522943 1 libtorch.cc:173] skipping model configuration auto-complete for 'some_model': not supported for pytorch backend\r\nI0605 04:55:45.523439 1 libtorch.cc:1022] TRITONBACKEND_ModelInstanceInitialize: some_model (device 0)\r\nI0605 04:55:45.775833 1 model_repository_manager.cc:1240] successfully loaded 'some_model' version 1\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nnvcr.io/nvidia/tritonserver:21.04-py3 or nvcr.io/nvidia/tritonserver:21.05-py3\r\n\r\nwhen I was using nvcr.io/nvidia/tritonserver:21.03-py3, didn't show this error.\r\n\r\nAre you using the Triton container or did you build it yourself?\r\ncontainer\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n1. triton model repository was put on azure storage\r\n2. Run triton deployment on AKS using nvcr.io/nvidia/tritonserver:21.04-py3 image with arguments: --model-control-mode=explicit --load-model=some_model --model-repository=as://tonytesttritonaccount/tony-model-repository/\r\n3. kubectl logs -f triton-inference-server-xxxx\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\nname: \"some_model\"\r\nplatform: \"pytorch_libtorch\"\r\nmax_batch_size: 10\r\ninput [\r\n  {\r\n    name: \"input__0\"\r\n    data_type: TYPE_INT64\r\n    dims: [4585]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"atc7__0\"\r\n    data_type: TYPE_FP32\r\n    dims: [990]\r\n  },\r\n  {\r\n    name: \"atc5__1\"\r\n    data_type: TYPE_FP32\r\n    dims: [467]\r\n  }\r\n]\r\n\r\ninstance_group [ {\r\n  count: 1\r\n  kind: KIND_GPU\r\n}]\r\n\r\nversion_policy { latest { num_versions: 1 } }\r\n\r\ndynamic_batching {\r\n  preferred_batch_size: [ 10 ]\r\n  max_queue_delay_microseconds: 5000\r\n}\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\"\r\n\r\nDon't expected to see this error, and when re-load model it should not have any downtime as the document claimed: https://github.com/triton-inference-server/server/blob/main/docs/model_management.md#model-control-mode-explicit\r\nhttps://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_21-04.html#rel_21-04\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2966/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2893", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2893/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2893/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2893/events", "html_url": "https://github.com/triton-inference-server/server/issues/2893", "id": 896493888, "node_id": "MDU6SXNzdWU4OTY0OTM4ODg=", "number": 2893, "title": "Triton json cause assert Abort or Segmentfault", "user": {"login": "zjjott", "id": 3733548, "node_id": "MDQ6VXNlcjM3MzM1NDg=", "avatar_url": "https://avatars.githubusercontent.com/u/3733548?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zjjott", "html_url": "https://github.com/zjjott", "followers_url": "https://api.github.com/users/zjjott/followers", "following_url": "https://api.github.com/users/zjjott/following{/other_user}", "gists_url": "https://api.github.com/users/zjjott/gists{/gist_id}", "starred_url": "https://api.github.com/users/zjjott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zjjott/subscriptions", "organizations_url": "https://api.github.com/users/zjjott/orgs", "repos_url": "https://api.github.com/users/zjjott/repos", "events_url": "https://api.github.com/users/zjjott/events{/privacy}", "received_events_url": "https://api.github.com/users/zjjott/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-05-20T08:46:44Z", "updated_at": "2021-07-13T17:20:13Z", "closed_at": "2021-07-13T17:20:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWe have try to implement some backend which return nested json object such as:\r\n```\r\n{\r\n    \"ret\":{\r\n        \"a\":\"1\",\r\n        \"b\": {\r\n           \"c\":[1,null]\r\n         }\r\n    }\r\n}\r\n\r\n```\r\nbut found triton_json always cause segmentfault, but rapidjson can deal with this object\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nI am using this version `triton_json.h`\r\nhttps://github.com/triton-inference-server/common/blob/main/include/triton/common/triton_json.h\r\n\r\n\r\n\r\n**To Reproduce**\r\na single file that include `triton_json.h` can reproduce this issue\r\n```cpp\r\n#include <iostream>\r\n#include <string>\r\n#include <ostream>\r\nclass Error\r\n{\r\npublic:\r\n    /// Create an error with the specified message.\r\n    /// \\param msg The message for the error\r\n    explicit Error(const std::string &msg = \"\");\r\n\r\n    /// Accessor for the message of this error.\r\n    /// \\return The messsage for the error. Empty if no error.\r\n    const std::string &Message() const { return msg_; }\r\n\r\n    /// Does this error indicate OK status?\r\n    /// \\return True if this error indicates \"ok\"/\"success\", false if\r\n    /// error indicates a failure.\r\n    bool IsOk() const { return msg_.empty(); }\r\n\r\n    /// Convenience \"success\" value. Can be used as Error::Success to\r\n    /// indicate no error.\r\n    static const Error Success;\r\n\r\nprivate:\r\n    friend std::ostream &operator<<(std::ostream &, const Error &);\r\n    std::string msg_;\r\n};\r\nconst Error Error::Success(\"\");\r\n\r\nError::Error(const std::string &msg) : msg_(msg) {}\r\n\r\nstd::ostream &\r\noperator<<(std::ostream &out, const Error &err)\r\n{\r\n    if (!err.msg_.empty())\r\n    {\r\n        out << err.msg_;\r\n    }\r\n    return out;\r\n}\r\n\r\n#ifndef TRITONJSON_STATUSTYPE\r\n#define TRITONJSON_STATUSTYPE Error\r\n#define TRITONJSON_STATUSRETURN(M) return Error(M)\r\n#define TRITONJSON_STATUSSUCCESS Error::Success\r\n#endif\r\n#include <triton/common/triton_json.h>\r\n\r\nusing namespace rapidjson;\r\nusing namespace std;\r\nvoid WriteDatatoJSON(\r\n    triton::common::TritonJson::Value *data_json)\r\n{\r\n    // 1. define try_json_body as root/not_root\r\n    triton::common::TritonJson::Value try_json_body(\r\n        triton::common::TritonJson::ValueType::OBJECT);\r\n    const std::string raw_s = string(R\"({\"text\":\"HelloJSON!\",\"hei2\":[\"test\",null]})\");\r\n    auto ret=try_json_body.Parse(raw_s);\r\n    if (!ret.IsOk())\r\n    {\r\n        cout << \"some error occurr\" << ret.Message()<< endl;\r\n    }\r\n   // HERE! i have tried std::move and Swap function but all useless\r\n    (*data_json) = std::move(try_json_body); //rootjson:sf otherwi\r\n    // data_json->Swap(try_json_body);\r\n    try_json_body.Release();\r\n}\r\nvoid InnerResponse(triton::common::TritonJson::Value* response_json)\r\n{\r\n    triton::common::TritonJson::Value ret_json(\r\n        *response_json,triton::common::TritonJson::ValueType::OBJECT);\r\n    triton::common::TritonJson::Value data_json(\r\n        *response_json, triton::common::TritonJson::ValueType::OBJECT);\r\n\r\n    WriteDatatoJSON(&data_json);\r\n    triton::common::TritonJson::WriteBuffer buffer;\r\n    buffer.Clear();\r\n    data_json.Write(&buffer);\r\n    cout << \"data_json:\"<< buffer.Contents() << endl;\r\n    response_json->Add(\"result\", std::move(data_json));\r\n    buffer.Clear();\r\n    // buffer.Clear();\r\n    response_json->Write(&buffer);\r\n    cout << \"response_json:\" << response_json << buffer.Contents() << endl;\r\n}\r\n\r\nint main()\r\n{\r\n    for(int i=0;i<3;i++){\r\n        cout<<\"hello idx:\"<<i<<endl;\r\n        triton::common::TritonJson::Value response_json(\r\n            triton::common::TritonJson::ValueType::OBJECT);\r\n        InnerResponse(&response_json);\r\n\r\n        triton::common::TritonJson::WriteBuffer buffer;\r\n        buffer.Clear();\r\n        response_json.Write(&buffer);\r\n        cout << &response_json << buffer.Contents() << endl;\r\n        }\r\n    \r\n}\r\n```\r\n\r\nand compile it with `triton_json.h`\r\n\r\n`g++ -g -std=c++17 -I/workspace/builddir/tritonserver/build/server/_deps/repo-common-src/include test_json.cc -o test_json  && ./test_json`\r\n\r\ncause this Abort:\r\n```\r\ntest_json: /usr/local/include/rapidjson/document.h:1078: rapidjson::GenericValue<Encoding, Allocator>::ConstMemberIterator rapidjson::GenericValue<Encoding, Allocator>::MemberEnd() const [with Encoding = rapidjson::UTF8<>; Allocator = rapidjson::MemoryPoolAllocator<>; rapidjson::GenericValue<Encoding, Allocator>::ConstMemberIterator = rapidjson::GenericMemberIterator<true, rapidjson::UTF8<>, rapidjson::MemoryPoolAllocator<> >]: Assertion `IsObject()' failed.\r\nAborted\r\n```\r\n\r\n**Expected behavior**\r\nor maybe my code is incorrect?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2893/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2877", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2877/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2877/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2877/events", "html_url": "https://github.com/triton-inference-server/server/issues/2877", "id": 894481492, "node_id": "MDU6SXNzdWU4OTQ0ODE0OTI=", "number": 2877, "title": "Triton does not run without gpu (cpu-only)", "user": {"login": "leeho", "id": 1549074, "node_id": "MDQ6VXNlcjE1NDkwNzQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1549074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leeho", "html_url": "https://github.com/leeho", "followers_url": "https://api.github.com/users/leeho/followers", "following_url": "https://api.github.com/users/leeho/following{/other_user}", "gists_url": "https://api.github.com/users/leeho/gists{/gist_id}", "starred_url": "https://api.github.com/users/leeho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leeho/subscriptions", "organizations_url": "https://api.github.com/users/leeho/orgs", "repos_url": "https://api.github.com/users/leeho/repos", "events_url": "https://api.github.com/users/leeho/events{/privacy}", "received_events_url": "https://api.github.com/users/leeho/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1079803578, "node_id": "MDU6TGFiZWwxMDc5ODAzNTc4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2021-05-18T15:02:31Z", "updated_at": "2023-04-03T20:50:43Z", "closed_at": "2021-08-11T17:53:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen running without gpu, the following error happens:\r\n\r\ntritonserver: error while loading shared libraries: libnvidia-ml.so.1: cannot open shared object file: No such file or directory\r\n\r\nAccordingly to documentation, without additional flags triton should just run without gpus:\r\n\r\nhttps://github.com/triton-inference-server/server/blob/main/docs/quickstart.md#run-on-cpu-only-system\r\n\r\nThe error is random, I was able to start the container several times with this error being intermittent, but after I installed some more libraries over the base container this error started to happen all the time.\r\n\r\n**Triton Information**\r\n\r\nDocker container: nvcr.io/nvidia/tritonserver:21.03-py3\r\n\r\nAre you using the Triton container or did you build it yourself?\r\n\r\nContainer\r\n\r\n**To Reproduce**\r\n\r\nRun the container on a system without gpus\r\n\r\n**Expected behavior**\r\n\r\nContainer starting\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2877/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2877/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2855", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2855/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2855/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2855/events", "html_url": "https://github.com/triton-inference-server/server/issues/2855", "id": 891072899, "node_id": "MDU6SXNzdWU4OTEwNzI4OTk=", "number": 2855, "title": "python backend streaming pybind11::error_already_set for perf_analyzer with concurrency > 1", "user": {"login": "Slyne", "id": 6286804, "node_id": "MDQ6VXNlcjYyODY4MDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/6286804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Slyne", "html_url": "https://github.com/Slyne", "followers_url": "https://api.github.com/users/Slyne/followers", "following_url": "https://api.github.com/users/Slyne/following{/other_user}", "gists_url": "https://api.github.com/users/Slyne/gists{/gist_id}", "starred_url": "https://api.github.com/users/Slyne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Slyne/subscriptions", "organizations_url": "https://api.github.com/users/Slyne/orgs", "repos_url": "https://api.github.com/users/Slyne/repos", "events_url": "https://api.github.com/users/Slyne/events{/privacy}", "received_events_url": "https://api.github.com/users/Slyne/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Tabrizian", "id": 10105175, "node_id": "MDQ6VXNlcjEwMTA1MTc1", "avatar_url": "https://avatars.githubusercontent.com/u/10105175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tabrizian", "html_url": "https://github.com/Tabrizian", "followers_url": "https://api.github.com/users/Tabrizian/followers", "following_url": "https://api.github.com/users/Tabrizian/following{/other_user}", "gists_url": "https://api.github.com/users/Tabrizian/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tabrizian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tabrizian/subscriptions", "organizations_url": "https://api.github.com/users/Tabrizian/orgs", "repos_url": "https://api.github.com/users/Tabrizian/repos", "events_url": "https://api.github.com/users/Tabrizian/events{/privacy}", "received_events_url": "https://api.github.com/users/Tabrizian/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2021-05-13T14:08:22Z", "updated_at": "2021-08-30T14:56:44Z", "closed_at": "2021-06-10T17:08:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nTry to perf_analyze a simple sequence model with concurrency > 1  (=1 no issues), get error message:\r\n```\r\nterminate called after throwing an instance of 'pybind11::error_already_set'\r\n  what():  error: unpack_from requires a buffer of at least 3489662884 bytes for unpacking 3489662880 bytes at offset 4 (actual buffer size is 132)\r\n\r\nAt:\r\n  /opt/tritonserver/backends/python/triton_python_backend_utils.py(117): deserialize_bytes_tensor\r\n```\r\n\r\n**Triton Information**\r\n21.04\r\n\r\nAre you using the Triton container or did you build it yourself?   Triton container\r\n\r\n**To Reproduce**\r\nPaste the model.py\r\n```\r\nimport numpy as np\r\nimport json\r\n\r\n# triton_python_backend_utils is available in every Triton Python model. You\r\n# need to use this module to create inference requests and responses. It also\r\n# contains some utility functions for extracting information from model_config\r\n# and converting Triton input/output types to numpy types.\r\nimport triton_python_backend_utils as pb_utils\r\nfrom multiprocessing.pool import ThreadPool\r\n\r\nclass Decoder(object):\r\n    def __init__(self, blank):\r\n        self.prev = ''\r\n        self.result = ''\r\n        self.blank_symbol = blank\r\n    \r\n    def decode(self, input, start, ready):\r\n        \"\"\"\r\n        input: a list of characters/a string\r\n        \"\"\"\r\n        if start:\r\n            self.prev = ''\r\n            self.result = ''\r\n        \r\n        if ready:\r\n            for li in input.decode(\"utf-8\"):\r\n                if li != self.prev:\r\n                    if li != self.blank_symbol:\r\n                        self.result += li\r\n                self.prev = li\r\n        r = np.array([[self.result]], dtype=np.dtype(object))\r\n        return r\r\n\r\nclass TritonPythonModel:\r\n    \"\"\"Your Python model must use the same class name. Every Python model\r\n    that is created must have \"TritonPythonModel\" as the class name.\r\n    \"\"\"\r\n\r\n    def initialize(self, args):\r\n        \"\"\"`initialize` is called only once when the model is being loaded.\r\n        Implementing `initialize` function is optional. This function allows\r\n        the model to intialize any state associated with this model.\r\n        Parameters\r\n        ----------\r\n        args : dict\r\n          Both keys and values are strings. The dictionary keys and values are:\r\n          * model_config: A JSON string containing the model configuration\r\n          * model_instance_kind: A string containing model instance kind\r\n          * model_instance_device_id: A string containing model instance device ID\r\n          * model_repository: Model repository path\r\n          * model_version: Model version\r\n          * model_name: Model name\r\n        \"\"\"\r\n\r\n        # You must parse model_config. JSON string is not parsed here\r\n        self.model_config = model_config = json.loads(args['model_config'])\r\n\r\n        # get max batch size\r\n        max_batch_size = max(model_config[\"max_batch_size\"], 1)\r\n\r\n        # get blank symbol from config\r\n        blank = self.model_config.get(\"blank_id\", '-')\r\n\r\n        # initialize decoders\r\n        self.decoders = [Decoder(blank) for i in range(max_batch_size)]\r\n\r\n        # Get OUTPUT0 configuration\r\n        output0_config = pb_utils.get_output_config_by_name(\r\n            model_config, \"OUTPUT0\")\r\n            \r\n        # Convert Triton types to numpy types\r\n        self.output0_dtype = pb_utils.triton_string_to_numpy(\r\n            output0_config['data_type'])\r\n\r\n    def batch_decode(self, batch_input, batch_start, batch_ready):\r\n        responses = []\r\n        args = []\r\n        idx = 0\r\n        for i,r,s in zip(batch_input, batch_ready, batch_start):\r\n            args.append([idx, i, r, s])\r\n            idx += 1\r\n        \r\n        with ThreadPool() as p:\r\n            responses = p.map(self.process_single_request, args)\r\n\r\n        return responses\r\n\r\n    def process_single_request(self, inp):\r\n        decoder_idx, input, ready, start = inp\r\n        response = self.decoders[decoder_idx].decode(input[0], start[0], ready[0])\r\n        output0_dtype = self.output0_dtype\r\n        #print(\"output dtype:\", output0_dtype, type(response))\r\n        #print(response.astype(output0_dtype))\r\n        out_tensor_0 = pb_utils.Tensor(\"OUTPUT0\", response.astype(output0_dtype))\r\n        inference_response = pb_utils.InferenceResponse(\r\n                output_tensors=[out_tensor_0])\r\n        return inference_response\r\n\r\n    def execute(self, requests):\r\n        \"\"\"`execute` MUST be implemented in every Python model. `execute`\r\n        function receives a list of pb_utils.InferenceRequest as the only\r\n        argument. This function is called when an inference request is made\r\n        for this model. Depending on the batching configuration (e.g. Dynamic\r\n        Batching) used, `requests` may contain multiple requests. Every\r\n        Python model, must create one pb_utils.InferenceResponse for every\r\n        pb_utils.InferenceRequest in `requests`. If there is an error, you can\r\n        set the error argument when creating a pb_utils.InferenceResponse\r\n        Parameters\r\n        ----------\r\n        requests : list\r\n          A list of pb_utils.InferenceRequest\r\n        Returns\r\n        -------\r\n        list\r\n          A list of pb_utils.InferenceResponse. The length of this list must\r\n          be the same as `requests`\r\n        \"\"\"\r\n        #print(\"START NEW\")\r\n        responses = []\r\n\r\n        batch_input = []\r\n        batch_ready = []\r\n        batch_start = []\r\n        batch_corrid = []\r\n        # Every Python backend must iterate over everyone of the requests\r\n        # and create a pb_utils.InferenceResponse for each of them.\r\n        for request in requests:\r\n            # Get INPUT0\r\n            in_0 = pb_utils.get_input_tensor_by_name(request, \"INPUT0\")\r\n            #in_0 ->  <triton_python_backend_utils.Tensor object\r\n            # in_0 -> ndarray['xxx']\r\n\r\n            batch_input += in_0.as_numpy().tolist()\r\n            \r\n            in_start = pb_utils.get_input_tensor_by_name(request, \"START\")\r\n            batch_start += in_start.as_numpy().tolist()\r\n            \r\n            in_ready = pb_utils.get_input_tensor_by_name(request, \"READY\")\r\n            batch_ready += in_ready.as_numpy().tolist()\r\n\r\n            in_corrid = pb_utils.get_input_tensor_by_name(request, \"CORRID\")\r\n            batch_corrid += in_corrid.as_numpy().tolist()\r\n\r\n        #print(\"corrid\", batch_corrid)\r\n        #print(\"batch input\", batch_input)\r\n        responses = self.batch_decode(batch_input, batch_start, batch_ready)\r\n        #print(\"batch response\", responses)\r\n        # You should return a list of pb_utils.InferenceResponse. Length\r\n        # of this list must match the length of `requests` list.\r\n        assert len(requests) == len(responses)\r\n        #print(\"send responses\", responses)\r\n        return responses\r\n\r\n    def finalize(self):\r\n        \"\"\"`finalize` is called only once when the model is being unloaded.\r\n        Implementing `finalize` function is OPTIONAL. This function allows\r\n        the model to perform any necessary clean ups before exit.\r\n        \"\"\"\r\n        print('Cleaning up...')\r\n```\r\n\r\nThe config.pbtxt\r\n```\r\nname: \"ctc_decode_model\"\r\nbackend: \"python\"\r\nmax_batch_size: 4\r\nsequence_batching{\r\n    max_sequence_idle_microseconds: 5000000\r\n    direct {\r\n      minimum_slot_utilization: 0.1\r\n      max_queue_delay_microseconds: 100000\r\n    }\r\n    control_input [\r\n        {\r\n            name: \"START\",\r\n            control [\r\n                {\r\n                    kind: CONTROL_SEQUENCE_START\r\n                    fp32_false_true: [0,1]\r\n                }\r\n            ]\r\n        },\r\n        {\r\n            name: \"READY\"\r\n            control [\r\n                {\r\n                    kind: CONTROL_SEQUENCE_READY\r\n                    fp32_false_true: [0, 1]\r\n                }\r\n            ]\r\n        },\r\n        {\r\n            name: \"CORRID\",\r\n            control [\r\n                {\r\n                    kind: CONTROL_SEQUENCE_CORRID\r\n                    data_type: TYPE_UINT64\r\n                }\r\n            ]\r\n        }\r\n    ]\r\n}\r\ninput [\r\n  {\r\n    name: \"INPUT0\"\r\n    data_type: TYPE_STRING\r\n    dims: [1] \r\n  }\r\n]\r\n\r\noutput [\r\n  {\r\n    name: \"OUTPUT0\"\r\n    data_type: TYPE_STRING\r\n    dims: [-1]\r\n  }\r\n]\r\n\r\ninstance_group [{ kind: KIND_CPU }]\r\n```\r\n\r\nThe commandline is \r\n```\r\nperf_client --streaming -i gRPC --concurrency-range 2:2:1 -m ctc_decode_model\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2855/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2855/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2853", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2853/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2853/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2853/events", "html_url": "https://github.com/triton-inference-server/server/issues/2853", "id": 890364075, "node_id": "MDU6SXNzdWU4OTAzNjQwNzU=", "number": 2853, "title": "Triton does not load the latest available model in explicit mode.", "user": {"login": "nsiddharth", "id": 473034, "node_id": "MDQ6VXNlcjQ3MzAzNA==", "avatar_url": "https://avatars.githubusercontent.com/u/473034?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nsiddharth", "html_url": "https://github.com/nsiddharth", "followers_url": "https://api.github.com/users/nsiddharth/followers", "following_url": "https://api.github.com/users/nsiddharth/following{/other_user}", "gists_url": "https://api.github.com/users/nsiddharth/gists{/gist_id}", "starred_url": "https://api.github.com/users/nsiddharth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nsiddharth/subscriptions", "organizations_url": "https://api.github.com/users/nsiddharth/orgs", "repos_url": "https://api.github.com/users/nsiddharth/repos", "events_url": "https://api.github.com/users/nsiddharth/events{/privacy}", "received_events_url": "https://api.github.com/users/nsiddharth/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2021-05-12T18:17:23Z", "updated_at": "2021-06-08T10:49:15Z", "closed_at": "2021-05-21T18:33:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Description**\r\nusing model control mode as explicit, it is expected that Triton will load the latest available model when a new model version is added and load_model is called. However, this does not happen and load_model returns only the already loaded model. This can be tested by adding a new version directory for the simple model provided in the examples. Note that a new version directory is added after Triton has started up. However, this works when unload_model is called followed by load model\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n2.8.0\r\nAre you using the Triton container or did you build it yourself?\r\nUsing the container\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nUse a simple script like so to test this behavior:\r\n```\r\nimport tritonclient.grpc as grpcclient\r\nfrom tritonclient.utils import InferenceServerException\r\n\r\nMODEL_NAME = \"simple\"\r\n\r\nURL = \"localhost:8001\"\r\ntriton_client = grpcclient.InferenceServerClient(url=URL, verbose=True)\r\n\r\ntriton_client.is_server_live()\r\nprint(triton_client.is_model_ready(MODEL_NAME))\r\ntriton_client.get_model_repository_index().models\r\n\r\n\"\"\"\r\nmodel loading and unloading \r\nThis should pickup the latest available model\r\nwithout having to call:\r\ntriton_client.unload_model(MODEL_NAME)\r\n\"\"\"\r\ntriton_client.load_model(MODEL_NAME)\r\n\"\"\"This should show that the latest model is available\"\"\"\r\ntriton_client.get_model_repository_index().models\r\n\r\n```\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nThe model used to test this was the simple model provided in the examples\r\n**Expected behavior**\r\nwhen a new version is added to the model repo like so:\r\n`aws s3 cp --recursive 2 s3://<Model_repo>/simple/2`\r\nand load_model is called, the latest model should be loaded and made available. This does not happen. \r\nHowever, this takes effect if unload_model is called before calling load_model again. We do not want this behavior in production where you need to unload the model causing downtime before you are able to load a new one.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2853/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2853/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2848", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2848/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2848/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2848/events", "html_url": "https://github.com/triton-inference-server/server/issues/2848", "id": 889474121, "node_id": "MDU6SXNzdWU4ODk0NzQxMjE=", "number": 2848, "title": "the preprocess VGG mean-substraction in image_client.cc ", "user": {"login": "zoidburg", "id": 11624459, "node_id": "MDQ6VXNlcjExNjI0NDU5", "avatar_url": "https://avatars.githubusercontent.com/u/11624459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zoidburg", "html_url": "https://github.com/zoidburg", "followers_url": "https://api.github.com/users/zoidburg/followers", "following_url": "https://api.github.com/users/zoidburg/following{/other_user}", "gists_url": "https://api.github.com/users/zoidburg/gists{/gist_id}", "starred_url": "https://api.github.com/users/zoidburg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zoidburg/subscriptions", "organizations_url": "https://api.github.com/users/zoidburg/orgs", "repos_url": "https://api.github.com/users/zoidburg/repos", "events_url": "https://api.github.com/users/zoidburg/events{/privacy}", "received_events_url": "https://api.github.com/users/zoidburg/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-05-12T01:19:56Z", "updated_at": "2021-08-11T19:41:45Z", "closed_at": "2021-08-11T19:41:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://github.com/triton-inference-server/server/blob/b4d6cc227acf5dfe985d7982592764014fbcf047/src/clients/c%2B%2B/examples/image_client.cc#L139\r\n\r\nThe image has been converted to RGB order before, so maybe the mean value should be (123, 117, 104), just like in the python implementation\r\nhttps://github.com/triton-inference-server/server/blob/b4d6cc227acf5dfe985d7982592764014fbcf047/src/clients/python/examples/image_client.py#L168", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2848/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2846", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2846/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2846/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2846/events", "html_url": "https://github.com/triton-inference-server/server/issues/2846", "id": 887531745, "node_id": "MDU6SXNzdWU4ODc1MzE3NDU=", "number": 2846, "title": "InferenceServerException: PyTorch execute failure: Expected Tensor but got Tuple [ BART Summarization ]", "user": {"login": "anshoomehra", "id": 24396120, "node_id": "MDQ6VXNlcjI0Mzk2MTIw", "avatar_url": "https://avatars.githubusercontent.com/u/24396120?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anshoomehra", "html_url": "https://github.com/anshoomehra", "followers_url": "https://api.github.com/users/anshoomehra/followers", "following_url": "https://api.github.com/users/anshoomehra/following{/other_user}", "gists_url": "https://api.github.com/users/anshoomehra/gists{/gist_id}", "starred_url": "https://api.github.com/users/anshoomehra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anshoomehra/subscriptions", "organizations_url": "https://api.github.com/users/anshoomehra/orgs", "repos_url": "https://api.github.com/users/anshoomehra/repos", "events_url": "https://api.github.com/users/anshoomehra/events{/privacy}", "received_events_url": "https://api.github.com/users/anshoomehra/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 19, "created_at": "2021-05-11T14:53:25Z", "updated_at": "2022-10-20T07:23:25Z", "closed_at": "2021-06-04T17:29:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nPorted PyTorch BART Summarization Model using JIT libraries and hosted on Triton Server. The inference is consistently failing with \" InferenceServerException: PyTorch execute failure: Expected Tensor but got Tuple \" error. I do not see anyway to define data type as Tensor (dtype: INT64) in config.pbtxt & defining it as TYPE_INT64 results in mentioned error.\r\n\r\n**Triton Information**\r\nUsing Prebuilt Container - nvcr.io/nvidia/tritonserver:21.04-py3\r\n\r\n**To Reproduce**\r\n1. Convert the model using JIT libraries (giving standard huggingface model for easy reproducibility, I am using a fine-tuned version of this baseline with no changes to the model architecture)\r\n\r\n```\r\nimport torch\r\nimport numpy as np\r\nfrom transformers import (\r\n    AutoModelForSeq2SeqLM,\r\n    AutoTokenizer,\r\n    AutoConfig\r\n)\r\n\r\nclass pyTorchToTorchScript(torch.nn.Module):\r\n    def __init__(self):\r\n        super(pyTorchToTorchScript, self).__init__()\r\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\", torchscript=True).cuda()\r\n    \r\n    def forward(self, input_ids):\r\n        return self.model(input_ids)\r\n \r\ninput_ids = torch.zeros((1, 1024), dtype=torch.long, device='cuda')   \r\npt_model = pyTorchToTorchScript().eval()\r\ntraced_script_module = torch.jit.trace(pt_model, input_ids)\r\n\r\ntraced_script_module.eval()\r\ntraced_script_module(input_ids)\r\ntraced_script_module.save(\"exportedModelsForTritan/bart_large_cnn_fl/1/model.pt\")\r\n```\r\n\r\n2. Test the JIT Converted Model\r\n\r\n```\r\nmodel = torch.jit.load(\"exportedModelsForTritan/bart_large_cnn_fl/1/model.pt\")\r\n\r\n## This works good with input as Tensor but fails for Input as Numpy Array \r\nexample_outputs = model(input_ids)\r\n```\r\n\r\n3. Build the config.pbtxt and load model in desired directory structure\r\n```\r\nname: \"bart_large_cnn_fl\"\r\nplatform: \"pytorch_libtorch\"\r\ninput [\r\n{\r\n    name: \"input__0\"\r\n    data_type: TYPE_INT64\r\n    dims: [1, 1024]\r\n } \r\n]\r\noutput {\r\n    name: \"output__0\"\r\n    data_type: TYPE_INT64\r\n    dims: [2, 50264]\r\n}\r\n```\r\n\r\n4. Load Model on Triton Server\r\n\r\n```\r\nnvidia-docker run \\\r\n--gpus=all --rm \\\r\n--name summarization_triton_server \\\r\n-p 8000:8000 -p 8001:8001 -p 8002:8002 \\\r\n--ip 0.0.0.0 \\\r\n-v /home/jupyter/summarization/exportedModelsForTritan/:/models \\\r\nnvcr.io/nvidia/tritonserver:21.04-py3 \\\r\ntritonserver --model-repository=/models --log-verbose=1\r\n```\r\n\r\n\r\n\r\n5. Run Inference [ Model is loaded without any issues ]\r\nInstalled : pip install nvidia-pyindex tritonclient[all]\r\n\r\n```\r\nimport tritonclient.grpc as grpcclient\r\nimport tritonclient.grpc.model_config_pb2 as model_config_pb2\r\n\r\nimport tritonclient.http as httpclient\r\nimport tritonclient.utils\r\n\r\nfrom tritonclient.utils import InferenceServerException\r\n\r\nimport torch\r\nimport numpy as np\r\n\r\ntriton_gprc_client = grpcclient.InferenceServerClient(url=\"0.0.0.0:8001\", verbose=True)\r\ntriton_http_client = httpclient.InferenceServerClient(url=\"0.0.0.0:8000\", verbose=True)\r\n\r\nmodel_name=\"bart_large_cnn_fl\"\r\nmodel_version=\"1\" \r\n\r\nmodel_metadata = triton_http_client.get_model_metadata(model_name=model_name)\r\n\r\n#Output Metadata>>>>\r\n#GET /v2/models/bart_large_cnn_fl, headers None\r\n# <HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '218'}>\r\n# bytearray(b'{\"name\":\"bart_large_cnn_fl\",\"versions\":[\"1\"],\"platform\":\"pytorch_libtorch\",\"inputs\":[{\"name\":\"input__0\",\"datatype\":\"INT64\",\"shape\":[1,1024]}],\"outputs\":[{\"name\":\"output__0\",\"datatype\":\"INT64\",\"shape\":[2,50264]}]}')\r\n\r\nmodel_config = triton_http_client.get_model_config(model_name=model_name)\r\n\r\n#Output Config>>>>\r\n# GET /v2/models/bart_large_cnn_fl/config, headers None\r\n# <HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '847'}>\r\n# bytearray(b'{\"name\":\"bart_large_cnn_fl\",\"platform\":\"pytorch_libtorch\",\"backend\":\"pytorch\",\"version_policy\":{\"latest\":{\"num_versions\":1}},\"max_batch_size\":0,\"input\":[{\"name\":\"input__0\",\"data_type\":\"TYPE_INT64\",\"format\":\"FORMAT_NONE\",\"dims\":[1,1024],\"is_shape_tensor\":false,\"allow_ragged_batch\":false}],\"output\":[{\"name\":\"output__0\",\"data_type\":\"TYPE_INT64\",\"dims\":[2,50264],\"label_filename\":\"\",\"is_shape_tensor\":false}],\"batch_input\":[],\"batch_output\":[],\"optimization\":{\"priority\":\"PRIORITY_DEFAULT\",\"input_pinned_memory\":{\"enable\":true},\"output_pinned_memory\":{\"enable\":true},\"gather_kernel_buffer_threshold\":0,\"eager_batching\":false},\"instance_group\":[{\"name\":\"bart_large_cnn_fl\",\"kind\":\"KIND_GPU\",\"count\":1,\"gpus\":[0],\"profile\":[]}],\"default_model_filename\":\"model.pt\",\"cc_model_filenames\":{},\"metric_tags\":{},\"parameters\":{},\"model_warmup\":[]}')\r\n\r\ninput_text=\"The BART Model with a language modeling head. Can be used for summarization.....\"\r\n\r\nFT_MODEL_PATH = \"fineTuning/fineTunedModels/bart_large_cnn_fl\"\r\n\r\nFT_MODEL_TOKENIZER = AutoTokenizer.from_pretrained(FT_MODEL_PATH)\r\ninput_tokenized = FT_MODEL_TOKENIZER.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding='max_length')#.to('cuda')\r\n\r\n## input_tokenized.dtype >>> torch.int64\r\n\r\ninput_ids = np.array(input_tokenized, dtype=np.int64)\r\ninput_ids = input_ids.reshape(1, 1024)\r\ninput = httpclient.InferInput('input__0', (1,  1024), 'INT64')\r\ninput.set_data_from_numpy(input_ids, binary_data=False)\r\n\r\noutput = httpclient.InferRequestedOutput('output__0',  binary_data=False)\r\n\r\nresponse = triton_http_client.infer(model_name, model_version=model_version, inputs=[input], outputs=[output])\r\n\r\n# Output Log >>>>>>\r\n# POST /v2/models/bart_large_cnn_fl/versions/1/infer, headers None\r\n# {\"inputs\":[{\"name\":\"input__0\",\"shape\":[1,1024],\"datatype\":\"INT64\",\"data\":[0,133,30634,7192,19,10,2777,19039,471,4,2615,28,341,13,39186,1938,4,152,1421,39449,2629,31,5048,12667,7153,45149,4,4254,5,2422,4684,14877,13,5,14569,6448,5,5560,36987,13,70,63,1421,36,16918,25,28882,50,6549,6,5032,2787,5,8135,33183,417,1033,6,3349,37215,3885,4753,4,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}],\"outputs\":[{\"name\":\"output__0\",\"parameters\":{\"binary_data\":false}}]}\r\n# <HTTPSocketPoolResponse status=400 headers={'content-type': 'application/json', 'content-length': '1618'}>\r\n\r\n\r\n```\r\n\r\n**ERROR CODE**\r\n```\r\n---------------------------------------------------------------------------\r\nInferenceServerException                  Traceback (most recent call last)\r\n<ipython-input-13-52ecde7ad3a9> in <module>\r\n----> 1 response = triton_http_client.infer(model_name, model_version=model_version, inputs=[input], outputs=[output])\r\n\r\n/opt/conda/lib/python3.7/site-packages/tritonclient/http/__init__.py in infer(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, headers, query_params)\r\n   1110                               headers=headers,\r\n   1111                               query_params=query_params)\r\n-> 1112         _raise_if_error(response)\r\n   1113 \r\n   1114         return InferResult(response, self._verbose)\r\n\r\n/opt/conda/lib/python3.7/site-packages/tritonclient/http/__init__.py in _raise_if_error(response)\r\n     61     error = _get_error(response)\r\n     62     if error is not None:\r\n---> 63         raise error\r\n     64 \r\n     65 \r\n\r\nInferenceServerException: PyTorch execute failure: Expected Tensor but got Tuple\r\nException raised from reportToTensorTypeError at ../aten/src/ATen/core/ivalue.cpp:846 (most recent call first):\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6c (0x7fe321d2339c in /opt/tritonserver/backends/pytorch/libc10.so)\r\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xfa (0x7fe321cf044c in /opt/tritonserver/backends/pytorch/libc10.so)\r\nframe #2: c10::IValue::reportToTensorTypeError() const + 0x68 (0x7fe30013e9b8 in /opt/tritonserver/backends/pytorch/libtorch_cpu.so)\r\nframe #3: <unknown function> + 0xfa88 (0x7fe32225ea88 in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so)\r\nframe #4: <unknown function> + 0x1580a (0x7fe32226480a in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so)\r\nframe #5: TRITONBACKEND_ModelInstanceExecute + 0x411 (0x7fe322265d11 in /opt/tritonserver/backends/pytorch/libtriton_pytorch.so)\r\nframe #6: <unknown function> + 0x2fd8f7 (0x7fe371b1f8f7 in /opt/tritonserver/bin/../lib/libtritonserver.so)\r\nframe #7: <unknown function> + 0xff030 (0x7fe371921030 in /opt/tritonserver/bin/../lib/libtritonserver.so)\r\nframe #8: <unknown function> + 0xd6d84 (0x7fe37135cd84 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\r\nframe #9: <unknown function> + 0x9609 (0x7fe3717f7609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #10: clone + 0x43 (0x7fe37104a293 in /usr/lib/x86_64-linux-gnu/libc.so.6)\r\n```\r\n\r\n**Expected behavior**\r\nExpecting model return logits. Not sure how to handle inputs at the time of inference.\r\n\r\n** Software Versions **\r\n```\r\ntorch==1.8.0\r\ntorchvision==0.9.0+cu111\r\ntransformers==4.5.1\r\ntritonclient==2.9.0\r\n```\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2846/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2843", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2843/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2843/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2843/events", "html_url": "https://github.com/triton-inference-server/server/issues/2843", "id": 886575587, "node_id": "MDU6SXNzdWU4ODY1NzU1ODc=", "number": 2843, "title": "so many server's handler caused server not work", "user": {"login": "CColten", "id": 8947579, "node_id": "MDQ6VXNlcjg5NDc1Nzk=", "avatar_url": "https://avatars.githubusercontent.com/u/8947579?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CColten", "html_url": "https://github.com/CColten", "followers_url": "https://api.github.com/users/CColten/followers", "following_url": "https://api.github.com/users/CColten/following{/other_user}", "gists_url": "https://api.github.com/users/CColten/gists{/gist_id}", "starred_url": "https://api.github.com/users/CColten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CColten/subscriptions", "organizations_url": "https://api.github.com/users/CColten/orgs", "repos_url": "https://api.github.com/users/CColten/repos", "events_url": "https://api.github.com/users/CColten/events{/privacy}", "received_events_url": "https://api.github.com/users/CColten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2981561833, "node_id": "MDU6TGFiZWwyOTgxNTYxODMz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/investigating", "name": "investigating", "color": "FEF2C0", "default": false, "description": "The developement team is investigating this issue"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-05-11T09:05:11Z", "updated_at": "2021-06-08T00:01:55Z", "closed_at": "2021-06-08T00:01:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nthe tensorrt server when run in a long time, it didn't handle any request \u3002\r\nwe started our services in some GPUs, the trtserver had so many handles when found one TensorRT didn't work, but others are OK;\r\nfrom the handles' info , we found \"protocol:UNIX \", \"protocol:TCP\", \"pipe\", \"eventpoll\" are the mainly causes.\r\n\r\nin our client, only using curl in \"server\\src\\clients\\c++\\api_v1\\library\\request_http.cc\" to request with a curl clean after.\r\nsearching about the \"pipe\", \"eventpoll\" , found that mainly caused by not shutdown the sock or other connetions.\r\n\r\nis any other had met the same question? why the server used so many handlers?\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nTensorRT Version: 19.10\r\nNVIDIA GPU: V100\r\nNVIDIA Driver Version: 410.79\r\nCUDA Version: 10.0\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nusing the Triton container \uff0819.10 and 20.07\uff09\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\nrunning a long time when the handlers' num increasing\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nwe put many models together and used,  long time later , one of the servers  hang.\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2843/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2836", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2836/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2836/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2836/events", "html_url": "https://github.com/triton-inference-server/server/issues/2836", "id": 883992429, "node_id": "MDU6SXNzdWU4ODM5OTI0Mjk=", "number": 2836, "title": "infrence of torch script model much slower with triton than python environment", "user": {"login": "yjegssl", "id": 39104616, "node_id": "MDQ6VXNlcjM5MTA0NjE2", "avatar_url": "https://avatars.githubusercontent.com/u/39104616?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yjegssl", "html_url": "https://github.com/yjegssl", "followers_url": "https://api.github.com/users/yjegssl/followers", "following_url": "https://api.github.com/users/yjegssl/following{/other_user}", "gists_url": "https://api.github.com/users/yjegssl/gists{/gist_id}", "starred_url": "https://api.github.com/users/yjegssl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yjegssl/subscriptions", "organizations_url": "https://api.github.com/users/yjegssl/orgs", "repos_url": "https://api.github.com/users/yjegssl/repos", "events_url": "https://api.github.com/users/yjegssl/events{/privacy}", "received_events_url": "https://api.github.com/users/yjegssl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2637780852, "node_id": "MDU6TGFiZWwyNjM3NzgwODUy", "url": "https://api.github.com/repos/triton-inference-server/server/labels/performance", "name": "performance", "color": "37F207", "default": false, "description": "A possible performance tune-up"}, {"id": 3649117940, "node_id": "LA_kwDOCQnI4s7ZgR70", "url": "https://api.github.com/repos/triton-inference-server/server/labels/pytorch%20ngc", "name": "pytorch ngc", "color": "685EB8", "default": false, "description": "This bug originates from the PyTorch build shipped in the PyTorch NGC container"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 37, "created_at": "2021-05-10T11:13:37Z", "updated_at": "2022-04-11T13:44:14Z", "closed_at": "2022-04-11T13:44:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nA clear and concise description of what the bug is.\r\nI convert models in an open source OCR project  with torch.jit.trace() and put it on triton to do inference, but the inference speed much slower than I just run the traced model in python environment\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n21.04 py3\r\nAre you using the Triton container or did you build it yourself?\r\ncontainer\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n`pip install easyocr`\r\n`\r\nimport easyocr\r\nimport torch\r\nreader = easyocr.Reader(['ch_sim','en'],quantize=False,gpu=False)\r\nimgH=640\r\nimgW=352\r\nmax_length = 36\r\nbatch_size = 8\r\nimage = torch.ones([1,3,imgH, imgW])\r\nimage = torch.autograd.Variable(image).cuda()\r\nimg = torch.ones([batch_zie, 1, 64, 256]).cuda()\r\ntext = torch.ones([1,int(max_length+1)]).cuda()\r\n\r\ndetector = reader.detector.cuda()\r\nscripted_detector = torch.jit.trace(detector, image)\r\nscripted_detector.save('scripted_detector.pt')\r\nrecognizer = reader.recognizer.cuda()\r\nscripted_recognizer = torch.jit.trace(recognizer,(img, text))\r\nscripted_recognizer.save('scripted_recognizer.pt')\r\n`\r\n'\r\nname: \"detector\"\r\nplatform: \"pytorch_libtorch\"\r\nmax_batch_size: 1\r\ninput[\r\n{\r\nname: \"input_0\"\r\ndata_type: TYPE_FP32\r\ndims: [3,352,640]\r\n}\r\n]\r\n\r\noutput[\r\n{\r\nname: \"output_0\"\r\ndata_type: TYPE_FP32\r\ndims: [-1, -1]\r\n},\r\n{\r\nname: \"output_1\"\r\ndata_type: TYPE_FP32\r\ndims: [-1, -1]\r\n}\r\n]\r\ninstance_group [\r\n{\r\ncount: 1\r\nkind: KIND_GPU\r\ngpus: [0]\r\n}\r\n]\r\n\r\n'\r\n`name: \"detector\"\r\nplatform: \"pytorch_libtorch\"\r\nmax_batch_size: 1\r\ninput[\r\n{\r\nname: \"input_0\"\r\ndata_type: TYPE_FP32\r\ndims: [3,352,640]\r\n}\r\n]\r\n\r\noutput[\r\n{\r\nname: \"output_0\"\r\ndata_type: TYPE_FP32\r\ndims: [-1, -1]\r\n},\r\n{\r\nname: \"output_1\"\r\ndata_type: TYPE_FP32\r\ndims: [-1, -1]\r\n}\r\n]\r\ninstance_group [\r\n{\r\ncount: 1\r\nkind: KIND_GPU\r\ngpus: [0]\r\n}\r\n]\r\n\r\n'\r\nname: \"detector\"\r\nplatform: \"pytorch_libtorch\"\r\nmax_batch_size: 1\r\ninput[\r\n{\r\nname: \"input_0\"\r\ndata_type: TYPE_FP32\r\ndims: [3,352,640]\r\n}\r\n]\r\n\r\noutput[\r\n{\r\nname: \"output_0\"\r\ndata_type: TYPE_FP32\r\ndims: [-1, -1]\r\n},\r\n{\r\nname: \"output_1\"\r\ndata_type: TYPE_FP32\r\ndims: [-1, -1]\r\n}\r\n]\r\ninstance_group [\r\n{\r\ncount: 1\r\nkind: KIND_GPU\r\ngpus: [0]\r\n}\r\n]\r\n\r\n'\r\n`name: \"recognizer\"\r\nplatform: \"pytorch_libtorch\"\r\nmax_batch_size: 1\r\ninput[\r\n{\r\nname: \"input_0\"\r\ndata_type: TYPE_FP32\r\ndims: [1,64,256]\r\n},\r\nname: \"input_1\"\r\ndata_type: TYPE_INT64\r\ndims: [1,37]\r\n]\r\n\r\noutput[\r\n{\r\nname: \"output_0\"\r\ndata_type: TYPE_FP32\r\ndims: [-1, -1]\r\n},\r\n]\r\ninstance_group [\r\n{\r\ncount: 1\r\nkind: KIND_GPU\r\ngpus: [0]\r\n}\r\n]\r\n`\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nvgg based cnn \r\nrcnn\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\ninference time similar or faster than just using easyOCR project with python, gpu", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2836/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2836/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2835", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2835/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2835/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2835/events", "html_url": "https://github.com/triton-inference-server/server/issues/2835", "id": 883798622, "node_id": "MDU6SXNzdWU4ODM3OTg2MjI=", "number": 2835, "title": "Unable to connect to S3 protocol of Dell EMC ECS EX300 ", "user": {"login": "jax79sg", "id": 2820927, "node_id": "MDQ6VXNlcjI4MjA5Mjc=", "avatar_url": "https://avatars.githubusercontent.com/u/2820927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jax79sg", "html_url": "https://github.com/jax79sg", "followers_url": "https://api.github.com/users/jax79sg/followers", "following_url": "https://api.github.com/users/jax79sg/following{/other_user}", "gists_url": "https://api.github.com/users/jax79sg/gists{/gist_id}", "starred_url": "https://api.github.com/users/jax79sg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jax79sg/subscriptions", "organizations_url": "https://api.github.com/users/jax79sg/orgs", "repos_url": "https://api.github.com/users/jax79sg/repos", "events_url": "https://api.github.com/users/jax79sg/events{/privacy}", "received_events_url": "https://api.github.com/users/jax79sg/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2021-05-10T09:16:18Z", "updated_at": "2021-05-19T14:44:20Z", "closed_at": "2021-05-18T20:26:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nWhen connecting to Dell EMC EC S EX300 configured with S3 buckets, the error comes out after about 30 seconds\r\n```\r\nerror: creating server: Internal - Could not get MetaData for bucket with name 123.123.123.123 due to exception: error message: Unable to connect to endpoint\r\n```\r\ni was able to connect using aws-cli using the exact same connection string. `s3://123.123.123.123/bucketname`\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n21.03 docker image, server version 2.8.\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTriton container\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n```\r\ndocker run --rm -p 8001:8001 -p 8002:8002 --env envfileContainAccessKeyAndPassword.env nvidia/tritonserver:21.03-py3 tritonserver --model-repository=s3://123.123.123.123:443/bucketname --log-verbose 9\r\n```\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\nNot applicable.\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\nServer running and models loaded", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2835/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2835/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2643", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2643/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2643/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2643/events", "html_url": "https://github.com/triton-inference-server/server/issues/2643", "id": 834778068, "node_id": "MDU6SXNzdWU4MzQ3NzgwNjg=", "number": 2643, "title": "Tritonserver crashes with segmentation fault", "user": {"login": "inversitas", "id": 13886460, "node_id": "MDQ6VXNlcjEzODg2NDYw", "avatar_url": "https://avatars.githubusercontent.com/u/13886460?v=4", "gravatar_id": "", "url": "https://api.github.com/users/inversitas", "html_url": "https://github.com/inversitas", "followers_url": "https://api.github.com/users/inversitas/followers", "following_url": "https://api.github.com/users/inversitas/following{/other_user}", "gists_url": "https://api.github.com/users/inversitas/gists{/gist_id}", "starred_url": "https://api.github.com/users/inversitas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/inversitas/subscriptions", "organizations_url": "https://api.github.com/users/inversitas/orgs", "repos_url": "https://api.github.com/users/inversitas/repos", "events_url": "https://api.github.com/users/inversitas/events{/privacy}", "received_events_url": "https://api.github.com/users/inversitas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2021-03-18T12:55:10Z", "updated_at": "2021-04-08T16:34:12Z", "closed_at": "2021-04-07T17:48:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "# Description\r\nTritonserver crashes with segmentation fault on inference call with wrong/missing header type.\r\n\r\n\r\n# Triton Information\r\nFor Release Tests: \r\n - nvcr.io/nvidia/tritonserver:21.02-py3\r\n - nvcr.io/nvidia/tritonserver:20.12-py3\r\n\r\nDebugging\r\n - branch r21.02\r\n\r\n# To Reproduce\r\nI am using ubuntu 20.04 for all my testing/debugging.\r\n## Assets\r\n### Model\r\nhttp://download.tensorflow.org/models/deeplab_mnv3_large_cityscapes_trainfine_2019_11_15.tar.gz\r\n\r\n### config.pbtxt\r\n```\r\nname: \"deeplabv3_mobilenetv3_cityscape\"\r\nplatform: \"tensorflow_graphdef\"\r\ninput [\r\n  {\r\n    name: \"ImageTensor\"\r\n    data_type: TYPE_UINT8\r\n    dims: [1,-1,-1,3]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"SemanticPredictions\"\r\n    data_type: TYPE_INT32\r\n    dims: [1,-1,-1]\r\n  }\r\n]\r\n```\r\n\r\n\r\n### query.json\r\nhttps://gist.github.com/inversitas/5578da304aa65552d0359bf4a51e8435/raw/594ae07417640933df4958417757f0553418b986/query.json\r\n\r\n## Demonstration using offical container\r\n### Start Triton Server\r\n```\r\ndocker run --rm -it -v /tritondebug/deeplabv3_mobilenetv3_cityscape:/models/deeplabv3_mobilenetv3_cityscape -p9000:8000 nvcr.io/nvidia/tritonserver:21.02-py3 tritonserver --model-repository=/models\r\n```\r\n\r\n### Execute Query\r\n```\r\ncurl --request POST --data @query.json http://localhost:9000/v2/models/deeplabv3_mobilenetv3_cityscape/versions/1/infer\r\n```\r\n\r\nThe server exits/crashes without an error message.\r\n\r\n## Additional Information\r\n### Potential Causes\r\nThe segmentation fault does not occur when providing the correct header. The following command executes as expected: \r\n```\r\ncurl --header \"Content-Type: application/json\" --request POST --data @query.json http://localhost:9000/v2/models/deeplabv3_mobilenetv3_cityscape/versions/1/infer\r\n```\r\n### Affected Versions\r\nOnly the releases 20.12 and 21.02 seem to be affected by the bug. Version 20.11 still successfully executes the query. \r\n\r\n### Debugging\r\nBuild version 21.02 with debugging enabled:\r\n```\r\n./build.py --build-dir=/tmp/citritonbuild --enable-logging --enable-stats --enable-tracing --enable-metrics --endpoint=http --build-type Debug --backend=tensorflow1:r21.02 --backend=tensorflow2:r21.02\r\n```\r\n\r\nStart the docker container with bash:\r\n```\r\ndocker run --rm -it -v /tritondebug/deeplabv3_mobilenetv3_cityscape:/models/deeplabv3_mobilenetv3_cityscape -p9000:8000 tritonserver:latest bash\r\n```\r\n\r\nInstall gdb inside the container and start the server using gdb:\r\n```\r\napt update\r\napt install -y gdb\r\ngdb -ex run --args tritonserver --model-repository=/models --log-verbose 10\r\n```\r\n\r\nAfter executing the query\r\n```\r\ncurl --request POST --data @query.json http://localhost:9000/v2/models/deeplabv3_mobilenetv3_cityscape/versions/1/infer\r\n```\r\nI get the following backtrace:\r\n```\r\nThread 39 \"tritonserver\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fb206c8a700 (LWP 793)]\r\n0x00005626cf4f8311 in evhtp_parse_query_wflags (\r\n    query=0x7fb1547f9040 \"{\\\"inputs\\\" : [{\\\"name\\\" : \\\"ImageTensor\\\", \\\"datatype\\\" : \\\"UINT8\\\", \\\"shape\\\" : [1, 960, 480, 3] , \\\"data\\\" : [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [\"..., len=5069384, flags=0)\r\n    at /tmp/tritonbuild/tritonserver/build/third-party-repo/src/third_party/libevhtp/libevhtp/evhtp.c:3547\r\n3547    /tmp/tritonbuild/tritonserver/build/third-party-repo/src/third_party/libevhtp/libevhtp/evhtp.c: No such file or directory.\r\n(gdb) bt\r\n#0  0x00005626cf4f8311 in evhtp_parse_query_wflags (\r\n    query=0x7fb1547f9040 \"{\\\"inputs\\\" : [{\\\"name\\\" : \\\"ImageTensor\\\", \\\"datatype\\\" : \\\"UINT8\\\", \\\"shape\\\" : [1, 960, 480, 3] , \\\"data\\\" : [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [\"..., len=5069384, flags=0)\r\n    at /tmp/tritonbuild/tritonserver/build/third-party-repo/src/third_party/libevhtp/libevhtp/evhtp.c:3547\r\n#1  0x00005626cf4f8953 in evhtp_parse_query (\r\n    query=0x7fb1547f9040 \"{\\\"inputs\\\" : [{\\\"name\\\" : \\\"ImageTensor\\\", \\\"datatype\\\" : \\\"UINT8\\\", \\\"shape\\\" : [1, 960, 480, 3] , \\\"data\\\" : [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [\"..., len=5069384)\r\n    at /tmp/tritonbuild/tritonserver/build/third-party-repo/src/third_party/libevhtp/libevhtp/evhtp.c:3787\r\n#2  0x00005626cf4f57fe in htp__request_parse_fini_ (p=0x7fb140000d20)\r\n    at /tmp/tritonbuild/tritonserver/build/third-party-repo/src/third_party/libevhtp/libevhtp/evhtp.c:2023\r\n#3  0x00005626cf4fbded in hook_on_msg_complete_run (p=0x7fb140000d20, hooks=0x5626cf6e5fa0 <request_psets>)\r\n    at /tmp/tritonbuild/tritonserver/build/third-party-repo/src/third_party/libevhtp/libevhtp/parser.c:338\r\n#4  0x00005626cf500426 in htparser_run (p=0x7fb140000d20, hooks=0x5626cf6e5fa0 <request_psets>, \r\n    data=0x7fb200791630 \", 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, \"..., len=2632)\r\n    at /tmp/tritonbuild/tritonserver/build/third-party-repo/src/third_party/libevhtp/libevhtp/parser.c:2208\r\n#5  0x00005626cf4f61ed in htp__connection_readcb_ (bev=0x7fb200679cd0, arg=0x7fb140000b60)\r\n    at /tmp/tritonbuild/tritonserver/build/third-party-repo/src/third_party/libevhtp/libevhtp/evhtp.c:2286\r\n#6  0x00005626cf4d5ac1 in bufferevent_run_deferred_callbacks_locked (cb=0x7fb200679e70, arg=0x7fb200679cd0)\r\n    at /tmp/tritonbuild/tritonserver/build/libevent/src/libevent/bufferevent.c:149\r\n#7  0x00005626cf4df250 in event_process_active_single_queue (base=0x7fb200659af0, activeq=0x7fb2000014d0, max_to_process=2147483647, endtime=0x0)\r\n    at /tmp/tritonbuild/tritonserver/build/libevent/src/libevent/event.c:1652\r\n#8  0x00005626cf4df795 in event_process_active (base=0x7fb200659af0) at /tmp/tritonbuild/tritonserver/build/libevent/src/libevent/event.c:1738\r\n#9  0x00005626cf4dff49 in event_base_loop (base=0x7fb200659af0, flags=0) at /tmp/tritonbuild/tritonserver/build/libevent/src/libevent/event.c:1961\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n#10 0x00005626cf5006b6 in _evthr_loop (args=0x5626d13b4d00)\r\n    at /tmp/tritonbuild/tritonserver/build/third-party-repo/src/third_party/libevhtp/libevhtp/thread.c:127\r\n#11 0x00007fb261dd9609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#12 0x00007fb261b02293 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n```\r\n\r\n# Expected Behaviour \r\nThe triton server should not crash and execute the query successfully. Alternatively the query could fail with an error message requiring the correct header type.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2643/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2607", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2607/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2607/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2607/events", "html_url": "https://github.com/triton-inference-server/server/issues/2607", "id": 827230704, "node_id": "MDU6SXNzdWU4MjcyMzA3MDQ=", "number": 2607, "title": "reload ensemble model may cause server crash", "user": {"login": "yushcs", "id": 32016547, "node_id": "MDQ6VXNlcjMyMDE2NTQ3", "avatar_url": "https://avatars.githubusercontent.com/u/32016547?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yushcs", "html_url": "https://github.com/yushcs", "followers_url": "https://api.github.com/users/yushcs/followers", "following_url": "https://api.github.com/users/yushcs/following{/other_user}", "gists_url": "https://api.github.com/users/yushcs/gists{/gist_id}", "starred_url": "https://api.github.com/users/yushcs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yushcs/subscriptions", "organizations_url": "https://api.github.com/users/yushcs/orgs", "repos_url": "https://api.github.com/users/yushcs/repos", "events_url": "https://api.github.com/users/yushcs/events{/privacy}", "received_events_url": "https://api.github.com/users/yushcs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-03-10T06:53:00Z", "updated_at": "2022-05-31T17:54:18Z", "closed_at": "2022-05-31T17:54:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nA clear and concise description of what the bug is.\r\nWhile version 1 of ensemble model is being served, put a version 2 into the repository, ensemble model would be reloaded. And it may cause server crash if there are clients sending requests to the ensemble model during the reloading.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\nr20.12\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTriton container is used\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n1\u3001start a client to send requests to the ensemble model continually\r\n2\u3001add version 2 to ensemble model repository\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nAlthough there is no need to change the version of ensemble model, server crash of any case should be avoid.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2607/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2607/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2461", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2461/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2461/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2461/events", "html_url": "https://github.com/triton-inference-server/server/issues/2461", "id": 794686751, "node_id": "MDU6SXNzdWU3OTQ2ODY3NTE=", "number": 2461, "title": "Triton server failed to load Tensorflow SavedModel", "user": {"login": "churinga", "id": 8494615, "node_id": "MDQ6VXNlcjg0OTQ2MTU=", "avatar_url": "https://avatars.githubusercontent.com/u/8494615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/churinga", "html_url": "https://github.com/churinga", "followers_url": "https://api.github.com/users/churinga/followers", "following_url": "https://api.github.com/users/churinga/following{/other_user}", "gists_url": "https://api.github.com/users/churinga/gists{/gist_id}", "starred_url": "https://api.github.com/users/churinga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/churinga/subscriptions", "organizations_url": "https://api.github.com/users/churinga/orgs", "repos_url": "https://api.github.com/users/churinga/repos", "events_url": "https://api.github.com/users/churinga/events{/privacy}", "received_events_url": "https://api.github.com/users/churinga/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2021-01-27T01:23:02Z", "updated_at": "2022-03-14T20:31:20Z", "closed_at": "2021-02-01T17:31:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nTriton inference server is started with command line argument `strict-model-config=false`, with a single Tensorflow saved model to load. The saved model does not have an explicit `config.pbtxt`, and according to the documentation, the model config should be able to be automatically detected. \r\n\r\nThe server fails to load the model with the following complaint: \r\n```\r\nE0127 00:06:37.106575 1 model_repository_manager.cc:899] failed to load '...' version 10: Invalid argument: model input cannot have empty reshape for non-batching model for ...\r\n```\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using?\r\n\r\nr20.09\r\n\r\nAre you using the Triton container or did you build it yourself?\r\n\r\nContainer. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\n\r\nThe model is saved in Tensorflow saved model format with the following Tensorflow function calls:\r\n``` python\r\n        builder = tf.saved_model.builder.SavedModelBuilder(model_path_srv)\r\n        inputs = {'query_word_ids': tf.saved_model.utils.build_tensor_info(self.query_word_ids),\r\n                  'title1_word_ids': tf.saved_model.utils.build_tensor_info(self.title1_word_ids),\r\n                  'title2_word_ids': tf.saved_model.utils.build_tensor_info(self.title2_word_ids),\r\n                  'title3_word_ids': tf.saved_model.utils.build_tensor_info(self.title3_word_ids),\r\n                  'dropout': tf.saved_model.utils.build_tensor_info(self.dropout)}\r\n\r\n        outputs = {'output': tf.saved_model.utils.build_tensor_info(self.labels_prob)}\r\n\r\n        prediction_signature = (\r\n            tf.saved_model.signature_def_utils.build_signature_def(\r\n                inputs=inputs,\r\n                outputs=outputs,\r\n                method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\nServer would throw the following error:\r\n```\r\nE0127 00:06:37.106575 1 model_repository_manager.cc:899] failed to load '...' version 10: Invalid argument: model input cannot have empty reshape for non-batching model for ...\r\n```", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2461/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2229", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/2229/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/2229/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/2229/events", "html_url": "https://github.com/triton-inference-server/server/issues/2229", "id": 737456417, "node_id": "MDU6SXNzdWU3Mzc0NTY0MTc=", "number": 2229, "title": "sequence_batch_scheduler.cc:399 The previous sequence did not end before this sequence start", "user": {"login": "Slyne", "id": 6286804, "node_id": "MDQ6VXNlcjYyODY4MDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/6286804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Slyne", "html_url": "https://github.com/Slyne", "followers_url": "https://api.github.com/users/Slyne/followers", "following_url": "https://api.github.com/users/Slyne/following{/other_user}", "gists_url": "https://api.github.com/users/Slyne/gists{/gist_id}", "starred_url": "https://api.github.com/users/Slyne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Slyne/subscriptions", "organizations_url": "https://api.github.com/users/Slyne/orgs", "repos_url": "https://api.github.com/users/Slyne/repos", "events_url": "https://api.github.com/users/Slyne/events{/privacy}", "received_events_url": "https://api.github.com/users/Slyne/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-11-06T04:51:45Z", "updated_at": "2020-11-11T23:42:19Z", "closed_at": "2020-11-11T23:42:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nTrying the perf_analyzer\uff1a\r\nperf_analyzer -u \"localhost:8001\" --streaming --concurrency-range 500 -m kaldi_online -f perf.csv -i gRPC --input-data output.json\r\nI got lots of warnnings like 'The previous sequence did not end before this sequence start'.\r\n\r\n1. I want to know how to interpret this warnning 'The previous sequence did not end before this sequence start'. And how should I edit the input-data in order to test the streaming sequence model properly.\r\n2. Is the latency for the whole sequence or just the last chunk ? \r\n\r\n\r\nThe model kaldi_online can be found: https://github.com/NVIDIA/DeepLearningExamples/tree/master/Kaldi/SpeechRecognition\r\nThe output.json looks like:\r\n{\r\n  \"data\" :\r\n    [\r\n      [\r\n        {\r\n          \"WAV_DATA\" : [1.0, -1.560, .......],\r\n          \"WAV_DIM\": [8160]\r\n        },\r\n        {\r\n          \"WAV_DATA\" : [1.0, -1.560, .......],\r\n          \"WAV_DIM\": [8160]\r\n        },\r\n        {\r\n           \"WAV_DATA\" : [1.0, -1.560, .......],\r\n          \"WAV_DIM\": [8160]\r\n        },\r\n        {\r\n           \"WAV_DATA\" : [1.0, -1.560, .......],\r\n          \"WAV_DIM\": [8160]\r\n        }\r\n      ],\r\n     [....],\r\n     [...],\r\n]\r\n}\r\n\r\n**Triton Information**\r\n20.10\r\nnvcr.io/nvidia/tritonserver:20.10-py3-clientsdk\r\n\r\nAre you using the Triton container or did you build it yourself?\r\nTriton container\r\n\r\n**To Reproduce**\r\nRefer to the previous info.\r\n\r\nDescribe the models (framework, inputs, outputs), ideally include the model configuration file (if using an ensemble include the model configuration file for that as well).\r\nModel details can be found in https://github.com/NVIDIA/DeepLearningExamples/tree/master/Kaldi/SpeechRecognition.\r\n\r\n**Expected behavior**\r\nJust wanna know how to interpret the output of perf client on sequence data.\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/2229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/2229/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/1284", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/1284/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/1284/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/1284/events", "html_url": "https://github.com/triton-inference-server/server/issues/1284", "id": 596419216, "node_id": "MDU6SXNzdWU1OTY0MTkyMTY=", "number": 1284, "title": "unexpected shape for output  when output is 4 dims", "user": {"login": "CColten", "id": 8947579, "node_id": "MDQ6VXNlcjg5NDc1Nzk=", "avatar_url": "https://avatars.githubusercontent.com/u/8947579?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CColten", "html_url": "https://github.com/CColten", "followers_url": "https://api.github.com/users/CColten/followers", "following_url": "https://api.github.com/users/CColten/following{/other_user}", "gists_url": "https://api.github.com/users/CColten/gists{/gist_id}", "starred_url": "https://api.github.com/users/CColten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CColten/subscriptions", "organizations_url": "https://api.github.com/users/CColten/orgs", "repos_url": "https://api.github.com/users/CColten/repos", "events_url": "https://api.github.com/users/CColten/events{/privacy}", "received_events_url": "https://api.github.com/users/CColten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-08T08:56:40Z", "updated_at": "2020-04-13T07:42:13Z", "closed_at": "2020-04-13T07:42:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "trtis-19.10-py3\r\n\r\nconfig is :\r\ninput [\r\n    {\r\n        name: \"image_tensor\"\r\n        data_type: TYPE_FP32\r\n        dims: [ -1, -1, 3 ]\r\n    }\r\n]\r\noutput [\r\n   {\r\n      name: \"softmax_tensor\"\r\n      data_type: TYPE_FP32\r\n      dims: [ 1, -1, -1, 2 ]\r\n   } \r\n]\r\n\r\nI run, the error is : \r\nfailed to update context stat : [inference:0 20] ....  unexpected shape for output \"softmax_tensor\", model configuration shape is [1,-1,-1,2] , inference shape is [1,1280,720,2] \r\n\r\ni had tested 3 dims outpu, all are right. when load 4 dims output model, it \r\noccured", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/1284/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/1284/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/1138", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/1138/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/1138/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/1138/events", "html_url": "https://github.com/triton-inference-server/server/issues/1138", "id": 572139153, "node_id": "MDU6SXNzdWU1NzIxMzkxNTM=", "number": 1138, "title": "perf_client unable to allocate memory on gpu", "user": {"login": "zhm9484", "id": 12964346, "node_id": "MDQ6VXNlcjEyOTY0MzQ2", "avatar_url": "https://avatars.githubusercontent.com/u/12964346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhm9484", "html_url": "https://github.com/zhm9484", "followers_url": "https://api.github.com/users/zhm9484/followers", "following_url": "https://api.github.com/users/zhm9484/following{/other_user}", "gists_url": "https://api.github.com/users/zhm9484/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhm9484/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhm9484/subscriptions", "organizations_url": "https://api.github.com/users/zhm9484/orgs", "repos_url": "https://api.github.com/users/zhm9484/repos", "events_url": "https://api.github.com/users/zhm9484/events{/privacy}", "received_events_url": "https://api.github.com/users/zhm9484/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-02-27T15:01:00Z", "updated_at": "2020-03-02T17:16:57Z", "closed_at": "2020-03-02T17:16:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "## gpu: `Tesla V100`\r\n## version:  `20.01-py3`  \r\n## trtserver start cmd\r\n```shell\r\nsudo nvidia-docker run -d  --rm --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 -v/root/models:/models tensorrtserver:20.01-py3 trtserver --model-repository=/models\r\n```  \r\n## perf_client cmd\r\n```shell\r\n./perf_client -m inception_graphdef -b 1 --shared-memory=cuda\r\n```\r\n## output\r\n```shell\r\n[ 0] INTERNAL - unable to allocate memory of 4004bytes on gpu for output InceptionV3/Predictions/Softmax\r\n```\r\n\r\nAnything else should I provide?", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/1138/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/1138/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/1106", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/1106/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/1106/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/1106/events", "html_url": "https://github.com/triton-inference-server/server/issues/1106", "id": 564947289, "node_id": "MDU6SXNzdWU1NjQ5NDcyODk=", "number": 1106, "title": "Concurrency problem due to allocation of output buffers on device", "user": {"login": "mrjackbo", "id": 45255179, "node_id": "MDQ6VXNlcjQ1MjU1MTc5", "avatar_url": "https://avatars.githubusercontent.com/u/45255179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrjackbo", "html_url": "https://github.com/mrjackbo", "followers_url": "https://api.github.com/users/mrjackbo/followers", "following_url": "https://api.github.com/users/mrjackbo/following{/other_user}", "gists_url": "https://api.github.com/users/mrjackbo/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrjackbo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrjackbo/subscriptions", "organizations_url": "https://api.github.com/users/mrjackbo/orgs", "repos_url": "https://api.github.com/users/mrjackbo/repos", "events_url": "https://api.github.com/users/mrjackbo/events{/privacy}", "received_events_url": "https://api.github.com/users/mrjackbo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-02-13T21:00:40Z", "updated_at": "2020-03-18T18:05:38Z", "closed_at": "2020-03-12T22:45:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\nRecently, I noticed a performance regression running multiple TensorRT models (3 models, 2 instances each) concurrently on a single GPU (Titan V). They are ensembled and fed through a single custom backend. However I feed them, the average GPU utilization remains around 60%. I know that it was easy to reach full utilization with this setup a while ago.\r\n\r\nSo I turned to the nvidia profiler, and was surprised to see that after each execution of a TensorRT model, the corresponding thread calls `cudaMalloc` for each output. Of course, `cudaMalloc` synchronizes all work on the device, and hence severely limits concurrency.\r\nThe culprit appears to be here:\r\nhttps://github.com/NVIDIA/tensorrt-inference-server/blob/9bbfdb8f9c90616f2a6a165e27a9059660a1b6d2/src/core/backend_context.cc#L378-L382\r\nBy default, this allocates an output buffer on the same device that `output` is located on, resulting in a `cudaMalloc` call and hence in synchronization of the device.\r\n\r\nTo confirm my suspicion, I patched the above location to always request allocation of an output buffer with `TRTSERVER_MEMORY_CPU`, and the CPU utilization returned to the expected high level.\r\n\r\n\r\n\r\n**TRTIS Information**\r\nWhat version of TRTIS are you using?\r\nmaster, 19.12, 20.01,...\r\nAre you using the TRTIS container or did you build it yourself?\r\nboth\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/1106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/1106/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/1079", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/1079/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/1079/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/1079/events", "html_url": "https://github.com/triton-inference-server/server/pull/1079", "id": 558411926, "node_id": "MDExOlB1bGxSZXF1ZXN0MzY5ODAwODc0", "number": 1079, "title": "Skipping L0_perf_client's very high concurrency test", "user": {"login": "tanmayv25", "id": 10909321, "node_id": "MDQ6VXNlcjEwOTA5MzIx", "avatar_url": "https://avatars.githubusercontent.com/u/10909321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanmayv25", "html_url": "https://github.com/tanmayv25", "followers_url": "https://api.github.com/users/tanmayv25/followers", "following_url": "https://api.github.com/users/tanmayv25/following{/other_user}", "gists_url": "https://api.github.com/users/tanmayv25/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanmayv25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanmayv25/subscriptions", "organizations_url": "https://api.github.com/users/tanmayv25/orgs", "repos_url": "https://api.github.com/users/tanmayv25/repos", "events_url": "https://api.github.com/users/tanmayv25/events{/privacy}", "received_events_url": "https://api.github.com/users/tanmayv25/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-31T22:40:51Z", "updated_at": "2020-02-04T00:27:37Z", "closed_at": "2020-02-04T00:27:34Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "draft": false, "pull_request": {"url": "https://api.github.com/repos/triton-inference-server/server/pulls/1079", "html_url": "https://github.com/triton-inference-server/server/pull/1079", "diff_url": "https://github.com/triton-inference-server/server/pull/1079.diff", "patch_url": "https://github.com/triton-inference-server/server/pull/1079.patch", "merged_at": "2020-02-04T00:27:34Z"}, "body": "The test should be added back with the fix to DLIS-1054.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/1079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/1079/timeline", "performed_via_github_app": null, "state_reason": null}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/837", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/837/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/837/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/837/events", "html_url": "https://github.com/triton-inference-server/server/issues/837", "id": 519200897, "node_id": "MDU6SXNzdWU1MTkyMDA4OTc=", "number": 837, "title": "Improve error messages: backend not installed -> Segfault ", "user": {"login": "ohlr", "id": 26937880, "node_id": "MDQ6VXNlcjI2OTM3ODgw", "avatar_url": "https://avatars.githubusercontent.com/u/26937880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ohlr", "html_url": "https://github.com/ohlr", "followers_url": "https://api.github.com/users/ohlr/followers", "following_url": "https://api.github.com/users/ohlr/following{/other_user}", "gists_url": "https://api.github.com/users/ohlr/gists{/gist_id}", "starred_url": "https://api.github.com/users/ohlr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ohlr/subscriptions", "organizations_url": "https://api.github.com/users/ohlr/orgs", "repos_url": "https://api.github.com/users/ohlr/repos", "events_url": "https://api.github.com/users/ohlr/events{/privacy}", "received_events_url": "https://api.github.com/users/ohlr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "CoderHam", "id": 11223643, "node_id": "MDQ6VXNlcjExMjIzNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/11223643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CoderHam", "html_url": "https://github.com/CoderHam", "followers_url": "https://api.github.com/users/CoderHam/followers", "following_url": "https://api.github.com/users/CoderHam/following{/other_user}", "gists_url": "https://api.github.com/users/CoderHam/gists{/gist_id}", "starred_url": "https://api.github.com/users/CoderHam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CoderHam/subscriptions", "organizations_url": "https://api.github.com/users/CoderHam/orgs", "repos_url": "https://api.github.com/users/CoderHam/repos", "events_url": "https://api.github.com/users/CoderHam/events{/privacy}", "received_events_url": "https://api.github.com/users/CoderHam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2019-11-07T11:13:26Z", "updated_at": "2019-11-15T20:24:42Z", "closed_at": "2019-11-15T20:24:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Is your feature request related to a problem? Please describe.**\r\nIf a backend is not installed but a model which uses that backend is loaded the trtis responds with:\r\n\r\n`9746 Segmentation fault (core dumped)`\r\n\r\n**Describe the solution you'd like**\r\nImprove description: something along the lines:\r\n\r\n`the requested backend (tensorflow) is not installed`\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/837/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/556", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/556/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/556/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/556/events", "html_url": "https://github.com/triton-inference-server/server/issues/556", "id": 480785875, "node_id": "MDU6SXNzdWU0ODA3ODU4NzU=", "number": 556, "title": "Header path issues in librequest.so", "user": {"login": "philipp-schmidt", "id": 25586333, "node_id": "MDQ6VXNlcjI1NTg2MzMz", "avatar_url": "https://avatars.githubusercontent.com/u/25586333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/philipp-schmidt", "html_url": "https://github.com/philipp-schmidt", "followers_url": "https://api.github.com/users/philipp-schmidt/followers", "following_url": "https://api.github.com/users/philipp-schmidt/following{/other_user}", "gists_url": "https://api.github.com/users/philipp-schmidt/gists{/gist_id}", "starred_url": "https://api.github.com/users/philipp-schmidt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/philipp-schmidt/subscriptions", "organizations_url": "https://api.github.com/users/philipp-schmidt/orgs", "repos_url": "https://api.github.com/users/philipp-schmidt/repos", "events_url": "https://api.github.com/users/philipp-schmidt/events{/privacy}", "received_events_url": "https://api.github.com/users/philipp-schmidt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-14T16:56:12Z", "updated_at": "2019-08-29T15:43:47Z", "closed_at": "2019-08-29T15:43:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to build a standalone c++ client, which should link and make use of librequest.so.\r\nFor this, I would like to use the tar file provided in the client docker, e.g. `v1.5.0dev.clients.tar.gz`\r\n\r\nThe content looks like this:\r\n\r\n```\r\n.\r\n|-- bin\r\n|   |-- ensemble_image_client\r\n|   |-- image_client\r\n|   |-- perf_client\r\n|   |-- simple_callback_client\r\n|   |-- simple_client\r\n|   |-- simple_sequence_client\r\n|   |-- simple_shm_client\r\n|   `-- simple_string_client\r\n|-- include\r\n|   |-- api.pb.h\r\n|   |-- model_config.pb.h\r\n|   |-- request.h\r\n|   |-- request_grpc.h\r\n|   |-- request_http.h\r\n|   |-- request_status.pb.h\r\n|   `-- server_status.pb.h\r\n|-- lib\r\n|   `-- librequest.so\r\n|-- python\r\n|   |-- ensemble_image_client.py\r\n|   |-- grpc_image_client.py\r\n|   |-- image_client.py\r\n|   |-- simple_callback_client.py\r\n|   |-- simple_client.py\r\n|   |-- simple_sequence_client.py\r\n|   |-- simple_string_client.py\r\n|   `-- tensorrtserver-1.5.0.dev0-py2.py3-none-linux_x86_64.whl\r\n`-- v1.5.0dev.clients.tar.gz\r\n```\r\n\r\nSo in my CMakeLists.txt I link and include the library like this:\r\n\r\n```cmake\r\n# TRTIS client request library\r\nINCLUDE_DIRECTORIES(/path/to/above/include)\r\nlink_directories(/path/to/above/lib)\r\n```\r\n\r\nIn my main I include `#include \"request_grpc.h\"`, which is correctly found. But in this \"public\" library header, I get the following error now:\r\n\r\n```bash\r\nIn file included from /path/to/main.cpp:32:0:\r\n/path/to/request_grpc.h:30:10: fatal error: src/clients/c++/request.h: No such file or directory\r\n #include \"src/clients/c++/request.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n```\r\n\r\nWhich makes sense, as those \"public\" library headers try to include from the path \"src/clients/c++/\", which of course does not exist in this context.\r\n\r\nSo how can I use the library headers? Or in more general terms: how would I install and use this library from e.g. /usr/lib and /usr/include?", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/556/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/513", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/513/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/513/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/513/events", "html_url": "https://github.com/triton-inference-server/server/issues/513", "id": 475076148, "node_id": "MDU6SXNzdWU0NzUwNzYxNDg=", "number": 513, "title": "perf_client will silently ignore -f flag and not output a CSV in static concurrency mode", "user": {"login": "jobegrabber", "id": 889310, "node_id": "MDQ6VXNlcjg4OTMxMA==", "avatar_url": "https://avatars.githubusercontent.com/u/889310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jobegrabber", "html_url": "https://github.com/jobegrabber", "followers_url": "https://api.github.com/users/jobegrabber/followers", "following_url": "https://api.github.com/users/jobegrabber/following{/other_user}", "gists_url": "https://api.github.com/users/jobegrabber/gists{/gist_id}", "starred_url": "https://api.github.com/users/jobegrabber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jobegrabber/subscriptions", "organizations_url": "https://api.github.com/users/jobegrabber/orgs", "repos_url": "https://api.github.com/users/jobegrabber/repos", "events_url": "https://api.github.com/users/jobegrabber/events{/privacy}", "received_events_url": "https://api.github.com/users/jobegrabber/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-31T10:58:18Z", "updated_at": "2019-08-09T23:52:15Z", "closed_at": "2019-08-09T23:52:15Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Unless running in dynamic concurrency mode, perf_client will not output a summary CSV and silently ignore the `-f` flag if it's provided.\r\nI would expect either the a CSV output file, a warning or an error to be produced instead.\r\n\r\nThe CSV writing depends on whether the [`summary` vector is non-empty](https://github.com/NVIDIA/tensorrt-inference-server/blob/f7d93fd6f352ef27f8164ac3905d00bb751a34e1/src/clients/c%2B%2B/perf_client.cc#L2099) - and this vector is only appended to in dynamic concurrency mode:\r\n```c++\r\nif (summary.size()) { // summary gets only filled in dynamic concurrency mode\r\n  // ...\r\n    if (!filename.empty()) {\r\n      // write csv\r\n```\r\n\r\nThe CSV is produced when inserting `summary.push_back(status_summary);` [hereafter](https://github.com/NVIDIA/tensorrt-inference-server/blob/f7d93fd6f352ef27f8164ac3905d00bb751a34e1/src/clients/c%2B%2B/perf_client.cc#L2070).", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/513/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/459", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/459/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/459/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/459/events", "html_url": "https://github.com/triton-inference-server/server/issues/459", "id": 467890159, "node_id": "MDU6SXNzdWU0Njc4OTAxNTk=", "number": 459, "title": "Can't use GCS bucket name as model repository root", "user": {"login": "jobegrabber", "id": 889310, "node_id": "MDQ6VXNlcjg4OTMxMA==", "avatar_url": "https://avatars.githubusercontent.com/u/889310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jobegrabber", "html_url": "https://github.com/jobegrabber", "followers_url": "https://api.github.com/users/jobegrabber/followers", "following_url": "https://api.github.com/users/jobegrabber/following{/other_user}", "gists_url": "https://api.github.com/users/jobegrabber/gists{/gist_id}", "starred_url": "https://api.github.com/users/jobegrabber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jobegrabber/subscriptions", "organizations_url": "https://api.github.com/users/jobegrabber/orgs", "repos_url": "https://api.github.com/users/jobegrabber/repos", "events_url": "https://api.github.com/users/jobegrabber/events{/privacy}", "received_events_url": "https://api.github.com/users/jobegrabber/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2019-07-14T22:10:17Z", "updated_at": "2019-07-17T22:39:22Z", "closed_at": "2019-07-15T15:47:15Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "In the current state I am unable to configure a GCS bucket root as a model repository root, i.e. TRTIS is unable to use `gs://somebucket` as the root path for the model repository. `gs://somebucket/somedir` works.\r\nI expect this to be the case since [`ParsePath` of `filesystem`](https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/core/filesystem.cc#L278) will fail if there's no object specified in the path. `ParsePath` is called during the setup via a call to [`IsDirectory` of `filesystem`](https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/core/model_repository_manager.cc#L785) of the model repository.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/459/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/435", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/435/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/435/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/435/events", "html_url": "https://github.com/triton-inference-server/server/issues/435", "id": 463936089, "node_id": "MDU6SXNzdWU0NjM5MzYwODk=", "number": 435, "title": "Caffe2 model crashes on loading", "user": {"login": "digger18", "id": 11904051, "node_id": "MDQ6VXNlcjExOTA0MDUx", "avatar_url": "https://avatars.githubusercontent.com/u/11904051?v=4", "gravatar_id": "", "url": "https://api.github.com/users/digger18", "html_url": "https://github.com/digger18", "followers_url": "https://api.github.com/users/digger18/followers", "following_url": "https://api.github.com/users/digger18/following{/other_user}", "gists_url": "https://api.github.com/users/digger18/gists{/gist_id}", "starred_url": "https://api.github.com/users/digger18/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/digger18/subscriptions", "organizations_url": "https://api.github.com/users/digger18/orgs", "repos_url": "https://api.github.com/users/digger18/repos", "events_url": "https://api.github.com/users/digger18/events{/privacy}", "received_events_url": "https://api.github.com/users/digger18/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-07-03T20:20:31Z", "updated_at": "2019-07-15T16:08:55Z", "closed_at": "2019-07-15T16:08:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Logs:\r\n```\r\n===============================\r\n\r\n== TensorRT Inference Server ==\r\n\r\n===============================\r\n\r\nNVIDIA Release 19.06 (build 6791108)\r\n\r\nCopyright (c) 2018-2019, NVIDIA CORPORATION. All rights reserved.\r\n\r\nCopyright 2019 The TensorFlow Authors. All rights reserved.\r\n\r\nCopyright 2019 The TensorFlow Serving Authors. All rights reserved.\r\n\r\nCopyright (c) 2016-present, Facebook Inc. All rights reserved.\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION. All rights reserved.\r\n\r\nNVIDIA modifications are covered by the license terms that apply to the underlying\r\n\r\nproject or file.\r\n\r\nI0703 19:37:57.576868 1 main.cc:325] Starting endpoints, 'inference:0' listening on\r\n\r\nI0703 19:37:57.577000 1 grpc_server.cc:263] Starting a GRPCService at 0.0.0.0:8001\r\n\r\nI0703 19:37:57.577008 1 grpc_server.cc:269] Register TensorRT GRPCService\r\n\r\nI0703 19:37:57.577018 1 grpc_server.cc:272] Register Infer RPC\r\n\r\nI0703 19:37:57.577021 1 grpc_server.cc:276] Register StreamInfer RPC\r\n\r\nI0703 19:37:57.577023 1 grpc_server.cc:281] Register Status RPC\r\n\r\nI0703 19:37:57.577026 1 grpc_server.cc:285] Register Profile RPC\r\n\r\nI0703 19:37:57.577029 1 grpc_server.cc:289] Register Health RPC\r\n\r\nI0703 19:37:57.577032 1 grpc_server.cc:301] Register Executor\r\n\r\nI0703 19:37:57.581147 1 http_server.cc:579] Starting HTTPService at 0.0.0.0:8000\r\n\r\nI0703 19:37:57.622260 1 main.cc:356] localhost:8002 for metric reporting\r\n\r\nI0703 19:37:57.623293 1 metrics.cc:149] found 1 GPUs supporting NVML metrics\r\n\r\nI0703 19:37:57.628690 1 metrics.cc:158] GPU 0: GeForce RTX 2080 Ti\r\n\r\nI0703 19:37:57.629104 1 server.cc:113] Initializing TensorRT Inference Server\r\n\r\nI0703 19:37:57.670572 1 server_status.cc:83] New status tracking for model 'body_reid'\r\n\r\nI0703 19:37:57.670667 1 model_repository_manager.cc:627] loading: body_reid:2\r\n\r\nI0703 19:37:57.764862 1 netdef_backend.cc:214] Creating instance body_reid_0_gpu0 on GPU 0 (7.5) using init_model.netdef and model.netdef\r\n\r\n[E init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\n\r\n[E init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\n\r\n[E init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\n\r\nE0703 19:38:00.085397 1 model_repository_manager.cc:761] failed to load 'body_reid' version 2: Internal: load failed for 'body_reid': [enforce fail at operator.cc:60] blob != nullptr. op Conv: Encountered a non-existing input blob: 0\r\n\r\nframe #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*) + 0x76 (0x7f8a8b673326 in /opt/tensorrtserver/lib/libc10.so)\r\n\r\nframe #1: caffe2::OperatorBase::OperatorBase(caffe2::OperatorDef const&, caffe2::Workspace*) + 0xa5f (0x7f8ae0abb33f in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #2: <unknown function> + 0x2b259f5 (0x7f8a8e3ac9f5 in /opt/tensorrtserver/lib/libcaffe2_gpu.so)\r\n\r\nframe #3: <unknown function> + 0x3099d62 (0x7f8a8e920d62 in /opt/tensorrtserver/lib/libcaffe2_gpu.so)\r\n\r\nframe #4: <unknown function> + 0x309af7e (0x7f8a8e921f7e in /opt/tensorrtserver/lib/libcaffe2_gpu.so)\r\n\r\nframe #5: std::_Function_handler<std::unique_ptr<caffe2::OperatorBase, std::default_delete<caffe2::OperatorBase> > (caffe2::OperatorDef const&, caffe2::Workspace*), std::unique_ptr<caffe2::OperatorBase, std::default_delete<caffe2::OperatorBase> > (*)(caffe2::OperatorDef const&, caffe2::Workspace*)>::_M_invoke(std::_Any_data const&, caffe2::OperatorDef const&, caffe2::Workspace*&&) + 0x23 (0x7f8ae0977cf3 in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #6: <unknown function> + 0x18574cd (0x7f8ae0ab84cd in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #7: caffe2::CreateOperator(caffe2::OperatorDef const&, caffe2::Workspace*, int) + 0x32a (0x7f8ae0ab95ba in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #8: caffe2::SimpleNet::SimpleNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x28f (0x7f8ae0b338af in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #9: <unknown function> + 0x18d433e (0x7f8ae0b3533e in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #10: std::_Function_handler<std::unique_ptr<caffe2::NetBase, std::default_delete<caffe2::NetBase> > (std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*), std::unique_ptr<caffe2::NetBase, std::default_delete<caffe2::NetBase> > (*)(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*)>::_M_invoke(std::_Any_data const&, std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*&&) + 0x23 (0x7f8ae0b2e2f3 in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #11: caffe2::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0xb59 (0x7f8ae0b1ea29 in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #12: caffe2::Workspace::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, bool) + 0x136 (0x7f8ae0b3b836 in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #13: caffe2::Workspace::CreateNet(caffe2::NetDef const&, bool) + 0x9f (0x7f8ae0b3cb7f in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #14: <unknown function> + 0x18f39f7 (0x7f8ae0b549f7 in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #15: Caffe2WorkspaceCreate + 0x1416 (0x7f8ae0b56a86 in /opt/tensorrtserver/lib/libcaffe2.so)\r\n\r\nframe #16: nvidia::inferenceserver::NetDefBackend::CreateExecutionContext(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<char, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<char, std::allocator<char> > > > > const&) + 0x844 (0x7f8ae7539e84 in /opt/tensorrtserver/lib/libtrtserver.so)\r\n\r\nframe #17: nvidia::inferenceserver::NetDefBackend::CreateExecutionContexts(std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<char, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::vector<char, std::allocator<char> > > > > const&) + 0x378 (0x7f8ae753b1e8 in /opt/tensorrtserver/lib/libtrtserver.so)\r\n\r\nframe #18: nvidia::inferenceserver::NetDefBackendFactory::CreateBackend(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, nvidia::inferenceserver::ModelConfig const&, std::unique_ptr<nvidia::inferenceserver::InferenceBackend, std::default_delete<nvidia::inferenceserver::InferenceBackend> >*) + 0x77a (0x7f8ae75367ca in /opt/tensorrtserver/lib/libtrtserver.so)\r\n\r\nframe #19: nvidia::inferenceserver::ModelRepositoryManager::BackendLifeCycle::CreateBackendHandle(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, long, nvidia::inferenceserver::ModelRepositoryManager::BackendLifeCycle::BackendInfo*) + 0x49e (0x7f8ae74f024e in /opt/tensorrtserver/lib/libtrtserver.so)\r\n\r\nframe #20: std::thread::_Impl<std::_Bind_simple<std::_Mem_fn<nvidia::inferenceserver::Status (nvidia::inferenceserver::ModelRepositoryManager::BackendLifeCycle::*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, long, nvidia::inferenceserver::ModelRepositoryManager::BackendLifeCycle::BackendInfo*)> (nvidia::inferenceserver::ModelRepositoryManager::BackendLifeCycle*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, long, nvidia::inferenceserver::ModelRepositoryManager::BackendLifeCycle::BackendInfo*)> >::_M_run() + 0x49 (0x7f8ae74e5b19 in /opt/tensorrtserver/lib/libtrtserver.so)\r\n\r\nframe #21: <unknown function> + 0xb8c80 (0x7f8a7b076c80 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\r\n\r\nframe #22: <unknown function> + 0x76ba (0x7f8a7b8586ba in /lib/x86_64-linux-gnu/libpthread.so.0)\r\n\r\nframe #23: clone + 0x6d (0x7f8a7aae541d in /lib/x86_64-linux-gnu/libc.so.6)\r\n```\r\n\r\nConfiguration:\r\n```\r\nname: \"model_name\"\r\nplatform: \"caffe2_netdef\"\r\nmax_batch_size: 64\r\ninput [ {\r\n  name: \"data\"\r\n  data_type: TYPE_FP32\r\n  format: FORMAT_NCHW\r\n  dims: [3, 384, 128]\r\n} ]\r\noutput [ {\r\n  name: \"View_1\"\r\n  data_type: TYPE_FP32\r\n  dims: [512, 1, 1]\r\n} ]\r\ninstance_group [ {\r\n  name: \"model_name\"\r\n  count: 1\r\n  gpus: 0\r\n  kind: KIND_GPU\r\n} ]\r\ndynamic_batching {\r\n  preferred_batch_size: [ 8, 16, 32, 64 ]\r\n  max_queue_delay_microseconds: 25000\r\n}\r\n```\r\n\r\nI have placed both init_model.netdef and model.netdef files on the model repository.\r\n\r\nWhat am I doing wrong?", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/435/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/432", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/432/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/432/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/432/events", "html_url": "https://github.com/triton-inference-server/server/issues/432", "id": 463796166, "node_id": "MDU6SXNzdWU0NjM3OTYxNjY=", "number": 432, "title": "Tensorflow shape detection fails with multiple versions.", "user": {"login": "nieksand", "id": 1396037, "node_id": "MDQ6VXNlcjEzOTYwMzc=", "avatar_url": "https://avatars.githubusercontent.com/u/1396037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nieksand", "html_url": "https://github.com/nieksand", "followers_url": "https://api.github.com/users/nieksand/followers", "following_url": "https://api.github.com/users/nieksand/following{/other_user}", "gists_url": "https://api.github.com/users/nieksand/gists{/gist_id}", "starred_url": "https://api.github.com/users/nieksand/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nieksand/subscriptions", "organizations_url": "https://api.github.com/users/nieksand/orgs", "repos_url": "https://api.github.com/users/nieksand/repos", "events_url": "https://api.github.com/users/nieksand/events{/privacy}", "received_events_url": "https://api.github.com/users/nieksand/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-07-03T14:47:13Z", "updated_at": "2019-07-09T23:07:32Z", "closed_at": "2019-07-09T22:57:26Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Issue**: Automatic shape detection for TensorFlow models seems to break when there is more that one version in the model repository.  (Details below).\r\n\r\n**Expected Behavior**: I would expect TRTIS to verify that all versions have the same shape and to then use that shape.\r\n\r\nI can reproduce on the 19.04, 19.05, and 19.06 releases.\r\n\r\n---\r\n\r\nMy config.pbtxt is minimal:\r\n```\r\nplatform: \"tensorflow_savedmodel\"\r\nversion_policy: { all { } }\r\n```\r\n\r\nEverything is fine with just one version:\r\n```\r\nprompt> find 1/\r\n1/\r\n1/model.savedmodel\r\n1/model.savedmodel/saved_model.pb\r\n```\r\n\r\nBut if I make a second version, even with the exact same model, shape detection no longer works...\r\n```\r\nprompt> cp -R 1/ 2/\r\n\r\nprompt> docker run --rm --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -v /data/cape/models_dev/repository/:/models/ -it nvcr.io/nvidia/tensorrtserver:19.06-py3 trtserver --model-store=/models/ --strict-model-config=false\r\n```\r\n\r\nError from TRTIS logs:\r\n```\r\nE0703 14:41:47.017690 1 model_repository_manager.cc:761] failed to load 'rcr_v3' version 2: Invalid argument: unable to load model 'rcr_v3', configuration expects 0 inputs, model provides 1\r\nE0703 14:41:47.017870 1 model_repository_manager.cc:761] failed to load 'rcr_v3' version 1: Invalid argument: unable to load model 'rcr_v3', configuration expects 0 inputs, model provides 1\r\n```\r\n\r\nStatus then shows `MODEL_UNAVAILABLE`.\r\n\r\nIf I explicitly set the shape in config.pbtxt, both versions load fine:\r\n```\r\nplatform: \"tensorflow_savedmodel\"\r\nversion_policy: { all { }}\r\ninput {\r\n  name: \"input\"\r\n  data_type: TYPE_FP32\r\n  dims: -1\r\n  dims: 224\r\n  dims: 224\r\n  dims: 3\r\n}\r\noutput {\r\n  name: \"output\"\r\n  data_type: TYPE_FP32\r\n  dims: -1\r\n  dims: 8\r\n}\r\n```", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/432/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/363", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/363/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/363/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/363/events", "html_url": "https://github.com/triton-inference-server/server/issues/363", "id": 454866865, "node_id": "MDU6SXNzdWU0NTQ4NjY4NjU=", "number": 363, "title": "TRTIS stops serving all models when uploading a new model", "user": {"login": "achbogga", "id": 14843747, "node_id": "MDQ6VXNlcjE0ODQzNzQ3", "avatar_url": "https://avatars.githubusercontent.com/u/14843747?v=4", "gravatar_id": "", "url": "https://api.github.com/users/achbogga", "html_url": "https://github.com/achbogga", "followers_url": "https://api.github.com/users/achbogga/followers", "following_url": "https://api.github.com/users/achbogga/following{/other_user}", "gists_url": "https://api.github.com/users/achbogga/gists{/gist_id}", "starred_url": "https://api.github.com/users/achbogga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/achbogga/subscriptions", "organizations_url": "https://api.github.com/users/achbogga/orgs", "repos_url": "https://api.github.com/users/achbogga/repos", "events_url": "https://api.github.com/users/achbogga/events{/privacy}", "received_events_url": "https://api.github.com/users/achbogga/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-06-11T19:34:58Z", "updated_at": "2019-08-08T15:55:43Z", "closed_at": "2019-08-08T15:55:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "The server crashes/stops serving all the models while a new model is being uploaded to the server. Part of the reason might be the server detects a new model change even before the full upload is complete, and fails to parse the part model file. This, however, should not result in a crash/stop to serving all the other models which are already there being served.\r\n\r\nPlease let me know how to fix this issue. Thanks.\r\n@deadeyegoodwin ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/363/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/363/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/298", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/298/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/298/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/298/events", "html_url": "https://github.com/triton-inference-server/server/issues/298", "id": 446013737, "node_id": "MDU6SXNzdWU0NDYwMTM3Mzc=", "number": 298, "title": "caffe2_netdef InferenceServerException", "user": {"login": "bezero", "id": 11209421, "node_id": "MDQ6VXNlcjExMjA5NDIx", "avatar_url": "https://avatars.githubusercontent.com/u/11209421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bezero", "html_url": "https://github.com/bezero", "followers_url": "https://api.github.com/users/bezero/followers", "following_url": "https://api.github.com/users/bezero/following{/other_user}", "gists_url": "https://api.github.com/users/bezero/gists{/gist_id}", "starred_url": "https://api.github.com/users/bezero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bezero/subscriptions", "organizations_url": "https://api.github.com/users/bezero/orgs", "repos_url": "https://api.github.com/users/bezero/repos", "events_url": "https://api.github.com/users/bezero/events{/privacy}", "received_events_url": "https://api.github.com/users/bezero/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2019-05-20T09:29:58Z", "updated_at": "2019-06-04T19:31:56Z", "closed_at": "2019-06-04T19:31:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a caffe model with the following config:\r\n```proto\r\nname: \"caffe_model\"\r\nplatform: \"caffe2_netdef\"\r\nmax_batch_size: 2\r\ninput [\r\n  {\r\n    name: \"actual_input_1\"\r\n    data_type: TYPE_FP32\r\n    format: FORMAT_NCHW\r\n    dims: [ 3, 1024, 1024 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"output1\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 64512, 22  ]\r\n  }\r\n]\r\n```\r\n\r\nServer loads with no error. While inference request I received ```InferenceServerException``` error:\r\n```\r\n---------------------------------------------------------------------------\r\nInferenceServerException                  Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorrtserver/api/__init__.py in run(self, inputs, outputs, batch_size, flags)\r\n    868 \r\n    869         # Run inference...\r\n--> 870         self._last_request_id = _raise_if_error(c_void_p(_crequest_infer_ctx_run(self._ctx)))\r\n    871 \r\n    872         return self._get_results(outputs, batch_size)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorrtserver/api/__init__.py in _raise_if_error(err)\r\n    178         _crequest_error_del(err)\r\n    179         if not isok:\r\n--> 180             raise ex\r\n    181         return ex.request_id()\r\n    182     return 0\r\n\r\nInferenceServerException: [inference:0 3] failed to run model 'caffe_model': [enforce fail at reshape_op.h:86] . Can not reshape a non-zero size (67584) tensor to zero size.\r\nError from operator: \r\ninput: \"636\" input: \"650\" output: \"651\" output: \"OC2_DUMMY_1\" name: \"\" type: \"Reshape\" device_option { device_type: 1 device_id: 1 }frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*) + 0x76 (0x7f745af9c0e6 in /opt/tensorrtserver/lib/libc10.so)\r\nframe #1: <unknown function> + 0x2e0e673 (0x7f745dfb9673 in /opt/tensorrtserver/lib/libcaffe2_gpu.so)\r\nframe #2: <unknown function> + 0x2e0ea2e (0x7f745dfb9a2e in /opt/tensorrtserver/lib/libcaffe2_gpu.so)\r\nframe #3: <unknown function> + 0x27866f2 (0x7f745d9316f2 in /opt/tensorrtserver/lib/libcaffe2_gpu.so)\r\nframe #4: caffe2::SimpleNet::Run() + 0x1c9 (0x7f74af024b49 in /opt/tensorrtserver/lib/libcaffe2.so)\r\nframe #5: caffe2::Workspace::RunNet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x4d (0x7f74af045a2d in /opt/tensorrtserver/lib/libcaffe2.so)\r\nframe #6: <unknown function> + 0x15cb017 (0x7f74af0a2017 in /opt/tensorrtserver/lib/libcaffe2.so)\r\nframe #7: <unknown function> + 0x92e89b (0x5593f0dfa89b in trtserver)\r\nframe #8: <unknown function> + 0x92f03f (0x5593f0dfb03f in trtserver)\r\nframe #9: <unknown function> + 0x92f5fa (0x5593f0dfb5fa in trtserver)\r\nframe #10: <unknown function> + 0x9768e2 (0x5593f0e428e2 in trtserver)\r\nframe #11: <unknown function> + 0xb8c80 (0x7f744bbabc80 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\r\nframe #12: <unknown function> + 0x76ba (0x7f744c7b36ba in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #13: clone + 0x6d (0x7f744b61a41d in /lib/x86_64-linux-gnu/libc.so.6)\r\n```\r\n\r\nTo verify that model is working correctly I used ```nvcr.io/nvidia/pytorch:19.04-py3``` container to run model manually with the following script and it worked:\r\n\r\n```py\r\nfrom caffe2.python import core, workspace, models\r\nimport numpy as np\r\nwith open('init_model.netdef', \"rb\") as f:\r\n    init_net = f.read()\r\nwith open('model.netdef', \"rb\") as f:\r\n    predict_net = f.read()\r\np = workspace.Predictor(init_net, predict_net)\r\nresults = p.run({'actual_input_1': np.zeros((10, 3, 1024, 1024), dtype=np.float32)})\r\nprint(results[0].shape)  # (10, 64512, 22)\r\n```\r\nWhere does this error come from?", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/298/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/285", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/285/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/285/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/285/events", "html_url": "https://github.com/triton-inference-server/server/issues/285", "id": 442608249, "node_id": "MDU6SXNzdWU0NDI2MDgyNDk=", "number": 285, "title": "Custom backends with errors are flagged as READY", "user": {"login": "SlipknotTN", "id": 10343467, "node_id": "MDQ6VXNlcjEwMzQzNDY3", "avatar_url": "https://avatars.githubusercontent.com/u/10343467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SlipknotTN", "html_url": "https://github.com/SlipknotTN", "followers_url": "https://api.github.com/users/SlipknotTN/followers", "following_url": "https://api.github.com/users/SlipknotTN/following{/other_user}", "gists_url": "https://api.github.com/users/SlipknotTN/gists{/gist_id}", "starred_url": "https://api.github.com/users/SlipknotTN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SlipknotTN/subscriptions", "organizations_url": "https://api.github.com/users/SlipknotTN/orgs", "repos_url": "https://api.github.com/users/SlipknotTN/repos", "events_url": "https://api.github.com/users/SlipknotTN/events{/privacy}", "received_events_url": "https://api.github.com/users/SlipknotTN/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-10T08:49:26Z", "updated_at": "2019-06-27T17:32:42Z", "closed_at": "2019-06-27T17:32:42Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "How to reproduce it:\r\n- Use addsub example\r\n- Change config.pbtxt setting wrong output names\r\n\r\ne.g.\r\n\r\n```\r\nname: \"add_sub_custom\"\r\nplatform: \"custom\"\r\nmax_batch_size: 0\r\ndefault_model_filename: \"libaddsub.so\"\r\ninput [\r\n  {\r\n    name: \"INPUT0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 16 ]\r\n  },\r\n  {\r\n    name: \"INPUT1\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 16 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"OUTPUT0_AAAAAA\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 16 ]\r\n  },\r\n  {\r\n    name: \"OUTPUT1\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 16 ]\r\n  }\r\n]\r\n```\r\n\r\ninstead of\r\n```\r\nname: \"add_sub_custom\"\r\nplatform: \"custom\"\r\nmax_batch_size: 0\r\ndefault_model_filename: \"libaddsub.so\"\r\ninput [\r\n  {\r\n    name: \"INPUT0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 16 ]\r\n  },\r\n  {\r\n    name: \"INPUT1\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 16 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"OUTPUT0\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 16 ]\r\n  },\r\n  {\r\n    name: \"OUTPUT1\"\r\n    data_type: TYPE_FP32\r\n    dims: [ 16 ]\r\n  }\r\n]\r\n```\r\n\r\nYou get this error in server log \r\n```\r\nE0510 08:34:22.755094 1000 dynamic_batch_scheduler.cc:162] Initialization failed for dynamic-batch scheduler thread 0: initialize error for 'add_sub_custom': (6) model outputs must be named 'OUTPUT0' and 'OUTPUT1'\r\n```\r\n\r\nbut model is flagged as READY (successfully loaded and MODEL_READY status).\r\n\r\nIf try to call the model, the client request hangs.\r\nIs it an intended behaviour? I think this case, custom model should not be loaded.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/285/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/285/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/269", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/269/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/269/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/269/events", "html_url": "https://github.com/triton-inference-server/server/issues/269", "id": 440657904, "node_id": "MDU6SXNzdWU0NDA2NTc5MDQ=", "number": 269, "title": "Inference performance hit a limit with one trtserver instance via grpc", "user": {"login": "TrojanXu", "id": 6835198, "node_id": "MDQ6VXNlcjY4MzUxOTg=", "avatar_url": "https://avatars.githubusercontent.com/u/6835198?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TrojanXu", "html_url": "https://github.com/TrojanXu", "followers_url": "https://api.github.com/users/TrojanXu/followers", "following_url": "https://api.github.com/users/TrojanXu/following{/other_user}", "gists_url": "https://api.github.com/users/TrojanXu/gists{/gist_id}", "starred_url": "https://api.github.com/users/TrojanXu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TrojanXu/subscriptions", "organizations_url": "https://api.github.com/users/TrojanXu/orgs", "repos_url": "https://api.github.com/users/TrojanXu/repos", "events_url": "https://api.github.com/users/TrojanXu/events{/privacy}", "received_events_url": "https://api.github.com/users/TrojanXu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-05-06T11:40:29Z", "updated_at": "2019-07-03T19:29:47Z", "closed_at": "2019-07-03T19:29:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Client: perf_client\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nTRTIS Docker image: 19.04\r\nCUDA Driver: 418.18\r\nGPU model and memory: 8 V100-16G\r\nModel used: resnet50_trt_fp16\r\n\r\n1. Deploy on one GPU,\r\n./perf_client -m resnet50_trt_fp16 -d -c64 -l2000 -p5000 -b8 -i grpc -u localhost:8001 -t4  --max-threads=16\r\n```\r\nRequest concurrency: 21\r\n  Client:\r\n    Request count: 1520\r\n    Throughput: 2432 infer/sec\r\n    Avg latency: 61525 usec (standard deviation 20319 usec)\r\n    Avg gRPC time: 43874 usec ((un)marshal request/response 1534 usec + response wait 42340 usec)\r\n  Server:\r\n    Request count: 1825\r\n    Avg request latency: 4787 usec (overhead 18 usec + queue 52 usec + compute 4717 usec)\r\n```\r\n2. But if you deploy the model to two gpus,\r\nThe total throughput is similar\r\n```\r\nRequest concurrency: 22\r\nClient:\r\nRequest count: 1435\r\nThroughput: 2296 infer/sec\r\nAvg latency: 67370 usec (standard deviation 23489 usec)\r\nAvg gRPC time: 46339 usec ((un)marshal request/response 1772 usec + response wait 44567 usec)\r\nServer:\r\nRequest count: 1740\r\nAvg request latency: 3783 usec (overhead 17 usec + queue 54 usec + compute 3712 usec)\r\n```\r\nGPU utilization drops to ~50%\r\n\r\nThis phenomena exists for different backends and similarly, close to a upper limit ~2400 infer/sec\r\nThis can be circumvented by launching multiple trtserver instances.\r\n\r\nConfiguring grpc-infer-thread-count has no effect.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/269/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/174", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/174/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/174/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/174/events", "html_url": "https://github.com/triton-inference-server/server/pull/174", "id": 424436206, "node_id": "MDExOlB1bGxSZXF1ZXN0MjYzNzkwODY5", "number": 174, "title": "DRIVER VERSIONS can be xxx.yy.zz if coming from side branches", "user": {"login": "cliffwoolley", "id": 9682398, "node_id": "MDQ6VXNlcjk2ODIzOTg=", "avatar_url": "https://avatars.githubusercontent.com/u/9682398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cliffwoolley", "html_url": "https://github.com/cliffwoolley", "followers_url": "https://api.github.com/users/cliffwoolley/followers", "following_url": "https://api.github.com/users/cliffwoolley/following{/other_user}", "gists_url": "https://api.github.com/users/cliffwoolley/gists{/gist_id}", "starred_url": "https://api.github.com/users/cliffwoolley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cliffwoolley/subscriptions", "organizations_url": "https://api.github.com/users/cliffwoolley/orgs", "repos_url": "https://api.github.com/users/cliffwoolley/repos", "events_url": "https://api.github.com/users/cliffwoolley/events{/privacy}", "received_events_url": "https://api.github.com/users/cliffwoolley/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-03-22T23:41:57Z", "updated_at": "2019-03-22T23:52:46Z", "closed_at": "2019-03-22T23:52:46Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "draft": false, "pull_request": {"url": "https://api.github.com/repos/triton-inference-server/server/pulls/174", "html_url": "https://github.com/triton-inference-server/server/pull/174", "diff_url": "https://github.com/triton-inference-server/server/pull/174.diff", "patch_url": "https://github.com/triton-inference-server/server/pull/174.patch", "merged_at": "2019-03-22T23:52:46Z"}, "body": "Fixes #173", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/174/timeline", "performed_via_github_app": null, "state_reason": null}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/147", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/147/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/147/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/147/events", "html_url": "https://github.com/triton-inference-server/server/issues/147", "id": 419795788, "node_id": "MDU6SXNzdWU0MTk3OTU3ODg=", "number": 147, "title": "Question: TRTIS Caffe2 does not support models with both CPU and GPU ops?", "user": {"login": "erickim555", "id": 1179697, "node_id": "MDQ6VXNlcjExNzk2OTc=", "avatar_url": "https://avatars.githubusercontent.com/u/1179697?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erickim555", "html_url": "https://github.com/erickim555", "followers_url": "https://api.github.com/users/erickim555/followers", "following_url": "https://api.github.com/users/erickim555/following{/other_user}", "gists_url": "https://api.github.com/users/erickim555/gists{/gist_id}", "starred_url": "https://api.github.com/users/erickim555/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erickim555/subscriptions", "organizations_url": "https://api.github.com/users/erickim555/orgs", "repos_url": "https://api.github.com/users/erickim555/repos", "events_url": "https://api.github.com/users/erickim555/events{/privacy}", "received_events_url": "https://api.github.com/users/erickim555/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1079803573, "node_id": "MDU6TGFiZWwxMDc5ODAzNTcz", "url": "https://api.github.com/repos/triton-inference-server/server/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-03-12T04:23:45Z", "updated_at": "2020-06-25T17:24:57Z", "closed_at": "2020-06-25T17:24:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://github.com/NVIDIA/tensorrt-inference-server/blob/0a73c5ce0d4c53ff55c23f05171b2c02d2c60c1d/src/servables/caffe2/netdef_bundle_c2.cc#L273-L290\r\n\r\nHello, I'm looking forward to using TRTIS to serve Caffe2 object detection models, specifically FRCNN-like models from Detectron: https://github.com/facebookresearch/Detectron\r\n\r\nHowever, in the above lines of the TRTIS source code, it looks like TRTIS is explicitly overwriting the `device_option` field of all ops to either GPU or CPU (depending on `gpu_device` input parameter). Thus, it disallows mixing CPU and GPU ops. This is problematic, as the Detectron FRCNN models contain custom C++ CPU ops, such as the CollectAndDistributeFpnRpnProposals op: [link](https://caffe2.ai/docs/operators-catalogue.html#collectanddistributefpnrpnproposals)\r\n\r\nIs it a fundamental TRTIS restriction that a Caffe2 model must either run purely on the GPU, or purely on the CPU? If not, then for our usecase I'll just comment out this block and build+deploy a custom TRTIS binary.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/147/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/140", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/140/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/140/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/140/events", "html_url": "https://github.com/triton-inference-server/server/issues/140", "id": 418736196, "node_id": "MDU6SXNzdWU0MTg3MzYxOTY=", "number": 140, "title": "HTTP 200 response on invalid output names", "user": {"login": "nieksand", "id": 1396037, "node_id": "MDQ6VXNlcjEzOTYwMzc=", "avatar_url": "https://avatars.githubusercontent.com/u/1396037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nieksand", "html_url": "https://github.com/nieksand", "followers_url": "https://api.github.com/users/nieksand/followers", "following_url": "https://api.github.com/users/nieksand/following{/other_user}", "gists_url": "https://api.github.com/users/nieksand/gists{/gist_id}", "starred_url": "https://api.github.com/users/nieksand/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nieksand/subscriptions", "organizations_url": "https://api.github.com/users/nieksand/orgs", "repos_url": "https://api.github.com/users/nieksand/repos", "events_url": "https://api.github.com/users/nieksand/events{/privacy}", "received_events_url": "https://api.github.com/users/nieksand/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-08T10:52:35Z", "updated_at": "2019-04-26T17:20:31Z", "closed_at": "2019-04-26T17:20:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I am using the http endpoint on `tensorrtserver:19.02-py3` with a TensorRT model.\r\n\r\nIf I pass the correct output tensor name to the http endpoint, everything is fine:\r\n```\r\n(Pdb) response.code\r\n200\r\n(Pdb) response.body\r\nb'\\x08\\xd2\\xb99\\xf0\\xb2\\x1f=\\x1eQd?!Dp=\\xf6\\xb52:$\\xe2\\x1a<model_name: \"roof_condition\"\\nmodel_version: 1\\nbatch_size: 1\\noutput {\\n  name: \"import/softmax_output/Softmax\"\\n  raw {\\n    dims: 1\\n    dims: 1\\n    dims: 6\\n    batch_byte_size: 24\\n  }\\n}\\n'\r\n(Pdb) [v for v in response.headers.items()]\r\n[('Nv-Inferresponse', 'model_name: \"roof_condition\" model_version: 1 batch_size: 1 output { name: \"import/softmax_output/Softmax\" raw { dims: 1 dims: 1 dims: 6 batch_byte_size: 24 } }'), ('Nv-Status', 'code: SUCCESS server_id: \"inference:0\" request_id: 10083'), ('Content-Type', 'application/octet-stream'), ('Date', 'Fri, 08 Mar 2019 10:55:42 GMT'), ('Content-Length', '207'), ('Connection', 'close')]\r\n```\r\n\r\nBut if I ask for a non-existent output name like \"potato\":\r\n* TRTIS still hands back an http 200 response\r\n* The `Nv-Status` header still indicates SUCCESS.\r\n```\r\n(Pdb) response.code\r\n200\r\n(Pdb) response.body\r\nb'model_name: \"roof_condition\"\\nmodel_version: 1\\nbatch_size: 1\\n'\r\n(Pdb) [v for v in response.headers.items()]\r\n[('Nv-Inferresponse', 'model_name: \"roof_condition\" model_version: 1 batch_size: 1'), ('Nv-Status', 'code: SUCCESS server_id: \"inference:0\" request_id: 10082'), ('Content-Type', 'application/octet-stream'), ('Date', 'Fri, 08 Mar 2019 10:42:55 GMT'), ('Content-Length', '60'), ('Connection', 'close')]\r\n```\r\n\r\nI think TRTIS should return an error (e.g. http 4xx) when asked for a non-existent output name.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/140/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/140/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/134", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/134/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/134/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/134/events", "html_url": "https://github.com/triton-inference-server/server/issues/134", "id": 418296522, "node_id": "MDU6SXNzdWU0MTgyOTY1MjI=", "number": 134, "title": "Unable to collect inference metrics for nullptr servable", "user": {"login": "nieksand", "id": 1396037, "node_id": "MDQ6VXNlcjEzOTYwMzc=", "avatar_url": "https://avatars.githubusercontent.com/u/1396037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nieksand", "html_url": "https://github.com/nieksand", "followers_url": "https://api.github.com/users/nieksand/followers", "following_url": "https://api.github.com/users/nieksand/following{/other_user}", "gists_url": "https://api.github.com/users/nieksand/gists{/gist_id}", "starred_url": "https://api.github.com/users/nieksand/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nieksand/subscriptions", "organizations_url": "https://api.github.com/users/nieksand/orgs", "repos_url": "https://api.github.com/users/nieksand/repos", "events_url": "https://api.github.com/users/nieksand/events{/privacy}", "received_events_url": "https://api.github.com/users/nieksand/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-07T13:00:11Z", "updated_at": "2019-03-21T15:48:06Z", "closed_at": "2019-03-21T15:48:06Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "If I shutdown TRTIS while new requests are still coming in, I get a slew of  \"Unable to collect inference metrics for nullptr servable\" messages near the end of the shutdown process.\r\n\r\nThis is just an annoyance.  It doesn't seem to hurt anything.\r\n\r\nI am using the http endpoint on `tensorrtserver:19.02-py3 ` with a GTX 1060.\r\n\r\nThis gist shows the hanky panky:\r\nhttps://gist.github.com/nieksand/007e70d49b2b6bba63144715d97f481e\r\n\r\nMy client script is just serially poking TRTIS: send single inference request, wait for the response, send the next/\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/134/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/134/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/122", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/122/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/122/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/122/events", "html_url": "https://github.com/triton-inference-server/server/issues/122", "id": 415865900, "node_id": "MDU6SXNzdWU0MTU4NjU5MDA=", "number": 122, "title": "Potential GPU memory leak for TensorFlow models?", "user": {"login": "qianl15", "id": 11331727, "node_id": "MDQ6VXNlcjExMzMxNzI3", "avatar_url": "https://avatars.githubusercontent.com/u/11331727?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qianl15", "html_url": "https://github.com/qianl15", "followers_url": "https://api.github.com/users/qianl15/followers", "following_url": "https://api.github.com/users/qianl15/following{/other_user}", "gists_url": "https://api.github.com/users/qianl15/gists{/gist_id}", "starred_url": "https://api.github.com/users/qianl15/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qianl15/subscriptions", "organizations_url": "https://api.github.com/users/qianl15/orgs", "repos_url": "https://api.github.com/users/qianl15/repos", "events_url": "https://api.github.com/users/qianl15/events{/privacy}", "received_events_url": "https://api.github.com/users/qianl15/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-02-28T23:09:59Z", "updated_at": "2022-02-17T10:01:14Z", "closed_at": "2019-03-04T16:52:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI was testing with `tensorrtserver:19.02-py3`. However, I found the GPU memory usage became near 100% after running one TensorFlow savedmodel. The memory usage didn't go down even when I unloaded that model. Moreover, I have set `--tf-gpu-memory-fraction=0.1` but it didn't help.\r\n\r\nSo is it possible that trtserver has GPU memory leak for TensorFlow models? Because the memory usage looks good with tensorrt plans.\r\n\r\nThanks", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/122/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/55", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/55/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/55/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/55/events", "html_url": "https://github.com/triton-inference-server/server/issues/55", "id": 403026130, "node_id": "MDU6SXNzdWU0MDMwMjYxMzA=", "number": 55, "title": "Multiple GPU scheduling", "user": {"login": "bezero", "id": 11209421, "node_id": "MDQ6VXNlcjExMjA5NDIx", "avatar_url": "https://avatars.githubusercontent.com/u/11209421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bezero", "html_url": "https://github.com/bezero", "followers_url": "https://api.github.com/users/bezero/followers", "following_url": "https://api.github.com/users/bezero/following{/other_user}", "gists_url": "https://api.github.com/users/bezero/gists{/gist_id}", "starred_url": "https://api.github.com/users/bezero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bezero/subscriptions", "organizations_url": "https://api.github.com/users/bezero/orgs", "repos_url": "https://api.github.com/users/bezero/repos", "events_url": "https://api.github.com/users/bezero/events{/privacy}", "received_events_url": "https://api.github.com/users/bezero/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-01-25T06:43:09Z", "updated_at": "2019-01-29T19:55:31Z", "closed_at": "2019-01-29T19:55:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have 2 GTX 1080ti available and a single servable model in my models repository. When I run tensorrtserver:18.12-py3 it does discover both gpu's and an instance of my model is created for both gpu's. However, while testing server with many requests the load is only on first gpu, the second one is idle and not used. When switched back to tensorrtserver:18.09-py3 (which I used previously) TRTIS uses both gpu resources as expected. ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/55/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/55/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/36", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/36/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/36/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/36/events", "html_url": "https://github.com/triton-inference-server/server/issues/36", "id": 397431782, "node_id": "MDU6SXNzdWUzOTc0MzE3ODI=", "number": 36, "title": "New labels file is not detected", "user": {"login": "bezero", "id": 11209421, "node_id": "MDQ6VXNlcjExMjA5NDIx", "avatar_url": "https://avatars.githubusercontent.com/u/11209421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bezero", "html_url": "https://github.com/bezero", "followers_url": "https://api.github.com/users/bezero/followers", "following_url": "https://api.github.com/users/bezero/following{/other_user}", "gists_url": "https://api.github.com/users/bezero/gists{/gist_id}", "starred_url": "https://api.github.com/users/bezero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bezero/subscriptions", "organizations_url": "https://api.github.com/users/bezero/orgs", "repos_url": "https://api.github.com/users/bezero/repos", "events_url": "https://api.github.com/users/bezero/events{/privacy}", "received_events_url": "https://api.github.com/users/bezero/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-01-09T15:48:03Z", "updated_at": "2019-01-24T00:50:02Z", "closed_at": "2019-01-24T00:50:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "I deployed a project which contains model foder (version 1), config file and labels_1.txt for the given version. When new version of the model is introduced, TRTIS automatically detects new version. However, if labels file for the new version is changed, TRTIS does not update \"label_filename\" unless I restart TRTIS.\r\n\r\nTo update labels I add labels_2.txt file to the project folder and update \"label_filename\" in the config file. Is it possible to detect these changes without restarting TRTIS.", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/36/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/36/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/27", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/27/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/27/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/27/events", "html_url": "https://github.com/triton-inference-server/server/issues/27", "id": 393981948, "node_id": "MDU6SXNzdWUzOTM5ODE5NDg=", "number": 27, "title": "cp: cannot stat 'bazel-bin/src/custom/addsub/libaddsub.so': No such file or directory", "user": {"login": "daemon11", "id": 7776997, "node_id": "MDQ6VXNlcjc3NzY5OTc=", "avatar_url": "https://avatars.githubusercontent.com/u/7776997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/daemon11", "html_url": "https://github.com/daemon11", "followers_url": "https://api.github.com/users/daemon11/followers", "following_url": "https://api.github.com/users/daemon11/following{/other_user}", "gists_url": "https://api.github.com/users/daemon11/gists{/gist_id}", "starred_url": "https://api.github.com/users/daemon11/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/daemon11/subscriptions", "organizations_url": "https://api.github.com/users/daemon11/orgs", "repos_url": "https://api.github.com/users/daemon11/repos", "events_url": "https://api.github.com/users/daemon11/events{/privacy}", "received_events_url": "https://api.github.com/users/daemon11/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-12-25T07:59:53Z", "updated_at": "2018-12-26T06:18:48Z", "closed_at": "2018-12-26T06:18:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I run:\r\ndocker build -t tensorrtserver_clients --target trtserver_build --build-arg \"PYVER=2.7\" --build-arg \"BUILD_CLIENTS_ONLY=1\" .\r\n\r\nI have some problems\uff1a\r\nINFO: Elapsed time: 94.683s, Critical Path: 14.74s\r\nINFO: 1848 processes: 1848 local.\r\nINFO: Build completed successfully, 2001 total actions\r\nINFO: Build completed successfully, 2001 total actions\r\ncp: cannot stat 'bazel-bin/src/custom/addsub/libaddsub.so': No such file or directory\r\nThe command '/bin/sh -c (cd /opt/tensorflow && ./nvbuild.sh --python$PYVER --configonly) &&     (cd tools && mv bazel.rc bazel.orig &&      cat bazel.orig /opt/tensorflow/.tf_configure.bazelrc > bazel.rc) &&     bash -c 'if [ \"$BUILD_CLIENTS_ONLY\" != \"1\" ]; then                bazel build -c opt --config=cuda                      src/servers/trtserver                      src/custom/...                      src/clients/...                      src/test/...;              else                bazel build -c opt src/clients/...;              fi' &&     (cd /opt/tensorrtserver && ln -s /workspace/qa qa) &&     mkdir -p /opt/tensorrtserver/bin &&     cp bazel-bin/src/clients/c++/image_client /opt/tensorrtserver/bin/. &&     cp bazel-bin/src/clients/c++/perf_client /opt/tensorrtserver/bin/. &&     cp bazel-bin/src/clients/c++/simple_client /opt/tensorrtserver/bin/. &&     mkdir -p /opt/tensorrtserver/lib &&     cp bazel-bin/src/clients/c++/librequest.so /opt/tensorrtserver/lib/. &&     cp bazel-bin/src/clients/c++/librequest.a /opt/tensorrtserver/lib/. &&     mkdir -p /opt/tensorrtserver/custom &&     cp bazel-bin/src/custom/addsub/libaddsub.so /opt/tensorrtserver/custom/. &&     mkdir -p /opt/tensorrtserver/pip &&     bazel-bin/src/clients/python/build_pip /opt/tensorrtserver/pip/. &&     bash -c 'if [ \"$BUILD_CLIENTS_ONLY\" != \"1\" ]; then                cp bazel-bin/src/servers/trtserver /opt/tensorrtserver/bin/.;                cp bazel-bin/src/test/caffe2plan /opt/tensorrtserver/bin/.;              fi' &&     bazel clean --expunge &&     rm -rf /root/.cache/bazel &&     rm -rf /tmp/*' returned a non-zero code: 1\r\n\r\nCan you help? Thank you\r\n", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/27/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/27/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/14", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/14/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/14/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/14/events", "html_url": "https://github.com/triton-inference-server/server/issues/14", "id": 390988259, "node_id": "MDU6SXNzdWUzOTA5ODgyNTk=", "number": 14, "title": "perf_client has limits on concurrency", "user": {"login": "mrjackbo", "id": 45255179, "node_id": "MDQ6VXNlcjQ1MjU1MTc5", "avatar_url": "https://avatars.githubusercontent.com/u/45255179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrjackbo", "html_url": "https://github.com/mrjackbo", "followers_url": "https://api.github.com/users/mrjackbo/followers", "following_url": "https://api.github.com/users/mrjackbo/following{/other_user}", "gists_url": "https://api.github.com/users/mrjackbo/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrjackbo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrjackbo/subscriptions", "organizations_url": "https://api.github.com/users/mrjackbo/orgs", "repos_url": "https://api.github.com/users/mrjackbo/repos", "events_url": "https://api.github.com/users/mrjackbo/events{/privacy}", "received_events_url": "https://api.github.com/users/mrjackbo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-12-14T06:54:40Z", "updated_at": "2019-01-29T19:55:30Z", "closed_at": "2019-01-29T19:55:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "I cannot max out my two GPUs using one instance of perf_client, but using multiple instances I can. \r\n\r\nA little bit more detail:\r\nUsing perf_client with GRPC and -a, the throughput does not increase when I go beyond -t 9.\r\nBut using four instances with -t 9 simultaneously for the same model, I get much higher total throughput. This is can be verified using trtis metrics. \r\nIs this expected behavior? I can share the precise command that I use when I am back at my desk, but I cannot share the model that I use. ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/14/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/14/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/triton-inference-server/server/issues/13", "repository_url": "https://api.github.com/repos/triton-inference-server/server", "labels_url": "https://api.github.com/repos/triton-inference-server/server/issues/13/labels{/name}", "comments_url": "https://api.github.com/repos/triton-inference-server/server/issues/13/comments", "events_url": "https://api.github.com/repos/triton-inference-server/server/issues/13/events", "html_url": "https://github.com/triton-inference-server/server/issues/13", "id": 390954867, "node_id": "MDU6SXNzdWUzOTA5NTQ4Njc=", "number": 13, "title": "Encountered out of memoryError when using perf_client test with various batch sizes", "user": {"login": "leeyoung48", "id": 4178149, "node_id": "MDQ6VXNlcjQxNzgxNDk=", "avatar_url": "https://avatars.githubusercontent.com/u/4178149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leeyoung48", "html_url": "https://github.com/leeyoung48", "followers_url": "https://api.github.com/users/leeyoung48/followers", "following_url": "https://api.github.com/users/leeyoung48/following{/other_user}", "gists_url": "https://api.github.com/users/leeyoung48/gists{/gist_id}", "starred_url": "https://api.github.com/users/leeyoung48/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leeyoung48/subscriptions", "organizations_url": "https://api.github.com/users/leeyoung48/orgs", "repos_url": "https://api.github.com/users/leeyoung48/repos", "events_url": "https://api.github.com/users/leeyoung48/events{/privacy}", "received_events_url": "https://api.github.com/users/leeyoung48/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1079803568, "node_id": "MDU6TGFiZWwxMDc5ODAzNTY4", "url": "https://api.github.com/repos/triton-inference-server/server/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-12-14T03:49:36Z", "updated_at": "2020-03-08T18:20:21Z", "closed_at": "2020-03-08T18:20:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying test Tesla P4 performance with TRT inference server. My GPU memory is 8GB.\r\nI tested using perf_client c++ sample and resetnet50_netdef model, starting with batchsize=1 , and then double the batchsize and go on.\r\nI found test result is OK when batchsize = 8 and test failed when batchsize = 16 due to out of memory error.\r\nwhat make me confused is the batchsize=4 and batchsize=8 test also failed after the batchsize16 test got bad result. As my thought batchsize < 16 tests should be always OK.\r\nCould you give me some explanation for that? Thanks. ", "reactions": {"url": "https://api.github.com/repos/triton-inference-server/server/issues/13/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/triton-inference-server/server/issues/13/timeline", "performed_via_github_app": null, "state_reason": "completed"}]