[{"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4977", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4977/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4977/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4977/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4977", "id": 1572156282, "node_id": "I_kwDOBhEUd85dtTN6", "number": 4977, "title": "Error when pretraing on MFM+MLM", "user": {"login": "guijuzhejiang", "id": 57936895, "node_id": "MDQ6VXNlcjU3OTM2ODk1", "avatar_url": "https://avatars.githubusercontent.com/u/57936895?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guijuzhejiang", "html_url": "https://github.com/guijuzhejiang", "followers_url": "https://api.github.com/users/guijuzhejiang/followers", "following_url": "https://api.github.com/users/guijuzhejiang/following{/other_user}", "gists_url": "https://api.github.com/users/guijuzhejiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/guijuzhejiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guijuzhejiang/subscriptions", "organizations_url": "https://api.github.com/users/guijuzhejiang/orgs", "repos_url": "https://api.github.com/users/guijuzhejiang/repos", "events_url": "https://api.github.com/users/guijuzhejiang/events{/privacy}", "received_events_url": "https://api.github.com/users/guijuzhejiang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2023-02-06T08:46:12Z", "updated_at": "2023-02-06T08:47:56Z", "closed_at": "2023-02-06T08:47:56Z", "author_association": "NONE", "active_lock_reason": null, "body": null, "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4977/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4977/timeline", "performed_via_github_app": null, "state_reason": "not_planned"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4934", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4934/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4934/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4934/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4934", "id": 1519473277, "node_id": "I_kwDOBhEUd85akVJ9", "number": 4934, "title": "Exception: Could not infer language pair, please provide it explicitly", "user": {"login": "FayZ676", "id": 94952795, "node_id": "U_kgDOBajdWw", "avatar_url": "https://avatars.githubusercontent.com/u/94952795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FayZ676", "html_url": "https://github.com/FayZ676", "followers_url": "https://api.github.com/users/FayZ676/followers", "following_url": "https://api.github.com/users/FayZ676/following{/other_user}", "gists_url": "https://api.github.com/users/FayZ676/gists{/gist_id}", "starred_url": "https://api.github.com/users/FayZ676/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FayZ676/subscriptions", "organizations_url": "https://api.github.com/users/FayZ676/orgs", "repos_url": "https://api.github.com/users/FayZ676/repos", "events_url": "https://api.github.com/users/FayZ676/events{/privacy}", "received_events_url": "https://api.github.com/users/FayZ676/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-01-04T19:30:52Z", "updated_at": "2023-01-07T20:15:44Z", "closed_at": "2023-01-07T20:15:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI am trying to finetune the `dense_125m` model from the [moe_lm example](https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm) on custom data as a test before stepping up to finetune the larger dense models on custom data. I have had some help through the issues and have managed to get to the inference step.\r\n### I am able to preprocess my data:\r\n```\r\nfairseq-preprocess --destdir data-bin/captions \\\r\n\t--only-source \\\r\n\t--srcdict en_dense_lm_125m/dict.txt \\\r\n\t--trainpref caption_datasets/howard/fairseq/train.txt \\\r\n\t--validpref caption_datasets/howard/fairseq/validation.txt \\\r\n\t--testpref caption_datasets/howard/fairseq/test.txt\r\n```\r\n### I am also able to fine tune on my data:\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 \\\r\nfairseq-train data-bin/captions \\\r\n\t--finetune-from-model en_dense_lm_125m/model.pt \\\r\n\t--task language_modeling --tokens-per-sample 512 \\\r\n\t--arch transformer_lm_gpt \\\r\n\t--batch-size 32 \\\r\n\t--optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n\t--max-epoch 1 \\\r\n\t--lr 0.01 \\\r\n\t--fp16\r\n```\r\n### But I am unable to run inference with either `fairseq-generate` or `fairseq-interactive`:\r\n```\r\n# when trying fairseq-generate\r\nfairseq-generate data-bin/captions \\\r\n  --path checkpoints/checkpoint_best.pt \\\r\n  --max-sentences 1 \\\r\n\t--beam 5\r\n\r\n# when trying fairseq-interactive\r\nfairseq-interactive data-bin/captions \\\r\n  --path checkpoints/checkpoint_best.pt \\\r\n  --max-sentences 1 \\\r\n\t--beam 5\r\n```\r\n\r\n### I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/faizi/miniconda3/envs/fairseq/bin/fairseq-generate\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq_cli/generate.py\", line 413, in cli_main\r\n    main(args)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq_cli/generate.py\", line 50, in main\r\n    return _main(cfg, sys.stdout)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq_cli/generate.py\", line 83, in _main\r\n    task = tasks.setup_task(cfg.task)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/tasks/__init__.py\", line 47, in setup_task\r\n    return task.setup_task(cfg, **kwargs)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/tasks/translation.py\", line 303, in setup_task\r\n    raise Exception(\r\nException: Could not infer language pair, please provide it explicitly\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): 0.12.2\r\n - PyTorch Version (e.g., 1.0): 1.13.1\r\n - OS (e.g., Linux): Ubuntu 22.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install --editable ./`\r\n - Python version: 3.8.13\r\n - CUDA/cuDNN version: 11.7\r\n - GPU models and configuration: 4x NVIDIA 3090\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4934/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4928", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4928/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4928/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4928/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4928", "id": 1513117401, "node_id": "I_kwDOBhEUd85aMFbZ", "number": 4928, "title": "fairseq-train not recognizing dictionary path's created by fairseq-preprocess", "user": {"login": "FayZ676", "id": 94952795, "node_id": "U_kgDOBajdWw", "avatar_url": "https://avatars.githubusercontent.com/u/94952795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FayZ676", "html_url": "https://github.com/FayZ676", "followers_url": "https://api.github.com/users/FayZ676/followers", "following_url": "https://api.github.com/users/FayZ676/following{/other_user}", "gists_url": "https://api.github.com/users/FayZ676/gists{/gist_id}", "starred_url": "https://api.github.com/users/FayZ676/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FayZ676/subscriptions", "organizations_url": "https://api.github.com/users/FayZ676/orgs", "repos_url": "https://api.github.com/users/FayZ676/repos", "events_url": "https://api.github.com/users/FayZ676/events{/privacy}", "received_events_url": "https://api.github.com/users/FayZ676/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-12-28T20:01:12Z", "updated_at": "2022-12-30T16:23:40Z", "closed_at": "2022-12-30T16:23:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI run the following preprocessing script\r\n```\r\nfairseq-preprocess --destdir data-bin/captions \\\r\n  --source-lang prompt --target-lang completion \\\r\n  --trainpref caption_datasets/howard/fairseq/train \\\r\n  --validpref caption_datasets/howard/fairseq/validation \\\r\n  --testpref caption_datasets/howard/fairseq/test\r\n``` \r\nand get the following files in `data-bin/captions/`\r\n<img width=\"1610\" alt=\"Screen Shot 2022-12-28 at 12 23 01 PM\" src=\"https://user-images.githubusercontent.com/94952795/209865833-bca7e618-11bc-4f05-a656-70fcfe8cdb04.png\">\r\n\r\nBut when I run the following train script\r\n```\r\nCUDA_VISIBLE_DEVICES=0 \\\r\nfairseq-train data-bin/captions \\\r\n\t--finetune-from-model en_dense_lm_125m/model.pt \\\r\n\t--arch transformer_lm_gpt \\\r\n\t--task language_modeling --tokens-per-sample 512 \\\r\n\t--batch-size 32 \\\r\n\t--optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n\t--max-epoch 1 \\\r\n\t--fp16\r\n```\r\n\r\nFairseq doesn't recognize the dictionary files and I get the following error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/faizi/miniconda3/envs/fairseq/bin/fairseq-train\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq_cli/train.py\", line 574, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/distributed/utils.py\", line 404, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq_cli/train.py\", line 87, in main\r\n    task = tasks.setup_task(cfg.task)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/tasks/__init__.py\", line 47, in setup_task\r\n    return task.setup_task(cfg, **kwargs)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/tasks/language_modeling.py\", line 171, in setup_task\r\n    dictionary, output_dictionary = cls.setup_dictionary(args, **kwargs)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/tasks/language_modeling.py\", line 155, in setup_dictionary\r\n    dictionary = Dictionary.load(os.path.join(paths[0], \"dict.txt\"))\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/data/dictionary.py\", line 226, in load\r\n    d.add_from_file(f)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/data/dictionary.py\", line 239, in add_from_file\r\n    raise fnfe\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/data/dictionary.py\", line 236, in add_from_file\r\n    with open(PathManager.get_local_path(f), \"r\", encoding=\"utf-8\") as fd:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'data-bin/captions/dict.txt'\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): 0.12.2\r\n - PyTorch Version (e.g., 1.0): 1.13.1\r\n - OS (e.g., Linux): Ubuntu 22.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 11.7\r\n - GPU models and configuration: 4x 3090's\r\n - Any other relevant information:\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4928/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4928/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4927", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4927/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4927/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4927/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4927", "id": 1513046260, "node_id": "I_kwDOBhEUd85aL0D0", "number": 4927, "title": "\"unrecognized argument\" inconsistency between stories example and other translation cases.", "user": {"login": "FayZ676", "id": 94952795, "node_id": "U_kgDOBajdWw", "avatar_url": "https://avatars.githubusercontent.com/u/94952795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FayZ676", "html_url": "https://github.com/FayZ676", "followers_url": "https://api.github.com/users/FayZ676/followers", "following_url": "https://api.github.com/users/FayZ676/following{/other_user}", "gists_url": "https://api.github.com/users/FayZ676/gists{/gist_id}", "starred_url": "https://api.github.com/users/FayZ676/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FayZ676/subscriptions", "organizations_url": "https://api.github.com/users/FayZ676/orgs", "repos_url": "https://api.github.com/users/FayZ676/repos", "events_url": "https://api.github.com/users/FayZ676/events{/privacy}", "received_events_url": "https://api.github.com/users/FayZ676/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-12-28T18:17:02Z", "updated_at": "2022-12-29T17:23:09Z", "closed_at": "2022-12-29T17:23:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIn the training command for the stories example we are able to specify `--source-lang` and `--target-lang`. However, when I try replicate in my own example I get the error `fairseq-train: error: unrecognized arguments: --source-lang prompt --target-lang completion`\r\n\r\nI am confused about this since in the [fairseq documentation](https://fairseq.readthedocs.io/en/latest/command_line_tools.html#fairseq-train) for `fairseq-train` there is no parameter `--source-lang` or `--target-lang` and yet I have no trouble running the training command for the stories example.\r\n\r\n### Stories Example Command\r\n```\r\nfairseq-train data-bin/writingPrompts \\\r\n\t-a fconv_self_att_wp \\\r\n\t--lr 0.25 --optimizer nag \\\r\n\t--clip-norm 0.1 \\\r\n\t--max-tokens 1500 \\\r\n\t--lr-scheduler reduce_lr_on_plateau \\\r\n\t--decoder-attention True --encoder-attention False \\\r\n\t--criterion label_smoothed_cross_entropy \\\r\n\t--weight-decay .0000001 \\\r\n\t--label-smoothing 0 \\\r\n\t--source-lang wp_source --target-lang wp_target \\\r\n\t--gated-attention True --self-attention True \\\r\n\t--project-input True \\\r\n\t--pretrained False \\\r\n\t--skip-invalid-size-inputs-valid-test\r\n```\r\n\r\n### My Command\r\n```\r\nCUDA_VISIBLE_DEVICES=0 \\\r\nfairseq-train data-bin/captions \\\r\n\t--finetune-from-model en_dense_lm_125m/model.pt \\\r\n\t--source-lang prompt --target-lang completion \\\r\n\t--arch transformer_lm_gpt \\\r\n\t--task translation \\\r\n\t--batch-size 32 \\\r\n\t--optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n\t--max-epoch 1 \\\r\n\t--fp16\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): 0.12.2\r\n - PyTorch Version (e.g., 1.0): 1.13.1\r\n - OS (e.g., Linux): Ubuntu 22.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 11.7\r\n - GPU models and configuration: 4x 3090\r\n - Any other relevant information:\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4927/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4926", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4926/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4926/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4926/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4926", "id": 1513042633, "node_id": "I_kwDOBhEUd85aLzLJ", "number": 4926, "title": "'unrecognized arguments: True' when specifying --joined-dictionary", "user": {"login": "FayZ676", "id": 94952795, "node_id": "U_kgDOBajdWw", "avatar_url": "https://avatars.githubusercontent.com/u/94952795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FayZ676", "html_url": "https://github.com/FayZ676", "followers_url": "https://api.github.com/users/FayZ676/followers", "following_url": "https://api.github.com/users/FayZ676/following{/other_user}", "gists_url": "https://api.github.com/users/FayZ676/gists{/gist_id}", "starred_url": "https://api.github.com/users/FayZ676/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FayZ676/subscriptions", "organizations_url": "https://api.github.com/users/FayZ676/orgs", "repos_url": "https://api.github.com/users/FayZ676/repos", "events_url": "https://api.github.com/users/FayZ676/events{/privacy}", "received_events_url": "https://api.github.com/users/FayZ676/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-12-28T18:11:13Z", "updated_at": "2022-12-30T16:24:18Z", "closed_at": "2022-12-30T16:24:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n`fairseq-preprocess: error: unrecognized arguments: True` when specifying `--joined-dictionary True`\r\n\r\n```\r\nfairseq-preprocess \\\r\n  --joined-dictionary True \\\r\n  --source-lang prompt --target-lang completion \\\r\n  --trainpref datasets/howard/fairseq/train \\\r\n  --validpref datasets/howard/fairseq/validation \\\r\n  --testpref datasets/howard/fairseq/test \\\r\n  --destdir data-bin/captions/\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): 0.12.2\r\n - PyTorch Version (e.g., 1.0): 1.13.1\r\n - OS (e.g., Linux): Ubuntu 22.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 11.7\r\n - GPU models and configuration: 4x 3090\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4926/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4915", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4915/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4915/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4915/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4915", "id": 1507020880, "node_id": "I_kwDOBhEUd85Z01BQ", "number": 4915, "title": "KeyError: 'iterations_in_epoch' when attempting to load model from checkpoint file for finetuning", "user": {"login": "FayZ676", "id": 94952795, "node_id": "U_kgDOBajdWw", "avatar_url": "https://avatars.githubusercontent.com/u/94952795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FayZ676", "html_url": "https://github.com/FayZ676", "followers_url": "https://api.github.com/users/FayZ676/followers", "following_url": "https://api.github.com/users/FayZ676/following{/other_user}", "gists_url": "https://api.github.com/users/FayZ676/gists{/gist_id}", "starred_url": "https://api.github.com/users/FayZ676/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FayZ676/subscriptions", "organizations_url": "https://api.github.com/users/FayZ676/orgs", "repos_url": "https://api.github.com/users/FayZ676/repos", "events_url": "https://api.github.com/users/FayZ676/events{/privacy}", "received_events_url": "https://api.github.com/users/FayZ676/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-12-21T23:07:42Z", "updated_at": "2022-12-22T21:26:04Z", "closed_at": "2022-12-22T21:26:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen attempting to load the `en_dense_lm_125M` for fine tuning I get `KeyError: 'iterations_in_epoch'`. The error output looks very similar to that in issue #3211. Although #3211 was solved, the solution was not clearly stated as it was supposedly solved in a pr.\r\n\r\n### To Reproduce\r\n\r\nRun command\r\n`fairseq-train ./data-bin/captions \\\r\n\t--finetune-from-model en_dense_lm_125m/model.pt \\\r\n\t--arch transformer_lm_gpt \\\r\n\t--task language_modeling --tokens-per-sample 512 \\\r\n\t--batch-size 4 \\\r\n\t--optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n\t--max-epoch 9 \\\r\n\t--fp16`\r\n\r\n### Error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/faizi/miniconda3/envs/fairseq/bin/fairseq-train\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq_cli/train.py\", line 574, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/distributed/utils.py\", line 379, in call_main\r\n    torch.multiprocessing.spawn(\r\n  File \"/home/faizi/miniconda3/envs/fairseq/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 240, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/faizi/miniconda3/envs/fairseq/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 198, in start_processes\r\n    while not context.join():\r\n  File \"/home/faizi/miniconda3/envs/fairseq/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 160, in join\r\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\r\ntorch.multiprocessing.spawn.ProcessRaisedException:\r\n\r\n-- Process 2 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/faizi/miniconda3/envs/fairseq/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/distributed/utils.py\", line 362, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq_cli/train.py\", line 165, in main\r\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/checkpoint_utils.py\", line 255, in load_checkpoint\r\n    extra_state = trainer.load_checkpoint(\r\n  File \"/home/faizi/projects/model-testing/fairseq/fairseq/trainer.py\", line 646, in load_checkpoint\r\n    and itr_state[\"iterations_in_epoch\"] == 0\r\nKeyError: 'iterations_in_epoch'\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version: main\r\n - PyTorch Version 1.13\r\n - OS: Ubuntu 22.04\r\n - How you installed fairseq: source\r\n - Build command you used:\r\n    \r\n    ```bash\r\n    git clone https://github.com/pytorch/fairseq\r\n    cd fairseq\r\n    pip install --editable ./\r\n    ````\r\n - Python version: 3.8.15\r\n - CUDA/cuDNN version: 11.7\r\n - GPU models and configuration: 4x 3090\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4915/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4843", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4843/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4843/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4843/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4843", "id": 1434211637, "node_id": "I_kwDOBhEUd85VfFU1", "number": 4843, "title": "fairseq install error", "user": {"login": "kli017", "id": 14877573, "node_id": "MDQ6VXNlcjE0ODc3NTcz", "avatar_url": "https://avatars.githubusercontent.com/u/14877573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kli017", "html_url": "https://github.com/kli017", "followers_url": "https://api.github.com/users/kli017/followers", "following_url": "https://api.github.com/users/kli017/following{/other_user}", "gists_url": "https://api.github.com/users/kli017/gists{/gist_id}", "starred_url": "https://api.github.com/users/kli017/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kli017/subscriptions", "organizations_url": "https://api.github.com/users/kli017/orgs", "repos_url": "https://api.github.com/users/kli017/repos", "events_url": "https://api.github.com/users/kli017/events{/privacy}", "received_events_url": "https://api.github.com/users/kli017/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-11-03T07:15:43Z", "updated_at": "2023-04-26T09:29:31Z", "closed_at": "2023-04-26T09:29:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I met a error when I trying to install fairseq with cmd **pip3 install --editable ./\"**\r\nthe error message like below:\r\n`Getting requirements to build editable ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  \u00d7 Getting requirements to build editable did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [23 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"/home/likai/miniconda3/envs/fairseq/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 363, in <module>\r\n          main()\r\n        File \"/home/likai/miniconda3/envs/fairseq/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 345, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n        File \"/home/likai/miniconda3/envs/fairseq/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 144, in get_requires_for_build_editable\r\n          return hook(config_settings)\r\n        File \"/tmp/pip-build-env-da1j7k9z/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 446, in get_requires_for_build_editable\r\n          return self.get_requires_for_build_wheel(config_settings)\r\n        File \"/tmp/pip-build-env-da1j7k9z/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 338, in get_requires_for_build_wheel\r\n          return self._get_build_requires(config_settings, requirements=['wheel'])\r\n        File \"/tmp/pip-build-env-da1j7k9z/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 320, in _get_build_requires\r\n          self.run_setup()\r\n        File \"/tmp/pip-build-env-da1j7k9z/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 335, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 12, in <module>\r\n        File \"/tmp/pip-build-env-da1j7k9z/overlay/lib/python3.8/site-packages/torch/__init__.py\", line 191, in <module>\r\n          _load_global_deps()\r\n        File \"/tmp/pip-build-env-da1j7k9z/overlay/lib/python3.8/site-packages/torch/__init__.py\", line 153, in _load_global_deps\r\n          ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\r\n        File \"/home/likai/miniconda3/envs/fairseq/lib/python3.8/ctypes/__init__.py\", line 373, in __init__\r\n          self._handle = _dlopen(self._name, mode)\r\n      OSError: /tmp/pip-build-env-da1j7k9z/overlay/lib/python3.8/site-packages/torch/lib/../../nvidia/cublas/lib/libcublas.so.11: symbol cublasLtHSHMatmulAlgoInit version libcublasLt.so.11 not defined in file libcublasLt.so.11 with link time reference\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n`\r\n\r\n\r\n### Environment\r\n - fairseq Version (e.g., 1.0 or main): \r\n - PyTorch Version: 1.10.1+cu111\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source): pip3 install --editable ./\r\n - Python version: python3.8.13\r\n - CUDA/cuDNN version: cuda11.1\r\n - GPU models and configuration: A100\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4843/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4843/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4838", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4838/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4838/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4838/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4838", "id": 1431751035, "node_id": "I_kwDOBhEUd85VVsl7", "number": 4838, "title": "Old commit broke lots of imports", "user": {"login": "franznowak", "id": 33263343, "node_id": "MDQ6VXNlcjMzMjYzMzQz", "avatar_url": "https://avatars.githubusercontent.com/u/33263343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/franznowak", "html_url": "https://github.com/franznowak", "followers_url": "https://api.github.com/users/franznowak/followers", "following_url": "https://api.github.com/users/franznowak/following{/other_user}", "gists_url": "https://api.github.com/users/franznowak/gists{/gist_id}", "starred_url": "https://api.github.com/users/franznowak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/franznowak/subscriptions", "organizations_url": "https://api.github.com/users/franznowak/orgs", "repos_url": "https://api.github.com/users/franznowak/repos", "events_url": "https://api.github.com/users/franznowak/events{/privacy}", "received_events_url": "https://api.github.com/users/franznowak/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-11-01T16:50:37Z", "updated_at": "2023-02-24T14:09:13Z", "closed_at": "2023-02-23T21:18:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nhttps://github.com/facebookresearch/fairseq/commit/f8b795f427a39c19a6b7245be240680617156948 moved `metrics.py` from `\\fairseq` to `\\fairseq\\logging`. This means that the imports in 35 files are broken because they still use `from fairseq import metrics`. However, it does not necessarily fail if you are running the import from within the fairseq directory.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. From outsite the fairseq directory, run cmd ``python -c 'from fairseq import metrics'``\r\n2. See error: \r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: cannot import name 'metrics' from 'fairseq' (unknown location)\r\n```\r\n\r\nI first noticed this issue when trying to run step on the example from https://github.com/microsoft/unilm/tree/master/deltalm which uses fairseq:\r\n\r\n1. Run the following from the deltalm directory:\r\n```\r\nbash examples/binary_iwslt14.sh \\      \r\n     /tmp/iwslt14/iwslt14.tokenized.de-en \\\r\n     /tmp/iwslt14/iwslt14.spm \\\r\n     /path/to/checkpoint/spm.model\r\n```\r\n2. See error:\r\n```\r\n\\Traceback (most recent call last):\r\n  File \"/Users/franz/src/unilm/deltalm/preprocess.py\", line 1, in <module>\r\n    import deltalm\r\n  File \"/Users/franz/src/unilm/deltalm/deltalm/__init__.py\", line 1, in <module>\r\n    import deltalm.models\r\n  File \"/Users/franz/src/unilm/deltalm/deltalm/models/__init__.py\", line 5, in <module>\r\n    from fairseq.models import MODEL_REGISTRY, ARCH_MODEL_INV_REGISTRY\r\n  File \"/Users/franz/src/unilm/deltalm/fairseq/fairseq/models/__init__.py\", line 236, in <module>\r\n    import_models(models_dir, \"fairseq.models\")\r\n  File \"/Users/franz/src/unilm/deltalm/fairseq/fairseq/models/__init__.py\", line 218, in import_models\r\n    importlib.import_module(namespace + \".\" + model_name)\r\n  File \"/Users/franz/opt/anaconda3/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/Users/franz/src/unilm/deltalm/fairseq/fairseq/models/speech_to_text/__init__.py\", line 9, in <module>\r\n    from .xm_transformer import * # noqa\r\n  File \"/Users/franz/src/unilm/deltalm/fairseq/fairseq/models/speech_to_text/xm_transformer.py\", line 15, in <module>\r\n    from fairseq.models.wav2vec import Wav2VecEncoder\r\n  File \"/Users/franz/src/unilm/deltalm/fairseq/fairseq/models/wav2vec/__init__.py\", line 6, in <module>\r\n    from .wav2vec import *  # noqa\r\n  File \"/Users/franz/src/unilm/deltalm/fairseq/fairseq/models/wav2vec/wav2vec.py\", line 25, in <module>\r\n    from fairseq.tasks import FairseqTask\r\n  File \"/Users/franz/src/unilm/deltalm/fairseq/fairseq/tasks/__init__.py\", line 15, in <module>\r\n    from .fairseq_task import FairseqTask, LegacyFairseqTask  # noqa\r\n  File \"/Users/franz/src/unilm/deltalm/fairseq/fairseq/tasks/fairseq_task.py\", line 13, in <module>\r\n    from fairseq.logging import metrics, search, tokenizer, utils\r\nImportError: cannot import name 'search' from 'fairseq.logging' (/Users/franz/src/unilm/deltalm/fairseq/fairseq/logging/__init__.py)\r\n(base) franz@student-net-ox-dock-1-a-415 deltalm % bash examples/binary_iwslt14.sh \\\r\n     /tmp/iwslt14/iwslt14.tokenized.de-en \\\r\n     /tmp/iwslt14/iwslt14.spm \\\r\n     /path/to/checkpoint/spm.model\r\n```\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\r\nMinimal means having the shortest code but still preserving the bug. -->\r\n```\r\nfrom fairseq import metrics\r\n```\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nImport should just work.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): 1.0 or main\r\n - OS (e.g., Linux): macOS Monterey 12.6\r\n - How you installed fairseq (`pip`, source): `pip install --editable fairseq/`\r\n - Python version: 3.9\r\n - Any other relevant information: weirdly, when installing `tensorboardX` and running from within fairseq directory, the first set of repro steps does not reproduce the issue (the second one still does). \r\n\r\n### Additional context\r\n\r\nFiles with faulty imports:\r\n```\r\n./tests/test_metrics.py:9:from fairseq import metrics\r\n./fairseq/model_parallel/criterions/vocab_parallel_cross_entropy.py:8:from fairseq import metrics, utils\r\n./fairseq/criterions/fairseq_criterion.py:9:from fairseq import metrics, utils\r\n./fairseq/criterions/cross_entropy.py:10:from fairseq import metrics, utils\r\n./fairseq/criterions/fastspeech2_loss.py:14:from fairseq import metrics, utils\r\n./fairseq/criterions/label_smoothed_cross_entropy_with_rdrop.py:11:from fairseq import metrics, utils\r\n./fairseq/criterions/ctc.py:16:from fairseq import metrics, utils\r\n./fairseq/criterions/adaptive_loss.py:10:from fairseq import metrics, utils\r\n./fairseq/criterions/model_criterion.py:12:from fairseq import metrics, utils\r\n./fairseq/criterions/wav2vec_criterion.py:12:from fairseq import metrics, utils\r\n./fairseq/criterions/label_smoothed_cross_entropy_with_ctc.py:12:from fairseq import metrics, utils\r\n./fairseq/criterions/legacy_masked_lm.py:10:from fairseq import metrics, utils\r\n./fairseq/criterions/nat_loss.py:10:from fairseq import metrics, utils\r\n./fairseq/criterions/label_smoothed_cross_entropy_latency_augmented.py:8:from fairseq import metrics, utils\r\n./fairseq/criterions/label_smoothed_cross_entropy.py:12:from fairseq import metrics, utils\r\n./fairseq/criterions/tacotron2_loss.py:17:from fairseq import metrics, utils\r\n./fairseq/criterions/hubert_criterion.py:13:from fairseq import metrics, utils\r\n./fairseq/criterions/speech_ulm_criterion.py:10:from fairseq import metrics\r\n./fairseq/criterions/sentence_prediction.py:12:from fairseq import metrics\r\n./fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py:8:from fairseq import metrics, utils\r\n./fairseq/criterions/speech_to_speech_criterion.py:12:from fairseq import metrics, utils\r\n./fairseq/criterions/masked_lm.py:11:from fairseq import metrics, modules, utils\r\n./fairseq/criterions/sentence_ranking.py:10:from fairseq import metrics, utils\r\n./fairseq/tasks/fairseq_task.py\r\n./fairseq/tasks/multilingual_translation.py\r\n./fairseq/tasks/online_backtranslation.py\r\n./fairseq/tasks/translation.py\r\n./examples/translation_moe/translation_moe_src/translation_moe.py:10:from fairseq import metrics, utils\r\n./examples/adaptive_span/adaptive_span_loss.py:10:from fairseq import metrics, utils\r\n./examples/MMPT/mmpt/losses/fairseqmmloss.py:11:from fairseq import metrics\r\n./examples/rxf/rxf_src/label_smoothed_cross_entropy_r3f.py:10:from fairseq import metrics, utils\r\n./examples/speech_text_joint_to_text/criterions/text_guide_cross_entropy_acc.py:11:from fairseq import metrics, utils\r\n./examples/speech_text_joint_to_text/criterions/multi_modality_compound.py:8:from fairseq import metrics, utils\r\n./examples/discriminative_reranking_nmt/tasks/discriminative_reranking_task.py:15:from fairseq import metrics\r\n./examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py:12:from fairseq import metrics, utils\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4838/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4838/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4835", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4835/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4835/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4835/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4835", "id": 1430905772, "node_id": "I_kwDOBhEUd85VSeOs", "number": 4835, "title": "AttributeError: 'dict' object has no attribute '_get_node_flag'", "user": {"login": "MLDeep16", "id": 89720138, "node_id": "MDQ6VXNlcjg5NzIwMTM4", "avatar_url": "https://avatars.githubusercontent.com/u/89720138?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MLDeep16", "html_url": "https://github.com/MLDeep16", "followers_url": "https://api.github.com/users/MLDeep16/followers", "following_url": "https://api.github.com/users/MLDeep16/following{/other_user}", "gists_url": "https://api.github.com/users/MLDeep16/gists{/gist_id}", "starred_url": "https://api.github.com/users/MLDeep16/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MLDeep16/subscriptions", "organizations_url": "https://api.github.com/users/MLDeep16/orgs", "repos_url": "https://api.github.com/users/MLDeep16/repos", "events_url": "https://api.github.com/users/MLDeep16/events{/privacy}", "received_events_url": "https://api.github.com/users/MLDeep16/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-11-01T05:44:41Z", "updated_at": "2023-01-10T02:13:23Z", "closed_at": "2022-11-01T16:49:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Issue while loading the model\r\n\r\nIn omegaconf/omegaconf.py\", line 670, in open_dict\r\n    prev_state = config._get_node_flag(\"struct\")\r\nAttributeError: 'dict' object has no attribute '_get_node_flag'\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4835/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4827", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4827/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4827/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4827/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4827", "id": 1422214955, "node_id": "I_kwDOBhEUd85UxUcr", "number": 4827, "title": "wav2vec2 inference using custom trained checkpoint that uses cfg instead of args", "user": {"login": "abarcovschi", "id": 26311007, "node_id": "MDQ6VXNlcjI2MzExMDA3", "avatar_url": "https://avatars.githubusercontent.com/u/26311007?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abarcovschi", "html_url": "https://github.com/abarcovschi", "followers_url": "https://api.github.com/users/abarcovschi/followers", "following_url": "https://api.github.com/users/abarcovschi/following{/other_user}", "gists_url": "https://api.github.com/users/abarcovschi/gists{/gist_id}", "starred_url": "https://api.github.com/users/abarcovschi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abarcovschi/subscriptions", "organizations_url": "https://api.github.com/users/abarcovschi/orgs", "repos_url": "https://api.github.com/users/abarcovschi/repos", "events_url": "https://api.github.com/users/abarcovschi/events{/privacy}", "received_events_url": "https://api.github.com/users/abarcovschi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-10-25T10:09:04Z", "updated_at": "2022-10-25T10:09:34Z", "closed_at": "2022-10-25T10:09:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nAfter failing to run inference with a finetuned wav2vec2 .pt checkpoint on a single audio file using the recognize.py script provided in the famous [issue #2561](https://github.com/facebookresearch/fairseq/issues/2651), I created my own script for this problem, which runs for me without any errors. The custom model was trained in the Hydra framework.\r\n\r\n**NOTE:** this script is for using models that have a 'cfg' field defined in the dictionary returned when loading the checkpoint, in contrast to checkpoints that have an 'args' field defined. Inspiration was taken from the [issue #3043](https://github.com/facebookresearch/fairseq/issues/3043), which also used a model with the 'cfg' field instead of the 'args' field.\r\n\r\n#### Code sample\r\n```python\r\n# run ASR inference using a wav2vec2 ASR model and a specified decoder on a single audio file.\r\n# used for wav2vec2 ASR checkpoints that were finetuned in the Hydra framework (loaded checkpoint has 'cfg' key but no 'args' key).\r\n\r\n\r\nimport torch\r\nimport soundfile as sf\r\nfrom argparse import Namespace\r\nimport torch.nn.functional as F\r\nfrom omegaconf import OmegaConf\r\nfrom fairseq.data import Dictionary\r\nfrom fairseq.data.data_utils import post_process\r\nfrom examples.speech_recognition.w2l_decoder import W2lViterbiDecoder\r\nfrom fairseq.models.wav2vec.wav2vec2_asr import Wav2VecCtc, Wav2Vec2CtcConfig\r\n\r\n\r\ndef get_config_dict(args):\r\n    if isinstance(args, Namespace):\r\n        # unpack Namespace into base dict obj\r\n        args = vars(args)\r\n    fields = Wav2Vec2CtcConfig.__dataclass_fields__\r\n    # create dict for attributes of Wav2Vec2CtcConfig with vals taken from the same key in args, if they exist\r\n    fields_dict = {}\r\n    # this means Wav2Vec2CtcConfig obj fields will be overwritten with vals from args, otherwise they will be default\r\n    for field in fields.keys():\r\n        if field in args:\r\n            fields_dict[field] = args[field]\r\n\r\n    return fields_dict\r\n\r\n\r\ndef get_feature(filepath):\r\n    def postprocess(feats, sample_rate):\r\n        if feats.dim == 2:\r\n            feats = feats.mean(-1)\r\n\r\n        assert feats.dim() == 1, feats.dim()\r\n\r\n        with torch.no_grad():\r\n            feats = F.layer_norm(feats, feats.shape)\r\n        return feats\r\n\r\n    wav, sample_rate = sf.read(filepath)\r\n    feats = torch.from_numpy(wav).float()\r\n    feats = postprocess(feats, sample_rate)\r\n    feats = feats.cuda()\r\n\r\n    return feats\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model_path = \"/path/to/checkpoint.pt\"\r\n    target_dict = Dictionary.load('/path/to/dict.ltr.txt')\r\n\r\n    w2v = torch.load(model_path)\r\n\r\n    args_dict = get_config_dict(w2v['cfg']['model'])\r\n    w2v_config_obj = OmegaConf.merge(OmegaConf.structured(Wav2Vec2CtcConfig), args_dict)\r\n\r\n    dummy_target_dict = {'target_dictionary' : target_dict.symbols}\r\n    dummy_target_dict = Namespace(**dummy_target_dict)\r\n\r\n    model = Wav2VecCtc.build_model(w2v_config_obj, dummy_target_dict)\r\n    model.load_state_dict(w2v[\"model\"], strict=True)\r\n    model = model.cuda()\r\n    model.eval()\r\n\r\n    sample, input = dict(), dict()\r\n    WAV_PATH = '/path/to/speech.wav'\r\n\r\n    # define additional decoder args\r\n    decoder_args = Namespace(**{'nbest': 1})\r\n    generator = W2lViterbiDecoder(decoder_args, target_dict)\r\n\r\n    feature = get_feature(WAV_PATH)\r\n    input[\"source\"] = feature.unsqueeze(0)\r\n\r\n    padding_mask = torch.BoolTensor(input[\"source\"].size(1)).fill_(False).unsqueeze(0)\r\n\r\n    input[\"padding_mask\"] = padding_mask\r\n    sample[\"net_input\"] = input\r\n\r\n    models = list()\r\n    models.append(model)\r\n\r\n    with torch.no_grad():\r\n        hypo = generator.generate(models, sample, prefix_tokens=None)\r\n\r\n    hyp_pieces = target_dict.string(hypo[0][0][\"tokens\"].int().cpu())\r\n\r\n\r\n    res = post_process(hyp_pieces, 'letter')\r\n    print(res)\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): 1.0.0a0+35cc605\r\n - flashlight version 1.0.0\r\n - Hydra version 1.0.7\r\n - PyTorch Version (e.g., 1.0): 1.12.1+cu113\r\n - OS (e.g., Linux): Ubuntu 20.04.3 LTS (GNU/Linux 5.4.0-122-generic x86_64)\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip3 install packaging && pip3 install --editable ./\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: 11.4\r\n - Cuda compilation tools, release 11.1, V11.1.105\r\n - GPU models and configuration: A6000\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4827/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4789", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4789/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4789/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4789/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4789", "id": 1409078637, "node_id": "I_kwDOBhEUd85T_NVt", "number": 4789, "title": "MBART ckpt loading error", "user": {"login": "stdoo", "id": 18691008, "node_id": "MDQ6VXNlcjE4NjkxMDA4", "avatar_url": "https://avatars.githubusercontent.com/u/18691008?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stdoo", "html_url": "https://github.com/stdoo", "followers_url": "https://api.github.com/users/stdoo/followers", "following_url": "https://api.github.com/users/stdoo/following{/other_user}", "gists_url": "https://api.github.com/users/stdoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/stdoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stdoo/subscriptions", "organizations_url": "https://api.github.com/users/stdoo/orgs", "repos_url": "https://api.github.com/users/stdoo/repos", "events_url": "https://api.github.com/users/stdoo/events{/privacy}", "received_events_url": "https://api.github.com/users/stdoo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-10-14T09:47:24Z", "updated_at": "2022-10-18T09:57:47Z", "closed_at": "2022-10-18T09:57:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "I was going to do some translation inference with MBART according to the instructions in the following page: https://github.com/facebookresearch/fairseq/blob/main/examples/mbart/README.md.\r\n\r\nEverything went well until this command:\r\n```\r\nmodel_dir=MBART_finetuned_enro # fix if you moved the checkpoint\r\n\r\nfairseq-generate path_2_data \\\r\n  --path $model_dir/model.pt \\\r\n  --task translation_from_pretrained_bart \\\r\n  --gen-subset test \\\r\n  -t ro_RO -s en_XX \\\r\n  --bpe 'sentencepiece' --sentencepiece-model $model_dir/sentence.bpe.model \\\r\n  --sacrebleu --remove-bpe 'sentencepiece' \\\r\n  --batch-size 32 --langs $langs > en_ro\r\n```\r\nThe error is like:\r\n\r\n> RuntimeError: Error(s) in loading state_dict for BARTModel:\r\n size mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([250027, 1024]) from checkpoint, the shape in current model is torch.Size([250004, 1024]).\r\n size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([250027, 1024]) from checkpoint, the shape in current model is torch.Size([250004, 1024]).\r\n size mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([250027, 1024]) from checkpoint, the shape in current model is torch.Size([250004, 1024]).\r\n\r\nIt seems that the ckpt provided cannot match the size of the BARTModel. \r\nHow to solve this problem?\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4789/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4766", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4766/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4766/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4766/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4766", "id": 1401562082, "node_id": "I_kwDOBhEUd85TiiPi", "number": 4766, "title": "Hugging face impletenation result in cuda device error.", "user": {"login": "knoriy", "id": 46536226, "node_id": "MDQ6VXNlcjQ2NTM2MjI2", "avatar_url": "https://avatars.githubusercontent.com/u/46536226?v=4", "gravatar_id": "", "url": "https://api.github.com/users/knoriy", "html_url": "https://github.com/knoriy", "followers_url": "https://api.github.com/users/knoriy/followers", "following_url": "https://api.github.com/users/knoriy/following{/other_user}", "gists_url": "https://api.github.com/users/knoriy/gists{/gist_id}", "starred_url": "https://api.github.com/users/knoriy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/knoriy/subscriptions", "organizations_url": "https://api.github.com/users/knoriy/orgs", "repos_url": "https://api.github.com/users/knoriy/repos", "events_url": "https://api.github.com/users/knoriy/events{/privacy}", "received_events_url": "https://api.github.com/users/knoriy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-10-07T18:59:12Z", "updated_at": "2022-12-01T16:39:18Z", "closed_at": "2022-12-01T16:37:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nUsing GPU and huggingface `load_model_ensemble_and_task_from_hf_hub` results in:\r\n```\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)\r\n```\r\nForcing all tensors and models on a device still returns the same error. Am I missing something?\r\n\r\n### To Reproduce\r\n\r\n```python\r\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\r\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\r\n\r\n\r\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\r\n    \"facebook/fastspeech2-en-200_speaker-cv4\",\r\n    arg_overrides={\"vocoder\": \"hifigan\", \"fp16\": False}\r\n)\r\nmodel = models[0]\r\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\r\ngenerator = task.build_generator(models, cfg)\r\n\r\ntext = \"Hello, this is a test run.\"\r\n\r\nsample = TTSHubInterface.get_model_input(task, text)\r\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\r\n```\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd 'python file.py'\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\nError in full:\r\n```\r\nFetching 9 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:00<00:00, 1002.25it/s]\r\n2022-10-07 18:52:27 | INFO | fairseq.tasks.speech_to_text | dictionary size (vocab.txt): 74\r\n2022-10-07 18:52:28 | INFO | fairseq.models.text_to_speech.vocoder | loaded HiFiGAN checkpoint from /fsx/home-knoriy/.cache/fairseq/models--facebook--fastspeech2-en-200_speaker-cv4/snapshots/e0218c23f36ee17ddef548a1ec04efe6ced9370c/hifigan.bin\r\n2022-10-07 18:52:29 | INFO | fairseq.models.text_to_speech.vocoder | loaded HiFiGAN checkpoint from /fsx/home-knoriy/.cache/fairseq/models--facebook--fastspeech2-en-200_speaker-cv4/snapshots/e0218c23f36ee17ddef548a1ec04efe6ced9370c/hifigan.bin\r\nTraceback (most recent call last):\r\n  File \"/fsx/knoriy/code/text-to-speech/fastspeech2.py\", line 53, in <module>\r\n    wav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/fairseq/models/text_to_speech/hub_interface.py\", line 132, in get_prediction\r\n    prediction = generator.generate(model, sample)\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/fairseq/speech_generator.py\", line 162, in generate\r\n    finalized = [\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/fairseq/speech_generator.py\", line 165, in <listcomp>\r\n    \"waveform\": self.get_waveform(\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/fairseq/speech_generator.py\", line 33, in get_waveform\r\n    return None if self.vocoder is None else self.vocoder(feat).squeeze(0)\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/fairseq/models/text_to_speech/vocoder.py\", line 201, in forward\r\n    return model(x.unsqueeze(0).transpose(1, 2)).detach().squeeze(0)\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/fairseq/models/text_to_speech/hifigan.py\", line 155, in forward\r\n    x = self.conv_pre(x)\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1148, in _call_impl\r\n    result = forward_call(*input, **kwargs)\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 307, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/fsx/home-knoriy/miniconda3/envs/tts/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 303, in _conv_forward\r\n    return F.conv1d(input, weight, bias, self.stride,\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)\r\n```\r\n\r\n#### Code sample\r\n```python\r\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\r\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\r\n\r\n\r\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\r\n    \"facebook/fastspeech2-en-200_speaker-cv4\",\r\n    arg_overrides={\"vocoder\": \"hifigan\", \"fp16\": False}\r\n)\r\nmodel = models[0]\r\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\r\ngenerator = task.build_generator(models, cfg)\r\n\r\ntext = \"Hello, this is a test run.\"\r\n\r\nsample = TTSHubInterface.get_model_input(task, text)\r\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\r\n```\r\n\r\n### Expected behavior\r\n\r\nAudio data, which contains a person saying, \"Hello, this is a test run.\"\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): 0.12.2\r\n - PyTorch Version (e.g., 1.0): 1.12.1\r\n - OS (e.g., Linux): ubuntu20.04\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.0\r\n - CUDA/cuDNN version: 11.6\r\n - GPU models and configuration: Nvidia A100\r\n\r\n### Additional context\r\n\r\nI've tested this with several other models with a similar error, CPU mode works fine.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4766/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4766/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4753", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4753/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4753/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4753/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4753", "id": 1394091619, "node_id": "I_kwDOBhEUd85TGCZj", "number": 4753, "title": "Facebook", "user": {"login": "Effyouu", "id": 114792201, "node_id": "U_kgDOBteXCQ", "avatar_url": "https://avatars.githubusercontent.com/u/114792201?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Effyouu", "html_url": "https://github.com/Effyouu", "followers_url": "https://api.github.com/users/Effyouu/followers", "following_url": "https://api.github.com/users/Effyouu/following{/other_user}", "gists_url": "https://api.github.com/users/Effyouu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Effyouu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Effyouu/subscriptions", "organizations_url": "https://api.github.com/users/Effyouu/orgs", "repos_url": "https://api.github.com/users/Effyouu/repos", "events_url": "https://api.github.com/users/Effyouu/events{/privacy}", "received_events_url": "https://api.github.com/users/Effyouu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-10-03T03:58:09Z", "updated_at": "2022-10-13T15:12:48Z", "closed_at": "2022-10-13T15:12:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\n\n<!-- A clear and concise description of what the bug is. -->\n\n### To Reproduce\n\nSteps to reproduce the behavior (**always include the command you ran**):\n\n1. Run cmd '....'\n2. See error\n\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\n\n\n#### Code sample\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\nMinimal means having the shortest code but still preserving the bug. -->\n\n### Expected behavior\n\n<!-- A clear and concise description of what you expected to happen. -->\n\n### Environment\n\n - fairseq Version (e.g., 1.0 or main):\n - PyTorch Version (e.g., 1.0)\n - OS (e.g., Linux):\n - How you installed fairseq (`pip`, source):\n - Build command you used (if compiling from source):\n - Python version:\n - CUDA/cuDNN version:\n - GPU models and configuration:\n - Any other relevant information:\n\n### Additional context\n\n<!-- Add any other context about the problem here. -->", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4753/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4689", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4689/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4689/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4689/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4689", "id": 1359127588, "node_id": "I_kwDOBhEUd85RAqQk", "number": 4689, "title": "BLEU score calculation error for character-based languages such as Chinese, Japanese, etc.", "user": {"login": "BrightXiaoHan", "id": 25839309, "node_id": "MDQ6VXNlcjI1ODM5MzA5", "avatar_url": "https://avatars.githubusercontent.com/u/25839309?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BrightXiaoHan", "html_url": "https://github.com/BrightXiaoHan", "followers_url": "https://api.github.com/users/BrightXiaoHan/followers", "following_url": "https://api.github.com/users/BrightXiaoHan/following{/other_user}", "gists_url": "https://api.github.com/users/BrightXiaoHan/gists{/gist_id}", "starred_url": "https://api.github.com/users/BrightXiaoHan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BrightXiaoHan/subscriptions", "organizations_url": "https://api.github.com/users/BrightXiaoHan/orgs", "repos_url": "https://api.github.com/users/BrightXiaoHan/repos", "events_url": "https://api.github.com/users/BrightXiaoHan/events{/privacy}", "received_events_url": "https://api.github.com/users/BrightXiaoHan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-09-01T16:41:50Z", "updated_at": "2022-09-01T19:44:28Z", "closed_at": "2022-09-01T19:44:28Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen training model using translation task, if the `target_lang` is char base language, such as zh(Chinese) and ja(Japanese), the score calculated by sacrebleu is problematic.\r\n\r\n### To Reproduce\r\nThis is my scripts:\r\nEnvironment variable. Create a file .env and source .env.\r\n```bash\r\nFAIRSEQ_SRC=/path/to/fairseq\r\nRAW_DATA_DIR=raw\r\nDATA_DIR=data\r\nDATA_BIN=databin\r\nCHECKPOINTS=checkpoints\r\nCUDA_VISIBLE_DEVICES=0\r\nMAX_TOKENS=4096  # decide by gpu memory\r\n```\r\nDownload wmt18 en-zh processed training data\r\n```bash\r\ncurl -o $RAW_DATA_DIR/corpus.gz  https://data.statmt.org/wmt18/translation-task/preprocessed/zh-en/corpus.gz\r\ngunzip $RAW_DATA_DIR/corpus.gz\r\ncut -f 1 $DATA_DIR/corpus > en.train\r\ncut -f 2 $DATA_DIR/corpus > zh.train\r\n\r\nsacrebleu -t wmt17 -l en-zh --echo src > $RAW_DATA_DIR/en.valid\r\nsacrebleu -t wmt17 -l en-zh --echo ref > $RAW_DATA_DIR/zh.valid\r\nsacrebleu -t wmt18 -l en-zh --echo src > $RAW_DATA_DIR/en.test\r\nsacrebleu -t wmt18 -l en-zh --echo ref > $RAW_DATA_DIR/zh.test\r\n```\r\n\r\nTrain sentencepiece model and tokenize data\r\n```bash\r\npython $FAIRSEQ_SRC/scripts/spm_train.py \\\r\n    --input=$RAW_DATA_DIR/en.train,$RAW_DATA_DIR/zh.train \\\r\n    --model_prefix=$DATA_DIR/sentencepiece.bpe \\\r\n    --vocab_size=32000 \\\r\n    --character_coverage=1.0 \\\r\n    --model_type=bpe \\\r\n    --input_sentence_size=2000000 \\\r\n    --shuffle_input_sentence=true\r\n\r\npython $FAIRSEQ_SRC/scripts/spm_encode.py \\\r\n    --model $DATA_DIR/sentencepiece.bpe.model \\\r\n    --inputs $RAW_DATA_DIR/en.train $RAW_DATA_DIR/zh.train \\\r\n    --outputs $DATA_DIR/train.en $DATA_DIR/train.zh\r\n\r\npython $FAIRSEQ_SRC/scripts/spm_encode.py \\\r\n    --model $DATA_DIR/sentencepiece.bpe.model \\\r\n    --inputs $RAW_DATA_DIR/en.valid $RAW_DATA_DIR/zh.valid \\\r\n    --outputs $DATA_DIR/valid.en $DATA_DIR/valid.zh\r\n\r\npython $FAIRSEQ_SRC/scripts/spm_encode.py \\\r\n    --model $DATA_DIR/sentencepiece.bpe.model \\\r\n    --inputs $RAW_DATA_DIR/en.test $RAW_DATA_DIR/zh.test \\\r\n    --outputs $DATA_DIR/test.en $DATA_DIR/test.zh\r\n```\r\n\r\nBinarize data\r\n```\r\nfairseq-preprocess --source-lang en --target-lang zh \\\r\n    --trainpref $DATA_DIR/train \\\r\n    --validpref $DATA_DIR/valid \\\r\n    --testpref $DATA_DIR/test \\\r\n    --joined-dictionary \\\r\n    --thresholdtgt 10 --thresholdsrc 10 \\\r\n    --destdir $DATA_BIN \\\r\n    --workers 40 \r\n```\r\nTrain normal transformer model without adapters.\r\n```\r\nfairseq-train \\\r\n    $DATA_BIN \\\r\n    --arch transformer --share-all-embeddings \\\r\n    --source-lang en --target-lang zh \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --dropout 0.1 --weight-decay 0.0001 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens $MAX_TOKENS \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\r\n    --eval-bleu-remove-bpe sentencepiece \\\r\n    --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\r\n    --save-dir $CHECKPOINTS\r\n```\r\n<img width=\"1445\" alt=\"\u622a\u5c4f2022-09-02 00 28 36\" src=\"https://user-images.githubusercontent.com/25839309/187965638-9b6688ab-2501-4683-809f-e10b5fd2924f.png\">\r\nFrom the output sample, the translation result is good, but the calculated bleu value is only 4.63. This is because the default tokenizer of sacrebleu cannot tokenize Chinese correctly.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\nThe latest api of sacrebleu supports the specified language and automatically selects the appropriate tokenize method.  Is it possible to use the latest sacrebleu api to solve this issue. I can also make a Pull request for it.\r\n\r\n/tree/main/fairseq/tasks/translation.py:494\r\n```py\r\n        if self.cfg.eval_tokenized_bleu:\r\n            return sacrebleu.corpus_bleu(hyps, [refs], tokenize=\"none\")\r\n        else:\r\n            return sacrebleu.corpus_bleu(hyps, [refs])\r\n```\r\nshould be changed to\r\n```py\r\n        if self.cfg.eval_tokenized_bleu:\r\n            return sacrebleu.metrics.BLEU().corpus_score(hyps, [refs])\r\n        else:\r\n            return sacrebleu.metrics.BLEU(trg_lang=self.cfg.target_lang).corpus_score(hyps, [refs])\r\n```\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version: main\r\n - PyTorch Version: any\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4689/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4677", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4677/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4677/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4677/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4677", "id": 1355848673, "node_id": "I_kwDOBhEUd85Q0Jvh", "number": 4677, "title": "Generating on Finetuned M2M100 models ", "user": {"login": "i55code", "id": 15132387, "node_id": "MDQ6VXNlcjE1MTMyMzg3", "avatar_url": "https://avatars.githubusercontent.com/u/15132387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/i55code", "html_url": "https://github.com/i55code", "followers_url": "https://api.github.com/users/i55code/followers", "following_url": "https://api.github.com/users/i55code/following{/other_user}", "gists_url": "https://api.github.com/users/i55code/gists{/gist_id}", "starred_url": "https://api.github.com/users/i55code/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/i55code/subscriptions", "organizations_url": "https://api.github.com/users/i55code/orgs", "repos_url": "https://api.github.com/users/i55code/repos", "events_url": "https://api.github.com/users/i55code/events{/privacy}", "received_events_url": "https://api.github.com/users/i55code/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-08-30T14:20:19Z", "updated_at": "2022-08-30T15:14:02Z", "closed_at": "2022-08-30T15:13:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nWhen I generate from a model finetuned from M2M100, it shows that: \r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)\r\n\r\nHow to fix this issue?\r\n\r\nThanks! \r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4677/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4622", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4622/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4622/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4622/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4622", "id": 1324268712, "node_id": "I_kwDOBhEUd85O7ryo", "number": 4622, "title": "Uninitialised bias parameters in RelPositionMultiHeadedAttention", "user": {"login": "sanchit-gandhi", "id": 93869735, "node_id": "U_kgDOBZhWpw", "avatar_url": "https://avatars.githubusercontent.com/u/93869735?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sanchit-gandhi", "html_url": "https://github.com/sanchit-gandhi", "followers_url": "https://api.github.com/users/sanchit-gandhi/followers", "following_url": "https://api.github.com/users/sanchit-gandhi/following{/other_user}", "gists_url": "https://api.github.com/users/sanchit-gandhi/gists{/gist_id}", "starred_url": "https://api.github.com/users/sanchit-gandhi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sanchit-gandhi/subscriptions", "organizations_url": "https://api.github.com/users/sanchit-gandhi/orgs", "repos_url": "https://api.github.com/users/sanchit-gandhi/repos", "events_url": "https://api.github.com/users/sanchit-gandhi/events{/privacy}", "received_events_url": "https://api.github.com/users/sanchit-gandhi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-08-01T11:29:42Z", "updated_at": "2022-08-01T17:08:29Z", "closed_at": "2022-08-01T17:08:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThe (u, v) bias terms in the MHA layer with relative position encodings are defined using `torch.Tensor`s:\r\nhttps://github.com/facebookresearch/fairseq/blob/4fe8583396191c22011350248119db98ec1b5cb8/fairseq/modules/espnet_multihead_attention.py#L127-L128\r\n\r\nSince `torch.Tensor` creates an un-initialised tensor, its use results in non-deterministic behaviour, poor initialisation, and nans if you have an unlucky init:\r\n```python\r\n>>>import torch\r\n>>> torch.Tensor(1, 1).sum()\r\ntensor(0.)\r\n>>> torch.Tensor(10, 10).sum()\r\ntensor(4.9312e-38)\r\n>>> torch.Tensor(100, 100).sum()\r\ntensor(nan)\r\n```\r\nSection 3.3 of the referenced paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf) does not specify the initialisation for the bias terms. Since `torch.Tensor` is typically populated with zeros, setting the initialisation to `torch.zeros` should provide a fix that is close to the intended behaviour.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4622/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4619", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4619/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4619/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4619/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4619", "id": 1322774046, "node_id": "I_kwDOBhEUd85O1-4e", "number": 4619, "title": "WandB hangs at the end of multi-gpu DDP training", "user": {"login": "mnoukhov", "id": 3391297, "node_id": "MDQ6VXNlcjMzOTEyOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/3391297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mnoukhov", "html_url": "https://github.com/mnoukhov", "followers_url": "https://api.github.com/users/mnoukhov/followers", "following_url": "https://api.github.com/users/mnoukhov/following{/other_user}", "gists_url": "https://api.github.com/users/mnoukhov/gists{/gist_id}", "starred_url": "https://api.github.com/users/mnoukhov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mnoukhov/subscriptions", "organizations_url": "https://api.github.com/users/mnoukhov/orgs", "repos_url": "https://api.github.com/users/mnoukhov/repos", "events_url": "https://api.github.com/users/mnoukhov/events{/privacy}", "received_events_url": "https://api.github.com/users/mnoukhov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-07-29T23:11:36Z", "updated_at": "2022-08-09T17:00:45Z", "closed_at": "2022-08-09T17:00:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIf you train with multigpu using ddp while logging with `wandb`, then the program doesn't exit as wandb hangs because there is no `wandb.finish()` call. \r\n\r\nSee https://docs.wandb.ai/guides/track/advanced/distributed-training#hanging-at-the-end-of-training\r\n\r\n### To Reproduce\r\n\r\n1. Run any regular training method with multiple gpus e.g. translation from `fairseq/examples/backtranslation`  \r\n\r\n\r\n2. After the training finishes, the program does not exit\r\n\r\n3. On killing the program (e.g. with Control+C) you get a stack trace about multiprocessing\r\n\r\n```bash\r\n2022-07-29 22:52:33 | INFO | fairseq_cli.train | done training in 162.1 seconds\r\n^CTraceback (most recent call last):\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\nProcess SpawnProcess-1:\r\n    \"__main__\", mod_spec)\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/mnt/home/fairseq/fairseq_cli/train.py\", line 570, in <module>\r\n    cli_main()\r\n  File \"/mnt/home/fairseq/fairseq_cli/train.py\", line 563, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/mnt/home/fairseq/fairseq/distributed/utils.py\", line 351, in call_main\r\n    join=True,\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 240, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 198, in start_processes\r\nTraceback (most recent call last):\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\r\n    util._exit_function()\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/multiprocessing/util.py\", line 357, in _exit_function\r\n    p.join()\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/multiprocessing/process.py\", line 140, in join\r\n    res = self._popen.wait(timeout)\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\r\n    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\r\n    pid, sts = os.waitpid(self.pid, flag)\r\n    while not context.join():\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 111, in join\r\n    timeout=timeout,\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\r\nKeyboardInterrupt\r\n    ready = selector.select(timeout)\r\n  File \"/mnt/home/miniconda/envs/fairseq/lib/python3.7/selectors.py\", line 415, in select\r\n    fd_event_list = self._selector.poll(timeout)\r\nKeyboardInterrupt\r\nTraceback (most recent call last):\r\n  File \"run_on_snow.py\", line 90, in <module>\r\n    subprocess.run(command, shell=True, check=True, text=True)\r\n  File \"/opt/conda/lib/python3.7/subprocess.py\", line 490, in run\r\n    stdout, stderr = process.communicate(input, timeout=timeout)\r\n  File \"/opt/conda/lib/python3.7/subprocess.py\", line 956, in communicate\r\n    self.wait()\r\n  File \"/opt/conda/lib/python3.7/subprocess.py\", line 1019, in wait\r\n    return self._wait(timeout=timeout)\r\n  File \"/opt/conda/lib/python3.7/subprocess.py\", line 1653, in _wait\r\n    (pid, sts) = self._try_wait(0)\r\n  File \"/opt/conda/lib/python3.7/subprocess.py\", line 1611, in _try_wait\r\n    (pid, sts) = os.waitpid(self.pid, wait_flags)\r\nKeyboardInterrupt\r\n```\r\n\r\n#### Code sample\r\n\r\nFrom `fairseq/examples/backtranslation`\r\n\r\n```bash\r\nCHECKPOINT_DIR=checkpoints_en_de_parallel\r\nCUDA_VISIBLE_DEVICES=0,1 fairseq-train --fp16 \\\r\n    data-bin/wmt18_en_de \\\r\n    --source-lang en --target-lang de \\\r\n    --arch transformer_wmt_en_de_big --share-all-embeddings \\\r\n    --dropout 0.3 --weight-decay 0.0 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --max-tokens 3584 --update-freq 16 \\\r\n    --max-update 2 \\\r\n    --save-dir $CHECKPOINT_DIR \\\r\n    --ddp-backend no_c10d \\ \r\n    --wandb-project MYWANDBPROJECT\r\n```\r\n\r\nProgram will finish training and then hang.\r\n\r\n### Expected behavior\r\n\r\nProgram should exit after training\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): main\r\n - PyTorch Version \r\n - OS (e.g., Linux): Debian Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source): pip install -e .\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 11.4\r\n - GPU models and configuration: 2x Tesla P100 (also tested with V100)\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4619/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4619/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4525", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4525/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4525/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4525/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4525", "id": 1289352976, "node_id": "I_kwDOBhEUd85M2fcQ", "number": 4525, "title": "'TH/TH.h' file not found error for PyTorch>=1.11", "user": {"login": "freewym", "id": 3506322, "node_id": "MDQ6VXNlcjM1MDYzMjI=", "avatar_url": "https://avatars.githubusercontent.com/u/3506322?v=4", "gravatar_id": "", "url": "https://api.github.com/users/freewym", "html_url": "https://github.com/freewym", "followers_url": "https://api.github.com/users/freewym/followers", "following_url": "https://api.github.com/users/freewym/following{/other_user}", "gists_url": "https://api.github.com/users/freewym/gists{/gist_id}", "starred_url": "https://api.github.com/users/freewym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/freewym/subscriptions", "organizations_url": "https://api.github.com/users/freewym/orgs", "repos_url": "https://api.github.com/users/freewym/repos", "events_url": "https://api.github.com/users/freewym/events{/privacy}", "received_events_url": "https://api.github.com/users/freewym/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-06-29T22:07:27Z", "updated_at": "2022-09-29T20:52:06Z", "closed_at": "2022-09-29T20:52:06Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n'TH/TH.h' file not found error at fairseq/clib/libnat_cuda/edit_dist.cu Line 11 for PyTorch>=1.11.\r\n\r\n### To Reproduce\r\nhave PyTorch>=1.11 installed\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd 'python setup.py build_ext --inplace'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): main\r\n - PyTorch Version (e.g., 1.0): 1.12\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4525/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4525/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4514", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4514/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4514/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4514/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4514", "id": 1285056437, "node_id": "I_kwDOBhEUd85MmGe1", "number": 4514, "title": "Delete this issue report please.", "user": {"login": "ejayguo", "id": 17350837, "node_id": "MDQ6VXNlcjE3MzUwODM3", "avatar_url": "https://avatars.githubusercontent.com/u/17350837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ejayguo", "html_url": "https://github.com/ejayguo", "followers_url": "https://api.github.com/users/ejayguo/followers", "following_url": "https://api.github.com/users/ejayguo/following{/other_user}", "gists_url": "https://api.github.com/users/ejayguo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ejayguo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ejayguo/subscriptions", "organizations_url": "https://api.github.com/users/ejayguo/orgs", "repos_url": "https://api.github.com/users/ejayguo/repos", "events_url": "https://api.github.com/users/ejayguo/events{/privacy}", "received_events_url": "https://api.github.com/users/ejayguo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-06-26T23:05:35Z", "updated_at": "2022-06-28T16:19:44Z", "closed_at": "2022-06-28T16:19:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Never mind. Sorry for bothering you.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4514/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4514/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4501", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4501/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4501/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4501/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4501", "id": 1276112528, "node_id": "I_kwDOBhEUd85MD-6Q", "number": 4501, "title": "PyPI install fails with \"FileNotFoundError: [Errno 2] No such file or directory: 'fairseq/version.txt'\"", "user": {"login": "PJ-Finlay", "id": 13067016, "node_id": "MDQ6VXNlcjEzMDY3MDE2", "avatar_url": "https://avatars.githubusercontent.com/u/13067016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PJ-Finlay", "html_url": "https://github.com/PJ-Finlay", "followers_url": "https://api.github.com/users/PJ-Finlay/followers", "following_url": "https://api.github.com/users/PJ-Finlay/following{/other_user}", "gists_url": "https://api.github.com/users/PJ-Finlay/gists{/gist_id}", "starred_url": "https://api.github.com/users/PJ-Finlay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PJ-Finlay/subscriptions", "organizations_url": "https://api.github.com/users/PJ-Finlay/orgs", "repos_url": "https://api.github.com/users/PJ-Finlay/repos", "events_url": "https://api.github.com/users/PJ-Finlay/events{/privacy}", "received_events_url": "https://api.github.com/users/PJ-Finlay/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "dianaml0", "id": 82468439, "node_id": "MDQ6VXNlcjgyNDY4NDM5", "avatar_url": "https://avatars.githubusercontent.com/u/82468439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dianaml0", "html_url": "https://github.com/dianaml0", "followers_url": "https://api.github.com/users/dianaml0/followers", "following_url": "https://api.github.com/users/dianaml0/following{/other_user}", "gists_url": "https://api.github.com/users/dianaml0/gists{/gist_id}", "starred_url": "https://api.github.com/users/dianaml0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dianaml0/subscriptions", "organizations_url": "https://api.github.com/users/dianaml0/orgs", "repos_url": "https://api.github.com/users/dianaml0/repos", "events_url": "https://api.github.com/users/dianaml0/events{/privacy}", "received_events_url": "https://api.github.com/users/dianaml0/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "dianaml0", "id": 82468439, "node_id": "MDQ6VXNlcjgyNDY4NDM5", "avatar_url": "https://avatars.githubusercontent.com/u/82468439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dianaml0", "html_url": "https://github.com/dianaml0", "followers_url": "https://api.github.com/users/dianaml0/followers", "following_url": "https://api.github.com/users/dianaml0/following{/other_user}", "gists_url": "https://api.github.com/users/dianaml0/gists{/gist_id}", "starred_url": "https://api.github.com/users/dianaml0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dianaml0/subscriptions", "organizations_url": "https://api.github.com/users/dianaml0/orgs", "repos_url": "https://api.github.com/users/dianaml0/repos", "events_url": "https://api.github.com/users/dianaml0/events{/privacy}", "received_events_url": "https://api.github.com/users/dianaml0/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2022-06-19T16:46:21Z", "updated_at": "2022-06-27T23:39:33Z", "closed_at": "2022-06-27T19:34:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI'm trying to install fairseq from PyPI and the install is failing to find 'fairseq/version.txt'.\r\n\r\nI was able to install successfully by cloning the code from GitHub and running `pip install .` in the root directory. My guess is that the installer is expecting to be in the root directory of the code to find `fairseq/version.txt` and breaks installing from PyPI.\r\n\r\n```\r\n$ pip install fairseq\r\nCollecting fairseq\r\n  Using cached fairseq-0.12.1.tar.gz (9.6 MB)\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Getting requirements to build wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [18 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"/home/argosopentech/temp/env/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 363, in <module>\r\n          main()\r\n        File \"/home/argosopentech/temp/env/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 345, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n        File \"/home/argosopentech/temp/env/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 130, in get_requires_for_build_wheel\r\n          return hook(config_settings)\r\n        File \"/tmp/pip-build-env-iwbeosu0/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 177, in get_requires_for_build_wheel\r\n          return self._get_build_requires(\r\n        File \"/tmp/pip-build-env-iwbeosu0/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 159, in _get_build_requires\r\n          self.run_setup()\r\n        File \"/tmp/pip-build-env-iwbeosu0/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 174, in run_setup\r\n          exec(compile(code, __file__, 'exec'), locals())\r\n        File \"setup.py\", line 27, in <module>\r\n          version = write_version_py()\r\n        File \"setup.py\", line 18, in write_version_py\r\n          with open(os.path.join(\"fairseq\", \"version.txt\")) as f:\r\n      FileNotFoundError: [Errno 2] No such file or directory: 'fairseq/version.txt'\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 Getting requirements to build wheel did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This error originates from a subprocess, and is likely not a problem with pip.\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version: 0.12.1\r\n - PyTorch Version: 1.11.0\r\n - OS: Ubuntu 22.04\r\n - Python version: 3.10.4 \r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4501/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4501/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4495", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4495/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4495/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4495/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4495", "id": 1272688308, "node_id": "I_kwDOBhEUd85L2660", "number": 4495, "title": "Error when getting coin.yaml for VLM", "user": {"login": "mozhdehrouhsedaghat", "id": 58995806, "node_id": "MDQ6VXNlcjU4OTk1ODA2", "avatar_url": "https://avatars.githubusercontent.com/u/58995806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mozhdehrouhsedaghat", "html_url": "https://github.com/mozhdehrouhsedaghat", "followers_url": "https://api.github.com/users/mozhdehrouhsedaghat/followers", "following_url": "https://api.github.com/users/mozhdehrouhsedaghat/following{/other_user}", "gists_url": "https://api.github.com/users/mozhdehrouhsedaghat/gists{/gist_id}", "starred_url": "https://api.github.com/users/mozhdehrouhsedaghat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mozhdehrouhsedaghat/subscriptions", "organizations_url": "https://api.github.com/users/mozhdehrouhsedaghat/orgs", "repos_url": "https://api.github.com/users/mozhdehrouhsedaghat/repos", "events_url": "https://api.github.com/users/mozhdehrouhsedaghat/events{/privacy}", "received_events_url": "https://api.github.com/users/mozhdehrouhsedaghat/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-06-15T20:12:24Z", "updated_at": "2022-06-15T20:19:54Z", "closed_at": "2022-06-15T20:19:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nWhen I run the following command for getting coin.yaml for the VML model (according to the provided instructions), I get the below error.\r\n\r\npython3 locallaunch.py projects/mtm/vlm/coin.yaml --dryrun\r\n\r\n[JobLauncher] job_key sweep\r\nTraceback (most recent call last):\r\n  File \"locallaunch.py\", line 148, in <module>\r\n    main(args)\r\n  File \"locallaunch.py\", line 133, in main\r\n    job(job_type=job_type, dryrun=args.dryrun)\r\n  File \"locallaunch.py\", line 35, in __call__\r\n    job = JobLauncher.JOB_CONFIG[self.job_key](\r\nKeyError: 'sweep'\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4495/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4495/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4479", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4479/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4479/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4479/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4479", "id": 1267907120, "node_id": "I_kwDOBhEUd85Lkrow", "number": 4479, "title": "'TranslationFromPretrainedBARTTask' object has no attribute 'args' under fairseq version 0.12.0", "user": {"login": "tianshuailu", "id": 22446406, "node_id": "MDQ6VXNlcjIyNDQ2NDA2", "avatar_url": "https://avatars.githubusercontent.com/u/22446406?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tianshuailu", "html_url": "https://github.com/tianshuailu", "followers_url": "https://api.github.com/users/tianshuailu/followers", "following_url": "https://api.github.com/users/tianshuailu/following{/other_user}", "gists_url": "https://api.github.com/users/tianshuailu/gists{/gist_id}", "starred_url": "https://api.github.com/users/tianshuailu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tianshuailu/subscriptions", "organizations_url": "https://api.github.com/users/tianshuailu/orgs", "repos_url": "https://api.github.com/users/tianshuailu/repos", "events_url": "https://api.github.com/users/tianshuailu/events{/privacy}", "received_events_url": "https://api.github.com/users/tianshuailu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-06-10T19:04:39Z", "updated_at": "2022-06-19T14:11:30Z", "closed_at": "2022-06-19T14:11:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run the following command\r\n\r\nPRETRAIN=mbart.cc25 # fix if you moved the downloaded checkpoint\r\nlangs=ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN\r\n\r\nfairseq-train path_2_data \\\r\n  --encoder-normalize-before --decoder-normalize-before \\\r\n  --arch mbart_large --layernorm-embedding \\\r\n  --task translation_from_pretrained_bart \\\r\n  --source-lang hi_IN --target-lang en_XX \\\r\n  --criterion label_smoothed_cross_entropy --label-smoothing 0.2 \\\r\n  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \\\r\n  --lr-scheduler polynomial_decay --lr 3e-05 --warmup-updates 2500 --total-num-update 40000 \\\r\n  --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \\\r\n  --max-tokens 1024 --update-freq 2 \\\r\n  --save-interval 1 --save-interval-updates 5000 --keep-interval-updates 10 --no-epoch-checkpoints \\\r\n  --seed 222 --log-format simple --log-interval 2 \\\r\n  --restore-file $PRETRAIN \\\r\n  --reset-optimizer --reset-meters --reset-dataloader --reset-lr-scheduler \\\r\n  --langs $langs \\\r\n  --ddp-backend legacy_ddp\r\n\r\n2. Got the following error\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 557, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 133, in main\r\n    task.load_dataset(valid_sub_split, combine=False, epoch=1)\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/tasks/translation_from_pretrained_bart.py\", line 66, in load_dataset\r\n    paths = utils.split_paths(self.args.data)\r\nAttributeError: 'TranslationFromPretrainedBARTTask' object has no attribute 'args'\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n### Expected behavior\r\nAfter running the above code, it should start training the model, which still works earlier today with fairseq version 0.10.2, but it seems that fairseq just got an update to 0.12.0 and now the code gives an error.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version 0.12.0:\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4479/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4479/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4462", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4462/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4462/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4462/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4462", "id": 1264083540, "node_id": "I_kwDOBhEUd85LWGJU", "number": 4462, "title": "Can not successfully run the evaluation script of speech_text_joint_to_text pre-training code", "user": {"login": "czy97", "id": 29651739, "node_id": "MDQ6VXNlcjI5NjUxNzM5", "avatar_url": "https://avatars.githubusercontent.com/u/29651739?v=4", "gravatar_id": "", "url": "https://api.github.com/users/czy97", "html_url": "https://github.com/czy97", "followers_url": "https://api.github.com/users/czy97/followers", "following_url": "https://api.github.com/users/czy97/following{/other_user}", "gists_url": "https://api.github.com/users/czy97/gists{/gist_id}", "starred_url": "https://api.github.com/users/czy97/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/czy97/subscriptions", "organizations_url": "https://api.github.com/users/czy97/orgs", "repos_url": "https://api.github.com/users/czy97/repos", "events_url": "https://api.github.com/users/czy97/events{/privacy}", "received_events_url": "https://api.github.com/users/czy97/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2022-06-08T01:52:32Z", "updated_at": "2022-06-28T13:18:35Z", "closed_at": "2022-06-13T02:41:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nThe evaluation code for the Librispeech ASR Pre-training in https://github.com/facebookresearch/fairseq/blob/main/examples/speech_text_joint_to_text/docs/pre-training.md seems not to be well tested\r\n\r\n### To Reproduce\r\nCommand:\r\n`python ./fairseq_cli/generate.py \\\r\n    $S2T_DATA_PATH \\\r\n    --task speech_text_joint_to_text \\\r\n    --max-tokens 800000  \\\r\n    --max-source-positions 800000 \\\r\n    --nbest 1 \\\r\n    --results-path $SAVE_PATH \\\r\n    --batch-size 512 \\\r\n    --path $FINAL_MODEL \\\r\n    --gen-subset $SUBSET \\\r\n    --config-yaml config.yaml \\\r\n    --scoring wer \\\r\n    --beam 10 --lenpen 1.0 --user-dir examples/speech_text_joint_to_text`\r\n\r\n1.  The evaluation command for Librispeech ASR Pre-training has an error. I think we should add \"--user-dir\" before \"examples/speech_text_joint_to_text\"\r\n2.  After fixing the above issue, I directly evaluate the [Fine-tuned model](https://dl.fbaipublicfiles.com/joint_speech_text_4_s2t/acl2022/librispeech/finetuned/checkpoint_ave_10.pt) provided in https://github.com/facebookresearch/fairseq/blob/main/examples/speech_text_joint_to_text/docs/pre-training.md. I got another error:  \"OSError: Model file not found: /fsx/yuntang/2021/joint_pretraining_ASR/pretrain03/checkpoints/expt10_960h.wd0.01.config.neuu.lr_0.001.elr_1e-06.mu800.0k.uf6.bs200.msp1024.mtp1024.mtt3072.mspch600.0k.mass750.0k.miss64.0k.mst750.0k.dsb3.mask0.3.mr0.1.ssmp0.3.sump0.7.mwd.noscale.gelu.default.all.nb.lpos.dp0.1.bart.ngpu16/checkpoint6.pt\"\r\n3. Then, I directly evaluate fine-tuned model trained by myself. Then I get the following error:\r\nTraceback (most recent call last):\r\n  File \"./fairseq_cli/generate.py\", line 417, in <module>\r\n    cli_main()\r\n  File \"./fairseq_cli/generate.py\", line 413, in cli_main\r\n    main(args)\r\n  File \"./fairseq_cli/generate.py\", line 48, in main\r\n    return _main(cfg, h)\r\n  File \"./fairseq_cli/generate.py\", line 201, in _main\r\n    hypos = task.inference_step(\r\n  File \"/tmp/code/examples/speech_text_joint_to_text/tasks/speech_text_joint.py\", line 216, in inference_step\r\n    return generator.generate(\r\n  File \"/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/tmp/code/fairseq/sequence_generator.py\", line 191, in generate\r\n    return self._generate(sample, **kwargs)\r\n  File \"/tmp/code/fairseq/sequence_generator.py\", line 266, in _generate\r\n    encoder_outs = self.model.reorder_encoder_out(encoder_outs, new_order)\r\n  File \"/tmp/code/fairseq/sequence_generator.py\", line 873, in reorder_encoder_out\r\n    model.encoder.reorder_encoder_out(encoder_outs[i], new_order)\r\n  File \"/tmp/code/examples/speech_text_joint_to_text/models/s2t_dualinputtransformer.py\", line 377, in reorder_encoder_out\r\n    return self.spch_encoder.reorder_encoder_out(encoder_out, new_order)\r\n  File \"/tmp/code/fairseq/models/speech_to_text/s2t_wav_transformer.py\", line 485, in reorder_encoder_out\r\n    return self.speech_encoder.reorder_encoder_out(encoder_out, new_order)\r\n  File \"/tmp/code/fairseq/models/speech_to_text/s2t_wav_transformer.py\", line 381, in reorder_encoder_out\r\n    if len(encoder_out[\"encoder_out\"]) == 0\r\nTypeError: tuple indices must be integers or slices, not str\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version : main\r\n - PyTorch Version (1.10.0)\r\n - OS: Linux\r\n - How you installed fairseq (`pip`, source): Yes\r\n - Python version: 3.8.12\r\n - CUDA/cuDNN version: cuda_11.1\r\n - GPU models and configuration: A100\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4462/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4429", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4429/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4429/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4429/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4429", "id": 1242688799, "node_id": "I_kwDOBhEUd85KEe0f", "number": 4429, "title": "Module Not Found Error", "user": {"login": "jayachandrakalakutagar", "id": 96902361, "node_id": "U_kgDOBcac2Q", "avatar_url": "https://avatars.githubusercontent.com/u/96902361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jayachandrakalakutagar", "html_url": "https://github.com/jayachandrakalakutagar", "followers_url": "https://api.github.com/users/jayachandrakalakutagar/followers", "following_url": "https://api.github.com/users/jayachandrakalakutagar/following{/other_user}", "gists_url": "https://api.github.com/users/jayachandrakalakutagar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jayachandrakalakutagar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jayachandrakalakutagar/subscriptions", "organizations_url": "https://api.github.com/users/jayachandrakalakutagar/orgs", "repos_url": "https://api.github.com/users/jayachandrakalakutagar/repos", "events_url": "https://api.github.com/users/jayachandrakalakutagar/events{/privacy}", "received_events_url": "https://api.github.com/users/jayachandrakalakutagar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-05-20T06:32:54Z", "updated_at": "2022-05-27T10:11:17Z", "closed_at": "2022-05-27T10:11:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nwas trying to run inference on pre-trained transformer TTS model downloaded from[ here](https://dl.fbaipublicfiles.com/fairseq/s2/vctk_transformer_phn.tar)\r\nNo module named 'fairseq'\r\nHelp me out to fix the bug\r\n![issue](https://user-images.githubusercontent.com/96902361/169467171-d768b581-766b-45bd-96b0-4ad7741bd20b.png)\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4429/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4416", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4416/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4416/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4416/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4416", "id": 1232918392, "node_id": "I_kwDOBhEUd85JfNd4", "number": 4416, "title": "Denoising task conflicting option string: --max-source-positions", "user": {"login": "clumsy", "id": 379115, "node_id": "MDQ6VXNlcjM3OTExNQ==", "avatar_url": "https://avatars.githubusercontent.com/u/379115?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clumsy", "html_url": "https://github.com/clumsy", "followers_url": "https://api.github.com/users/clumsy/followers", "following_url": "https://api.github.com/users/clumsy/following{/other_user}", "gists_url": "https://api.github.com/users/clumsy/gists{/gist_id}", "starred_url": "https://api.github.com/users/clumsy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clumsy/subscriptions", "organizations_url": "https://api.github.com/users/clumsy/orgs", "repos_url": "https://api.github.com/users/clumsy/repos", "events_url": "https://api.github.com/users/clumsy/events{/privacy}", "received_events_url": "https://api.github.com/users/clumsy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-05-11T16:38:55Z", "updated_at": "2022-06-28T19:44:18Z", "closed_at": "2022-06-28T19:44:18Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nLaunching denoising pretraining with BART model fails with:\r\n```\r\nargparse.ArgumentError: argument --max-source-positions: conflicting option string: --max-source-positions\r\n```\r\n\r\nThis seem to be happening because denoising task is still using argparse and not OmegaConf. And the same `--max-source-positions` option is already registered for a different task.\r\n\r\n### To Reproduce\r\n\r\nUsing main / v0.11.0  branches `denoising` task does not work with CLI and unit test.\r\n\r\n#### Code sample\r\n\r\nHere's a unit test that fails:\r\n```python\r\n# Copyright (c) Facebook, Inc. and its affiliates.\r\n#\r\n# This source code is licensed under the MIT license found in the\r\n# LICENSE file in the root directory of this source tree.\r\n\r\nimport os\r\nimport unittest\r\nfrom tempfile import TemporaryDirectory\r\n\r\nfrom fairseq import options\r\nfrom fairseq.binarizer import FileBinarizer, VocabularyDatasetBinarizer\r\nfrom fairseq.dataclass.utils import convert_namespace_to_omegaconf\r\nfrom fairseq.tasks.denoising import DenoisingTask\r\nfrom tests.utils import build_vocab, make_data\r\n\r\n\r\nclass TestDenoising(unittest.TestCase):\r\n    def test_denoising(self):\r\n        with TemporaryDirectory() as dirname:\r\n\r\n            # prep input file\r\n            raw_file = os.path.join(dirname, \"raw\")\r\n            data = make_data(out_file=raw_file)\r\n            vocab = build_vocab(data)\r\n\r\n            # binarize\r\n            binarizer = VocabularyDatasetBinarizer(vocab, append_eos=False)\r\n            split = \"train\"\r\n            bin_file = os.path.join(dirname, split)\r\n            dataset_impl = \"mmap\"\r\n            FileBinarizer.multiprocess_dataset(\r\n                input_file=raw_file,\r\n                binarizer=binarizer,\r\n                dataset_impl=dataset_impl,\r\n                vocab_size=len(vocab),\r\n                output_prefix=bin_file,\r\n            )\r\n\r\n            # setup task\r\n            train_args = options.parse_args_and_arch(\r\n                options.get_training_parser(),\r\n                [\r\n                    \"--task\",\r\n                    \"denoising\",\r\n                    \"--arch\",\r\n                    \"bart_base\",\r\n                    \"--seed\",\r\n                    \"42\",\r\n                    \"--mask-length\",\r\n                    \"word\",\r\n                    \"--permute-sentences\",\r\n                    \"1\",\r\n                    \"--rotate\",\r\n                    \"0\",\r\n                    \"--replace-length\",\r\n                    \"-1\",\r\n                    \"--mask\",\r\n                    \"0.2\",\r\n                    dirname,\r\n                ],\r\n            )\r\n            cfg = convert_namespace_to_omegaconf(train_args)\r\n            task = DenoisingTask(cfg.task, binarizer.dict)\r\n\r\n            # load datasets\r\n            original_dataset = task._load_dataset_split(bin_file, 1, False)\r\n            task.load_dataset(split)\r\n            masked_dataset = task.dataset(split)\r\n\r\n            iterator = task.get_batch_iterator(\r\n                dataset=masked_dataset,\r\n                max_tokens=65_536,\r\n                max_positions=4_096,\r\n            ).next_epoch_itr(shuffle=False)\r\n            mask_index = task.source_dictionary.index(\"<mask>\")\r\n            for batch in iterator:\r\n                for sample in range(len(batch)):\r\n                    net_input = batch[\"net_input\"]\r\n                    masked_src_tokens = net_input[\"src_tokens\"][sample]\r\n                    masked_src_length = net_input[\"src_lengths\"][sample]\r\n                    masked_tgt_tokens = batch[\"target\"][sample]\r\n\r\n                    sample_id = batch[\"id\"][sample]\r\n                    original_tokens = original_dataset[sample_id]\r\n                    original_tokens = original_tokens.masked_select(\r\n                        masked_src_tokens[:masked_src_length] == mask_index\r\n                    )\r\n                    masked_tokens = masked_tgt_tokens.masked_select(\r\n                        masked_src_tokens == mask_index\r\n                    )\r\n\r\n                    assert masked_tokens.equal(original_tokens)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n```\r\n\r\n### Expected behavior\r\n\r\nDenoising task should work.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): `main`, `v0.11.0` branches\r\n - PyTorch Version (e.g., 1.0): `v0.10.2`\r\n - OS (e.g., Linux): `AL2`\r\n - How you installed fairseq (`pip`, source):  `source`\r\n - Build command you used (if compiling from source): `sudo pip install --no-deps -v .`\r\n - Python version: `3.8.12`\r\n - CUDA/cuDNN version: `11.3`\r\n - GPU models and configuration: `8 NVIDIA\u00ae V100 Tensor Core GPUs`\r\n - Any other relevant information:\r\n\r\n### Additional context\r\nN/A\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4416/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4400", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4400/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4400/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4400/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4400", "id": 1227528237, "node_id": "I_kwDOBhEUd85JKpgt", "number": 4400, "title": "ValueError: Buffer dtype mismatch, expected 'int64_t' but got 'long'", "user": {"login": "YumingZHAI", "id": 20256921, "node_id": "MDQ6VXNlcjIwMjU2OTIx", "avatar_url": "https://avatars.githubusercontent.com/u/20256921?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YumingZHAI", "html_url": "https://github.com/YumingZHAI", "followers_url": "https://api.github.com/users/YumingZHAI/followers", "following_url": "https://api.github.com/users/YumingZHAI/following{/other_user}", "gists_url": "https://api.github.com/users/YumingZHAI/gists{/gist_id}", "starred_url": "https://api.github.com/users/YumingZHAI/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YumingZHAI/subscriptions", "organizations_url": "https://api.github.com/users/YumingZHAI/orgs", "repos_url": "https://api.github.com/users/YumingZHAI/repos", "events_url": "https://api.github.com/users/YumingZHAI/events{/privacy}", "received_events_url": "https://api.github.com/users/YumingZHAI/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-05-06T07:43:13Z", "updated_at": "2022-07-14T07:08:13Z", "closed_at": "2022-07-14T07:08:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- When I run fairseq-train to fine-tune a pre-trained model, I got the error \"ValueError: Buffer dtype mismatch, expected 'int64_t' but got 'long'\"  -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run cmd :\r\nos.system(\"fairseq-train data_bin \\\r\n  --finetune-from-model 418M_last_checkpoint.pt \\\r\n  --save-dir m2m-checkpoint/ \\\r\n  --task translation_multi_simple_epoch \\\r\n  --encoder-normalize-before \\\r\n  --lang-pairs yo-en \\\r\n  --batch-size 10 \\\r\n  --decoder-normalize-before \\\r\n  --encoder-langtok src \\\r\n  --decoder-langtok \\\r\n  --criterion cross_entropy \\\r\n  --optimizer adafactor \\\r\n  --lr-scheduler cosine \\\r\n  --lr 3e-05 \\\r\n  --max-update 40000 \\\r\n  --update-freq 2 \\\r\n  --save-interval 1 \\\r\n  --save-interval-updates 5000 \\\r\n  --keep-interval-updates 10 \\\r\n  --no-epoch-checkpoints \\\r\n  --log-format simple \\\r\n  --log-interval 2 \\\r\n  --patience 10 \\\r\n  --arch transformer_wmt_en_de_big \\\r\n  --encoder-layers 12 --decoder-layers 12 \\\r\n  --share-decoder-input-output-embed --share-all-embeddings \\\r\n  --ddp-backend no_c10d \\\r\n  --max-epoch 3\")\r\n\r\n2. See error\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\fairseq-train-script.py\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"c:\\users\\administrator\\pycharmprojects\\bfsu-nmt\\fairseq\\fairseq_cli\\train.py\", line 557, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"c:\\users\\administrator\\pycharmprojects\\bfsu-nmt\\fairseq\\fairseq\\distributed\\utils.py\", line 369, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"c:\\users\\administrator\\pycharmprojects\\bfsu-nmt\\fairseq\\fairseq_cli\\train.py\", line 164, in main\r\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(\r\n  File \"c:\\users\\administrator\\pycharmprojects\\bfsu-nmt\\fairseq\\fairseq\\checkpoint_utils.py\", line 272, in load_checkpoint\r\n    epoch_itr = trainer.get_train_iterator(\r\n  File \"c:\\users\\administrator\\pycharmprojects\\bfsu-nmt\\fairseq\\fairseq\\trainer.py\", line 718, in get_train_iterator\r\n    return data_utils.batch_by_size(\r\n  File \"c:\\users\\administrator\\pycharmprojects\\bfsu-nmt\\fairseq\\fairseq\\data\\data_utils.py\", line 348, in batch_by_size\r\n    return batch_by_size_vec(\r\n  File \"fairseq\\data\\data_utils_fast.pyx\", line 20, in fairseq.data.data_utils_fast.batch_by_size_vec\r\nValueError: Buffer dtype mismatch, expected 'int64_t' but got 'long'\r\n\r\n### Environment\r\n\r\n - fairseq Version: 1.0.0a0\r\n - PyTorch Version:1.11.0+cu113 \r\n - OS: Windows 10\r\n - How you installed fairseq: source \r\n - Build command you used (if compiling from source): pip install --editable ./\r\n - Python version: 3.9.7 \r\n - CUDA/cuDNN version: 11.3 \r\n - GPU models and configuration: NVIDIA GeForce GTX 1660 Ti \r\n - Any other relevant information:\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4400/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4400/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4397", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4397/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4397/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4397/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4397", "id": 1226306971, "node_id": "I_kwDOBhEUd85JF_Wb", "number": 4397, "title": "ModuleNotFoundError: No module named 'xformers'", "user": {"login": "raphaelmerx", "id": 5139869, "node_id": "MDQ6VXNlcjUxMzk4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/5139869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raphaelmerx", "html_url": "https://github.com/raphaelmerx", "followers_url": "https://api.github.com/users/raphaelmerx/followers", "following_url": "https://api.github.com/users/raphaelmerx/following{/other_user}", "gists_url": "https://api.github.com/users/raphaelmerx/gists{/gist_id}", "starred_url": "https://api.github.com/users/raphaelmerx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raphaelmerx/subscriptions", "organizations_url": "https://api.github.com/users/raphaelmerx/orgs", "repos_url": "https://api.github.com/users/raphaelmerx/repos", "events_url": "https://api.github.com/users/raphaelmerx/events{/privacy}", "received_events_url": "https://api.github.com/users/raphaelmerx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "dianaml0", "id": 82468439, "node_id": "MDQ6VXNlcjgyNDY4NDM5", "avatar_url": "https://avatars.githubusercontent.com/u/82468439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dianaml0", "html_url": "https://github.com/dianaml0", "followers_url": "https://api.github.com/users/dianaml0/followers", "following_url": "https://api.github.com/users/dianaml0/following{/other_user}", "gists_url": "https://api.github.com/users/dianaml0/gists{/gist_id}", "starred_url": "https://api.github.com/users/dianaml0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dianaml0/subscriptions", "organizations_url": "https://api.github.com/users/dianaml0/orgs", "repos_url": "https://api.github.com/users/dianaml0/repos", "events_url": "https://api.github.com/users/dianaml0/events{/privacy}", "received_events_url": "https://api.github.com/users/dianaml0/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "dianaml0", "id": 82468439, "node_id": "MDQ6VXNlcjgyNDY4NDM5", "avatar_url": "https://avatars.githubusercontent.com/u/82468439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dianaml0", "html_url": "https://github.com/dianaml0", "followers_url": "https://api.github.com/users/dianaml0/followers", "following_url": "https://api.github.com/users/dianaml0/following{/other_user}", "gists_url": "https://api.github.com/users/dianaml0/gists{/gist_id}", "starred_url": "https://api.github.com/users/dianaml0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dianaml0/subscriptions", "organizations_url": "https://api.github.com/users/dianaml0/orgs", "repos_url": "https://api.github.com/users/dianaml0/repos", "events_url": "https://api.github.com/users/dianaml0/events{/privacy}", "received_events_url": "https://api.github.com/users/dianaml0/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-05-05T07:43:28Z", "updated_at": "2022-05-09T16:01:28Z", "closed_at": "2022-05-09T16:01:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nSeems like there's a missing dependency `xformers` when importing fairseq\r\n\r\n### To Reproduce\r\n\r\n* Install fairseq latest in editable mode\r\n* Run `import fairseq`\r\n\r\nYou get:\r\n```python\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/__init__.py\", line 33, in <module>\r\n    import fairseq.criterions  # noqa\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/criterions/__init__.py\", line 36, in <module>\r\n    importlib.import_module(\"fairseq.criterions.\" + file_name)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/criterions/ctc.py\", line 19, in <module>\r\n    from fairseq.tasks import FairseqTask\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/tasks/__init__.py\", line 136, in <module>\r\n    import_tasks(tasks_dir, \"fairseq.tasks\")\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/tasks/__init__.py\", line 117, in import_tasks\r\n    importlib.import_module(namespace + \".\" + task_name)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/tasks/semisupervised_translation.py\", line 22, in <module>\r\n    from fairseq.models import FairseqMultiModel\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/models/__init__.py\", line 235, in <module>\r\n    import_models(models_dir, \"fairseq.models\")\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/models/__init__.py\", line 217, in import_models\r\n    importlib.import_module(namespace + \".\" + model_name)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/models/hubert/__init__.py\", line 6, in <module>\r\n    from .hubert import *  # noqa\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/models/hubert/hubert.py\", line 20, in <module>\r\n    from fairseq.models.wav2vec.wav2vec2 import (\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/models/wav2vec/__init__.py\", line 6, in <module>\r\n    from .wav2vec import *  # noqa\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/models/wav2vec/wav2vec.py\", line 18, in <module>\r\n    from fairseq.modules import (\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/modules/__init__.py\", line 33, in <module>\r\n    from .multihead_attention import MultiheadAttention\r\n  File \"/usr/local/lib/python3.7/dist-packages/fairseq/modules/multihead_attention.py\", line 13, in <module>\r\n    from xformers.components.attention import build_attention\r\nModuleNotFoundError: No module named 'xformers'\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): main\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): `pip install -e fairseq` from main\r\n - Python version: 3.7\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4397/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4397/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4374", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4374/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4374/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4374/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4374", "id": 1212004781, "node_id": "I_kwDOBhEUd85IPbmt", "number": 4374, "title": "fairseq-generate generates the sentences has a lot of duplications", "user": {"login": "yuanbinhuan", "id": 44355058, "node_id": "MDQ6VXNlcjQ0MzU1MDU4", "avatar_url": "https://avatars.githubusercontent.com/u/44355058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbinhuan", "html_url": "https://github.com/yuanbinhuan", "followers_url": "https://api.github.com/users/yuanbinhuan/followers", "following_url": "https://api.github.com/users/yuanbinhuan/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbinhuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbinhuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbinhuan/subscriptions", "organizations_url": "https://api.github.com/users/yuanbinhuan/orgs", "repos_url": "https://api.github.com/users/yuanbinhuan/repos", "events_url": "https://api.github.com/users/yuanbinhuan/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbinhuan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-04-22T08:22:23Z", "updated_at": "2022-05-17T17:26:20Z", "closed_at": "2022-04-22T08:22:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4374/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4374/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4326", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4326/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4326/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4326/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4326", "id": 1190824299, "node_id": "I_kwDOBhEUd85G-olr", "number": 4326, "title": "RuntimeError in fairseq-train", "user": {"login": "LeonKennedy", "id": 4124675, "node_id": "MDQ6VXNlcjQxMjQ2NzU=", "avatar_url": "https://avatars.githubusercontent.com/u/4124675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LeonKennedy", "html_url": "https://github.com/LeonKennedy", "followers_url": "https://api.github.com/users/LeonKennedy/followers", "following_url": "https://api.github.com/users/LeonKennedy/following{/other_user}", "gists_url": "https://api.github.com/users/LeonKennedy/gists{/gist_id}", "starred_url": "https://api.github.com/users/LeonKennedy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LeonKennedy/subscriptions", "organizations_url": "https://api.github.com/users/LeonKennedy/orgs", "repos_url": "https://api.github.com/users/LeonKennedy/repos", "events_url": "https://api.github.com/users/LeonKennedy/events{/privacy}", "received_events_url": "https://api.github.com/users/LeonKennedy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-04-03T05:29:49Z", "updated_at": "2022-09-23T07:09:18Z", "closed_at": "2022-09-23T07:09:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\njust run fairseq-train\r\n\r\n### To Reproduce\r\n\r\n1. conda create -n fair python=3.7.13\r\n2. pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu112\r\n3. cd fairseq\r\n4. pip install --editable ./\r\n5. fairseq-train \r\nsee error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/hello/anaconda3/envs/fair/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/mnt/data4t/code/fairseq/fairseq_cli/train.py\", line 542, in cli_main\r\n    args = options.parse_args_and_arch(parser, modify_parser=modify_parser)\r\n  File \"/mnt/data4t/code/fairseq/fairseq/options.py\", line 153, in parse_args_and_arch\r\n    raise RuntimeError()\r\nRuntimeError\r\n\r\n### Environment\r\n\r\n - fairseq Version : main\r\n - PyTorch Version 1.11\r\n - OS (e.g., Linux): ubauntu20.04\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: \r\n - CUDA/cuDNN version: \r\n\r\n### Additional context\r\n\r\ni not found fairseq command in /home/hello/anaconda3/envs/fair/bin/\r\n\r\n>> ls /home/hello/anaconda3/envs/fair/bin/\r\nfairseq-eval-lm  fairseq-generate  fairseq-hydra-train  fairseq-interactive  fairseq-preprocess  fairseq-score  fairseq-train  fairseq-validate\r\n\r\n\r\n>> python -c \"import fairseq;print(fairseq.__version__)\"\r\n1.0.0a0+06c65c8\r\n\r\ndid i miss some step?", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4326/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4326/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4267", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4267/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4267/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4267/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4267", "id": 1167216531, "node_id": "I_kwDOBhEUd85Fkk-T", "number": 4267, "title": "[error][data2vec]omegaconf.errors.ConfigKeyError: Key 'cache_in_scratch' not in 'AudioFinetuningConfig'", "user": {"login": "Dawn-970", "id": 66781518, "node_id": "MDQ6VXNlcjY2NzgxNTE4", "avatar_url": "https://avatars.githubusercontent.com/u/66781518?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dawn-970", "html_url": "https://github.com/Dawn-970", "followers_url": "https://api.github.com/users/Dawn-970/followers", "following_url": "https://api.github.com/users/Dawn-970/following{/other_user}", "gists_url": "https://api.github.com/users/Dawn-970/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dawn-970/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dawn-970/subscriptions", "organizations_url": "https://api.github.com/users/Dawn-970/orgs", "repos_url": "https://api.github.com/users/Dawn-970/repos", "events_url": "https://api.github.com/users/Dawn-970/events{/privacy}", "received_events_url": "https://api.github.com/users/Dawn-970/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-03-12T08:04:22Z", "updated_at": "2022-03-12T13:48:30Z", "closed_at": "2022-03-12T13:48:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd 'fairseq-hydra-train \\\r\n    task.normalize=true \\\r\n    common.user_dir=/mypath1 \\\r\n    distributed_training.distributed_world_size=1 \\\r\n    task.data=/mypath2 \\\r\n    model.w2v_path=/mypath3 \\\r\n    --config-dir /mypath4 \\\r\n    --config-name base_100h  '\r\n2. See error\r\nomegaconf.errors.ConfigKeyError: Key 'cache_in_scratch' not in 'AudioFinetuningConfig'\r\n        full_key: cache_in_scratch\r\n        reference_type=Optional[AudioFinetuningConfig]\r\n        object_type=AudioFinetuningConfig\r\n\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version :main\r\n - PyTorch Version 1.11.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source):cloned the main branch from repo\r\n - Python version:3.8.12\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4267/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4246", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4246/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4246/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4246/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4246", "id": 1155868408, "node_id": "I_kwDOBhEUd85E5Sb4", "number": 4246, "title": "ImportError: fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so: undefined symbol", "user": {"login": "clumsy", "id": 379115, "node_id": "MDQ6VXNlcjM3OTExNQ==", "avatar_url": "https://avatars.githubusercontent.com/u/379115?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clumsy", "html_url": "https://github.com/clumsy", "followers_url": "https://api.github.com/users/clumsy/followers", "following_url": "https://api.github.com/users/clumsy/following{/other_user}", "gists_url": "https://api.github.com/users/clumsy/gists{/gist_id}", "starred_url": "https://api.github.com/users/clumsy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clumsy/subscriptions", "organizations_url": "https://api.github.com/users/clumsy/orgs", "repos_url": "https://api.github.com/users/clumsy/repos", "events_url": "https://api.github.com/users/clumsy/events{/privacy}", "received_events_url": "https://api.github.com/users/clumsy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-03-01T22:09:20Z", "updated_at": "2022-11-09T07:01:26Z", "closed_at": "2022-03-03T18:06:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n`ImportError: /usr/local/lib/python3.8/dist-packages/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE` when fine-tuning __bart_large__ model using translation task with the latest v0.11.0\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd using a __custom__ dataset:\r\n```\r\npython -m torch.distributed.launch --nproc_per_node=4 \\\r\n    --nnodes=1 --node_rank=0 \\\r\n    --master_addr=$ip \\\r\n    --master_port=12345 \\\r\n    $(which fairseq-train) $DATA \\\r\n    --max-tokens $MAX_TOKENS \\\r\n    --task translation \\\r\n    --source-lang iq --target-lang oq \\\r\n    --truncate-source \\\r\n    --layernorm-embedding \\\r\n    --share-all-embeddings \\\r\n    --share-decoder-input-output-embed \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --required-batch-size-multiple 1 \\\r\n    --arch bart_large \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --label-smoothing 0.1 \\\r\n    --dropout 0.1 --attention-dropout 0.1 \\\r\n    --weight-decay 0.01 --optimizer adam --adam-betas \"(0.9, 0.999)\" --adam-eps 1e-08 \\\r\n    --clip-norm 0.1 \\\r\n    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\r\n    --fp16 --update-freq $UPDATE_FREQ \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --find-unused-parameters;\r\n```\r\n3. See error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq_cli/train.py\", line 535, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/distributed/utils.py\", line 354, in call_main\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq_cli/train.py\", line 94, in main\r\n    model = fsdp_wrap(task.build_model(cfg.model))\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/tasks/translation.py\", line 369, in build_model\r\n    model = super().build_model(cfg, from_checkpoint)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/tasks/fairseq_task.py\", line 335, in build_model\r\n    model = models.build_model(cfg, self, from_checkpoint)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/models/__init__.py\", line 106, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/models/transformer/transformer_legacy.py\", line 133, in build_model\r\n    return super().build_model(cfg, task)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/models/transformer/transformer_base.py\", line 97, in build_model\r\n    encoder = cls.build_encoder(cfg, src_dict, encoder_embed_tokens)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/models/transformer/transformer_legacy.py\", line 143, in build_encoder\r\n    return super().build_encoder(\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/models/transformer/transformer_base.py\", line 119, in build_encoder\r\n    return TransformerEncoderBase(cfg, src_dict, embed_tokens)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/models/transformer/transformer_encoder.py\", line 79, in __init__\r\n    self.layernorm_embedding = LayerNorm(embed_dim, export=cfg.export)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fairseq/modules/layer_norm.py\", line 32, in LayerNorm\r\n    return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\r\n  File \"/usr/local/lib/python3.8/dist-packages/apex/normalization/fused_layer_norm.py\", line 166, in __init__\r\n    fused_layer_norm_cuda = importlib.import_module(\"fused_layer_norm_cuda\")\r\n  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 657, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 556, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1166, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /usr/local/lib/python3.8/dist-packages/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\r\n```\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\nThis works fine on 0.10.2, but I would like to start using `--ddp-backend fully_sharded` mode thus switching to (unreleased) 0.11.0\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): `0.11.0`\r\n - PyTorch Version (e.g., 1.0): `1.10.0`\r\n - OS (e.g., Linux): `Linux x86_64`\r\n - How you installed fairseq (`pip`, source): `pip3 install of v0.11.0`\r\n - Build command you used (if compiling from source): `pip3 install --no-dependencies . (PyTorch installation details below)`\r\n - Python version: `3.8.12`\r\n - CUDA/cuDNN version: `11.3`\r\n - GPU models and configuration: `Tesla V100-SXM2-16GB`\r\n - Any other relevant information: `Using Apex fork: https://github.com/szhengac/apex using --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\"`\r\n\r\n### Additional context\r\n\r\nBuild as part of a Docker image base on `nvidia/cuda:11.3.0-devel-ubuntu20.04`.\r\n\r\nCompiling PyTorch from source:\r\n```DS_BUILD_OPS=1 TORCH_CUDA_ARCH_LIST=\"6.0 6.1 7.0 7.5 8.0\" pip3 install -v . --global-option=\"build_ext\" --global-option=\"-j$(nproc)\"```\r\n\r\nNot sure, but it might be related to https://discuss.pytorch.org/t/undefined-symbol-when-import-lltm-cpp-extension/32627", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4246/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4246/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4242", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4242/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4242/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4242/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4242", "id": 1152194847, "node_id": "I_kwDOBhEUd85ErRkf", "number": 4242, "title": "AddTargetDataset pads target sequences first, then adds EOS", "user": {"login": "shalymin-amzn", "id": 98762373, "node_id": "U_kgDOBeL-hQ", "avatar_url": "https://avatars.githubusercontent.com/u/98762373?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shalymin-amzn", "html_url": "https://github.com/shalymin-amzn", "followers_url": "https://api.github.com/users/shalymin-amzn/followers", "following_url": "https://api.github.com/users/shalymin-amzn/following{/other_user}", "gists_url": "https://api.github.com/users/shalymin-amzn/gists{/gist_id}", "starred_url": "https://api.github.com/users/shalymin-amzn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shalymin-amzn/subscriptions", "organizations_url": "https://api.github.com/users/shalymin-amzn/orgs", "repos_url": "https://api.github.com/users/shalymin-amzn/repos", "events_url": "https://api.github.com/users/shalymin-amzn/events{/privacy}", "received_events_url": "https://api.github.com/users/shalymin-amzn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-02-26T19:39:19Z", "updated_at": "2022-03-01T05:27:05Z", "closed_at": "2022-03-01T05:27:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n`AddTargetDataset` pads target sequences first, then adds `EOS`. This results in the following target batches:\r\n```\r\n[\r\n  [ 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0 ... 0 EOS]\r\n  [ 1 2 3 4 5 6 7 8 9 0 0 0 0 0 0 ... 0 EOS]\r\n]\r\n```\r\n\r\nOne ends up using this config with `Wav2Vec2Seq2Seq` and I guess with any autoregressive task.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\nTargets like that:\r\n```\r\n[\r\n  [ 1 2 3 4 5 EOS 0 0 0 0 0 0 0 0 0 0 ... 0]\r\n  [ 1 2 3 4 5 6 7 8 9 EOS 0 0 0 0 0 0 ... 0]\r\n]\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): latest dev\r\n - PyTorch Version (e.g., 1.0) 10\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4242/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4235", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4235/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4235/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4235/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4235", "id": 1148893333, "node_id": "I_kwDOBhEUd85EeriV", "number": 4235, "title": "size mismatch When Running CommonsenseQATask", "user": {"login": "Hanser14Forever", "id": 48411454, "node_id": "MDQ6VXNlcjQ4NDExNDU0", "avatar_url": "https://avatars.githubusercontent.com/u/48411454?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hanser14Forever", "html_url": "https://github.com/Hanser14Forever", "followers_url": "https://api.github.com/users/Hanser14Forever/followers", "following_url": "https://api.github.com/users/Hanser14Forever/following{/other_user}", "gists_url": "https://api.github.com/users/Hanser14Forever/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hanser14Forever/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hanser14Forever/subscriptions", "organizations_url": "https://api.github.com/users/Hanser14Forever/orgs", "repos_url": "https://api.github.com/users/Hanser14Forever/repos", "events_url": "https://api.github.com/users/Hanser14Forever/events{/privacy}", "received_events_url": "https://api.github.com/users/Hanser14Forever/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-02-24T05:58:48Z", "updated_at": "2022-02-27T10:09:56Z", "closed_at": "2022-02-27T10:09:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "Traceback (most recent call last):\r\n  File \"/home2/gyxiao/Workplace/Code/fairseq/fairseq/trainer.py\", line 279, in load_checkpoint\r\n    state[\"model\"], strict=True, args=self.args\r\n  File \"/home2/gyxiao/Workplace/Code/fairseq/fairseq/models/fairseq_model.py\", line 96, in load_state_dict\r\n    return super().load_state_dict(new_state_dict, strict)\r\n  File \"/home2/gyxiao/.conda/envs/newpy36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1052, in load_state_dict\r\n    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\nRuntimeError: Error(s) in loading state_dict for RobertaModel:\r\n        Missing key(s) in state_dict: \"encoder.sentence_encoder.layers.12.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.12.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.12.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.12.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.12.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.12.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.12.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.12.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.12.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.12.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.12.fc1.weight\", \"encoder.sentence_encoder.layers.12.fc1.bias\", \"encoder.sentence_encoder.layers.12.fc2.weight\", \"encoder.sentence_encoder.layers.12.fc2.bias\", \"encoder.sentence_encoder.layers.12.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.12.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.13.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.13.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.13.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.13.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.13.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.13.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.13.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.13.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.13.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.13.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.13.fc1.weight\", \"encoder.sentence_encoder.layers.13.fc1.bias\", \"encoder.sentence_encoder.layers.13.fc2.weight\", \"encoder.sentence_encoder.layers.13.fc2.bias\", \"encoder.sentence_encoder.layers.13.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.13.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.14.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.14.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.14.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.14.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.14.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.14.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.14.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.14.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.14.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.14.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.14.fc1.weight\", \"encoder.sentence_encoder.layers.14.fc1.bias\", \"encoder.sentence_encoder.layers.14.fc2.weight\", \"encoder.sentence_encoder.layers.14.fc2.bias\", \"encoder.sentence_encoder.layers.14.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.14.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.15.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.15.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.15.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.15.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.15.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.15.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.15.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.15.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.15.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.15.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.15.fc1.weight\", \"encoder.sentence_encoder.layers.15.fc1.bias\", \"encoder.sentence_encoder.layers.15.fc2.weight\", \"encoder.sentence_encoder.layers.15.fc2.bias\", \"encoder.sentence_encoder.layers.15.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.15.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.16.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.16.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.16.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.16.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.16.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.16.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.16.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.16.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.16.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.16.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.16.fc1.weight\", \"encoder.sentence_encoder.layers.16.fc1.bias\", \"encoder.sentence_encoder.layers.16.fc2.weight\", \"encoder.sentence_encoder.layers.16.fc2.bias\", \"encoder.sentence_encoder.layers.16.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.16.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.17.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.17.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.17.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.17.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.17.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.17.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.17.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.17.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.17.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.17.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.17.fc1.weight\", \"encoder.sentence_encoder.layers.17.fc1.bias\", \"encoder.sentence_encoder.layers.17.fc2.weight\", \"encoder.sentence_encoder.layers.17.fc2.bias\", \"encoder.sentence_encoder.layers.17.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.17.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.18.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.18.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.18.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.18.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.18.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.18.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.18.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.18.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.18.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.18.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.18.fc1.weight\", \"encoder.sentence_encoder.layers.18.fc1.bias\", \"encoder.sentence_encoder.layers.18.fc2.weight\", \"encoder.sentence_encoder.layers.18.fc2.bias\", \"encoder.sentence_encoder.layers.18.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.18.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.19.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.19.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.19.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.19.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.19.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.19.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.19.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.19.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.19.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.19.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.19.fc1.weight\", \"encoder.sentence_encoder.layers.19.fc1.bias\", \"encoder.sentence_encoder.layers.19.fc2.weight\", \"encoder.sentence_encoder.layers.19.fc2.bias\", \"encoder.sentence_encoder.layers.19.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.19.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.20.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.20.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.20.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.20.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.20.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.20.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.20.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.20.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.20.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.20.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.20.fc1.weight\", \"encoder.sentence_encoder.layers.20.fc1.bias\", \"encoder.sentence_encoder.layers.20.fc2.weight\", \"encoder.sentence_encoder.layers.20.fc2.bias\", \"encoder.sentence_encoder.layers.20.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.20.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.21.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.21.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.21.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.21.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.21.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.21.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.21.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.21.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.21.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.21.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.21.fc1.weight\", \"encoder.sentence_encoder.layers.21.fc1.bias\", \"encoder.sentence_encoder.layers.21.fc2.weight\", \"encoder.sentence_encoder.layers.21.fc2.bias\", \"encoder.sentence_encoder.layers.21.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.21.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.22.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.22.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.22.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.22.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.22.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.22.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.22.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.22.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.22.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.22.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.22.fc1.weight\", \"encoder.sentence_encoder.layers.22.fc1.bias\", \"encoder.sentence_encoder.layers.22.fc2.weight\", \"encoder.sentence_encoder.layers.22.fc2.bias\", \"encoder.sentence_encoder.layers.22.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.22.final_layer_norm.bias\", \"encoder.sentence_encoder.layers.23.self_attn.k_proj.weight\", \"encoder.sentence_encoder.layers.23.self_attn.k_proj.bias\", \"encoder.sentence_encoder.layers.23.self_attn.v_proj.weight\", \"encoder.sentence_encoder.layers.23.self_attn.v_proj.bias\", \"encoder.sentence_encoder.layers.23.self_attn.q_proj.weight\", \"encoder.sentence_encoder.layers.23.self_attn.q_proj.bias\", \"encoder.sentence_encoder.layers.23.self_attn.out_proj.weight\", \"encoder.sentence_encoder.layers.23.self_attn.out_proj.bias\", \"encoder.sentence_encoder.layers.23.self_attn_layer_norm.weight\", \"encoder.sentence_encoder.layers.23.self_attn_layer_norm.bias\", \"encoder.sentence_encoder.layers.23.fc1.weight\", \"encoder.sentence_encoder.layers.23.fc1.bias\", \"encoder.sentence_encoder.layers.23.fc2.weight\", \"encoder.sentence_encoder.layers.23.fc2.bias\", \"encoder.sentence_encoder.layers.23.final_layer_norm.weight\", \"encoder.sentence_encoder.layers.23.final_layer_norm.bias\". \r\n        size mismatch for encoder.sentence_encoder.embed_tokens.weight: copying a param with shape torch.Size([50265, 768]) from checkpoint, the shape in current model is torch.Size([50265, 1024]).\r\n        size mismatch for encoder.sentence_encoder.embed_positions.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([514, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.0.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.1.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.2.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.3.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.4.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.5.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.6.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.7.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.8.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.9.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.10.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.self_attn.out_proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.self_attn_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.self_attn_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.layers.11.final_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.emb_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.sentence_encoder.emb_layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.lm_head.weight: copying a param with shape torch.Size([50265, 768]) from checkpoint, the shape in current model is torch.Size([50265, 1024]).\r\n        size mismatch for encoder.lm_head.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n        size mismatch for encoder.lm_head.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.lm_head.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n        size mismatch for encoder.lm_head.layer_norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home2/gyxiao/.conda/envs/newpy36/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home2/gyxiao/Workplace/Code/fairseq/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/home2/gyxiao/Workplace/Code/fairseq/fairseq/distributed_utils.py\", line 258, in call_main\r\n    main(args, **kwargs)\r\n  File \"/home2/gyxiao/Workplace/Code/fairseq/fairseq_cli/train.py\", line 114, in main\r\n    disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n  File \"/home2/gyxiao/Workplace/Code/fairseq/fairseq/checkpoint_utils.py\", line 173, in load_checkpoint\r\n    reset_meters=reset_meters,\r\n  File \"/home2/gyxiao/Workplace/Code/fairseq/fairseq/trainer.py\", line 288, in load_checkpoint\r\n    \"please ensure that the architectures match.\".format(filename)\r\nException: Cannot load model parameters from checkpoint /home2/gyxiao/Workplace/Code/roberta/roberta_base/model.pt; please ensure that the architectures match.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4235/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4219", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4219/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4219/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4219/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4219", "id": 1139586402, "node_id": "I_kwDOBhEUd85D7LVi", "number": 4219, "title": "[Error] [Win] omegaconf.errors.ConfigKeyError: Key 'max_source_positions' not in 'LanguageModelingConfig'", "user": {"login": "affanmehmood", "id": 46571643, "node_id": "MDQ6VXNlcjQ2NTcxNjQz", "avatar_url": "https://avatars.githubusercontent.com/u/46571643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/affanmehmood", "html_url": "https://github.com/affanmehmood", "followers_url": "https://api.github.com/users/affanmehmood/followers", "following_url": "https://api.github.com/users/affanmehmood/following{/other_user}", "gists_url": "https://api.github.com/users/affanmehmood/gists{/gist_id}", "starred_url": "https://api.github.com/users/affanmehmood/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/affanmehmood/subscriptions", "organizations_url": "https://api.github.com/users/affanmehmood/orgs", "repos_url": "https://api.github.com/users/affanmehmood/repos", "events_url": "https://api.github.com/users/affanmehmood/events{/privacy}", "received_events_url": "https://api.github.com/users/affanmehmood/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-02-16T06:13:46Z", "updated_at": "2022-02-16T07:45:22Z", "closed_at": "2022-02-16T07:44:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI am trying to run the Dense models given [here](https://github.com/pytorch/fairseq/tree/main/examples/moe_lm). The dense_125m runs fine but when I change the models to dense_355m or dense_13b I get this error.\r\n\r\n  File \"app.py\", line 3, in <module>\r\n    lm = TransformerLanguageModel.from_pretrained(model_dir, bpe='gpt2')\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\fairseq\\models\\fairseq_model.py\", line 272, in from_pretrained\r\n    **kwargs,\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\fairseq\\hub_utils.py\", line 75, in from_pretrained\r\n    arg_overrides=kwargs,\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\fairseq\\checkpoint_utils.py\", line 432, in load_model_ensemble_and_task\r\n    task = tasks.setup_task(cfg.task)\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\fairseq\\tasks\\__init__.py\", line 39, in setup_task\r\n    cfg = merge_with_parent(dc(), cfg)\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\fairseq\\dataclass\\utils.py\", line 490, in merge_with_parent\r\n    merged_cfg = OmegaConf.merge(dc, cfg)\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\omegaconf.py\", line 321, in merge\r\n    target.merge_with(*others[1:])\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\basecontainer.py\", line 331, in merge_with\r\n    self._format_and_raise(key=None, value=None, cause=e)\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\_utils.py\", line 629, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\basecontainer.py\", line 329, in merge_with\r\n    self._merge_with(*others)\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\basecontainer.py\", line 347, in _merge_with\r\n    BaseContainer._map_merge(self, other)\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\basecontainer.py\", line 314, in _map_merge\r\n    dest[key] = src._get_node(key)\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\dictconfig.py\", line 259, in __setitem__\r\n    key=key, value=value, type_override=ConfigKeyError, cause=e\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\_utils.py\", line 629, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"C:\\Users\\DiveDeepAI\\Desktop\\fairseq\\venv\\lib\\site-packages\\omegaconf\\_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.ConfigKeyError: Key 'max_source_positions' not in 'LanguageModelingConfig'\r\n        full_key: max_source_positions\r\n        reference_type=Optional[LanguageModelingConfig]\r\n        object_type=LanguageModelingConfig\r\n\r\n\r\n#### Code sample\r\n\r\nfrom fairseq.models.transformer_lm import TransformerLanguageModel\r\nmodel_dir = 'models/en_dense_lm_355m'\r\nlm = TransformerLanguageModel.from_pretrained(model_dir, bpe='gpt2')\r\nlm = lm.eval()  # disable dropout\r\nlm = lm.half()  # use FP16 for evaluation\r\nlm = lm.cuda()  # move to GPU\r\n\r\ndef get_logprobs(prompt):\r\n    import re\r\n    prompt = re.sub('\\n+' , '\\n', prompt)  # collapse repeated newlines, which indicate separate documents\r\n    return lm.score(prompt, replace_newlines_with_eos=True)['positional_scores']\r\n\r\ndef COPA_eval(prompt, alternative1, alternative2):\r\n    lprob1 = get_logprobs(prompt + \"\\n\" + alternative1).sum()\r\n    lprob2 = get_logprobs(prompt + \"\\n\" + alternative2).sum()\r\n    return 1 if lprob1 > lprob2 else 2\r\n\r\nres = COPA_eval(\"The man broke his toe. What was the CAUSE of this?\", \"He got a hole in his sock.\", \"He dropped a hammer on his foot.\")\r\nprint('res', res)\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version : latest main branch\r\n - PyTorch Version : torch==1.10.2+cu113\r\n - OS : Windows\r\n - How you installed fairseq: cloned the main branch from repo\r\n - Python version : 3.7.9\r\n - CUDA/cuDNN version: 11.3\r\n - GPU models and configuration: RTX 2060 and RTX 3090 (seperate machines)\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4219/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4198", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4198/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4198/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4198/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4198", "id": 1127594196, "node_id": "I_kwDOBhEUd85DNbjU", "number": 4198, "title": "Roberta pre-training using torch-xla fails on GPU", "user": {"login": "hannanjgaws", "id": 72148590, "node_id": "MDQ6VXNlcjcyMTQ4NTkw", "avatar_url": "https://avatars.githubusercontent.com/u/72148590?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hannanjgaws", "html_url": "https://github.com/hannanjgaws", "followers_url": "https://api.github.com/users/hannanjgaws/followers", "following_url": "https://api.github.com/users/hannanjgaws/following{/other_user}", "gists_url": "https://api.github.com/users/hannanjgaws/gists{/gist_id}", "starred_url": "https://api.github.com/users/hannanjgaws/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hannanjgaws/subscriptions", "organizations_url": "https://api.github.com/users/hannanjgaws/orgs", "repos_url": "https://api.github.com/users/hannanjgaws/repos", "events_url": "https://api.github.com/users/hannanjgaws/events{/privacy}", "received_events_url": "https://api.github.com/users/hannanjgaws/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-02-08T18:07:44Z", "updated_at": "2022-03-14T01:12:18Z", "closed_at": "2022-02-12T02:31:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nI am trying to pre-train Roberta base using torch-xla on GPU. I am following the [Pretraining RoBERTa using your own data tutorial](https://github.com/pytorch/fairseq/blob/main/examples/roberta/README.pretraining.md) to preprocess the WikiText-103 dataset and train RoBERTa base using 8 NVIDIA A100 GPUs. I added `common.tpu=True` to enable the XLA flow. \r\n\r\nThe first training epoch starts, but then hangs indefinitely. I expect that using the XLA flow should work on GPU. \r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Preprocess the WikiText-103 dataset following https://github.com/pytorch/fairseq/blob/main/examples/roberta/README.pretraining.md#1-preprocess-the-data\r\n```\r\nmkdir $HOME/pytorch-tutorial-data\r\ncd $HOME/pytorch-tutorial-data\r\n\r\nwget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\r\nunzip wikitext-103-raw-v1.zip\r\n\r\nmkdir -p gpt2_bpe\r\nwget -O gpt2_bpe/encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\r\nwget -O gpt2_bpe/vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\r\n\r\nfor SPLIT in train valid test; do \\\r\n    python -m examples.roberta.multiprocessing_bpe_encoder \\\r\n        --encoder-json gpt2_bpe/encoder.json \\\r\n        --vocab-bpe gpt2_bpe/vocab.bpe \\\r\n        --inputs wikitext-103-raw/wiki.${SPLIT}.raw \\\r\n        --outputs wikitext-103-raw/wiki.${SPLIT}.bpe \\\r\n        --keep-empty \\\r\n        --workers 60; \\\r\ndone\r\n\r\nwget -O gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\r\n\r\nfairseq-preprocess \\\r\n    --only-source \\\r\n    --srcdict gpt2_bpe/dict.txt \\\r\n    --trainpref wikitext-103-raw/wiki.train.bpe \\\r\n    --validpref wikitext-103-raw/wiki.valid.bpe \\\r\n    --testpref wikitext-103-raw/wiki.test.bpe \\\r\n    --destdir data-bin/wikitext-103 \\\r\n    --workers 60\r\n```\r\n\r\n2. Set the following envvars to specify the data directory and use all 8 GPUs\r\n```\r\nexport DATA_DIR=${HOME}/pytorch-tutorial-data/data-bin/wikitext-103\r\nexport GPU_NUM_DEVICES=8\r\n```\r\n3. Apply the following patch to FairSeq to fix an OOM issue:\r\n```\r\ndiff --git a/fairseq/dataclass/configs.py b/fairseq/dataclass/configs.py\r\nindex 75407389..cbddcb75 100644\r\n--- a/fairseq/dataclass/configs.py\r\n+++ b/fairseq/dataclass/configs.py\r\n@@ -4,6 +4,7 @@\r\n # LICENSE file in the root directory of this source tree.\r\n \r\n import sys\r\n+import os\r\n from dataclasses import _MISSING_TYPE, dataclass, field\r\n from typing import Any, List, Optional\r\n \r\n@@ -243,13 +244,13 @@ class CommonConfig(FairseqDataclass):\r\n @dataclass\r\n class DistributedTrainingConfig(FairseqDataclass):\r\n     distributed_world_size: int = field(\r\n-        default=max(1, torch.cuda.device_count()),\r\n+        default=os.environ.get('GPU_NUM_DEVICES', 1),\r\n         metadata={\r\n             \"help\": \"total number of GPUs across all nodes (default: all visible GPUs)\"\r\n         },\r\n     )\r\n     distributed_num_procs: Optional[int] = field(\r\n-        default=max(1, torch.cuda.device_count()),\r\n+        default=os.environ.get('GPU_NUM_DEVICES', 1),\r\n         metadata={\r\n             \"help\": \"total number of processes to fork (default: all visible GPUs)\"\r\n         },\r\n@@ -354,7 +355,7 @@ class DistributedTrainingConfig(FairseqDataclass):\r\n         default=3, metadata={\"help\": \"Local SGD allreduce frequency\"}\r\n     )\r\n     nprocs_per_node: int = field(\r\n-        default=max(1, torch.cuda.device_count()),\r\n+        default=os.environ.get('GPU_NUM_DEVICES', 1),\r\n         metadata={\r\n             \"help\": \"number of GPUs in each node. An allreduce operation across GPUs in \"\r\n             \"a node is very fast. Hence, we do allreduce across GPUs in a node, \"\r\n```\r\n\r\n4.  Run the Roberta base training command. Add `common.tpu=True` to enable to XLA flow.\r\n```\r\ncd fairseq\r\n\r\nfairseq-hydra-train -m --config-dir examples/roberta/config/pretraining \\\r\n--config-name base task.data=$DATA_DIR common.tpu=True\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\nThe first training epoch starts, but hangs indefinitely.\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\r\nMinimal means having the shortest code but still preserving the bug. -->\r\nCode described above.\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nI expect that I should be able to use the TPU torch-xla flow to pre-train Roberta base on GPU. \r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): fairseq==1.0.0a0+4a7835b\r\n - PyTorch Version (e.g., 1.0): torch==1.10.1, torch-xla==1.10, torchvision==0.11.2\r\n - OS (e.g., Linux): Amazon Linux 2\r\n - How you installed fairseq (`pip`, source): from source\r\n - Build command you used (if compiling from source):\r\n  ```\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./\r\ncd ~/\r\n ```\r\n - Python version: Python 3.7.12\r\n - CUDA/cuDNN version: V11.2.152\r\n - GPU models and configuration: 8x A100\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4198/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4198/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4167", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4167/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4167/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4167/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4167", "id": 1118749564, "node_id": "I_kwDOBhEUd85CrsN8", "number": 4167, "title": "Translation M2M100 Bug: Untranslated sentence.", "user": {"login": "Jourdelune", "id": 64205064, "node_id": "MDQ6VXNlcjY0MjA1MDY0", "avatar_url": "https://avatars.githubusercontent.com/u/64205064?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jourdelune", "html_url": "https://github.com/Jourdelune", "followers_url": "https://api.github.com/users/Jourdelune/followers", "following_url": "https://api.github.com/users/Jourdelune/following{/other_user}", "gists_url": "https://api.github.com/users/Jourdelune/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jourdelune/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jourdelune/subscriptions", "organizations_url": "https://api.github.com/users/Jourdelune/orgs", "repos_url": "https://api.github.com/users/Jourdelune/repos", "events_url": "https://api.github.com/users/Jourdelune/events{/privacy}", "received_events_url": "https://api.github.com/users/Jourdelune/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-01-30T23:12:13Z", "updated_at": "2022-06-08T07:37:34Z", "closed_at": "2022-06-08T07:37:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nHello, I tried to translate this sentence with the m2m100 model from English to French however the sentence is not completely translated, a part is simply ignored. You can see this bug here: https://huggingface.co/spaces/Iker/Translate-100-languages\r\n\r\n### To Reproduce\r\n\r\nto reproduce, you can either test the link above or simply run this code on google colabs for example.\r\n![image](https://user-images.githubusercontent.com/64205064/151721910-7bc59e58-e767-41d2-904f-cd32bfee467b.png)\r\n\r\n\r\n#### Code sample\r\n\r\n```py\r\n!pip install transformers\r\n!pip install sentencepiece\r\n```\r\n```py\r\nfrom transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\r\n\r\ntext = \"when mentioned nothing seems to happen but using some command it works normally. The problem would be the mention?\"\r\n\r\n\r\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\r\ntokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\r\n\r\ntokenizer.src_lang = \"en\"\r\nencoded_hi = tokenizer(text, return_tensors=\"pt\")\r\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\r\nprint(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\r\n```\r\n\r\n### Expected behavior\r\n\r\nHe should translate the sentence as:\r\nQuand on le mentionne, rien ne semble se passer mais en utilisant certaines commandes, cela fonctionne normalement. Le probl\u00e8me serait la mention ?\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0) 1.10\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip3 install transformers, sentencepiece\r\n - Python version: python3.7\r\n - CUDA/cuDNN version: None (cpu)\r\n - GPU models and configuration: None (cpu)\r\n\r\n### Additional context\r\n\r\nThis bug is also present on ctranslate2, a library that allows to accelerate model transformers, so it's not only a bug related to hugging face.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4167/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4167/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4162", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4162/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4162/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4162/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4162", "id": 1117986488, "node_id": "I_kwDOBhEUd85Cox64", "number": 4162, "title": "Best test metric value in the training log is actually best valid metric", "user": {"login": "ishalyminov", "id": 1062768, "node_id": "MDQ6VXNlcjEwNjI3Njg=", "avatar_url": "https://avatars.githubusercontent.com/u/1062768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ishalyminov", "html_url": "https://github.com/ishalyminov", "followers_url": "https://api.github.com/users/ishalyminov/followers", "following_url": "https://api.github.com/users/ishalyminov/following{/other_user}", "gists_url": "https://api.github.com/users/ishalyminov/gists{/gist_id}", "starred_url": "https://api.github.com/users/ishalyminov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ishalyminov/subscriptions", "organizations_url": "https://api.github.com/users/ishalyminov/orgs", "repos_url": "https://api.github.com/users/ishalyminov/repos", "events_url": "https://api.github.com/users/ishalyminov/events{/privacy}", "received_events_url": "https://api.github.com/users/ishalyminov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-01-29T01:14:13Z", "updated_at": "2022-02-25T22:38:59Z", "closed_at": "2022-02-25T22:38:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI see the following in the validation results during training:\r\n\r\n[2022-01-18 05:41:02,006][fairseq_cli.train][INFO] - begin validation on \"test\" subset\r\n[2022-01-18 05:43:26,995][test][INFO] -\r\n {\"epoch\": 150, \"test_loss\": \"24.118\", \"test_ntokens\": \"674.778\", \"test_nsentences\": \"13.3364\",\r\n  \"test_nll_loss\": \"0.477\", \"test_uer\": \"7.599\", \"test_wer\": \"22.706\", \"test_raw_wer\": \"22.706\",\r\n  \"test_wps\": \"1999.9\", \"test_wpb\": \"674.8\", \"test_bsz\": \"13.3\", \"test_num_updates\": \"20000\",\r\n  \"test_best_wer\": \"14.46\"}\r\n\r\nThe problem is, there's never such a low test WER anywhere in my training log.\r\n\r\nI think I found the bug location - it's this line: https://github.com/pytorch/fairseq/blob/main/fairseq_cli/train.py#L502\r\nBest metric is stored globally as checkpoint_utils.save_checkpoint.best - it's determined elsewhere based on the 1st eval subset only (=valid in my case).\r\nThen, every subset's current value (=valid_wer, test_wer) gets compared to it in that function.\r\n\r\nThis doesn't affect the training progress (as the checkpoint_utils.save_checkpoint.best is still updated correctly), it's just I think it makes sense to store the best metric for each subset individually and use that as the comparand in that function (or do not output \"best_wer\" for eval subsets at all as it's logged during checkpoint saving anyway).\r\n\r\nSee also: https://groups.google.com/g/fairseq-users/c/7nk3rJmvlg8\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4162/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4162/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4149", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4149/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4149/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4149/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4149", "id": 1111797242, "node_id": "I_kwDOBhEUd85CRK36", "number": 4149, "title": "data2vec: ValueError: Empty module name", "user": {"login": "Ramlinbird", "id": 11084132, "node_id": "MDQ6VXNlcjExMDg0MTMy", "avatar_url": "https://avatars.githubusercontent.com/u/11084132?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ramlinbird", "html_url": "https://github.com/Ramlinbird", "followers_url": "https://api.github.com/users/Ramlinbird/followers", "following_url": "https://api.github.com/users/Ramlinbird/following{/other_user}", "gists_url": "https://api.github.com/users/Ramlinbird/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ramlinbird/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ramlinbird/subscriptions", "organizations_url": "https://api.github.com/users/Ramlinbird/orgs", "repos_url": "https://api.github.com/users/Ramlinbird/repos", "events_url": "https://api.github.com/users/Ramlinbird/events{/privacy}", "received_events_url": "https://api.github.com/users/Ramlinbird/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-01-23T07:20:45Z", "updated_at": "2022-01-24T02:57:07Z", "closed_at": "2022-01-23T07:22:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "I installed fairseq and soundfile, try the example codes at `examples/data2vec/README.md`, \r\n```\r\npython fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/audio/pretraining --config-name base_librispeech task.data=./datas2/ common.user_dir=examples/data2vec/\r\n## ./datas2 saved the train.tsv and valid.tsv generated by sourdfile\r\n```\r\nIt raised the error:\r\n![image](https://user-images.githubusercontent.com/11084132/150668606-f6502c89-c301-456b-ac4d-16e3a86428d1.png)", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4149/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4149/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4128", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4128/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4128/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4128/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4128", "id": 1100792122, "node_id": "I_kwDOBhEUd85BnME6", "number": 4128, "title": "wav2vec_ctc is not found in MODEL_REGISTRY", "user": {"login": "ishalyminov", "id": 1062768, "node_id": "MDQ6VXNlcjEwNjI3Njg=", "avatar_url": "https://avatars.githubusercontent.com/u/1062768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ishalyminov", "html_url": "https://github.com/ishalyminov", "followers_url": "https://api.github.com/users/ishalyminov/followers", "following_url": "https://api.github.com/users/ishalyminov/following{/other_user}", "gists_url": "https://api.github.com/users/ishalyminov/gists{/gist_id}", "starred_url": "https://api.github.com/users/ishalyminov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ishalyminov/subscriptions", "organizations_url": "https://api.github.com/users/ishalyminov/orgs", "repos_url": "https://api.github.com/users/ishalyminov/repos", "events_url": "https://api.github.com/users/ishalyminov/events{/privacy}", "received_events_url": "https://api.github.com/users/ishalyminov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-01-12T21:09:42Z", "updated_at": "2022-01-13T16:48:27Z", "closed_at": "2022-01-13T16:48:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen running a wav2vec2 fine-tuning task with fairseq-hydra-train, an error arises that (as I confirmed in pdb) model name `wav2vec_ctc` is not found in MODEL_REGISTRY\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run `fairseq-hydra-train     --config-dir /home/ec2-user/SageMaker/fairseq/examples/wav2vec/config/finetuning     --config-name commonvoice_10h`\r\n2. Model section from my config `commonvoice_10h`: \r\n```\r\nmodel:\r\n  _name: wav2vec_ctc\r\n  w2v_path: /home/ishalyminov/data/fairseq/examples/wav2vec/xlsr_53_56k.pt\r\n  apply_mask: true\r\n  mask_prob: 0.75\r\n  mask_channel_prob: 0.25\r\n  mask_channel_length: 64\r\n  layerdrop: 0.1\r\n  activation_dropout: 0.1\r\n  feature_grad_mult: 0.0\r\n  freeze_finetune_updates: 10000\r\n```\r\n3. See error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ec2-user/SageMaker/fairseq/fairseq_cli/hydra_train.py\", line 27, in hydra_main\r\n    _hydra_main(cfg)\r\n  File \"/home/ec2-user/SageMaker/fairseq/fairseq_cli/hydra_train.py\", line 56, in _hydra_main\r\n    distributed_utils.call_main(cfg, pre_main, **kwargs)\r\n  File \"/home/ec2-user/SageMaker/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/ec2-user/SageMaker/fairseq/fairseq_cli/train.py\", line 94, in main\r\n    model = task.build_model(cfg.model)\r\n  File \"/home/ec2-user/SageMaker/fairseq/fairseq/tasks/audio_finetuning.py\", line 193, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/home/ec2-user/SageMaker/fairseq/fairseq/tasks/audio_pretraining.py\", line 197, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/home/ec2-user/SageMaker/fairseq/fairseq/tasks/fairseq_task.py\", line 335, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/home/ec2-user/SageMaker/fairseq/fairseq/models/__init__.py\", line 100, in build_model\r\n    f\"Could not infer model type from {cfg}. \"\r\nKeyError: \"'_name'\"\r\n```\r\n### Expected behavior\r\n\r\nModel trains\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): main\r\n - PyTorch Version (e.g., 1.0) 1.10.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip install -e .\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4128/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4122", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4122/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4122/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4122/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4122", "id": 1098916584, "node_id": "I_kwDOBhEUd85BgCLo", "number": 4122, "title": "HuBERT breaks with new wav2vec2 config updates: required_seq_len_multiple key error", "user": {"login": "RF5", "id": 23717819, "node_id": "MDQ6VXNlcjIzNzE3ODE5", "avatar_url": "https://avatars.githubusercontent.com/u/23717819?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RF5", "html_url": "https://github.com/RF5", "followers_url": "https://api.github.com/users/RF5/followers", "following_url": "https://api.github.com/users/RF5/following{/other_user}", "gists_url": "https://api.github.com/users/RF5/gists{/gist_id}", "starred_url": "https://api.github.com/users/RF5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RF5/subscriptions", "organizations_url": "https://api.github.com/users/RF5/orgs", "repos_url": "https://api.github.com/users/RF5/repos", "events_url": "https://api.github.com/users/RF5/events{/privacy}", "received_events_url": "https://api.github.com/users/RF5/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-01-11T09:40:16Z", "updated_at": "2022-02-04T07:56:39Z", "closed_at": "2022-02-04T07:56:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n- [This commit](https://github.com/pytorch/fairseq/commit/c2b771b1beed56d03134aa0a807fbbec766e484c#diff-57ada49f3dea7d9ad6d452c8c3c020464596c9461220c0ba3a2669f35a7f0e4a) introduced a new config argument `required_seq_len_multiple` to wav2vec2 and its associated `TransformerEncoder`.\r\n- This `TransformerEncoder` is used by HuBERT for its main transformer encoder as well.\r\n- [The HuBERT config](https://github.com/pytorch/fairseq/blob/main/fairseq/models/hubert/hubert.py#L37) has not been updated to contain this config argument. \r\n- As a result, whenever constructing a HuBERT model, an error `Key 'required_seq_len_multiple' not in 'HubertConfig'` is thrown. \r\n\r\n### To Reproduce\r\n\r\n```python\r\nfrom fairseq.models.hubert.hubert import (\r\n    HubertModel, \r\n    HubertConfig,\r\n    HubertPretrainingConfig,  \r\n)\r\nfrom fairseq.data.dictionary import Dictionary\r\n\r\ncfg = HubertConfig()\r\nhubert_task_cfg = HubertPretrainingConfig()\r\ncfg.label_rate = hubert_task_cfg.label_rate\r\ndicto = Dictionary()\r\n\r\nmodel = HubertModel(cfg, hubert_task_cfg, dicto)\r\n```\r\n\r\nRunning this will yield the stack trace:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-13-405dbe7b09c6> in <module>()\r\n     12 \r\n     13 \r\n---> 14 model = HubertModel(cfg, hubert_task_cfg, dicto)\r\n\r\n1 frames\r\n/usr/local/lib/python3.7/dist-packages/fairseq/models/hubert/hubert.py in __init__(self, cfg, task_cfg, dictionaries)\r\n    268         )\r\n    269 \r\n--> 270         self.encoder = TransformerEncoder(cfg)\r\n    271         self.layer_norm = LayerNorm(self.embed)\r\n    272 \r\n\r\n/usr/local/lib/python3.7/dist-packages/fairseq/models/wav2vec/wav2vec2.py in __init__(self, args)\r\n    904         self.dropout = args.dropout\r\n    905         self.embedding_dim = args.encoder_embed_dim\r\n--> 906         self.required_seq_len_multiple = args.required_seq_len_multiple\r\n    907 \r\n    908         self.pos_conv = nn.Conv1d(\r\n\r\nAttributeError: 'HubertConfig' object has no attribute 'required_seq_len_multiple'\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo error should occur.\r\n\r\n### Environment\r\n\r\n - fairseq Version: main\r\n - PyTorch Version: 1.10\r\n - OS: Linux\r\n - How you installed fairseq: `pip install git+https://github.com/pytorch/fairseq.git`\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A\r\n\r\n### Possible solution\r\n\r\nAdd the same default parameter `required_seq_len_multiple` to HubertConfig.\r\ni.e. [right here](https://github.com/pytorch/fairseq/blob/main/fairseq/models/hubert/hubert.py#L211) add:\r\n\r\n```python\r\n    required_seq_len_multiple: int = field(\r\n        default=1,\r\n        metadata={\r\n            \"help\": \"pad the input to encoder such that the sequence length is divisible by multiple\"\r\n        },\r\n    )\r\n```\r\nThe error is then no more. \r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4122/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4121", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4121/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4121/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4121/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4121", "id": 1098274997, "node_id": "I_kwDOBhEUd85Bdli1", "number": 4121, "title": "Installing Fairseq from source on Colab fails ( ImportError: cannot import name <> from 'fairseq' (unknown location)", "user": {"login": "gowtham1997", "id": 9641196, "node_id": "MDQ6VXNlcjk2NDExOTY=", "avatar_url": "https://avatars.githubusercontent.com/u/9641196?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gowtham1997", "html_url": "https://github.com/gowtham1997", "followers_url": "https://api.github.com/users/gowtham1997/followers", "following_url": "https://api.github.com/users/gowtham1997/following{/other_user}", "gists_url": "https://api.github.com/users/gowtham1997/gists{/gist_id}", "starred_url": "https://api.github.com/users/gowtham1997/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gowtham1997/subscriptions", "organizations_url": "https://api.github.com/users/gowtham1997/orgs", "repos_url": "https://api.github.com/users/gowtham1997/repos", "events_url": "https://api.github.com/users/gowtham1997/events{/privacy}", "received_events_url": "https://api.github.com/users/gowtham1997/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2022-01-10T19:14:25Z", "updated_at": "2023-03-31T10:24:06Z", "closed_at": "2022-01-16T06:13:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nI am trying to install Fairseq from the source on colab and it seems to be running into import errors.\r\n\r\n[Here](https://colab.research.google.com/drive/1EdFPAvlG69hpTsKxDPKg2dreMtI_LKrR?usp=sharing) is a reproducible colab notebook. \r\n\r\nI also tried adding cloned location to PYTHONPATH as suggested [here](https://github.com/pytorch/fairseq/issues/2546) but that doesn't seem to work as well.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4121/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4116", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4116/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4116/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4116/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4116", "id": 1097094153, "node_id": "I_kwDOBhEUd85BZFQJ", "number": 4116, "title": "Different inference result between fairseq-generate and BARTModel.from_pretrained().translate()", "user": {"login": "XiangyuTang", "id": 47786354, "node_id": "MDQ6VXNlcjQ3Nzg2MzU0", "avatar_url": "https://avatars.githubusercontent.com/u/47786354?v=4", "gravatar_id": "", "url": "https://api.github.com/users/XiangyuTang", "html_url": "https://github.com/XiangyuTang", "followers_url": "https://api.github.com/users/XiangyuTang/followers", "following_url": "https://api.github.com/users/XiangyuTang/following{/other_user}", "gists_url": "https://api.github.com/users/XiangyuTang/gists{/gist_id}", "starred_url": "https://api.github.com/users/XiangyuTang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/XiangyuTang/subscriptions", "organizations_url": "https://api.github.com/users/XiangyuTang/orgs", "repos_url": "https://api.github.com/users/XiangyuTang/repos", "events_url": "https://api.github.com/users/XiangyuTang/events{/privacy}", "received_events_url": "https://api.github.com/users/XiangyuTang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-01-09T03:21:40Z", "updated_at": "2022-01-10T04:20:00Z", "closed_at": "2022-01-10T04:20:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nFor the same model and the same dict in the translation task, when fairseq-generate method and Load BART method(e.g. `BARTModel.from_pretrained()`) were used to predict the case of the same input, it was found that their inference results were inconsistent.\r\n\r\nIn the following reference linking\uff1a[issues/2934](https://github.com/pytorch/fairseq/issues/2934), [some one said](https://github.com/pytorch/fairseq/issues/2934#issuecomment-736531753):\r\n\r\nAh, you\u2019re using the translation task rather than denoising. Probably there\u2019s a mismatch with the hub interface relating to the beginning of sentence token, and you need to **remove the \\<s> token** from here: \r\n\r\n`bpe_sentence = \"<s> \" + tokens + \" </s>\" `\r\n\r\nThat helped me a little bit but there is still something wrong in BARTModel.from_pretrained().translate(), the BART model can not output the same result as fairseq-generate.\r\n\r\nBTW: I have tried to unify the parameters(e.g. fairseq-generate --xx) of both methods, but it is not clear about the parameter setting entry of Load BART method.\r\n\r\n### To Reproduce\r\n\r\n1. Run cmd \r\n\r\nFairseq-generate method:\r\n\r\n```\r\nfairseq-generate \\\r\n    --path ${MODEL_DIR}/${pt_file} ${DATA_DIR} \\\r\n    --gen-subset ${MODE} \\\r\n    --nbest 1 \\\r\n    --max-tokens 2048 \\\r\n    --source-lang src --target-lang tgt \\\r\n    --results-path ${MODEL_DIR}/output_340 \\\r\n    --beam 5 \\\r\n    --bpe gpt2 \\\r\n    --remove-bpe > logs.txt 2>&1 &\r\n```\r\n \r\nLoad BART method:\r\n\r\n```python\r\nfrom fairseq.models.bart import BARTModel\r\n\r\ndef load_pretrained_bart(path):\r\n    bart = BARTModel.from_pretrained(path, checkpoint_file=ckpt_file,bpe=\"gpt2\")\r\n    bart.eval()\r\n    bart.cuda(f'cuda:{gpu}')\r\n    return bart\r\n\r\ntranslate_model = load_pretrained_bart(checkpoint_dir)\r\nresults = translate_model.translate(\r\n        sentences=sequences,\r\n        beam=5\r\n    )\r\n\r\n```\r\n2. See error\r\n\r\nfairseq-generate log:\r\n```\r\n2022-01-05 15:35:19 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/xxx/ckpt1229/checkpoint340.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': '/xxx/checkpoint/ckpt1229/output_340'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '/data_ext/v-xiantang/UniversalParser/dataset_post_txy/dsl_1229_only_cmd/bin/', 'source_lang': 'src', 'target_lang': 'tgt', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None}\r\n2022-01-05 15:35:20 | INFO | fairseq.tasks.translation | [src] dictionary: 50264 types\r\n2022-01-05 15:35:20 | INFO | fairseq.tasks.translation | [tgt] dictionary: 50264 types\r\n2022-01-05 15:35:20 | INFO | fairseq_cli.generate | loading model(s) from /xxxxx/xxx/xxx/checkpoint/ckpt1229/checkpoint340.pt\r\n2022-01-05 15:35:44 | INFO | fairseq.data.data_utils | loaded 500 examples from: /xxx/bin/test.src-tgt.src\r\n2022-01-05 15:35:44 | INFO | fairseq.data.data_utils | loaded 500 examples from: /xxx/bin/test.src-tgt.tgt\r\n2022-01-05 15:35:44 | INFO | fairseq.tasks.translation | /xxx/bin/ test src-tgt 500 examples\r\nS-38\tthe [IRT]second[/IRT] cell of column [IRT]C[/IRT] [SEP] [DT]Year[/DT] [SD]Brand[/SD] [SD]Category[/SD] [SD]Model[/SD] [ME]Sales[/ME]\r\nT-38\tColumn(C).Cell(2)\r\nH-38\t-0.12754178047180176\t39470 7 34 737 28780 7 17 8\r\nD-38\t-0.12754178047180176\tColumn(C).Cell(2)\r\n```\r\nBARTModel.from_pretrained().translate():\r\n```\r\n\"query\": \"the [IRT]second[/IRT] cell of column [IRT]C[/IRT] [SEP] [DT]Year[/DT] [SD]Brand[/SD] [SD]Category[/SD] [SD]Model[/SD] [ME]Sales[/ME]\",\r\n\"predict\": \"Column(C).Cell(2).Clear()\",\r\n\"expected\": \"Column(C).Cell(2)\",\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\nSee above.\r\n\r\n### Expected behavior\r\n\r\nFor the same model and the same dict in the translation task, when fairseq-generate method and Load BART method were used to predict the case of the same input, it is expected that their inference results are the same.\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version : fairseq == 1.0.0a0+801a646\r\n - PyTorch Version : 1.7.0\r\n - OS : Linux\r\n - How you installed fairseq (`pip`, source):\r\n  ```\r\n    pip install git+https://github.com/pytorch/fairseq.git@801a64683164680562c77b688d9ca77fc3e0cea7\r\n    pip list | grep fairseq\r\n```\r\n - Python version: 3.7.0\r\n - CUDA/cuDNN version: 10.1\r\n\r\n\r\n### Additional context\r\n\r\nTranslation task.\r\nWe use the Fairseq - CLI tool for training and the Load BART method for deployment/inference.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4116/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4116/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4073", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4073/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4073/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4073/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4073", "id": 1077163254, "node_id": "I_kwDOBhEUd85ANDT2", "number": 4073, "title": "[wav2vec2] 'pretraining model' training with SPGI Speech Dataset. Getting error \"Exception: The dataset is empty. This could indicate that all elements in the dataset have been skipped. Try increasing the max number of allowed tokens or using a larger dataset.\"", "user": {"login": "rishabhjain16", "id": 19801035, "node_id": "MDQ6VXNlcjE5ODAxMDM1", "avatar_url": "https://avatars.githubusercontent.com/u/19801035?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rishabhjain16", "html_url": "https://github.com/rishabhjain16", "followers_url": "https://api.github.com/users/rishabhjain16/followers", "following_url": "https://api.github.com/users/rishabhjain16/following{/other_user}", "gists_url": "https://api.github.com/users/rishabhjain16/gists{/gist_id}", "starred_url": "https://api.github.com/users/rishabhjain16/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rishabhjain16/subscriptions", "organizations_url": "https://api.github.com/users/rishabhjain16/orgs", "repos_url": "https://api.github.com/users/rishabhjain16/repos", "events_url": "https://api.github.com/users/rishabhjain16/events{/privacy}", "received_events_url": "https://api.github.com/users/rishabhjain16/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-12-10T19:27:55Z", "updated_at": "2023-01-11T14:52:46Z", "closed_at": "2022-02-15T17:52:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am getting the following error whenever I try to train my pretraining model using this command `CUDA_VISIBLE_DEVICES=0,1,3 fairseq-hydra-train     task.data=/workspace/w2v2/data/common/     --config-dir examples/wav2vec/config/pretraining     --config-name wav2vec2_large_librivox`. I am training on 3 V100 GPUs. \r\n\r\nI have tried increasing a number of parameters in the config file but couldn't seem to resolve this issue. Any help is appreciated. I have also tried changing the max token size but that didn't resolve the issue either. I am using a docker container to run my experiments. \r\n\r\n```\r\nroot@264b829ee4e2:/workspace/w2v2/fairseq# CUDA_VISIBLE_DEVICES=0,1,3 fairseq-hydra-train     task.data=/workspace/w2v2/data/common/     --config-dir examples/wav2vec/config/pretraining     --config-name wav2vec2_large_librivox\r\n2021-12-10 19:08:49 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:15178\r\n2021-12-10 19:08:49 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:15178\r\n2021-12-10 19:08:49 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1\r\n2021-12-10 19:08:49 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0\r\n2021-12-10 19:08:49 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for 2 nodes.\r\n2021-12-10 19:08:49 | INFO | fairseq.distributed.utils | initialized host 264b829ee4e2 as rank 0\r\n2021-12-10 19:08:49 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for 2 nodes.\r\n2021-12-10 19:08:49 | INFO | fairseq.distributed.utils | initialized host 264b829ee4e2 as rank 1\r\n264b829ee4e2:165756:165756 [0] NCCL INFO Bootstrap : Using [0]eth0:172.17.0.2<0>\r\n264b829ee4e2:165756:165756 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\n\r\n264b829ee4e2:165756:165756 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed\r\n\r\n264b829ee4e2:165756:165756 [0] transport/net_ib.cc:148 NCCL WARN NET/IB : Unable to open device mlx5_1\r\n\r\n264b829ee4e2:165756:165756 [0] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed\r\n\r\n264b829ee4e2:165756:165756 [0] transport/net_ib.cc:148 NCCL WARN NET/IB : Unable to open device mlx5_0\r\n264b829ee4e2:165756:165756 [0] NCCL INFO NET/IB : No device found.\r\n264b829ee4e2:165756:165756 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>\r\n264b829ee4e2:165756:165756 [0] NCCL INFO Using network Socket\r\nNCCL version 2.7.8+cuda11.1\r\n264b829ee4e2:165757:165757 [1] NCCL INFO Bootstrap : Using [0]eth0:172.17.0.2<0>\r\n264b829ee4e2:165757:165757 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\n\r\n264b829ee4e2:165757:165757 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed\r\n\r\n264b829ee4e2:165757:165757 [1] transport/net_ib.cc:148 NCCL WARN NET/IB : Unable to open device mlx5_1\r\n\r\n264b829ee4e2:165757:165757 [1] misc/ibvwrap.cc:212 NCCL WARN Call to ibv_open_device failed\r\n\r\n264b829ee4e2:165757:165757 [1] transport/net_ib.cc:148 NCCL WARN NET/IB : Unable to open device mlx5_0\r\n264b829ee4e2:165757:165757 [1] NCCL INFO NET/IB : No device found.\r\n264b829ee4e2:165757:165757 [1] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>\r\n264b829ee4e2:165757:165757 [1] NCCL INFO Using network Socket\r\n264b829ee4e2:165757:165901 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\r\n264b829ee4e2:165756:165900 [0] NCCL INFO Channel 00/04 :    0   1\r\n264b829ee4e2:165756:165900 [0] NCCL INFO Channel 01/04 :    0   1\r\n264b829ee4e2:165757:165901 [1] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1 [1] 0/-1/-1->1->-1|-1->1->0/-1/-1 [2] -1/-1/-1->1->0|0->1->-1/-1/-1 [3] 0/-1/-1->1->-1|-1->1->0/-1/-1\r\n264b829ee4e2:165756:165900 [0] NCCL INFO Channel 02/04 :    0   1\r\n264b829ee4e2:165756:165900 [0] NCCL INFO Channel 03/04 :    0   1\r\n264b829ee4e2:165757:165901 [1] NCCL INFO Setting affinity for GPU 1 to 5555,55555555,55555555\r\n264b829ee4e2:165756:165900 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\r\n264b829ee4e2:165756:165900 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] -1/-1/-1->0->1|1->0->-1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] -1/-1/-1->0->1|1->0->-1/-1/-1\r\n264b829ee4e2:165756:165900 [0] NCCL INFO Setting affinity for GPU 0 to 5555,55555555,55555555\r\n264b829ee4e2:165757:165901 [1] NCCL INFO Channel 00 : 1[3b000] -> 0[18000] via P2P/IPC\r\n264b829ee4e2:165756:165900 [0] NCCL INFO Channel 00 : 0[18000] -> 1[3b000] via P2P/IPC\r\n264b829ee4e2:165757:165901 [1] NCCL INFO Channel 01 : 1[3b000] -> 0[18000] via P2P/IPC\r\n264b829ee4e2:165756:165900 [0] NCCL INFO Channel 01 : 0[18000] -> 1[3b000] via P2P/IPC\r\n264b829ee4e2:165757:165901 [1] NCCL INFO Channel 02 : 1[3b000] -> 0[18000] via P2P/IPC\r\n264b829ee4e2:165756:165900 [0] NCCL INFO Channel 02 : 0[18000] -> 1[3b000] via P2P/IPC\r\n264b829ee4e2:165757:165901 [1] NCCL INFO Channel 03 : 1[3b000] -> 0[18000] via P2P/IPC\r\n264b829ee4e2:165756:165900 [0] NCCL INFO Channel 03 : 0[18000] -> 1[3b000] via P2P/IPC\r\n264b829ee4e2:165757:165901 [1] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer\r\n264b829ee4e2:165756:165900 [0] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer\r\n264b829ee4e2:165756:165900 [0] NCCL INFO comm 0x7ffdb8002010 rank 0 nranks 2 cudaDev 0 busId 18000 - Init COMPLETE\r\n264b829ee4e2:165757:165901 [1] NCCL INFO comm 0x7ffdc4002010 rank 1 nranks 2 cudaDev 1 busId 3b000 - Init COMPLETE\r\n264b829ee4e2:165756:165756 [0] NCCL INFO Launch mode Parallel\r\n[2021-12-10 19:08:55,324][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging':False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False,'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15178', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 120000000, 'batch_size': 4, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 120000000, 'batch_size_valid': 4, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 100000000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum':0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 2048, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 768, 'layer_norm_first': True, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': True, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0,'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.1, 0.999995]}, 'task': {'_name': 'audio_pretraining', 'data': '/workspace/w2v2/data/common/', 'labels': None, 'binarized_dataset': False,'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 32000000, 'min_sample_size': 32000, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': False, 'text_compression_level': 'none'}, 'criterion': {'_name': 'wav2vec', 'infonce': True, 'loss_weights': [0.1, 0.0], 'log_keys': ['prob_perplexity', 'code_perplexity', 'temp']}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 3200000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 100000000, 'lr': [0.005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters':{'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers':False}}\r\n[2021-12-10 19:09:08,391][fairseq_cli.train][INFO] - Wav2Vec2Model(\r\n  (feature_extractor): ConvFeatureExtractionModel(\r\n    (conv_layers): ModuleList(\r\n      (0): Sequential(\r\n        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\r\n        (1): Dropout(p=0.0, inplace=False)\r\n        (2): Sequential(\r\n          (0): TransposeLast()\r\n          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n          (2): TransposeLast()\r\n        )\r\n        (3): GELU()\r\n      )\r\n      (1): Sequential(\r\n        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\r\n        (1): Dropout(p=0.0, inplace=False)\r\n        (2): Sequential(\r\n          (0): TransposeLast()\r\n          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n          (2): TransposeLast()\r\n        )\r\n        (3): GELU()\r\n      )\r\n      (2): Sequential(\r\n        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\r\n        (1): Dropout(p=0.0, inplace=False)\r\n        (2): Sequential(\r\n          (0): TransposeLast()\r\n          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n          (2): TransposeLast()\r\n        )\r\n        (3): GELU()\r\n      )\r\n      (3): Sequential(\r\n        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\r\n        (1): Dropout(p=0.0, inplace=False)\r\n        (2): Sequential(\r\n          (0): TransposeLast()\r\n          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n          (2): TransposeLast()\r\n        )\r\n        (3): GELU()\r\n      )\r\n      (4): Sequential(\r\n        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\r\n        (1): Dropout(p=0.0, inplace=False)\r\n        (2): Sequential(\r\n          (0): TransposeLast()\r\n          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n          (2): TransposeLast()\r\n        )\r\n        (3): GELU()\r\n      )\r\n      (5): Sequential(\r\n        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\r\n        (1): Dropout(p=0.0, inplace=False)\r\n        (2): Sequential(\r\n          (0): TransposeLast()\r\n          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n          (2): TransposeLast()\r\n        )\r\n        (3): GELU()\r\n      )\r\n      (6): Sequential(\r\n        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\r\n        (1): Dropout(p=0.0, inplace=False)\r\n        (2): Sequential(\r\n          (0): TransposeLast()\r\n          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n          (2): TransposeLast()\r\n        )\r\n        (3): GELU()\r\n      )\r\n    )\r\n  )\r\n  (post_extract_proj): Linear(in_features=512, out_features=2048, bias=True)\r\n  (dropout_input): Dropout(p=0.0, inplace=False)\r\n  (dropout_features): Dropout(p=0.0, inplace=False)\r\n  (quantizer): GumbelVectorQuantizer(\r\n    (weight_proj): Linear(in_features=512, out_features=640, bias=True)\r\n  )\r\n  (project_q): Linear(in_features=768, out_features=768, bias=True)\r\n  (encoder): TransformerEncoder(\r\n    (pos_conv): Sequential(\r\n      (0): Conv1d(2048, 2048, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\r\n      (1): SamePad()\r\n      (2): GELU()\r\n    )\r\n    (layers): ModuleList(\r\n      (0): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (4): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (5): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (6): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (7): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (8): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (9): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (10): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (11): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (12): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (13): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (14): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (15): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (16): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (17): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (18): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (19): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (20): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (21): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (22): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (23): TransformerSentenceEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n        )\r\n        (dropout1): Dropout(p=0.0, inplace=False)\r\n        (dropout2): Dropout(p=0.0, inplace=False)\r\n        (dropout3): Dropout(p=0.0, inplace=False)\r\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=2048, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=2048, bias=True)\r\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n  (final_proj): Linear(in_features=2048, out_features=768, bias=True)\r\n)\r\n[2021-12-10 19:09:08,396][fairseq_cli.train][INFO] - task: AudioPretrainingTask\r\n[2021-12-10 19:09:08,396][fairseq_cli.train][INFO] - model: Wav2Vec2Model\r\n[2021-12-10 19:09:08,396][fairseq_cli.train][INFO] - criterion: Wav2vecCriterion\r\n[2021-12-10 19:09:08,399][fairseq_cli.train][INFO] - num. shared model params: 847,409,920 (num. trained: 847,409,920)\r\n[2021-12-10 19:09:08,401][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)\r\n[2021-12-10 19:09:11,150][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 2145983, skipped 88341 samples\r\n[2021-12-10 19:09:11,997][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank:0\r\n[2021-12-10 19:09:12,734][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 2 nodes.\r\n264b829ee4e2:165756:165985 [0] NCCL INFO Channel 00/04 :    0   1\r\n264b829ee4e2:165757:165986 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\r\n264b829ee4e2:165756:165985 [0] NCCL INFO Channel 01/04 :    0   1\r\n264b829ee4e2:165756:165985 [0] NCCL INFO Channel 02/04 :    0   1\r\n264b829ee4e2:165757:165986 [1] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1 [1] 0/-1/-1->1->-1|-1->1->0/-1/-1 [2] -1/-1/-1->1->0|0->1->-1/-1/-1 [3] 0/-1/-1->1->-1|-1->1->0/-1/-1\r\n264b829ee4e2:165756:165985 [0] NCCL INFO Channel 03/04 :    0   1\r\n264b829ee4e2:165757:165986 [1] NCCL INFO Setting affinity for GPU 1 to 5555,55555555,55555555\r\n264b829ee4e2:165756:165985 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64\r\n264b829ee4e2:165756:165985 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] -1/-1/-1->0->1|1->0->-1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] -1/-1/-1->0->1|1->0->-1/-1/-1\r\n264b829ee4e2:165756:165985 [0] NCCL INFO Setting affinity for GPU 0 to 5555,55555555,55555555\r\n264b829ee4e2:165757:165986 [1] NCCL INFO Channel 00 : 1[3b000] -> 0[18000] via P2P/IPC\r\n264b829ee4e2:165756:165985 [0] NCCL INFO Channel 00 : 0[18000] -> 1[3b000] via P2P/IPC\r\n264b829ee4e2:165757:165986 [1] NCCL INFO Channel 01 : 1[3b000] -> 0[18000] via P2P/IPC\r\n264b829ee4e2:165756:165985 [0] NCCL INFO Channel 01 : 0[18000] -> 1[3b000] via P2P/IPC\r\n264b829ee4e2:165757:165986 [1] NCCL INFO Channel 02 : 1[3b000] -> 0[18000] via P2P/IPC\r\n264b829ee4e2:165756:165985 [0] NCCL INFO Channel 02 : 0[18000] -> 1[3b000] via P2P/IPC\r\n264b829ee4e2:165757:165986 [1] NCCL INFO Channel 03 : 1[3b000] -> 0[18000] via P2P/IPC\r\n264b829ee4e2:165756:165985 [0] NCCL INFO Channel 03 : 0[18000] -> 1[3b000] via P2P/IPC\r\n264b829ee4e2:165757:165986 [1] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer\r\n264b829ee4e2:165756:165985 [0] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer\r\n264b829ee4e2:165757:165986 [1] NCCL INFO comm 0x7ffd98002010 rank 1 nranks 2 cudaDev 1 busId 3b000 - Init COMPLETE\r\n264b829ee4e2:165756:165985 [0] NCCL INFO comm 0x7ffd8c002010 rank 0 nranks 2 cudaDev 0 busId 18000 - Init COMPLETE\r\n264b829ee4e2:165756:165756 [0] NCCL INFO Launch mode Parallel\r\n[2021-12-10 19:09:12,758][fairseq.utils][INFO] - ***********************CUDA enviroments for all 2 workers***********************\r\n[2021-12-10 19:09:12,758][fairseq.utils][INFO] - rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB\r\n[2021-12-10 19:09:12,758][fairseq.utils][INFO] - rank   1: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB\r\n[2021-12-10 19:09:12,758][fairseq.utils][INFO] - ***********************CUDA enviroments for all 2 workers***********************\r\n[2021-12-10 19:09:12,759][fairseq_cli.train][INFO] - training on 2 devices (GPUs/TPUs)\r\n[2021-12-10 19:09:12,759][fairseq_cli.train][INFO] - max tokens per device = 120000000 and max sentences per device = 4\r\n[2021-12-10 19:09:12,760][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt\r\n[2021-12-10 19:09:12,760][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt\r\n[2021-12-10 19:09:12,761][fairseq.trainer][INFO] - loading train data for epoch 1\r\n[2021-12-10 19:09:12,762][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 0, skipped 0 samples\r\nTraceback (most recent call last):\r\n  File \"/workspace/w2v2/fairseq/fairseq_cli/hydra_train.py\", line 28, in hydra_main\r\n    _hydra_main(cfg)\r\n  File \"/workspace/w2v2/fairseq/fairseq_cli/hydra_train.py\", line 53, in _hydra_main\r\n    distributed_utils.call_main(cfg, pre_main, **kwargs)\r\n  File \"/workspace/w2v2/fairseq/fairseq/distributed/utils.py\", line 351, in call_main\r\n    join=True,\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 188, in start_processes\r\n    while not context.join():\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 150, in join\r\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\r\ntorch.multiprocessing.spawn.ProcessRaisedException:\r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/workspace/w2v2/fairseq/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/workspace/w2v2/fairseq/fairseq_cli/train.py\", line 159, in main\r\n    disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n  File \"/workspace/w2v2/fairseq/fairseq/checkpoint_utils.py\", line 273, in load_checkpoint\r\n    epoch=1, load_dataset=True, **passthrough_args\r\n  File \"/workspace/w2v2/fairseq/fairseq/trainer.py\", line 650, in get_train_iterator\r\n    self.reset_dummy_batch(batch_iterator.first_batch)\r\n  File \"/workspace/w2v2/fairseq/fairseq/data/iterators.py\", line 315, in first_batch\r\n    \"The dataset is empty. This could indicate \"\r\nException: The dataset is empty. This could indicate that all elements in the dataset have been skipped. Try increasing the max number of allowed tokens or using a larger dataset.\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4073/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4060", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4060/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4060/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4060/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4060", "id": 1072366473, "node_id": "I_kwDOBhEUd84_6wOJ", "number": 4060, "title": "Unable to install fairseq", "user": {"login": "yannis1962", "id": 749949, "node_id": "MDQ6VXNlcjc0OTk0OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/749949?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yannis1962", "html_url": "https://github.com/yannis1962", "followers_url": "https://api.github.com/users/yannis1962/followers", "following_url": "https://api.github.com/users/yannis1962/following{/other_user}", "gists_url": "https://api.github.com/users/yannis1962/gists{/gist_id}", "starred_url": "https://api.github.com/users/yannis1962/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yannis1962/subscriptions", "organizations_url": "https://api.github.com/users/yannis1962/orgs", "repos_url": "https://api.github.com/users/yannis1962/repos", "events_url": "https://api.github.com/users/yannis1962/events{/privacy}", "received_events_url": "https://api.github.com/users/yannis1962/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-12-06T16:47:04Z", "updated_at": "2021-12-06T17:08:18Z", "closed_at": "2021-12-06T17:08:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nAfter a regular installation of fairseq through pip, there was no fairseq-train, fairseq-preprocess and the like\r\n\r\nSo I attempted what is written in the github instructions:\r\n\r\ngit clone https://github.com/pytorch/fairseq\r\nce fairseq\r\nCFLAGS=\"-stdlib=libc++\" pip install --editable ./ --user\r\n\r\nand here is what I got:\r\n\r\nObtaining file:///home/yharalam/hagiwara/fairseq\r\n  Installing build dependencies ... done\r\n  Checking if build backend supports build_editable ... done\r\n  Getting requirements to build wheel ... done\r\n  Installing backend dependencies ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nRequirement already satisfied: cffi in /usr/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (1.14.5)\r\nRequirement already satisfied: cython in /home/yharalam/.local/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (0.29.25)\r\nRequirement already satisfied: torch in /home/yharalam/.local/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (1.10.0)\r\nRequirement already satisfied: omegaconf<2.1 in /home/yharalam/.local/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (2.0.6)\r\nRequirement already satisfied: sacrebleu>=1.4.12 in /home/yharalam/.local/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (2.0.0)\r\nRequirement already satisfied: tqdm in /home/yharalam/.local/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (4.62.3)\r\nRequirement already satisfied: torchaudio>=0.8.0 in /home/yharalam/.local/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (0.10.0)\r\nRequirement already satisfied: bitarray in /home/yharalam/.local/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (2.3.4)\r\nRequirement already satisfied: hydra-core<1.1,>=1.0.7 in /home/yharalam/.local/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (1.0.7)\r\nRequirement already satisfied: numpy in /home/yharalam/.local/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (1.21.4)\r\nRequirement already satisfied: regex in /home/yharalam/.local/lib/python3.9/site-packages (from fairseq==1.0.0a0+0dfd6b6) (2021.11.10)\r\nRequirement already satisfied: antlr4-python3-runtime==4.8 in /home/yharalam/.local/lib/python3.9/site-packages (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+0dfd6b6) (4.8)\r\nRequirement already satisfied: PyYAML>=5.1.* in /home/yharalam/.local/lib/python3.9/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+0dfd6b6) (6.0)\r\nRequirement already satisfied: typing-extensions in /home/yharalam/.local/lib/python3.9/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+0dfd6b6) (4.0.1)\r\nRequirement already satisfied: colorama in /usr/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+0dfd6b6) (0.4.4)\r\nRequirement already satisfied: tabulate>=0.8.9 in /home/yharalam/.local/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+0dfd6b6) (0.8.9)\r\nRequirement already satisfied: portalocker in /home/yharalam/.local/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+0dfd6b6) (2.3.2)\r\nRequirement already satisfied: pycparser in /usr/lib/python3.9/site-packages (from cffi->fairseq==1.0.0a0+0dfd6b6) (2.20)\r\nInstalling collected packages: fairseq\r\n  Attempting uninstall: fairseq\r\n    Found existing installation: fairseq 0.10.0\r\n    Uninstalling fairseq-0.10.0:\r\n      Successfully uninstalled fairseq-0.10.0\r\n  Running setup.py develop for fairseq\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /usr/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/home/yharalam/hagiwara/fairseq/setup.py'\"'\"'; __file__='\"'\"'/home/yharalam/hagiwara/fairseq/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps --user --prefix=\r\n         cwd: /home/yharalam/hagiwara/fairseq/\r\n    Complete output (32 lines):\r\n    running develop\r\n    /tmp/pip-build-env-0h5ic1bh/overlay/lib/python3.9/site-packages/setuptools/command/easy_install.py:156: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\r\n      warnings.warn(\r\n    WARNING: The user site-packages directory is disabled.\r\n    /tmp/pip-build-env-0h5ic1bh/overlay/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n      warnings.warn(\r\n    error: can't create or remove files in install directory\r\n    \r\n    The following error occurred while trying to add or remove files in the\r\n    installation directory:\r\n    \r\n        [Errno 13] Permission denied: '/usr/lib/python3.9/site-packages/test-easy-install-116815.write-test'\r\n    \r\n    The installation directory you specified (via --install-dir, --prefix, or\r\n    the distutils default setting) was:\r\n    \r\n        /usr/lib/python3.9/site-packages/\r\n    \r\n    Perhaps your account does not have write access to this directory?  If the\r\n    installation directory is a system-owned directory, you may need to sign in\r\n    as the administrator or \"root\" account.  If you do not have administrative\r\n    access to this machine, you may wish to choose a different installation\r\n    directory, preferably one that is listed in your PYTHONPATH environment\r\n    variable.\r\n    \r\n    For information on other options, you may wish to consult the\r\n    documentation at:\r\n    \r\n      https://setuptools.pypa.io/en/latest/deprecated/easy_install.html\r\n    \r\n    Please make the appropriate changes for your system and try again.\r\n    \r\n    ----------------------------------------\r\n  Rolling back uninstall of fairseq\r\n  Moving to /home/yharalam/.local/bin/fairseq-eval-lm\r\n   from /tmp/pip-uninstall-03hk2tvs/fairseq-eval-lm\r\n  Moving to /home/yharalam/.local/bin/fairseq-generate\r\n   from /tmp/pip-uninstall-03hk2tvs/fairseq-generate\r\n  Moving to /home/yharalam/.local/bin/fairseq-interactive\r\n   from /tmp/pip-uninstall-03hk2tvs/fairseq-interactive\r\n  Moving to /home/yharalam/.local/bin/fairseq-preprocess\r\n   from /tmp/pip-uninstall-03hk2tvs/fairseq-preprocess\r\n  Moving to /home/yharalam/.local/bin/fairseq-score\r\n   from /tmp/pip-uninstall-03hk2tvs/fairseq-score\r\n  Moving to /home/yharalam/.local/bin/fairseq-train\r\n   from /tmp/pip-uninstall-03hk2tvs/fairseq-train\r\n  Moving to /home/yharalam/.local/bin/fairseq-validate\r\n   from /tmp/pip-uninstall-03hk2tvs/fairseq-validate\r\n  Moving to /home/yharalam/.local/lib/python3.9/site-packages/fairseq-0.10.0.dist-info/\r\n   from /home/yharalam/.local/lib/python3.9/site-packages/~airseq-0.10.0.dist-info\r\n  Moving to /home/yharalam/.local/lib/python3.9/site-packages/fairseq/\r\n   from /home/yharalam/.local/lib/python3.9/site-packages/~airseq\r\n  Moving to /home/yharalam/.local/lib/python3.9/site-packages/fairseq_cli/\r\n   from /home/yharalam/.local/lib/python3.9/site-packages/~airseq_cli\r\nERROR: Command errored out with exit status 1: /usr/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/home/yharalam/hagiwara/fairseq/setup.py'\"'\"'; __file__='\"'\"'/home/yharalam/hagiwara/fairseq/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps --user --prefix= Check the logs for full command output.\r\n\r\nwhat am I doing wrong? I asked for --user option, why is the installation script trying to access /usr/lib ?\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): trying to install the latest\r\n - PyTorch Version (e.g., 1.0)  1.10.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): explained above\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.2\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4060/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4057", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4057/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4057/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4057/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4057", "id": 1071848586, "node_id": "I_kwDOBhEUd84_4xyK", "number": 4057, "title": "omegaconf.errors.ConfigAttributeError: Key 'checkpoint_activations' not in 'HubertConfig'", "user": {"login": "EmreOzkose", "id": 17765576, "node_id": "MDQ6VXNlcjE3NzY1NTc2", "avatar_url": "https://avatars.githubusercontent.com/u/17765576?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EmreOzkose", "html_url": "https://github.com/EmreOzkose", "followers_url": "https://api.github.com/users/EmreOzkose/followers", "following_url": "https://api.github.com/users/EmreOzkose/following{/other_user}", "gists_url": "https://api.github.com/users/EmreOzkose/gists{/gist_id}", "starred_url": "https://api.github.com/users/EmreOzkose/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EmreOzkose/subscriptions", "organizations_url": "https://api.github.com/users/EmreOzkose/orgs", "repos_url": "https://api.github.com/users/EmreOzkose/repos", "events_url": "https://api.github.com/users/EmreOzkose/events{/privacy}", "received_events_url": "https://api.github.com/users/EmreOzkose/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-12-06T08:09:41Z", "updated_at": "2021-12-09T21:52:43Z", "closed_at": "2021-12-07T07:21:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nHi,\r\n\r\nWhen I tried to load a hubert model, I got this error:\r\n\r\n```\r\nPython 3.8.12 (default, Oct 12 2021, 13:49:34) \r\n[GCC 7.5.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> import fairseq\r\n>>> ckpt_path = \"/path/to/fairseq/pretrained_models/hubert_xtralarge_ll60k_finetune_ls960_modified.pt\"\r\n>>> models, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([ckpt_path])\r\n2021-12-06 10:55:12 | INFO | fairseq.tasks.hubert_pretraining | current directory is /path/to/fairseq/scripts\r\n2021-12-06 10:55:12 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/abdo/old_checkpoint02/datasets/librispeech/960h/raw_repeated', 'fine_tuning': False, 'labels': ['ltr'], 'label_dir': None, 'label_rate': -1, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 300000, 'min_sample_size': None, 'single_target': True, 'random_crop': False, 'pad_audio': False}\r\n2021-12-06 10:55:12 | INFO | fairseq.tasks.hubert_pretraining | current directory is /path/to/fairseq/scripts\r\n2021-12-06 10:55:12 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/abdo/old_checkpoint02/datasets/librispeech/960h/raw_repeated', 'fine_tuning': False, 'labels': ['lyr9.km500'], 'label_dir': '/path/to/fairseq/scripts', 'label_rate': 50, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\r\n2021-12-06 10:55:12 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50, 'extractor_mode': layer_norm, 'encoder_layers': 48, 'encoder_embed_dim': 1280, 'encoder_ffn_embed_dim': 5120, 'encoder_attention_heads': 16, 'activation_fn': gelu, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 1024, 'untie_final_proj': True, 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 64, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': True}\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/path/to/fairseq/fairseq_latest/fairseq/checkpoint_utils.py\", line 462, in load_model_ensemble_and_task\r\n    model = task.build_model(cfg.model)\r\n  File \"/path/to/fairseq/fairseq_latest/fairseq/tasks/fairseq_task.py\", line 335, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/path/to/fairseq/fairseq_latest/fairseq/models/__init__.py\", line 105, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/path/to/fairseq/fairseq_latest/fairseq/models/hubert/hubert_asr.py\", line 146, in build_model\r\n    w2v_encoder = HubertEncoder(cfg, task.target_dictionary)\r\n  File \"/path/to/fairseq/fairseq_latest/fairseq/models/hubert/hubert_asr.py\", line 272, in __init__\r\n    model = task.build_model(w2v_args.model)\r\n  File \"/path/to/fairseq/fairseq_latest/fairseq/tasks/fairseq_task.py\", line 335, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/path/to/fairseq/fairseq_latest/fairseq/models/__init__.py\", line 105, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/path/to/fairseq/fairseq_latest/fairseq/models/hubert/hubert.py\", line 302, in build_model\r\n    model = HubertModel(cfg, task.cfg, task.dictionaries)\r\n  File \"/path/to/fairseq/fairseq_latest/fairseq/models/hubert/hubert.py\", line 265, in __init__\r\n    self.encoder = TransformerEncoder(cfg)\r\n  File \"/path/to/fairseq/fairseq_latest/fairseq/models/wav2vec/wav2vec2.py\", line 858, in __init__\r\n    if args.checkpoint_activations:\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 305, in __getattr__\r\n    self._format_and_raise(key=key, value=None, cause=e)\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/base.py\", line 95, in _format_and_raise\r\n    format_and_raise(\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/_utils.py\", line 629, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 303, in __getattr__\r\n    return self._get_impl(key=key, default_value=DEFAULT_VALUE_MARKER)\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 361, in _get_impl\r\n    node = self._get_node(key=key)\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 383, in _get_node\r\n    self._validate_get(key)\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 135, in _validate_get\r\n    self._format_and_raise(\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/base.py\", line 95, in _format_and_raise\r\n    format_and_raise(\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/path/to/miniconda3/envs/fairseq/lib/python3.8/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.ConfigAttributeError: Key 'checkpoint_activations' not in 'HubertConfig'\r\n\tfull_key: w2v_args.checkpoint_activations\r\n\treference_type=Optional[HubertConfig]\r\n\tobject_type=HubertConfig\r\n```\r\n\r\n### To Reproduce\r\n\r\nI am following [here](https://github.com/pytorch/fairseq/blob/main/examples/hubert/README.md).\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): '1.0.0a0+0dfd6b6'\r\n - PyTorch Version '1.10.0+cu102'\r\n - OS (e.g., Linux): Ubuntu\r\n - How you installed fairseq (`pip`, source): [pip install --editable ./](https://github.com/pytorch/fairseq#requirements-and-installation)\r\n - Python version: 3.8.12\r\n - CUDA/cuDNN version: CUDA Version: 11.1\r\n - GPU models and configuration: Tesla P100\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4057/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4055", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4055/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4055/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4055/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4055", "id": 1071360355, "node_id": "I_kwDOBhEUd84_26lj", "number": 4055, "title": "Scoring with sacrebleu broken with sacrebleu>=2.", "user": {"login": "erip", "id": 2348806, "node_id": "MDQ6VXNlcjIzNDg4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/2348806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erip", "html_url": "https://github.com/erip", "followers_url": "https://api.github.com/users/erip/followers", "following_url": "https://api.github.com/users/erip/following{/other_user}", "gists_url": "https://api.github.com/users/erip/gists{/gist_id}", "starred_url": "https://api.github.com/users/erip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erip/subscriptions", "organizations_url": "https://api.github.com/users/erip/orgs", "repos_url": "https://api.github.com/users/erip/repos", "events_url": "https://api.github.com/users/erip/events{/privacy}", "received_events_url": "https://api.github.com/users/erip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-12-05T03:14:03Z", "updated_at": "2021-12-24T20:12:02Z", "closed_at": "2021-12-24T18:14:27Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThe `TOKENIZERS` attribute was [removed from `sacrebleu.tokenizers`](https://github.com/mjpost/sacrebleu/commit/078c440168c6adc89ba75fe6d63f0d922d42bcfe#diff-f05d865cb9464a2f95d61f884fc0372fba4969e79fa0fa34a2efbc7dfd9c09a2L25) in the 2.0.0 release, so trying to score when it is installed yields an error.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. `pip install -U sacrebleu`\r\n2. `fairseq-generate --scoring sacrebleu ... \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n\r\nN/A\r\n\r\n### Expected behavior\r\n\r\nNo errors. :-)\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): main\r\n - PyTorch Version (e.g., 1.0) 1.10\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): n/a\r\n - Python version: n/a\r\n - CUDA/cuDNN version: n/a\r\n - GPU models and configuration:n/a\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4055/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4055/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4036", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4036/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4036/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4036/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4036", "id": 1061632459, "node_id": "I_kwDOBhEUd84_RznL", "number": 4036, "title": "Speech-to-text preprocessing copies tuple for 2**15 times instead of doing 16-bit conversion ", "user": {"login": "George0828Zhang", "id": 13522703, "node_id": "MDQ6VXNlcjEzNTIyNzAz", "avatar_url": "https://avatars.githubusercontent.com/u/13522703?v=4", "gravatar_id": "", "url": "https://api.github.com/users/George0828Zhang", "html_url": "https://github.com/George0828Zhang", "followers_url": "https://api.github.com/users/George0828Zhang/followers", "following_url": "https://api.github.com/users/George0828Zhang/following{/other_user}", "gists_url": "https://api.github.com/users/George0828Zhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/George0828Zhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/George0828Zhang/subscriptions", "organizations_url": "https://api.github.com/users/George0828Zhang/orgs", "repos_url": "https://api.github.com/users/George0828Zhang/repos", "events_url": "https://api.github.com/users/George0828Zhang/events{/privacy}", "received_events_url": "https://api.github.com/users/George0828Zhang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2021-11-23T19:23:00Z", "updated_at": "2022-02-28T17:24:13Z", "closed_at": "2022-02-28T17:24:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIn [extract_fbank_features](https://github.com/pytorch/fairseq/blob/7f5ec30b25d906b564cc0ee0e50d1d9193940f58/examples/speech_to_text/data_utils.py#L85) of `examples/speech_to_text/data_utils.py`, this line supposedly converts the waveform to 16-bit signed integer:\r\n```python\r\n    # Kaldi compliance: 16-bit signed integers\r\n    _waveform = _waveform * (2 ** 15)\r\n    _waveform = _waveform[0].numpy()\r\n```\r\nhowever, `_waveform` as returned by [convert_waveform](https://github.com/pytorch/fairseq/blob/7f5ec30b25d906b564cc0ee0e50d1d9193940f58/fairseq/data/audio/audio_utils.py#L20) is **actually a tuple**, this basically repeats the tuple for 2**15 times...  which does not change the content of the waveform at all ! (and possibly wastes lots of memory)\r\n\r\n### To Reproduce\r\n\r\nRun any prep_*_data.py and examine the extracted features, the numbers are extremely negative.\r\n\r\n#### Code sample\r\n\r\n```python\r\nimport torchaudio\r\nfrom fairseq.data.audio.audio_utils import convert_waveform\r\n\r\nwaveform, sr = torchaudio.load(\"/some/path/to.mp3\")\r\n_waveform = convert_waveform(waveform, sr, to_mono=True)\r\n_waveform = _waveform * (2 ** 15)  # tuple multiplication...\r\nprint(_waveform)\r\n```\r\n\r\n### Expected behavior\r\n\r\n`_waveform` is expected to be in the range of 16-bit signed integer after this multiplication.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4036/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4035", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4035/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4035/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4035/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4035", "id": 1061486868, "node_id": "I_kwDOBhEUd84_RQEU", "number": 4035, "title": "Compatibility issue to Hubert from recent wav2vec2 updates", "user": {"login": "ftshijt", "id": 22814472, "node_id": "MDQ6VXNlcjIyODE0NDcy", "avatar_url": "https://avatars.githubusercontent.com/u/22814472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ftshijt", "html_url": "https://github.com/ftshijt", "followers_url": "https://api.github.com/users/ftshijt/followers", "following_url": "https://api.github.com/users/ftshijt/following{/other_user}", "gists_url": "https://api.github.com/users/ftshijt/gists{/gist_id}", "starred_url": "https://api.github.com/users/ftshijt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ftshijt/subscriptions", "organizations_url": "https://api.github.com/users/ftshijt/orgs", "repos_url": "https://api.github.com/users/ftshijt/repos", "events_url": "https://api.github.com/users/ftshijt/events{/privacy}", "received_events_url": "https://api.github.com/users/ftshijt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-11-23T16:30:01Z", "updated_at": "2021-12-14T19:55:55Z", "closed_at": "2021-12-14T19:55:55Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nAccording to [recent updates](https://github.com/pytorch/fairseq/commit/7fd643552041f9aa81dd3085068176a97ef62e84) to wav2vec2 transformer encoder, the original hubert encoder initialization cannot work due to missing `checkpiont_activations`\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n```\r\nfrom fairseq.models.hubert.hubert import (\r\n    HubertModel,  # noqa: H301\r\n    HubertConfig,  # noqa: H301\r\n    HubertPretrainingConfig,  # noqa: H301\r\n)\r\nfrom fairseq.data.dictionary import Dictionary\r\n\r\ncfg = HubertConfig()\r\nhubert_task_cfg = HubertPretrainingConfig()\r\n\r\nmodel = HubertModel(cfg, hubert_task_cfg, dictionaries)\r\n```\r\n\r\nThe Error message would be:\r\n```\r\n    def __init__(self, args):\r\n        super().__init__()\r\n    \r\n        self.dropout = args.dropout\r\n        self.embedding_dim = args.encoder_embed_dim\r\n    \r\n        self.pos_conv = nn.Conv1d(\r\n            self.embedding_dim,\r\n            self.embedding_dim,\r\n            kernel_size=args.conv_pos,\r\n            padding=args.conv_pos // 2,\r\n            groups=args.conv_pos_groups,\r\n        )\r\n        dropout = 0\r\n        std = math.sqrt((4 * (1.0 - dropout)) / (args.conv_pos * self.embedding_dim))\r\n        nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\r\n        nn.init.constant_(self.pos_conv.bias, 0)\r\n    \r\n        self.pos_conv = nn.utils.weight_norm(self.pos_conv, name=\"weight\", dim=2)\r\n        self.pos_conv = nn.Sequential(self.pos_conv, SamePad(args.conv_pos), nn.GELU())\r\n    \r\n        layers = []\r\n        for _ in range(args.encoder_layers):\r\n            layer = TransformerSentenceEncoderLayer(\r\n                    embedding_dim=self.embedding_dim,\r\n                    ffn_embedding_dim=args.encoder_ffn_embed_dim,\r\n                    num_attention_heads=args.encoder_attention_heads,\r\n                    dropout=self.dropout,\r\n                    attention_dropout=args.attention_dropout,\r\n                    activation_dropout=args.activation_dropout,\r\n                    activation_fn=args.activation_fn,\r\n                    layer_norm_first=args.layer_norm_first,\r\n            )\r\n>           if args.checkpoint_activations:\r\nE           AttributeError: 'HubertConfig' object has no attribute 'checkpoint_activations'\r\n\r\ntools/fairseq/fairseq/models/wav2vec/wav2vec2.py:856: AttributeError\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main):\r\n - PyTorch Version 1.10\r\n - OS: linux\r\n - How you installed fairseq: pip\r\n - Python version: 3.9\r\n - CUDA/cuDNN version: 10.2\r\n\r\n### Possible solution\r\n\r\nAn easy solution would be adding `checkpoint_activations` in the system. If your guys think it's OK. I can submit a PR towards that.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4035/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4035/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4011", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4011/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4011/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4011/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/4011", "id": 1050702813, "node_id": "I_kwDOBhEUd84-oHPd", "number": 4011, "title": "LM shallow fusion bug", "user": {"login": "orangelulu", "id": 38940699, "node_id": "MDQ6VXNlcjM4OTQwNjk5", "avatar_url": "https://avatars.githubusercontent.com/u/38940699?v=4", "gravatar_id": "", "url": "https://api.github.com/users/orangelulu", "html_url": "https://github.com/orangelulu", "followers_url": "https://api.github.com/users/orangelulu/followers", "following_url": "https://api.github.com/users/orangelulu/following{/other_user}", "gists_url": "https://api.github.com/users/orangelulu/gists{/gist_id}", "starred_url": "https://api.github.com/users/orangelulu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/orangelulu/subscriptions", "organizations_url": "https://api.github.com/users/orangelulu/orgs", "repos_url": "https://api.github.com/users/orangelulu/repos", "events_url": "https://api.github.com/users/orangelulu/events{/privacy}", "received_events_url": "https://api.github.com/users/orangelulu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-11-11T08:31:57Z", "updated_at": "2021-11-11T09:02:12Z", "closed_at": "2021-11-11T09:02:12Z", "author_association": "NONE", "active_lock_reason": null, "body": null, "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4011/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3976", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3976/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3976/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3976/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3976", "id": 1034418175, "node_id": "I_kwDOBhEUd849p_f_", "number": 3976, "title": "Why am I getting a segfault when trying to import the data.Dictionary?", "user": {"login": "cinjon", "id": 615351, "node_id": "MDQ6VXNlcjYxNTM1MQ==", "avatar_url": "https://avatars.githubusercontent.com/u/615351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cinjon", "html_url": "https://github.com/cinjon", "followers_url": "https://api.github.com/users/cinjon/followers", "following_url": "https://api.github.com/users/cinjon/following{/other_user}", "gists_url": "https://api.github.com/users/cinjon/gists{/gist_id}", "starred_url": "https://api.github.com/users/cinjon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cinjon/subscriptions", "organizations_url": "https://api.github.com/users/cinjon/orgs", "repos_url": "https://api.github.com/users/cinjon/repos", "events_url": "https://api.github.com/users/cinjon/events{/privacy}", "received_events_url": "https://api.github.com/users/cinjon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-10-24T13:33:51Z", "updated_at": "2021-10-25T15:38:28Z", "closed_at": "2021-10-25T15:38:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen I run `from fairseq.data import Dictionary` from inside a module, I get a segmentation fault. \r\nHowever, when I run it from the same conda environment, but inside of a python shell, it loads just fine.\r\nI am getting very little feedback on what's going wrong from the segfault, just `Segmentation fault (core dumped)`. Any idea what could be causing this?\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Put `from fairseq.data import Dictionary` into the module.\r\n2. Run that module and observe the error.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n### Expected behavior\r\n\r\nThe expected behavior is that this loads no problem like it does in the shell.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): I tried both the version from the repo via `pip install --editable` as well as from the pip package - `pip install fairseq`.\r\n - PyTorch Version (e.g., 1.0): 1.10.0\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): Both from repo and from pip package.\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.13\r\n - CUDA/cuDNN version: CUDA 11.0\r\n - GPU models and configuration: Tesla T4\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3976/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3971", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3971/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3971/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3971/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3971", "id": 1033364407, "node_id": "I_kwDOBhEUd849l-O3", "number": 3971, "title": "error when install megatron", "user": {"login": "trangtv57", "id": 12123427, "node_id": "MDQ6VXNlcjEyMTIzNDI3", "avatar_url": "https://avatars.githubusercontent.com/u/12123427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trangtv57", "html_url": "https://github.com/trangtv57", "followers_url": "https://api.github.com/users/trangtv57/followers", "following_url": "https://api.github.com/users/trangtv57/following{/other_user}", "gists_url": "https://api.github.com/users/trangtv57/gists{/gist_id}", "starred_url": "https://api.github.com/users/trangtv57/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trangtv57/subscriptions", "organizations_url": "https://api.github.com/users/trangtv57/orgs", "repos_url": "https://api.github.com/users/trangtv57/repos", "events_url": "https://api.github.com/users/trangtv57/events{/privacy}", "received_events_url": "https://api.github.com/users/trangtv57/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-10-22T09:18:34Z", "updated_at": "2022-06-01T04:01:16Z", "closed_at": "2022-05-06T15:05:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nhi,\r\nI want to training language model with transformer_lm by multi GPU, and program show error \r\n`ModuleNotFoundError: No module named 'fairseq.model_parallel.megatron.mpu'`\r\nafter that error show suggest install megatron:\r\n```\r\nPlease install the megatron submodule:\r\n  git submodule update --init fairseq/model_parallel/megatron\r\n```\r\nbut when i am running this command, another error occurr:\r\n```\r\nfatal: reference is not a tree: adb23324c222aad0aad89308e70302d996a5eaeb\r\nUnable to checkout 'adb23324c222aad0aad89308e70302d996a5eaeb' in submodule path 'fairseq/model_parallel/megatron'\r\n```\r\nI clone and using the lastest version of fairseq. How can i fix this problem, thankyou\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3971/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3971/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3952", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3952/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3952/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3952/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3952", "id": 1024563104, "node_id": "I_kwDOBhEUd849EZeg", "number": 3952, "title": "Link to pre-trained VCTK tts_transformer model is broken", "user": {"login": "slegroux", "id": 825751, "node_id": "MDQ6VXNlcjgyNTc1MQ==", "avatar_url": "https://avatars.githubusercontent.com/u/825751?v=4", "gravatar_id": "", "url": "https://api.github.com/users/slegroux", "html_url": "https://github.com/slegroux", "followers_url": "https://api.github.com/users/slegroux/followers", "following_url": "https://api.github.com/users/slegroux/following{/other_user}", "gists_url": "https://api.github.com/users/slegroux/gists{/gist_id}", "starred_url": "https://api.github.com/users/slegroux/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/slegroux/subscriptions", "organizations_url": "https://api.github.com/users/slegroux/orgs", "repos_url": "https://api.github.com/users/slegroux/repos", "events_url": "https://api.github.com/users/slegroux/events{/privacy}", "received_events_url": "https://api.github.com/users/slegroux/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-10-12T22:56:19Z", "updated_at": "2022-01-31T22:07:34Z", "closed_at": "2022-01-31T22:07:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThe link for downloading the vctk transformer model is broken:\r\n\r\nhttps://dl.fbaipublicfiles.com/fairseq/s2/vctk_transformer_phn.tar\r\n\r\n\r\n### To Reproduce\r\n\r\nGo to https://github.com/pytorch/fairseq/blob/main/examples/speech_synthesis/docs/vctk_example.md\r\nClick on the download link for tts_transformer\r\nSee 404\r\n\r\n\r\n#### Code sample\r\nN/A\r\n\r\n### Expected behavior\r\n\r\nBrowser should start downloading pretrained model.\r\n\r\n### Environment\r\nN/A\r\n\r\n### Additional context\r\nN/A\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3952/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3939", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3939/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3939/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3939/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3939", "id": 1021102351, "node_id": "I_kwDOBhEUd8483MkP", "number": 3939, "title": "Encoder/decoder mismatch in joint speech to text example", "user": {"login": "tberckmann", "id": 58150584, "node_id": "MDQ6VXNlcjU4MTUwNTg0", "avatar_url": "https://avatars.githubusercontent.com/u/58150584?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tberckmann", "html_url": "https://github.com/tberckmann", "followers_url": "https://api.github.com/users/tberckmann/followers", "following_url": "https://api.github.com/users/tberckmann/following{/other_user}", "gists_url": "https://api.github.com/users/tberckmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/tberckmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tberckmann/subscriptions", "organizations_url": "https://api.github.com/users/tberckmann/orgs", "repos_url": "https://api.github.com/users/tberckmann/repos", "events_url": "https://api.github.com/users/tberckmann/events{/privacy}", "received_events_url": "https://api.github.com/users/tberckmann/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-10-08T13:31:44Z", "updated_at": "2021-11-04T02:51:36Z", "closed_at": "2021-11-04T02:51:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nNote to reviewers: this is labeled as \"needs triage\" but I already wrote and tested a fix: see the bottom of this issue for a link to the branch.\r\n\r\nTraining step on joint speech to text example hits a python exception due to mismatched tensor sizes in matrix multiplication (since decoder embedding size doesn't match the encoder size)\r\n\r\n### To Reproduce\r\n\r\n1. Perform data preprocessing as shown in https://github.com/pytorch/fairseq/blob/main/examples/speech_text_joint_to_text/docs/ende-mustc.md\r\n2. Run the training under \"Jointly trained model from scratch.\" One difference is that I did not use the parallel text data.\r\n\r\nError output is as follows:\r\n\r\nTraceback (most recent call last):\r\n.....\r\n  File \"/home/berckmann/si2/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 216, in forward\r\n    x, extra = self.extract_features(\r\n  File \"/home/berckmann/si2/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 238, in extract_features\r\n    return self.extract_features_scriptable(\r\n  File \"/home/berckmann/si2/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 340, in extract_features_scriptable\r\n    x, layer_attn, _ = layer(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/berckmann/si2/fairseq/fairseq/modules/transformer_layer.py\", line 388, in forward\r\n    x, attn = self.encoder_attn(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/berckmann/si2/fairseq/fairseq/modules/multihead_attention.py\", line 216, in forward\r\n    k = self.k_proj(key)\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\", line 96, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 1847, in linear\r\n    return torch._C._nn.linear(input, weight, bias)\r\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (666x256 and 512x256)\r\n\r\n#### Code sample\r\n\r\nThis is the code that triggered the issue:\r\n\r\n$python_exec fairseq/train.py ${MANIFEST_ROOT} \\\r\n    --save-dir ${save_dir} \\\r\n    --num-workers 4 \\\r\n    --task speech_text_joint_to_text \\\r\n    --arch dualinputs2ttransformer_s \\\r\n    --user-dir examples/speech_text_joint_to_text \\\r\n    --max-epoch 100 --update-mix-data \\\r\n    --optimizer adam --lr-scheduler inverse_sqrt \\\r\n    --lr 0.001 --update-freq 8 --clip-norm 10.0 \\\r\n    --criterion guided_label_smoothed_cross_entropy_with_accuracy \\\r\n    --label-smoothing 0.1 --max-tokens $max_token_cnt --max-tokens-text $max_token_cnt \\\r\n    --max-positions-text 400 --seed 2 --speech-encoder-layers 12 \\\r\n    --text-encoder-layers 6 --encoder-shared-layers 6 --decoder-layers 6 \\\r\n    --dropout 0.1 --warmup-updates 20000  \\\r\n    --text-sample-ratio 0.25 \\\r\n    --text-input-cost-ratio 0.5 --enc-grad-mult 2.0 --add-speech-eos \\\r\n    --log-format json --langpairs en-de --noise-token '\"'\"'\u2581NOISE'\"'\"' \\\r\n    --mask-text-ratio 0.0 --max-tokens-valid 20000 --ddp-backend no_c10d \\\r\n    --log-interval 100 --data-buffer-size 50 --config-yaml config.yaml \\\r\n    --keep-last-epochs 10 --valid-subset dev_st --train-subset train_st \\\r\n    --tensorboard-logdir logs/tensorb_log\r\n\r\n### Expected behavior\r\n\r\nTraining should complete \r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main): git hash 72bb4447d7\r\n - PyTorch Version (e.g., 1.0): 1.9.1+cu111\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip3 install --editable ./\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 11.1\r\n - GPU models and configuration: Various\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nWrote code for the fix already, which fixed the problem for me locally:\r\n\r\nhttps://github.com/tberckmann/fairseq/tree/joint_s2t_fixes\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3939/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3929", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3929/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3929/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3929/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3929", "id": 1016012562, "node_id": "I_kwDOBhEUd848jx8S", "number": 3929, "title": "Error in inference of Hindi Model", "user": {"login": "MrityunjoyS", "id": 30874320, "node_id": "MDQ6VXNlcjMwODc0MzIw", "avatar_url": "https://avatars.githubusercontent.com/u/30874320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MrityunjoyS", "html_url": "https://github.com/MrityunjoyS", "followers_url": "https://api.github.com/users/MrityunjoyS/followers", "following_url": "https://api.github.com/users/MrityunjoyS/following{/other_user}", "gists_url": "https://api.github.com/users/MrityunjoyS/gists{/gist_id}", "starred_url": "https://api.github.com/users/MrityunjoyS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MrityunjoyS/subscriptions", "organizations_url": "https://api.github.com/users/MrityunjoyS/orgs", "repos_url": "https://api.github.com/users/MrityunjoyS/repos", "events_url": "https://api.github.com/users/MrityunjoyS/events{/privacy}", "received_events_url": "https://api.github.com/users/MrityunjoyS/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-10-05T08:04:26Z", "updated_at": "2022-04-28T02:58:27Z", "closed_at": "2022-04-28T02:58:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've finetuned wav2vec2.0 model using custom hindi dataset and dict file.\r\nMy dict.ltr.txt file :\r\n\u0901 221552\r\n\u0902 2532935\r\n\u0903 7964\r\n\u0905 743852\r\n\u0906 486561\r\n\u0907 228473\r\n\u0908 147737\r\n\u0909 574563\r\n\u090a 37016\r\n\u090b 8300\r\n\u090f 986893\r\n\u0910 22905\r\n\u0911 22600\r\n\u0913 74015\r\n\u0914 692314\r\n\u0915 5256799\r\n\u0916 515586\r\n\u0917 965339\r\n\u0918 147515\r\n\u091a 854760\r\n\u091b 331551\r\n\u091c 1024320\r\n\u091d 140483\r\n\u091e 7963\r\n\u091f 420346\r\n\u0920 125848\r\n\u0921 552437\r\n\u0922 66851\r\n\u0923 95966\r\n\u0924 2128375\r\n\u0925 847606\r\n\u0926 1237555\r\n\u0927 265262\r\n\u0928 2562397\r\n\u092a 1958588\r\n\u092b 257928\r\n\u092c 1311026\r\n\u092d 493523\r\n\u092e 2415669\r\n\u092f 1620380\r\n\u0930 4351756\r\n\u0932 1804089\r\n\u0935 1192983\r\n\u0936 559844\r\n\u0937 199197\r\n\u0938 2783472\r\n\u0939 3549070\r\n\u093c 494196\r\n\u093e 6052649\r\n\u093f 2356618\r\n\u0940 2555418\r\n\u0941 1068016\r\n\u0942 596680\r\n\u0943 73764\r\n\u0945 7511\r\n\u0947 5044035\r\n\u0948 1443480\r\n\u0949 74500\r\n\u094b 1627689\r\n\u094c 111129\r\n\u094d 2224056\r\n\r\n**After my model is trained I'm trying on inferecing my model but getting below error :-**\r\n\r\n```\r\nINFO:__main__:| decoding with criterion ctc\r\nINFO:__main__:| loading model(s) from /root/model_and_dict_hi/checkpoint_best.pt\r\nINFO:fairseq.data.audio.raw_audio_dataset:loaded 496, skipped 0 samples\r\nTraceback (most recent call last):\r\n  File \"examples/speech_recognition/infer.py\", line 427, in <module>\r\n    cli_main()\r\n  File \"examples/speech_recognition/infer.py\", line 423, in cli_main\r\n    main(args)\r\n  File \"examples/speech_recognition/infer.py\", line 239, in main\r\n    task.load_dataset(args.gen_subset, task_cfg=saved_cfg.task)\r\n  File \"/root/fairseq/fairseq/tasks/audio_pretraining.py\", line 220, in load_dataset\r\n    line for i, line in enumerate(f)\r\n  File \"/root/fairseq/fairseq/tasks/audio_pretraining.py\", line 220, in <listcomp>\r\n    line for i, line in enumerate(f)\r\n  File \"/usr/lib/python3.6/encodings/ascii.py\", line 26, in decode\r\n    return codecs.ascii_decode(input, self.errors)[0]\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe0 in position 0: ordinal not in range(128)\r\n```\r\nSo I'm using **hindi character's which are obviously not ascii**, So what configuration changes must be done.\r\nPlease suggest what I might be doing wrong. Thank you in advance .", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3929/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3913", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3913/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3913/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3913/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3913", "id": 1007948046, "node_id": "I_kwDOBhEUd848FBEO", "number": 3913, "title": "sequence generator with prefix_tokens", "user": {"login": "rgwt123", "id": 22995589, "node_id": "MDQ6VXNlcjIyOTk1NTg5", "avatar_url": "https://avatars.githubusercontent.com/u/22995589?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgwt123", "html_url": "https://github.com/rgwt123", "followers_url": "https://api.github.com/users/rgwt123/followers", "following_url": "https://api.github.com/users/rgwt123/following{/other_user}", "gists_url": "https://api.github.com/users/rgwt123/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgwt123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgwt123/subscriptions", "organizations_url": "https://api.github.com/users/rgwt123/orgs", "repos_url": "https://api.github.com/users/rgwt123/repos", "events_url": "https://api.github.com/users/rgwt123/events{/privacy}", "received_events_url": "https://api.github.com/users/rgwt123/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-09-27T10:02:38Z", "updated_at": "2022-06-23T10:03:44Z", "closed_at": "2022-06-23T10:03:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSometimes using generator.generate with prefix_tokens can produce shorter results compared to given prefix.\r\nAdding `lprobs[:, self.eos] = -math.inf` in `line 355 of sequence_generator.py` seems to fix this problem. But the comment below says \"does not apply if using prefix_tokens\".\r\n```python\r\n            # handle prefix tokens (possibly with different lengths)\r\n            if (\r\n                prefix_tokens is not None\r\n                and step < prefix_tokens.size(1)\r\n                and step < max_len\r\n            ):\r\n                lprobs, tokens, scores = self._prefix_tokens(\r\n                    step, lprobs, scores, tokens, prefix_tokens, beam_size\r\n                )\r\n                lprobs[:, self.eos] = -math.inf # add this\r\n            elif step < self.min_len:\r\n                # minimum length constraint (does not apply if using prefix_tokens)\r\n                lprobs[:, self.eos] = -math.inf\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\r\nMinimal means having the shortest code but still preserving the bug. -->\r\n```python\r\ndef make_batch(sents):\r\n    sents = [\r\n        self.task.source_dictionary.encode_line(\r\n            ' '.join(sent), add_if_not_exist=False\r\n        ).long()\r\n        for sent in sents\r\n    ]\r\n    lengths = [sent.numel() for sent in sents]\r\n    max_len = max(lengths)\r\n    nums = len(sents)\r\n    res = sents[-1].new(nums, max_len).fill_(self.task.source_dictionary.pad_index)\r\n    for idx, sent in enumerate(sents):\r\n        res[idx, max_len-lengths[idx]:].copy_(sent)\r\n    res_lengths = torch.LongTensor(lengths)\r\n    return  {\"net_input\": {\"src_tokens\": res.cuda(), \"src_lengths\": res_lengths.cuda()}}\r\n      \r\ndef make_prefix_batch(sents):\r\n    sents = [\r\n        self.task.target_dictionary.encode_line(\r\n            ' '.join(sent), add_if_not_exist=False\r\n        )[:-1].long()\r\n        for sent in sents\r\n    ]\r\n    lengths = [sent.numel() for sent in sents]\r\n    max_len = max(lengths)\r\n    nums = len(sents)\r\n    res = sents[-1].new(nums, max_len).fill_(self.task.source_dictionary.pad_index)\r\n    for idx, sent in enumerate(sents):\r\n        res[idx, max_len-lengths[idx]:].copy_(sent)\r\n    return res.cuda()\r\n\r\nbatch_input_ids = make_batch([spm.encode(sentence)])\r\nif prefix != '':\r\n    batch_prefix_ids = make_prefix_batch([prefix.split()])\r\nelse:\r\n    batch_prefix_ids = None\r\ntrans = self.generator.generate(self.models, batch_input_ids, prefix_tokens=batch_prefix_ids)\r\ntran_ids = trans[0][0]['tokens'].int().cpu()\r\ntran_text = self.task.target_dictionary.string(tran_ids)\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3913/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3913/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3911", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3911/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3911/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3911/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3911", "id": 1007378248, "node_id": "I_kwDOBhEUd848C19I", "number": 3911, "title": "nat_loss.py label smoothing", "user": {"login": "haorannlp", "id": 52477842, "node_id": "MDQ6VXNlcjUyNDc3ODQy", "avatar_url": "https://avatars.githubusercontent.com/u/52477842?v=4", "gravatar_id": "", "url": "https://api.github.com/users/haorannlp", "html_url": "https://github.com/haorannlp", "followers_url": "https://api.github.com/users/haorannlp/followers", "following_url": "https://api.github.com/users/haorannlp/following{/other_user}", "gists_url": "https://api.github.com/users/haorannlp/gists{/gist_id}", "starred_url": "https://api.github.com/users/haorannlp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/haorannlp/subscriptions", "organizations_url": "https://api.github.com/users/haorannlp/orgs", "repos_url": "https://api.github.com/users/haorannlp/repos", "events_url": "https://api.github.com/users/haorannlp/events{/privacy}", "received_events_url": "https://api.github.com/users/haorannlp/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-09-26T12:30:19Z", "updated_at": "2021-09-30T13:18:36Z", "closed_at": "2021-09-30T13:18:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIn line 68 of `nat_loss.py`, the label smoothed nat loss is computed as follows:\r\n\r\n```\r\nloss = nll_loss * (1 - label_smoothing) - mean_ds(logits) * label_smoothing\r\n                \r\n```\r\nI think the smoothing part should be `mean_ds(logits) * label_smoothing / (logits.size(-1)-1)`\r\n\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue.\r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or main):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3911/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3907", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3907/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3907/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3907/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3907", "id": 1005436492, "node_id": "I_kwDOBhEUd8477b5M", "number": 3907, "title": "transformer_config.py is not ready, it causes many bugs in `--arch`", "user": {"login": "xu-song", "id": 13825126, "node_id": "MDQ6VXNlcjEzODI1MTI2", "avatar_url": "https://avatars.githubusercontent.com/u/13825126?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xu-song", "html_url": "https://github.com/xu-song", "followers_url": "https://api.github.com/users/xu-song/followers", "following_url": "https://api.github.com/users/xu-song/following{/other_user}", "gists_url": "https://api.github.com/users/xu-song/gists{/gist_id}", "starred_url": "https://api.github.com/users/xu-song/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xu-song/subscriptions", "organizations_url": "https://api.github.com/users/xu-song/orgs", "repos_url": "https://api.github.com/users/xu-song/repos", "events_url": "https://api.github.com/users/xu-song/events{/privacy}", "received_events_url": "https://api.github.com/users/xu-song/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-09-23T13:27:11Z", "updated_at": "2021-09-27T11:37:43Z", "closed_at": "2021-09-27T11:37:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nhttps://github.com/pytorch/fairseq/blob/main/fairseq/models/transformer/transformer_config.py\r\n\r\n`--arch` does not work in main branch. This is a critical bug.\r\n\r\n### Related issues\r\n\r\n- https://github.com/pytorch/fairseq/issues/3775\r\n- https://github.com/pytorch/fairseq/issues/3790\r\n\r\n### Related commit\r\n\r\n- https://github.com/pytorch/fairseq/commit/129d8594ccdc6644be84dc249e16489e049f4bfd\r\n\r\n### Environment\r\n\r\n - fairseq Version: main\r\n \r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3907/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3907/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3889", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3889/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3889/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3889/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3889", "id": 999339091, "node_id": "I_kwDOBhEUd847kLRT", "number": 3889, "title": "\"Only right padding is supported\" Error triggered while training MMA-Hard model", "user": {"login": "ereday", "id": 13196191, "node_id": "MDQ6VXNlcjEzMTk2MTkx", "avatar_url": "https://avatars.githubusercontent.com/u/13196191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ereday", "html_url": "https://github.com/ereday", "followers_url": "https://api.github.com/users/ereday/followers", "following_url": "https://api.github.com/users/ereday/following{/other_user}", "gists_url": "https://api.github.com/users/ereday/gists{/gist_id}", "starred_url": "https://api.github.com/users/ereday/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ereday/subscriptions", "organizations_url": "https://api.github.com/users/ereday/orgs", "repos_url": "https://api.github.com/users/ereday/repos", "events_url": "https://api.github.com/users/ereday/events{/privacy}", "received_events_url": "https://api.github.com/users/ereday/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-09-17T13:10:52Z", "updated_at": "2021-09-20T18:27:58Z", "closed_at": "2021-09-18T08:28:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI was trying to train MMA-Hard model (a Simultaneous Translation model) on the WMT15 de-en data. After training started and 100-120 iterations done, I got \"Only right padding is supported\" error. You can find complete error message below. \r\n\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\nI've installed fairseq library today (September 17, 2021)  (commit id: f6abcc2a6732) using following commands:\r\n\r\n```\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./\r\n```\r\n\r\nI obtained the data using the bash script provided in fairseq repository. Preprocessing/Binarization was also done using the commands provided in the readme: \r\n\r\n```\r\n# Preprocess/binarize the data\r\nTEXT=./wmt15.tokenized.de-en\r\nfairseq-preprocess --source-lang de --target-lang en \\\r\n    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\r\n    --destdir data-bin/wmt15.tokenized.de-en \\\r\n    --workers 20\r\n```\r\n\r\nTraining command as follows:\r\n\r\n```\r\nfairseq-train \\\r\ndata-bin/wmt15.tokenized.de-en \\\r\n --simul-type hard_aligned --mass-preservation \\\r\n--criterion latency_augmented_label_smoothed_cross_entropy \\\r\n--latency-var-weight 0.1 --max-update 50000 \\\r\n--arch transformer_monotonic_iwslt_de_en \\\r\n--optimizer adam --adam-betas '(0.9, 0.98)' --lr-scheduler 'inverse_sqrt' \\\r\n--warmup-init-lr 1e-7  --warmup-updates 4000 \\\r\n--lr 5e-4 --stop-min-lr 1e-9 --clip-norm 0.0 --weight-decay 0.0001 \\\r\n--dropout 0.3 --label-smoothing 0.1 --max-tokens 3584\r\n\r\n```\r\n\r\nAs mentioned above, after a few iterations, I got the following error message:\r\n```\r\nTraceback (most recent call last):                                                                                             \r\n  File \"/home/anaconda3/envs/py37/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/fairseq/fairseq_cli/train.py\", line 507, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/fairseq/fairseq_cli/train.py\", line 180, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/home/anaconda3/envs/py37/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/fairseq/fairseq_cli/train.py\", line 291, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/anaconda3/envs/py37/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/fairseq/fairseq/trainer.py\", line 761, in train_step\r\n    **extra_kwargs,\r\n  File \"/home/fairseq/fairseq/tasks/fairseq_task.py\", line 492, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/fairseq/fairseq/criterions/label_smoothed_cross_entropy_latency_augmented.py\", line 93, in forward\r\n    net_output = model(**sample[\"net_input\"])\r\n  File \"/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/fairseq/fairseq/models/transformer/transformer_base.py\", line 157, in forward\r\n    return_all_hiddens=return_all_hiddens,\r\n  File \"/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 222, in forward\r\n    alignment_heads=alignment_heads,\r\n  File \"/home/fairseq/examples/simultaneous_translation/models/transformer_monotonic_attention.py\", line 222, in extract_features\r\n    else None,\r\n  File \"/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/fairseq/examples/simultaneous_translation/modules/monotonic_transformer_layer.py\", line 152, in forward\r\n    need_head_weights=need_head_weights,\r\n  File \"/home/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/fairseq/examples/simultaneous_translation/modules/monotonic_multihead_attention.py\", line 351, in forward\r\n    \"Only right padding is supported.\"\r\nAssertionError: Only right padding is supported.\r\n```\r\n\r\nHere is some information about my environment:\r\n- My python version is 3.7 \r\n- Pytorch version is 1.9\r\n- Fairseq version is  installed locally after cloning & updating the repository to latest version.\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3889/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3884", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3884/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3884/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3884/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3884", "id": 998667167, "node_id": "I_kwDOBhEUd847hnOf", "number": 3884, "title": "Bugs in S2T prep_mustc_data.py", "user": {"login": "gegallego", "id": 31229713, "node_id": "MDQ6VXNlcjMxMjI5NzEz", "avatar_url": "https://avatars.githubusercontent.com/u/31229713?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gegallego", "html_url": "https://github.com/gegallego", "followers_url": "https://api.github.com/users/gegallego/followers", "following_url": "https://api.github.com/users/gegallego/following{/other_user}", "gists_url": "https://api.github.com/users/gegallego/gists{/gist_id}", "starred_url": "https://api.github.com/users/gegallego/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gegallego/subscriptions", "organizations_url": "https://api.github.com/users/gegallego/orgs", "repos_url": "https://api.github.com/users/gegallego/repos", "events_url": "https://api.github.com/users/gegallego/events{/privacy}", "received_events_url": "https://api.github.com/users/gegallego/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-09-16T21:44:44Z", "updated_at": "2021-12-22T02:18:29Z", "closed_at": "2021-12-22T02:18:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThere are two bugs in [`prep_mustc_data.py`](https://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/examples/speech_to_text/prep_mustc_data.py).\r\n\r\n### Bug in the call of [`sf.write`](https://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/examples/speech_to_text/prep_mustc_data.py#L130-L133)\r\n\r\nhttps://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/examples/speech_to_text/prep_mustc_data.py#L130-L133\r\n\r\nWhile `_wavform` has the format `(channels x frames)`, the [`sf.write`](https://github.com/bastibe/python-soundfile/blob/744efb4b01abc72498a96b09115b42a4cabd85e4/soundfile.py#L263-L316) function expects the format `(frames x channels)`.\r\n\r\nThis caused a `RuntimeError: Error opening 'XXXXX': Format not recognised.`.\r\n\r\nTo solve it, it is needed to transpose `_wavform`: `_wavform.T.numpy()`\r\n\r\n### Bug in the call of [`get_zip_manifest`](https://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/examples/speech_to_text/prep_mustc_data.py#L159)\r\n\r\nhttps://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/examples/speech_to_text/prep_mustc_data.py#L159\r\n\r\nThis function has the argument `is_audio`, which is `False` by default.\r\n\r\nIt should be set accordingly depending on if using `--use-audio-input` or not:\r\n```python\r\naudio_paths, audio_lengths = get_zip_manifest(\r\n    zip_path,\r\n    is_audio=args.use_audio_input,\r\n)\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3884/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3882", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3882/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3882/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3882/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3882", "id": 998478676, "node_id": "I_kwDOBhEUd847g5NU", "number": 3882, "title": "Bugs in S2T data_utils.py", "user": {"login": "gegallego", "id": 31229713, "node_id": "MDQ6VXNlcjMxMjI5NzEz", "avatar_url": "https://avatars.githubusercontent.com/u/31229713?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gegallego", "html_url": "https://github.com/gegallego", "followers_url": "https://api.github.com/users/gegallego/followers", "following_url": "https://api.github.com/users/gegallego/following{/other_user}", "gists_url": "https://api.github.com/users/gegallego/gists{/gist_id}", "starred_url": "https://api.github.com/users/gegallego/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gegallego/subscriptions", "organizations_url": "https://api.github.com/users/gegallego/orgs", "repos_url": "https://api.github.com/users/gegallego/repos", "events_url": "https://api.github.com/users/gegallego/events{/privacy}", "received_events_url": "https://api.github.com/users/gegallego/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-09-16T17:56:23Z", "updated_at": "2021-12-22T02:18:29Z", "closed_at": "2021-12-22T02:18:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThere are two bugs in this file.\r\n\r\n### Bug in [`extract_fbank_features`](https://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/examples/speech_to_text/data_utils.py#L73-L98) function\r\n\r\nThe following line:\r\nhttps://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/examples/speech_to_text/data_utils.py#L83\r\n\r\nShould be substituted by:\r\n```python\r\n_waveform, _ = convert_waveform(waveform, sample_rate, to_mono=True)\r\n```\r\n\r\nBecause the function [`convert_waveform`](https://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/fairseq/data/audio/audio_utils.py#L19) also returns the sample_rate:\r\nhttps://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/fairseq/data/audio/audio_utils.py#L61\r\n\r\n### Bug in [`create_zip`](https://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/examples/speech_to_text/data_utils.py#L101-L105) function\r\n\r\nThe following line:\r\nhttps://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/examples/speech_to_text/data_utils.py#L102\r\n\r\nDoes not work when executing [`prep_mustc_data.py`](https://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/examples/speech_to_text/prep_mustc_data.py) with `--use-audio-input`, because the files extension is `.flac`.\r\n\r\nIt is necessary to add the following line below:\r\n```python\r\npaths.extend(data_root.glob(\"*.flac\"))\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3882/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3882/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3876", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3876/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3876/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3876/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3876", "id": 996421162, "node_id": "I_kwDOBhEUd847ZC4q", "number": 3876, "title": "Simultaneous translation model (MMA) can't generate", "user": {"login": "kurtisxx", "id": 90444478, "node_id": "MDQ6VXNlcjkwNDQ0NDc4", "avatar_url": "https://avatars.githubusercontent.com/u/90444478?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kurtisxx", "html_url": "https://github.com/kurtisxx", "followers_url": "https://api.github.com/users/kurtisxx/followers", "following_url": "https://api.github.com/users/kurtisxx/following{/other_user}", "gists_url": "https://api.github.com/users/kurtisxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/kurtisxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kurtisxx/subscriptions", "organizations_url": "https://api.github.com/users/kurtisxx/orgs", "repos_url": "https://api.github.com/users/kurtisxx/repos", "events_url": "https://api.github.com/users/kurtisxx/events{/privacy}", "received_events_url": "https://api.github.com/users/kurtisxx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-09-14T20:52:23Z", "updated_at": "2022-02-09T04:38:22Z", "closed_at": "2021-09-15T07:04:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI've trained the command provided in the README to train a simultaneous machine translation model (MMA-Hard) model. The command I used is the following:\r\n\r\n`CUDA_VISIBLE_DEVICES=0 python train.py ./data-bin/wmt15_de_en --simul-type hard_aligned     --mass-preservation     --criterion latency_augmented_label_smoothed_cross_entropy      --latency-weight-var  0.1     --max-update 50000     --arch transformer_monotonic_iwslt_de_en     --optimizer adam --adam-betas '(0.9, 0.98)'     --lr-scheduler 'inverse_sqrt'     --warmup-init-lr 1e-7  --warmup-updates 4000     --lr 5e-4 --stop-min-lr 1e-9 --clip-norm 0.0 --weight-decay 0.0001    --dropout 0.3     --label-smoothing 0.1    --max-tokens 3584\r\n`\r\nAfter traning completed, I wanted to  generate translations using use the last checkpoint saved during training. For this I run following command:\r\n\r\n`fairseq-generate ./data-bin/wmt15_de_en/ --path checkpoints/checkpoint_last.pt --simul-type hard_aligned --max-tokens 3584\r\n`\r\nI ended up with the following error message:\r\n```\r\n\r\n  File \"anaconda3/envs/pytorch_latest_p36/bin/fairseq-generate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())\r\n  File \"fairseq/fairseq_cli/generate.py\", line 404, in cli_main\r\n    main(args)\r\n  File \"fairseq/fairseq_cli/generate.py\", line 49, in main\r\n    return _main(cfg, sys.stdout)\r\n  File \"fairseq/fairseq_cli/generate.py\", line 206, in _main\r\n    constraints=constraints,\r\n  File \"fairseq/fairseq/tasks/fairseq_task.py\", line 501, in inference_step\r\n    models, sample, prefix_tokens=prefix_tokens, constraints=constraints\r\n  File \"anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 26, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"fairseq/fairseq/sequence_generator.py\", line 182, in generate\r\n    return self._generate(sample, **kwargs)\r\n  File \"fairseq/fairseq/sequence_generator.py\", line 325, in _generate\r\n    self.temperature,\r\n  File \"fairseq/fairseq/sequence_generator.py\", line 778, in forward_decoder\r\n    incremental_state=incremental_states[i],\r\n  File \"fairseq/fairseq/models/transformer.py\", line 806, in forward\r\n    alignment_heads=alignment_heads,\r\n  File \"fairseq/examples/simultaneous_translation/models/transformer_monotonic_attention.py\", line 214, in extract_features\r\n    .gather(1, curr_steps.t())\r\nRuntimeError: Index tensor must have the same number of dimensions as input tensor\r\n```\r\n\r\nWhat could be the reason for that? I'm really DISAPPOINTED that there are millions of problems in the official code of a published paper and no support at all.. @xutaima \r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 252d5a9ae93e68254cfb1896fb5624cf11cda15e ( commit id)\r\n - PyTorch Version (e.g., 1.0) 1.7\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): git checkout 252d5a9ae93e68254cfb1896fb5624cf11cda15e && pip install \u2014editable ./\r\n - Python version: 3.6.13\r\n - CUDA/cuDNN version:10.1\r\n - GPU models and configuration: v100\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3876/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3876/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3870", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3870/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3870/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3870/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3870", "id": 994953550, "node_id": "MDU6SXNzdWU5OTQ5NTM1NTA=", "number": 3870, "title": "Textless NLP. Missed code_dict data", "user": {"login": "patkinm", "id": 2562324, "node_id": "MDQ6VXNlcjI1NjIzMjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/2562324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patkinm", "html_url": "https://github.com/patkinm", "followers_url": "https://api.github.com/users/patkinm/followers", "following_url": "https://api.github.com/users/patkinm/following{/other_user}", "gists_url": "https://api.github.com/users/patkinm/gists{/gist_id}", "starred_url": "https://api.github.com/users/patkinm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patkinm/subscriptions", "organizations_url": "https://api.github.com/users/patkinm/orgs", "repos_url": "https://api.github.com/users/patkinm/repos", "events_url": "https://api.github.com/users/patkinm/events{/privacy}", "received_events_url": "https://api.github.com/users/patkinm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2021-09-13T14:20:26Z", "updated_at": "2021-10-27T08:43:10Z", "closed_at": "2021-10-08T17:03:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nMissed code_dict data for fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/utils.py \r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd \r\n!PYTHONPATH=. \\\r\npython /home/ubuntu/fairseq/examples/textless_nlp/gslm/tools/resynthesize_speech.py\\\r\n--feature_type hubert     \\\r\n--acoustic_model_path /home/ubuntu/hubert_base_ls960.pt\\\r\n--layer 12     \\\r\n--kmeans_model_path /home/ubuntu/km.bin  \\\r\n--tts_model_path /home/ubuntu/tts_checkpoint_best.pt    \\\r\n--waveglow_path  /home/ubuntu/waveglow_256channels_new.pt \\\r\n--max_decoder_steps 2000\r\n\r\n2. See error\r\n[2021-09-12 12:18:20,486] [INFO]: Namespace(acoustic_model_path='/home/ubuntu/hubert_base_ls960.pt', denoiser_strength=0.1, feature_type='hubert', kmeans_model_path='/home/ubuntu/km.bin', layer=12, max_decoder_steps=2000, tts_model_path='/home/ubuntu/tts_checkpoint_best.pt', waveglow_path='/home/ubuntu/waveglow_256channels_new.pt')\r\n[2021-09-12 12:18:20,486] [INFO]: Loading acoustic model from /home/ubuntu/tts_checkpoint_best.pt...\r\n[2021-09-12 12:18:25,261] [INFO]: current directory is /home/ubuntu/fairseq\r\n[2021-09-12 12:18:25,262] [INFO]: HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\r\n[2021-09-12 12:18:25,275] [INFO]: HubertModel Config: {'_name': 'hubert', 'label_rate': 50, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False}\r\n[2021-09-12 12:18:27,194] [WARNING]: ===Jit: state[\"model\"] does not contain key \"_metadata\"=====\r\n[2021-09-12 12:18:27,194] [WARNING]: ===Jit: we will be filling in with current model's meta-data instead.\r\n[2021-09-12 12:18:31,669] [INFO]: Loading K-means model from /home/ubuntu/km.bin ...\r\n/home/ubuntu/miniconda3/envs/fairseq/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\r\n  UserWarning)\r\n[2021-09-12 12:18:31,670] [INFO]: Loading TTS model from /home/ubuntu/tts_checkpoint_best.pt...\r\n[2021-09-12 12:18:32,221] [INFO]: Loading Waveglow model from /home/ubuntu/waveglow_256channels_new.pt...\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/fairseq/examples/textless_nlp/gslm/tools/resynthesize_speech.py\", line 138, in <module>\r\n    main(args, logger)\r\n  File \"/home/ubuntu/fairseq/examples/textless_nlp/gslm/tools/resynthesize_speech.py\", line 100, in main\r\n    tts_dataset = TacotronInputDataset(hparams)\r\n  File \"/home/ubuntu/fairseq/examples/textless_nlp/gslm/unit2speech/tts_data.py\", line 24, in __init__\r\n    self.code_dict = load_code_dict(hparams.code_dict)\r\n  File \"/home/ubuntu/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/utils.py\", line 65, in load_code_dict\r\n    with open(path, 'r') as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/checkpoint/kushall/experiments/speechbot/tts/filelists/cpc_85_clustering-LL6k_gru2_kmeans100/code_dict_cpc_km100'\r\n\r\nI've tried to use all unit to speech HuBERT models:\r\n\r\nHuBERT Base + KM50\r\nHuBERT Base + KM100 \r\nHuBERT Base + KM200\r\nHuBERT Base + KM500\r\n\r\nSame issue...\r\n\r\nMaybe I'm doing something wrong. \r\n\r\nThank you,\r\nMichael\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3870/reactions", "total_count": 5, "+1": 5, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3870/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3865", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3865/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3865/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3865/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3865", "id": 994487701, "node_id": "MDU6SXNzdWU5OTQ0ODc3MDE=", "number": 3865, "title": " w2v -> hubert in fairseq/models/hubert_asr.py", "user": {"login": "yuseungwoo", "id": 42972413, "node_id": "MDQ6VXNlcjQyOTcyNDEz", "avatar_url": "https://avatars.githubusercontent.com/u/42972413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuseungwoo", "html_url": "https://github.com/yuseungwoo", "followers_url": "https://api.github.com/users/yuseungwoo/followers", "following_url": "https://api.github.com/users/yuseungwoo/following{/other_user}", "gists_url": "https://api.github.com/users/yuseungwoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuseungwoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuseungwoo/subscriptions", "organizations_url": "https://api.github.com/users/yuseungwoo/orgs", "repos_url": "https://api.github.com/users/yuseungwoo/repos", "events_url": "https://api.github.com/users/yuseungwoo/events{/privacy}", "received_events_url": "https://api.github.com/users/yuseungwoo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-09-13T06:02:23Z", "updated_at": "2021-09-13T06:03:13Z", "closed_at": "2021-09-13T06:02:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3865/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3854", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3854/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3854/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3854/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3854", "id": 992128346, "node_id": "MDU6SXNzdWU5OTIxMjgzNDY=", "number": 3854, "title": "Wikitext language model training is broken", "user": {"login": "RuABraun", "id": 12561072, "node_id": "MDQ6VXNlcjEyNTYxMDcy", "avatar_url": "https://avatars.githubusercontent.com/u/12561072?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RuABraun", "html_url": "https://github.com/RuABraun", "followers_url": "https://api.github.com/users/RuABraun/followers", "following_url": "https://api.github.com/users/RuABraun/following{/other_user}", "gists_url": "https://api.github.com/users/RuABraun/gists{/gist_id}", "starred_url": "https://api.github.com/users/RuABraun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RuABraun/subscriptions", "organizations_url": "https://api.github.com/users/RuABraun/orgs", "repos_url": "https://api.github.com/users/RuABraun/repos", "events_url": "https://api.github.com/users/RuABraun/events{/privacy}", "received_events_url": "https://api.github.com/users/RuABraun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-09-09T11:47:18Z", "updated_at": "2021-09-09T12:00:52Z", "closed_at": "2021-09-09T12:00:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nExecuted the commands described [here](https://github.com/pytorch/fairseq/tree/master/examples/language_model) to train a LM does not work.\r\n\r\n### To Reproduce\r\n\r\n1. Do preprocessing\r\n2. Call\r\n```\r\nfairseq-train --task language_modeling wikidata-bin --save-dir wikimodel --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --fp16 --max-update 50000\r\n```\r\nGet error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/idiap/temp/rbraun/programs/anaconda3/envs/speech/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/remote/idiap.svm/temp.speech01/rbraun/code/fairseq/fairseq_cli/train.py\", line 496, in cli_main\r\n    cfg = convert_namespace_to_omegaconf(args)\r\n  File \"/remote/idiap.svm/temp.speech01/rbraun/code/fairseq/fairseq/dataclass/utils.py\", line 389, in convert_namespace_to_omegaconf\r\n    composed_cfg = compose(\"config\", overrides=overrides, strict=False)\r\n  File \"/idiap/temp/rbraun/programs/anaconda3/envs/speech/lib/python3.8/site-packages/hydra/experimental/compose.py\", line 31, in compose\r\n    cfg = gh.hydra.compose_config(\r\n  File \"/idiap/temp/rbraun/programs/anaconda3/envs/speech/lib/python3.8/site-packages/hydra/_internal/hydra.py\", line 507, in compose_config\r\n    cfg = self.config_loader.load_configuration(\r\n  File \"/idiap/temp/rbraun/programs/anaconda3/envs/speech/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 151, in load_configuration\r\n    return self._load_configuration(\r\n  File \"/idiap/temp/rbraun/programs/anaconda3/envs/speech/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 256, in _load_configuration\r\n    cfg = self._merge_defaults_into_config(\r\n  File \"/idiap/temp/rbraun/programs/anaconda3/envs/speech/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 805, in _merge_defaults_into_config\r\n    hydra_cfg = merge_defaults_list_into_config(hydra_cfg, user_list)\r\n  File \"/idiap/temp/rbraun/programs/anaconda3/envs/speech/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 787, in merge_defaults_list_into_config\r\n    merged_cfg = self._merge_config(\r\n  File \"/idiap/temp/rbraun/programs/anaconda3/envs/speech/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 704, in _merge_config\r\n    raise MissingConfigException(msg, new_cfg)\r\nhydra.errors.MissingConfigException: Could not load _self_\r\n```\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master, commit 50b65368639ac25663764e1d4b3cf46b821975ec\r\n - PyTorch Version (e.g., 1.0): 1.8.0\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `python -m pip install -e`\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: 11.1\r\n - GPU models and configuration: 1080ti, ampere\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3854/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3845", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3845/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3845/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3845/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3845", "id": 988419148, "node_id": "MDU6SXNzdWU5ODg0MTkxNDg=", "number": 3845, "title": "The shell script examples/speech_to_text/data_utils.py import fairseq.data.audio.audio_utils error.", "user": {"login": "Cuihao1208", "id": 20596481, "node_id": "MDQ6VXNlcjIwNTk2NDgx", "avatar_url": "https://avatars.githubusercontent.com/u/20596481?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Cuihao1208", "html_url": "https://github.com/Cuihao1208", "followers_url": "https://api.github.com/users/Cuihao1208/followers", "following_url": "https://api.github.com/users/Cuihao1208/following{/other_user}", "gists_url": "https://api.github.com/users/Cuihao1208/gists{/gist_id}", "starred_url": "https://api.github.com/users/Cuihao1208/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Cuihao1208/subscriptions", "organizations_url": "https://api.github.com/users/Cuihao1208/orgs", "repos_url": "https://api.github.com/users/Cuihao1208/repos", "events_url": "https://api.github.com/users/Cuihao1208/events{/privacy}", "received_events_url": "https://api.github.com/users/Cuihao1208/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-09-05T04:12:58Z", "updated_at": "2022-01-31T22:09:11Z", "closed_at": "2022-01-31T22:09:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI got the following error when try to run [S2T Example: Speech Recognition (ASR) on LibriSpeech](https://github.com/pytorch/fairseq/blob/master/examples/speech_to_text/docs/librispeech_example.md#s2t-example-speech-recognition-asr-on-librispeech). Then error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"examples/speech_to_text/prep_librispeech_data.py\", line 14, in <module>\r\n    from examples.speech_to_text.data_utils import (\r\n  File \"/fairseq-master/examples/speech_to_text/data_utils.py\", line 17, in <module>\r\n    from fairseq.data.audio.audio_utils import (\r\nImportError: cannot import name '_convert_to_mono' from 'fairseq.data.audio.audio_utils' (/fairseq-master/fairseq/data/audio/audio_utils.py)\r\n```\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior :\r\n`python examples/speech_to_text/prep_librispeech_data.py \r\n  --output-root ${LS_ROOT} --vocab-type unigram --vocab-size 10000`\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\nline 17-19 in examples/speech_to_text/data_utils.py:\r\n```\r\nfrom fairseq.data.audio.audio_utils import (\r\n    _convert_to_mono, _get_kaldi_fbank, _get_torchaudio_fbank\r\n)\r\n```\r\n\r\nThe func **\"_convert_to_mono\"** in fairseq/data/audio/audio_utils.py is deleted on 28 Jul:\r\n[Add speech/text joint training for speech to text task](https://github.com/pytorch/fairseq/commit/7ca95a66d64411deea49cb8710195ed2e0699f0a#diff-b9288c5137e5626007e93a4fe8893bd11e307c4c47fa5f3610cb7773db6f979c)\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nHope to update related references when updating a script.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3845/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3845/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3821", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3821/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3821/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3821/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3821", "id": 979801612, "node_id": "MDU6SXNzdWU5Nzk4MDE2MTI=", "number": 3821, "title": "Error when running fairseq-generate on Speech-to-text translation example", "user": {"login": "longdct2706", "id": 7610397, "node_id": "MDQ6VXNlcjc2MTAzOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/7610397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/longdct2706", "html_url": "https://github.com/longdct2706", "followers_url": "https://api.github.com/users/longdct2706/followers", "following_url": "https://api.github.com/users/longdct2706/following{/other_user}", "gists_url": "https://api.github.com/users/longdct2706/gists{/gist_id}", "starred_url": "https://api.github.com/users/longdct2706/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/longdct2706/subscriptions", "organizations_url": "https://api.github.com/users/longdct2706/orgs", "repos_url": "https://api.github.com/users/longdct2706/repos", "events_url": "https://api.github.com/users/longdct2706/events{/privacy}", "received_events_url": "https://api.github.com/users/longdct2706/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2021-08-26T03:44:32Z", "updated_at": "2021-09-16T03:17:50Z", "closed_at": "2021-09-16T03:17:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI got the following error when try to run [speech translation example on MuST-C dataset](https://github.com/pytorch/fairseq/blob/master/examples/speech_to_text/docs/mustc_example.md), in the inference step\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"miniconda3/envs/fairseq/bin/fairseq-generate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())\r\n  File \"fairseq/fairseq_cli/generate.py\", line 409, in cli_main\r\n    args = options.parse_args_and_arch(parser)\r\n  File \"fairseq/fairseq/options.py\", line 148, in parse_args_and_arch\r\n    TASK_REGISTRY[args.task].add_args(parser)\r\n  File \"fairseq/fairseq/tasks/speech_to_text.py\", line 39, in add_args\r\n    help=\"max number of tokens in the source sequence\",\r\n  File \"miniconda3/envs/fairseq/lib/python3.7/argparse.py\", line 1373, in add_argument\r\n    return self._add_action(action)\r\n  File \"miniconda3/envs/fairseq/lib/python3.7/argparse.py\", line 1736, in _add_action\r\n    self._optionals._add_action(action)\r\n  File \"miniconda3/envs/fairseq/lib/python3.7/argparse.py\", line 1577, in _add_action\r\n    action = super(_ArgumentGroup, self)._add_action(action)\r\n  File \"miniconda3/envs/fairseq/lib/python3.7/argparse.py\", line 1387, in _add_action\r\n    self._check_conflict(action)\r\n  File \"miniconda3/envs/fairseq/lib/python3.7/argparse.py\", line 1526, in _check_conflict\r\n    conflict_handler(action, confl_optionals)\r\n  File \"miniconda3/envs/fairseq/lib/python3.7/argparse.py\", line 1535, in _handle_conflict_error\r\n    raise ArgumentError(action, message % conflict_string)\r\nargparse.ArgumentError: argument --max-source-positions: conflicting option string: --max-source-positions\r\n```\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior\r\n\r\n```\r\nfairseq-train ${DATADIR} \\\r\n  --config-yaml config_st.yaml --train-subset train_st --valid-subset dev_st \\\r\n  --save-dir ${ST_SAVE_DIR} --num-workers 20 --max-tokens 40000 --max-update 100000 \\\r\n  --task speech_to_text --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --report-accuracy \\\r\n  --arch s2t_transformer_s --optimizer adam --lr 2e-3 --lr-scheduler inverse_sqrt \\\r\n  --warmup-updates 10000 --clip-norm 10.0 --seed 1 --update-freq 1 \\\r\n  --log-format simple --log-interval 100  --keep-last-epochs 10 \\\r\n  --skip-invalid-size-inputs-valid-test\r\n\r\nfairseq-generate ${DATADIR} \\\r\n  --config-yaml config_st.yaml --gen-subset tst-COMMON_st --task speech_to_text \\\r\n  --path ${ST_SAVE_DIR}/${CHECKPOINT_FILENAME} \\\r\n  --max-tokens 50000 --beam 5 --scoring sacrebleu\r\n ```\r\n \r\n `fairseq-train` command works normally, but when I run `fairseq-generate`, it outputs  the above error.\r\n\r\n### Expected behavior\r\n\r\n`fairseq-generate` perform inference on tst-COMMON set and output BLEU score\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.7.1\r\n - OS (e.g., Linux): Ubuntu 20.04\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.10\r\n - CUDA/cuDNN version: 11.0\r\n - GPU models and configuration: A100 GPU\r\n - Any other relevant information", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3821/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3821/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3809", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3809/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3809/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3809/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3809", "id": 975345520, "node_id": "MDU6SXNzdWU5NzUzNDU1MjA=", "number": 3809, "title": "ImportError of speech_to_text example", "user": {"login": "arabae", "id": 46676700, "node_id": "MDQ6VXNlcjQ2Njc2NzAw", "avatar_url": "https://avatars.githubusercontent.com/u/46676700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arabae", "html_url": "https://github.com/arabae", "followers_url": "https://api.github.com/users/arabae/followers", "following_url": "https://api.github.com/users/arabae/following{/other_user}", "gists_url": "https://api.github.com/users/arabae/gists{/gist_id}", "starred_url": "https://api.github.com/users/arabae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arabae/subscriptions", "organizations_url": "https://api.github.com/users/arabae/orgs", "repos_url": "https://api.github.com/users/arabae/repos", "events_url": "https://api.github.com/users/arabae/events{/privacy}", "received_events_url": "https://api.github.com/users/arabae/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-08-20T07:29:24Z", "updated_at": "2021-09-16T03:11:41Z", "closed_at": "2021-09-16T03:11:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nHi,\r\n\r\nRun cmd `python3 ./examples/speech_to_text/prep_librispeech_data.py --output-root ./data`\r\n\r\nDuring the example, the following error occurs when performing the prep_librispech_data.py of speech_to_text:\r\n![image](https://user-images.githubusercontent.com/46676700/130195303-6e822961-6bb1-4b01-82ae-db4935d8df7e.png)\r\n\r\nI think we should import \"**convert_to_mono**\" instead of \"_convert_to_mono\" to fairseq.data.audio.utils.\r\n\r\nhttps://github.com/pytorch/fairseq/blob/1f7ef9ed1e1061f8c7f88f8b94c7186834398690/examples/speech_to_text/data_utils.py#L17-L19\r\nhttps://github.com/pytorch/fairseq/blob/1f7ef9ed1e1061f8c7f88f8b94c7186834398690/examples/speech_to_text/data_utils.py#L81\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 1.0\r\n - PyTorch Version (e.g., 1.0): 1.9.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8.8\r\n - CUDA/cuDNN version: 11.2/8.1\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3809/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3806", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3806/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3806/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3806/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3806", "id": 974640074, "node_id": "MDU6SXNzdWU5NzQ2NDAwNzQ=", "number": 3806, "title": "--eval bleu metric used in fairseq-train raises AttributeError in sacrebleu module ", "user": {"login": "Aiden-Frost", "id": 43773124, "node_id": "MDQ6VXNlcjQzNzczMTI0", "avatar_url": "https://avatars.githubusercontent.com/u/43773124?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aiden-Frost", "html_url": "https://github.com/Aiden-Frost", "followers_url": "https://api.github.com/users/Aiden-Frost/followers", "following_url": "https://api.github.com/users/Aiden-Frost/following{/other_user}", "gists_url": "https://api.github.com/users/Aiden-Frost/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aiden-Frost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aiden-Frost/subscriptions", "organizations_url": "https://api.github.com/users/Aiden-Frost/orgs", "repos_url": "https://api.github.com/users/Aiden-Frost/repos", "events_url": "https://api.github.com/users/Aiden-Frost/events{/privacy}", "received_events_url": "https://api.github.com/users/Aiden-Frost/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-08-19T13:02:29Z", "updated_at": "2021-08-22T03:30:52Z", "closed_at": "2021-08-22T03:30:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen using the --eval bleu metric in fairseq-train, during validation it raises: `AttributeError: module 'sacrebleu' has no attribute 'compute_bleu'\r\n`\r\n### To Reproduce\r\n\r\n```\r\nfairseq-train final_bin/ \\\r\n--arch transformer \\\r\n--max-source-positions=200 \\\r\n--max-target-positions 200 \\\r\n--dropout 0.2 \\\r\n--encoder-embed-dim 768 \\\r\n--encoder-ffn-embed-dim 1536 \\\r\n--encoder-layers 4 \\\r\n--encoder-attention-heads 16 \\\r\n--encoder-learned-pos \\\r\n--decoder-embed-dim 768 \\\r\n--decoder-ffn-embed-dim 1536 \\\r\n--decoder-layers 4 \\\r\n--decoder-attention-heads 16 \\\r\n--decoder-learned-pos \\\r\n--optimizer adam \\\r\n--adam-betas \"(0.9,0.98)\" \\\r\n--clip-norm 0.0 \\\r\n--lr 5e-4 \\\r\n--warmup-init-lr 1e-07 \\\r\n--warmup-updates 4000 \\\r\n--batch-size 64 \\\r\n--seed 1 \\\r\n--save-dir model/ \\\r\n--wandb-project \"hermes\" \\\r\n--ddp-backend=no_c10d \\\r\n--skip-invalid-size-inputs-valid-test \\\r\n--update-freq 3 \\\r\n--lr-scheduler inverse_sqrt \\\r\n--criterion label_smoothed_cross_entropy \\\r\n--label-smoothing 0.1 \\\r\n--eval-bleu \\\r\n--eval-bleu-print-samples \\\r\n--best-checkpoint-metric bleu \r\n```\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run the above command on fairseq-preprocess data\r\n2. See error\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n### Error Stack Trace\r\n```\r\n/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\nTo keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\r\n  return torch.floor_divide(self, other)\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq_cli/train.py\", line 507, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq_cli/train.py\", line 180, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/opt/conda/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq_cli/train.py\", line 306, in train\r\n    cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq_cli/train.py\", line 392, in validate_and_save\r\n    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq_cli/train.py\", line 462, in validate\r\n    trainer.valid_step(sample)\r\n  File \"/opt/conda/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq/trainer.py\", line 993, in valid_step\r\n    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq/trainer.py\", line 1363, in _reduce_and_log_stats\r\n    logging_output = agg.get_smoothed_values()\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq/logging/meters.py\", line 304, in get_smoothed_values\r\n    for key in self.keys()\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq/logging/meters.py\", line 305, in <listcomp>\r\n    if not key.startswith(\"_\")\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq/logging/meters.py\", line 295, in get_smoothed_value\r\n    return meter.fn(self)\r\n  File \"/home/jupyter/kn-en/fairseq/fairseq/tasks/translation.py\", line 423, in compute_bleu\r\n    fn_sig = inspect.getfullargspec(sacrebleu.compute_bleu)[0]\r\nAttributeError: module 'sacrebleu' has no attribute 'compute_bleu'\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nWhen the --eval metric is set to loss (default) the model trains as expected.\r\n### Environment\r\n\r\n - fairseq Version - master\r\n - PyTorch Version - 1.9.0+cu111\r\n - OS (e.g., Linux) - Linux 18.0.4\r\n - How you installed fairseq - `git clone https://github.com/pytorch/fairseq`\r\n - Build command you used -  `pip install --editable ./`\r\n - Python version - 3.7.10\r\n - CUDA/cuDNN version - 11.0\r\n - GPU models and configuration - Tesla T4\r\n - Any other relevant information - Complete setup on GCP AI platform notebook.\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\nThis exact command used to work 2 months back in google colab.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3806/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3806/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3805", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3805/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3805/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3805/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3805", "id": 974625436, "node_id": "MDU6SXNzdWU5NzQ2MjU0MzY=", "number": 3805, "title": "Hello, So I have been training a transformer for Neural Machine Translation", "user": {"login": "Aiden-Frost", "id": 43773124, "node_id": "MDQ6VXNlcjQzNzczMTI0", "avatar_url": "https://avatars.githubusercontent.com/u/43773124?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aiden-Frost", "html_url": "https://github.com/Aiden-Frost", "followers_url": "https://api.github.com/users/Aiden-Frost/followers", "following_url": "https://api.github.com/users/Aiden-Frost/following{/other_user}", "gists_url": "https://api.github.com/users/Aiden-Frost/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aiden-Frost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aiden-Frost/subscriptions", "organizations_url": "https://api.github.com/users/Aiden-Frost/orgs", "repos_url": "https://api.github.com/users/Aiden-Frost/repos", "events_url": "https://api.github.com/users/Aiden-Frost/events{/privacy}", "received_events_url": "https://api.github.com/users/Aiden-Frost/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-08-19T12:45:38Z", "updated_at": "2021-08-19T12:46:05Z", "closed_at": "2021-08-19T12:46:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3805/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3805/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3761", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3761/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3761/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3761/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3761", "id": 960191718, "node_id": "MDU6SXNzdWU5NjAxOTE3MTg=", "number": 3761, "title": "Some bugs about model's architecture", "user": {"login": "trestad", "id": 55476033, "node_id": "MDQ6VXNlcjU1NDc2MDMz", "avatar_url": "https://avatars.githubusercontent.com/u/55476033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trestad", "html_url": "https://github.com/trestad", "followers_url": "https://api.github.com/users/trestad/followers", "following_url": "https://api.github.com/users/trestad/following{/other_user}", "gists_url": "https://api.github.com/users/trestad/gists{/gist_id}", "starred_url": "https://api.github.com/users/trestad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trestad/subscriptions", "organizations_url": "https://api.github.com/users/trestad/orgs", "repos_url": "https://api.github.com/users/trestad/repos", "events_url": "https://api.github.com/users/trestad/events{/privacy}", "received_events_url": "https://api.github.com/users/trestad/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-08-04T10:08:17Z", "updated_at": "2021-09-20T19:32:00Z", "closed_at": "2021-09-20T19:32:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nI find that when I use cmd \"git clone https://github.com/pytorch/fairseq\" to install fairseq, there are some bugs. For example, in  \r\n \u201ctransformer_iwslt_de_en\u201d,  the parameter \"--encoder/decoder-ffn-embed-dim\" should be 1024, but even though I have used \"-arch transformer_iwslt_de_en\" in the train command, I still got a model with architecture of \"base_architecture\" where \"--encoder-ffn-embed-dim\" is 2048. \r\nWhen I turn to the stable release, this bug never shows.\r\nI hope you can fix this.\r\n\r\n2. See error\r\n\r\n![image](https://user-images.githubusercontent.com/55476033/128163353-ae8084d1-3a7d-4d59-8ee3-7c2f0ba39595.png)\r\n\r\n![image](https://user-images.githubusercontent.com/55476033/128163478-045418b3-4f59-4eee-8c78-b67b036a02c5.png)\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3761/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3761/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3672", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3672/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3672/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3672/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3672", "id": 934524486, "node_id": "MDU6SXNzdWU5MzQ1MjQ0ODY=", "number": 3672, "title": "Compatibility with Hydra 1.1 release", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-01T07:48:34Z", "updated_at": "2021-07-01T13:45:12Z", "closed_at": "2021-07-01T13:45:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "Latest hydra 1.1 release made some BC breaking changes in APIs, namely they removed kwarg `strict` in `experimental` which was available in 1.0.6. https://github.com/facebookresearch/hydra/blob/v1.0.6/hydra/experimental/compose.py#L13 \r\n\r\nfairseq is using the old API so it's not working with the latest hydra release. One example to repro is\r\n```\r\nroberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\r\n```\r\nThis should be a simple fix but it's probably worth reporting to hydra about reducing future BC breaking changes. ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3672/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3672/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3671", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3671/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3671/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3671/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3671", "id": 933985627, "node_id": "MDU6SXNzdWU5MzM5ODU2Mjc=", "number": 3671, "title": "ImportError: cannot import name 'MaskedLanguagePairDataset'", "user": {"login": "abhijithneilabraham", "id": 35420019, "node_id": "MDQ6VXNlcjM1NDIwMDE5", "avatar_url": "https://avatars.githubusercontent.com/u/35420019?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhijithneilabraham", "html_url": "https://github.com/abhijithneilabraham", "followers_url": "https://api.github.com/users/abhijithneilabraham/followers", "following_url": "https://api.github.com/users/abhijithneilabraham/following{/other_user}", "gists_url": "https://api.github.com/users/abhijithneilabraham/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhijithneilabraham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhijithneilabraham/subscriptions", "organizations_url": "https://api.github.com/users/abhijithneilabraham/orgs", "repos_url": "https://api.github.com/users/abhijithneilabraham/repos", "events_url": "https://api.github.com/users/abhijithneilabraham/events{/privacy}", "received_events_url": "https://api.github.com/users/abhijithneilabraham/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-06-30T18:27:58Z", "updated_at": "2021-06-30T21:06:42Z", "closed_at": "2021-06-30T21:06:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I try to run \r\n```\r\nfrom fairseq.data import MaskedLanguagePairDataset \r\n```\r\nI get the following error:\r\n\r\n```\r\nImportError: cannot import name 'MaskedLanguagePairDataset'\r\n```\r\n\r\nI am running fairseq==0.8.0\r\nWhere has this been moved to?\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3671/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3671/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3669", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3669/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3669/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3669/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3669", "id": 933832636, "node_id": "MDU6SXNzdWU5MzM4MzI2MzY=", "number": 3669, "title": "w2l_decoder.py gives AssertionError on line 144 ", "user": {"login": "tensorfoo", "id": 53809493, "node_id": "MDQ6VXNlcjUzODA5NDkz", "avatar_url": "https://avatars.githubusercontent.com/u/53809493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tensorfoo", "html_url": "https://github.com/tensorfoo", "followers_url": "https://api.github.com/users/tensorfoo/followers", "following_url": "https://api.github.com/users/tensorfoo/following{/other_user}", "gists_url": "https://api.github.com/users/tensorfoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/tensorfoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tensorfoo/subscriptions", "organizations_url": "https://api.github.com/users/tensorfoo/orgs", "repos_url": "https://api.github.com/users/tensorfoo/repos", "events_url": "https://api.github.com/users/tensorfoo/events{/privacy}", "received_events_url": "https://api.github.com/users/tensorfoo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-06-30T15:28:53Z", "updated_at": "2021-06-30T19:17:42Z", "closed_at": "2021-06-30T19:17:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "The assertion in question:\r\n \r\n```\r\nfor i, (word, spellings) in enumerate(self.lexicon.items()):\r\n                word_idx = self.word_dict.get_index(word)\r\n                _, score = self.lm.score(start_state, word_idx)\r\n                for spelling in spellings:\r\n                    print(spelling)\r\n                    spelling_idxs = [tgt_dict.index(token) for token in spelling]\r\n                    assert (\r\n                        tgt_dict.unk() not in spelling_idxs\r\n                    ), f\"{spelling} {spelling_idxs}\"\r\n                    self.trie.insert(spelling_idxs, word_idx, score)\r\n            self.trie.smear(SmearingMode.MAX)\r\n```\r\n\r\nDoes anyone know what could be causing this? I have successfully trained a language model and manually inspected the arpa file and tested the binary and it seems to be fine. It seems it's going through the lexicon file, word by word, and it finds a word it doesn't like - then fails. Where should I start to begin debugging? I've been stuck for a few days. Viterbi decoding works well but i'd like to use kenlm decoding. ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3669/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3669/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3638", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3638/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3638/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3638/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3638", "id": 927950644, "node_id": "MDU6SXNzdWU5Mjc5NTA2NDQ=", "number": 3638, "title": "Runtime Errors when begins to pre-train wav2vec2.0 in virtual machine environment with multi GPUs", "user": {"login": "Megumu2597", "id": 60489903, "node_id": "MDQ6VXNlcjYwNDg5OTAz", "avatar_url": "https://avatars.githubusercontent.com/u/60489903?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Megumu2597", "html_url": "https://github.com/Megumu2597", "followers_url": "https://api.github.com/users/Megumu2597/followers", "following_url": "https://api.github.com/users/Megumu2597/following{/other_user}", "gists_url": "https://api.github.com/users/Megumu2597/gists{/gist_id}", "starred_url": "https://api.github.com/users/Megumu2597/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Megumu2597/subscriptions", "organizations_url": "https://api.github.com/users/Megumu2597/orgs", "repos_url": "https://api.github.com/users/Megumu2597/repos", "events_url": "https://api.github.com/users/Megumu2597/events{/privacy}", "received_events_url": "https://api.github.com/users/Megumu2597/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-06-23T07:33:13Z", "updated_at": "2021-07-08T16:22:53Z", "closed_at": "2021-07-08T16:22:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nHow can I get pretraining WAV2VEC2.0 running with virtual machine instances with 8GPUs (made by gcloud compute instances create) ?\r\n\r\n\r\n\r\n### To Reproduce\r\nWhen I run the command below, with configure file for 8GPUs, Runtime error occurs saying that \"please report this to the fairseq developers\" between loading training data and training.\r\n\r\nI have confirmed pretrining works well with 4 GPUs in my environments.\r\nThe only one different between 4GPU and 8GPU is \"distributed_world_size\" argument in config file. \r\n\r\n\r\nfairseq-hydra-train \\\r\n    task.data=/path/to/data \\\r\n    --config-dir /path/to/fairseq-py/examples/wav2vec/config/pretraining \\\r\n    --config-name wav2vec2_for_8gpu\r\n\r\nin config file (wac2vec2_for_8gpu.yaml),\r\ndataset:\r\n  num_workers: 6\r\n  max_tokens: 1200000\r\n  skip_invalid_size_inputs_valid_test: true\r\n  batch_size: 4\r\n\r\ndistributed_training:\r\n  distributed_world_size: 8\r\n  ddp_backend: no_c10d\r\n\r\nother arguments follow the sample config. (config/wav2vec2_large_librivox.yaml)\r\n\r\n###Error messge \r\n\r\n[2021-06-23 06:03:11,782][fairseq.trainer][INFO] - loading train data for epoch 19\r\n[2021-06-23 06:03:11,983][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 48745, skipped 13803 samples\r\nTraceback (most recent call last):\r\n  File \"../fairseq/fairseq_cli/hydra_train.py\", line 80, in <module>\r\n    cli_main()\r\n  File \"../fairseq/fairseq_cli/hydra_train.py\", line 76, in cli_main\r\n    hydra_main()\r\n  File \"/usr/local/lib/python3.6/dist-packages/hydra/main.py\", line 37, in decorated_main\r\n    strict=strict,\r\n  File \"/usr/local/lib/python3.6/dist-packages/hydra/_internal/utils.py\", line 347, in _run_hydra\r\n    lambda: hydra.run(\r\n  File \"/usr/local/lib/python3.6/dist-packages/hydra/_internal/utils.py\", line 201, in run_and_report\r\n    raise ex\r\n  File \"/usr/local/lib/python3.6/dist-packages/hydra/_internal/utils.py\", line 198, in run_and_report\r\n    return func()\r\n  File \"/usr/local/lib/python3.6/dist-packages/hydra/_internal/utils.py\", line 350, in <lambda>\r\n    overrides=args.overrides,\r\n  File \"/usr/local/lib/python3.6/dist-packages/hydra/_internal/hydra.py\", line 112, in run\r\n    configure_logging=with_log_configuration,\r\n  File \"/usr/local/lib/python3.6/dist-packages/hydra/core/utils.py\", line 127, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"../fairseq/fairseq_cli/hydra_train.py\", line 45, in hydra_main\r\n    distributed_utils.call_main(cfg, pre_main)\r\n  File \"/app/fairseq/fairseq/distributed/utils.py\", line 351, in call_main\r\n    join=True,\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 119, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/app/fairseq/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/app/fairseq/fairseq_cli/train.py\", line 157, in main\r\n    disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n  File \"/app/fairseq/fairseq/checkpoint_utils.py\", line 259, in load_checkpoint\r\n    epoch_itr.load_state_dict(itr_state)\r\n  File \"/app/fairseq/fairseq/data/iterators.py\", line 423, in load_state_dict\r\n    \"Cannot resume training due to dataloader mismatch, please \"\r\nRuntimeError: Cannot resume training due to dataloader mismatch, please report this to the fairseq developers. You can relaunch training with `--reset-dataloader` and it should work.\r\n\r\n### Expected behavior\r\n I have confirmed that it works with 4GPU environments, and will move on to begin learning like this.\r\n\r\n[2021-06-23 06:36:38,954][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 48745, skipped 13803 samples\r\n[2021-06-23 06:36:40,121][fairseq.trainer][INFO] - begin training epoch 19\r\n\r\n### Environment\r\n - fairseq Version  master:\r\n - torch==1.6.0+cu101 \r\n - torchvision==0.7.0+cu101\r\n - OS Linux:\r\n - CPU&GPU are made by Compute Engine virtual machine instances(gcloud compute instances create)\r\n \u30fbn1-standard-32\uff08vCPU x 32\u3001memory 120 GB\uff09\r\n \u30fb8 x NVIDIA Tesla V100(8GPU)\r\n\r\n### Additional context\r\n##Fixed problem\r\nBefore stacking on this error I faced another error (https://github.com/huggingface/transformers/issues/3660)\r\nI solve the problem by enhancing CPU from n1-standard-4(vCPU x 4\u3001memory 15 GB\uff09 to n1-standard-32\uff08vCPU x 32\u3001memory 120 GB\uff09.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3638/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3638/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3634", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3634/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3634/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3634/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3634", "id": 926125332, "node_id": "MDU6SXNzdWU5MjYxMjUzMzI=", "number": 3634, "title": "enum Choices can't be pickled in multi-GPU training", "user": {"login": "minghao-wu", "id": 17817832, "node_id": "MDQ6VXNlcjE3ODE3ODMy", "avatar_url": "https://avatars.githubusercontent.com/u/17817832?v=4", "gravatar_id": "", "url": "https://api.github.com/users/minghao-wu", "html_url": "https://github.com/minghao-wu", "followers_url": "https://api.github.com/users/minghao-wu/followers", "following_url": "https://api.github.com/users/minghao-wu/following{/other_user}", "gists_url": "https://api.github.com/users/minghao-wu/gists{/gist_id}", "starred_url": "https://api.github.com/users/minghao-wu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/minghao-wu/subscriptions", "organizations_url": "https://api.github.com/users/minghao-wu/orgs", "repos_url": "https://api.github.com/users/minghao-wu/repos", "events_url": "https://api.github.com/users/minghao-wu/events{/privacy}", "received_events_url": "https://api.github.com/users/minghao-wu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-06-21T11:36:05Z", "updated_at": "2021-07-01T06:11:31Z", "closed_at": "2021-06-26T12:12:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI am using fairseq for my new research project. I wrote my own task, dataset, criterion and related dataclasses.\r\n\r\n When running on single GPU, everything works very well. When running on multiple GPUs, an error is reported.\r\n```\r\n\r\n2021-06-21 19:20:06 | INFO | fairseq.trainer | begin training epoch 1\r\n2021-06-21 19:20:06 | INFO | fairseq_cli.train | Start iterating over samples\r\nTraceback (most recent call last):\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq_cli/train.py\", line 498, in <module>\r\n    cli_main()\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq_cli/train.py\", line 491, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq/distributed/utils.py\", line 351, in call_main\r\n    join=True,\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 188, in start_processes\r\n    while not context.join():\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 150, in join\r\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\r\ntorch.multiprocessing.spawn.ProcessRaisedException:\r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq_cli/train.py\", line 169, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq_cli/train.py\", line 275, in train\r\n    for i, samples in enumerate(progress):\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq/logging/progress_bar.py\", line 256, in __iter__\r\n    for i, obj in enumerate(self.iterable, start=self.n):\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq/data/iterators.py\", line 59, in __iter__\r\n    for x in self.iterable:\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq/data/iterators.py\", line 528, in _chunk_iterator\r\n    for x in itr:\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq/data/iterators.py\", line 59, in __iter__\r\n    for x in self.iterable:\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq/data/iterators.py\", line 650, in __next__\r\n    raise item\r\n  File \"/home/hiai-wmh/discnmt-project/discnmt/fairseq/fairseq/data/iterators.py\", line 581, in run\r\n    for item in self._source:\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 359, in __iter__\r\n    return self._get_iterator()\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 305, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 918, in __init__\r\n    w.start()\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/multiprocessing/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/multiprocessing/context.py\", line 223, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/home/hiai-wmh/anaconda3/envs/discnmt/lib/python3.7/multiprocessing/reduction.py\", line 65, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <enum 'Choices'>: attribute lookup Choices on fairseq.dataclass.constants failed\r\n\r\n```\r\n\r\n### To Reproduce\r\n\r\nI really have no clue on how to provide a code snippet to reproduce this error.\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI am trying to train my custom task with multiple GPUs.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.9.0\r\n - OS (e.g., Linux): Linux (Ubuntu 16.04.7 LTS)\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install --editable ./\r\n - Python version: Python 3.7.10\r\n - CUDA/cuDNN version: 10.2/No idea...\r\n - GPU models and configuration: Tesla V100\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\nSomething similar yet still different was reported at https://github.com/pytorch/fairseq/issues/3503 and https://github.com/pytorch/fairseq/issues/3482.\r\nThe solution given in https://github.com/pytorch/fairseq/issues/3503 and https://github.com/pytorch/fairseq/issues/3482 is to insert following snippt at line 460 in fairseq/fairseq/dataclass/utils.py and I replace `MyConfigClassWhichNeedsToBePickled` with my own dataclass. \r\nThis solution does not work in my case.\r\n```python3\r\ncfg = OmegaConf.merge(\r\n    OmegaConf.structured(\r\n         MyConfigClassWhichNeedsToBePickled\r\n    ),\r\n    OmegaConf.create(\r\n        OmegaConf.to_yaml(cfg, resolve=True)\r\n    )\r\n)\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3634/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3634/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3620", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3620/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3620/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3620/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3620", "id": 921836280, "node_id": "MDU6SXNzdWU5MjE4MzYyODA=", "number": 3620, "title": "HuBERT - cannot load fine-tuned checkpoints", "user": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-06-15T21:58:04Z", "updated_at": "2022-02-07T16:02:17Z", "closed_at": "2022-02-07T16:02:17Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nIt is currently not possible to load the fine-tuned weights of Hubert.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n```python\r\ncheckpoint_path=\"./hubert_large_ll60k_finetune_ls960.pt\"\r\nfairseq.checkpoint_utils.load_model_ensemble_and_task([checkpoint_path], strict=False)\r\n```\r\n\r\nAlso, see this [colab](https://colab.research.google.com/drive/1qKIhqj8Fcdi5LuFNdNBlqMkBYKlnNNXq?usp=sharing) for reproducibility.\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\nOne should be able to load the fine-tuned checkpoint.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3620/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3620/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3612", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3612/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3612/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3612/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3612", "id": 919695557, "node_id": "MDU6SXNzdWU5MTk2OTU1NTc=", "number": 3612, "title": "KeyError: 'speech_pretraining', when loading wav2vec pretrained model as in the instructions", "user": {"login": "aviasd", "id": 41818563, "node_id": "MDQ6VXNlcjQxODE4NTYz", "avatar_url": "https://avatars.githubusercontent.com/u/41818563?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aviasd", "html_url": "https://github.com/aviasd", "followers_url": "https://api.github.com/users/aviasd/followers", "following_url": "https://api.github.com/users/aviasd/following{/other_user}", "gists_url": "https://api.github.com/users/aviasd/gists{/gist_id}", "starred_url": "https://api.github.com/users/aviasd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aviasd/subscriptions", "organizations_url": "https://api.github.com/users/aviasd/orgs", "repos_url": "https://api.github.com/users/aviasd/repos", "events_url": "https://api.github.com/users/aviasd/events{/privacy}", "received_events_url": "https://api.github.com/users/aviasd/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-06-13T01:48:35Z", "updated_at": "2021-06-17T00:16:29Z", "closed_at": "2021-06-17T00:15:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI just run the code from the [instructions](https://github.com/pytorch/fairseq/blob/master/examples/wav2vec/README.md#wav2vec) in Google Colab:\r\n\r\n```\r\nimport torch\r\nimport fairseq\r\n\r\ncp_path = '/path/to/wav2vec.pt'\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\nmodel = model[0]\r\nmodel.eval()\r\n\r\nwav_input_16khz = torch.randn(1,10000)\r\nz = model.feature_extractor(wav_input_16khz)\r\nc = model.feature_aggregator(z)\r\n\r\n```\r\n\r\nAnd I got:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-3-617c5917eacb> in <module>()\r\n      3 \r\n      4 cp_path = '/content/wav2vec_large.pt'\r\n----> 5 model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\n      6 model = model[0]\r\n      7 model.eval()\r\n\r\n2 frames\r\n/usr/local/lib/python3.7/dist-packages/fairseq/checkpoint_utils.py in load_model_ensemble_and_task(filenames, arg_overrides, task, strict, suffix, num_shards)\r\n    277             if not PathManager.exists(filename):\r\n    278                 raise IOError(\"Model file not found: {}\".format(filename))\r\n--> 279             state = load_checkpoint_to_cpu(filename, arg_overrides)\r\n    280             if shard_idx == 0:\r\n    281                 args = state[\"args\"]\r\n\r\n/usr/local/lib/python3.7/dist-packages/fairseq/checkpoint_utils.py in load_checkpoint_to_cpu(path, arg_overrides)\r\n    230         for arg_name, arg_val in arg_overrides.items():\r\n    231             setattr(args, arg_name, arg_val)\r\n--> 232     state = _upgrade_state_dict(state)\r\n    233     return state\r\n    234 \r\n\r\n/usr/local/lib/python3.7/dist-packages/fairseq/checkpoint_utils.py in _upgrade_state_dict(state)\r\n    432 \r\n    433     # set any missing default values in the task, model or other registries\r\n--> 434     registry.set_defaults(state[\"args\"], tasks.TASK_REGISTRY[state[\"args\"].task])\r\n    435     registry.set_defaults(state[\"args\"], models.ARCH_MODEL_REGISTRY[state[\"args\"].arch])\r\n    436     for registry_name, REGISTRY in registry.REGISTRIES.items():\r\n\r\nKeyError: 'speech_pretraining'\r\n``` \r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Open Google Colab\r\n2. Run`!pip install --upgrade fairseq`\r\n3. Run`!pip install --upgrade torch`\r\n4. Run `!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_large.pt`\r\n5. Run \r\n```\r\nimport torch\r\nimport fairseq\r\n\r\ncp_path = '/content/wav2vec_large.pt'\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\n\r\n```\r\n6. Get error.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-5-617c5917eacb> in <module>()\r\n      3 \r\n      4 cp_path = '/content/wav2vec_large.pt'\r\n----> 5 model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\n      6 model = model[0]\r\n      7 model.eval()\r\n\r\n2 frames\r\n/usr/local/lib/python3.7/dist-packages/fairseq/checkpoint_utils.py in load_model_ensemble_and_task(filenames, arg_overrides, task, strict, suffix, num_shards)\r\n    277             if not PathManager.exists(filename):\r\n    278                 raise IOError(\"Model file not found: {}\".format(filename))\r\n--> 279             state = load_checkpoint_to_cpu(filename, arg_overrides)\r\n    280             if shard_idx == 0:\r\n    281                 args = state[\"args\"]\r\n\r\n/usr/local/lib/python3.7/dist-packages/fairseq/checkpoint_utils.py in load_checkpoint_to_cpu(path, arg_overrides)\r\n    230         for arg_name, arg_val in arg_overrides.items():\r\n    231             setattr(args, arg_name, arg_val)\r\n--> 232     state = _upgrade_state_dict(state)\r\n    233     return state\r\n    234 \r\n\r\n/usr/local/lib/python3.7/dist-packages/fairseq/checkpoint_utils.py in _upgrade_state_dict(state)\r\n    432 \r\n    433     # set any missing default values in the task, model or other registries\r\n--> 434     registry.set_defaults(state[\"args\"], tasks.TASK_REGISTRY[state[\"args\"].task])\r\n    435     registry.set_defaults(state[\"args\"], models.ARCH_MODEL_REGISTRY[state[\"args\"].arch])\r\n    436     for registry_name, REGISTRY in registry.REGISTRIES.items():\r\n\r\nKeyError: 'speech_pretraining'\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n\r\n```\r\n\r\n!pip install --upgrade fairseq &> /dev/null\r\n!pip install --upgrade torch &> /dev/null\r\n!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_large.pt\r\n\r\nimport torch\r\nimport fairseq\r\n\r\ncp_path = '/content/wav2vec_large.pt'\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nThe model should be loaded without a problem.\r\n\r\n### Environment\r\n\r\n - pip version: 19.3.1\r\n - fairseq Version (e.g., 1.0 or master): 0.10.2\r\n - PyTorch Version (e.g., 1.0): 1.8.1+cu101\r\n - OS (e.g., Linux): Ubuntu 18.04.5 LTS (Google Colab)\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version:  3.7.10\r\n - CUDA/cuDNN version: cuda_11.0_bu.TC445_37.28845127_0\r\n - GPU models and configuration: \r\n ```\r\nSun Jun 13 01:47:37 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   36C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3612/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3597", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3597/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3597/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3597/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3597", "id": 914746570, "node_id": "MDU6SXNzdWU5MTQ3NDY1NzA=", "number": 3597, "title": "remove obvious error messages when training with `--cpu`", "user": {"login": "stet-stet", "id": 25744069, "node_id": "MDQ6VXNlcjI1NzQ0MDY5", "avatar_url": "https://avatars.githubusercontent.com/u/25744069?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stet-stet", "html_url": "https://github.com/stet-stet", "followers_url": "https://api.github.com/users/stet-stet/followers", "following_url": "https://api.github.com/users/stet-stet/following{/other_user}", "gists_url": "https://api.github.com/users/stet-stet/gists{/gist_id}", "starred_url": "https://api.github.com/users/stet-stet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stet-stet/subscriptions", "organizations_url": "https://api.github.com/users/stet-stet/orgs", "repos_url": "https://api.github.com/users/stet-stet/repos", "events_url": "https://api.github.com/users/stet-stet/events{/privacy}", "received_events_url": "https://api.github.com/users/stet-stet/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-06-08T10:07:53Z", "updated_at": "2021-06-09T05:33:43Z", "closed_at": "2021-06-09T05:33:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI am not sure if this should be classified with a bug, but when training a `levenstein_transformer` with a CPU, I am faced with this abonimation of a log output!\r\n\r\n```\r\ncannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\ncannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\nepoch 024:  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                            | 4/38 [00:00<00:07,  4.56it/s]cannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\ncannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\nepoch 024:  13%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                           | 5/38 [00:01<00:06,  4.98it/s]cannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\ncannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\nepoch 024:  16%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                         | 6/38 [00:01<00:06,  5.33it/s]cannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\ncannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\nepoch 024:  18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                       | 7/38 [00:01<00:05,  5.51it/s]cannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\ncannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\nepoch 024:  21%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                     | 8/38 [00:01<00:05,  5.65it/s]cannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\ncannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\nepoch 024:  24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                    | 9/38 [00:01<00:05,  5.73it/s]cannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\ncannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version\r\n```\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n```\r\nfairseq-train ./distilled --save-dir checkpoints --ddp-backend=legacy_ddp --criterion=nat_loss --task translation_lev --arch levenshtein_transformer --noise random_delete --share-all-embeddings --optimizer adam --adam-betas '(0.9,0.98)'  --lr 0.01 --lr-scheduler inverse_sqrt  --dropout 0.2 --weight-decay 0.01  --encoder-layers 1 --decoder-layers 1  --encoder-embed-dim 64 --encoder-ffn-embed-dim 256 --decoder-embed-dim 64 --decoder-ffn-embed-dim 256  --decoder-attention-heads 4 --encoder-attention-heads 4  --max-tokens 4000 --cpu\r\n```\r\n\r\nthe `--cpu` flag seems to be the culprit, but again, by specifying `--cpu`, we already know that we will `not import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version`.\r\n\r\n\r\n\r\n#### Code sample\r\n(See above)\r\n\r\n### Expected behavior\r\nTo not print `cannot import name 'libnat_cuda' from 'fairseq' (/home/manymanymanythanks/fairseq/fairseq/__init__.py)... fall back to CPU version` when we are using the `--cpu` flag \r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.8.1+cu102\r\n - OS (e.g., Linux): Debian\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source): \r\n ```\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./\r\npython setup.py build_ext --inplace\r\n```\r\n - Python version: 3.9.5\r\n - CUDA/cuDNN version: none\r\n - GPU models and configuration: none\r\n - Any other relevant information: actually I don't think any of the information above is relevant...?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3597/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3574", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3574/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3574/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3574/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3574", "id": 903166531, "node_id": "MDU6SXNzdWU5MDMxNjY1MzE=", "number": 3574, "title": "'Wav2VecModel' object has no attribute 'cfg'", "user": {"login": "leo19941227", "id": 33196053, "node_id": "MDQ6VXNlcjMzMTk2MDUz", "avatar_url": "https://avatars.githubusercontent.com/u/33196053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leo19941227", "html_url": "https://github.com/leo19941227", "followers_url": "https://api.github.com/users/leo19941227/followers", "following_url": "https://api.github.com/users/leo19941227/following{/other_user}", "gists_url": "https://api.github.com/users/leo19941227/gists{/gist_id}", "starred_url": "https://api.github.com/users/leo19941227/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leo19941227/subscriptions", "organizations_url": "https://api.github.com/users/leo19941227/orgs", "repos_url": "https://api.github.com/users/leo19941227/repos", "events_url": "https://api.github.com/users/leo19941227/events{/privacy}", "received_events_url": "https://api.github.com/users/leo19941227/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-05-27T03:21:29Z", "updated_at": "2021-06-01T23:44:04Z", "closed_at": "2021-06-01T23:44:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen loading wav2vec and vq-wav2vec checkpoint with `fairseq.checkpoint_utils.load_model_ensemble_and_task`, the first line will result in error since there is no `cfg` attribute. Perhaps it is a typo of `actualized_cfg = getattr(model, \"cfg\", None)`? After adding the default None value, both wav2vec and vq-wav2vec can correctly load the checkpoint.\r\n\r\nhttps://github.com/pytorch/fairseq/blob/c8223e350cfc616bb47196151d1223683e483b6d/fairseq/tasks/audio_pretraining.py#L306-L309\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n```bash\r\n# 1. Download wav2vec checkpoint\r\nwget https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_large.pt\r\n\r\n# 2. Run the following code sample (the same as the one in examples/wav2vec/README.md)\r\npython3 load.py\r\n\r\n# 3. See error\r\nTraceback (most recent call last):\r\n  File \"load.py\", line 5, in <module>\r\n    model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\n  File \"/home/leo/d/Benchmarking/fairseq/fairseq/checkpoint_utils.py\", line 446, in load_model_ensemble_and_task\r\n    model = task.build_model(cfg.model)\r\n  File \"/home/leo/d/Benchmarking/fairseq/fairseq/tasks/audio_pretraining.py\", line 306, in build_model\r\n    actualized_cfg = getattr(model, \"cfg\")\r\n  File \"/home/leo/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 778, in __getattr__\r\n    raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\r\ntorch.nn.modules.module.ModuleAttributeError: 'Wav2VecModel' object has no attribute 'cfg'\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n```python\r\n# load.py\r\n\r\nimport torch\r\nimport fairseq\r\n\r\ncp_path = '/path/to/wav2vec.pt'\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\nmodel = model[0]\r\nmodel.eval()\r\n\r\nwav_input_16khz = torch.randn(1,10000)\r\nz = model.feature_extractor(wav_input_16khz)\r\nc = model.feature_aggregator(z)\r\n```\r\n\r\n### Expected behavior\r\n\r\nSuccessfully load the checkpoint.\r\n\r\n### Environment\r\n\r\n - fairseq Version: master at c8223e350cfc616bb47196151d1223683e483b6d\r\n - PyTorch Version: 1.7.0\r\n - OS: Linux\r\n - How you installed fairseq: `pip install -e ./`\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: 10.2\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3574/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3567", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3567/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3567/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3567/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3567", "id": 900626985, "node_id": "MDU6SXNzdWU5MDA2MjY5ODU=", "number": 3567, "title": "wav2vecU missing import", "user": {"login": "BirgerMoell", "id": 1704131, "node_id": "MDQ6VXNlcjE3MDQxMzE=", "avatar_url": "https://avatars.githubusercontent.com/u/1704131?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BirgerMoell", "html_url": "https://github.com/BirgerMoell", "followers_url": "https://api.github.com/users/BirgerMoell/followers", "following_url": "https://api.github.com/users/BirgerMoell/following{/other_user}", "gists_url": "https://api.github.com/users/BirgerMoell/gists{/gist_id}", "starred_url": "https://api.github.com/users/BirgerMoell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BirgerMoell/subscriptions", "organizations_url": "https://api.github.com/users/BirgerMoell/orgs", "repos_url": "https://api.github.com/users/BirgerMoell/repos", "events_url": "https://api.github.com/users/BirgerMoell/events{/privacy}", "received_events_url": "https://api.github.com/users/BirgerMoell/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-05-25T10:44:44Z", "updated_at": "2021-05-25T11:44:42Z", "closed_at": "2021-05-25T11:44:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nNo module named 'speechproc' when running wav2vecU vads.py script inside colab\r\n\r\nTraceback (most recent call last):\r\n  File \"scripts/vads.py\", line 10, in <module>\r\n    import speechproc\r\nModuleNotFoundError: No module named 'speechproc'\r\n\r\n### To Reproduce\r\nInside a colab that has cloned the repo and downloaded common voice.\r\nhttps://colab.research.google.com/drive/1JEHyCaeI5TKdOO9VEJ0RyRH_wCpjadPm?usp=sharing\r\n\r\n1. !python scripts/vads.py /content/cv-corpus-6.1-2020-12-11/sv-SE/train.tsv train.vads\r\n\r\n2. Traceback (most recent call last):\r\n  File \"scripts/vads.py\", line 10, in <module>\r\n    import speechproc\r\nModuleNotFoundError: No module named 'speechproc'\r\n\r\n\r\n### Expected behavior\r\nExpects the script to run and make a train.vads file\r\n\r\n### Environment\r\n\r\n - fairseq Version (master):\r\n - PyTorch Version (1.8.1+cu101)\r\n - OS (Linux, colab):\r\n - How you installed fairseq (source):\r\n - Build command you used (git clone):\r\n - Python version: 3.7.10\r\n - CUDA/cuDNN version: colab setup\r\n - GPU models and configuration: colab setup\r\n - Any other relevant information: colab setup\r\n\r\n### Additional context\r\nRunning inside google colab", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3567/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3558", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3558/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3558/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3558/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3558", "id": 893519620, "node_id": "MDU6SXNzdWU4OTM1MTk2MjA=", "number": 3558, "title": "Current commit leads to StringScalar error with wav2vec_manifest.py generated tsvs", "user": {"login": "olafthiele", "id": 30040326, "node_id": "MDQ6VXNlcjMwMDQwMzI2", "avatar_url": "https://avatars.githubusercontent.com/u/30040326?v=4", "gravatar_id": "", "url": "https://api.github.com/users/olafthiele", "html_url": "https://github.com/olafthiele", "followers_url": "https://api.github.com/users/olafthiele/followers", "following_url": "https://api.github.com/users/olafthiele/following{/other_user}", "gists_url": "https://api.github.com/users/olafthiele/gists{/gist_id}", "starred_url": "https://api.github.com/users/olafthiele/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/olafthiele/subscriptions", "organizations_url": "https://api.github.com/users/olafthiele/orgs", "repos_url": "https://api.github.com/users/olafthiele/repos", "events_url": "https://api.github.com/users/olafthiele/events{/privacy}", "received_events_url": "https://api.github.com/users/olafthiele/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634592838, "node_id": "MDU6TGFiZWwyNjM0NTkyODM4", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/wav2vec", "name": "wav2vec", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-05-17T16:49:38Z", "updated_at": "2021-05-22T05:15:48Z", "closed_at": "2021-05-22T05:15:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Current master shows following error for hydra train with @kahne's commit 8f1a34af7cb6e92fa3c443e61803e0ff347a784e \r\n\r\n> Traceback (most recent call last):\r\n>   File \"/home/ubuntu/wav2vec/fairseq/fairseq_cli/hydra_train.py\", line 45, in hydra_main\r\n>     distributed_utils.call_main(cfg, pre_main)\r\n>   File \"/home/ubuntu/wav2vec/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\r\n>     main(cfg, **kwargs)\r\n>   File \"/home/ubuntu/wav2vec/fairseq/fairseq_cli/train.py\", line 157, in main\r\n>     disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n>   File \"/home/ubuntu/wav2vec/fairseq/fairseq/checkpoint_utils.py\", line 238, in load_checkpoint\r\n>     epoch=1, load_dataset=True, **passthrough_args\r\n>   File \"/home/ubuntu/wav2vec/fairseq/fairseq/trainer.py\", line 578, in get_train_iterator\r\n>     self.reset_dummy_batch(batch_iterator.first_batch)\r\n>   File \"/home/ubuntu/wav2vec/fairseq/fairseq/data/iterators.py\", line 321, in first_batch\r\n>     return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])\r\n>   File \"/home/ubuntu/wav2vec/fairseq/fairseq/data/iterators.py\", line 321, in <listcomp>\r\n>     return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])\r\n>   File \"/home/ubuntu/wav2vec/fairseq/fairseq/data/audio/raw_audio_dataset.py\", line 302, in __getitem__\r\n>     path_or_fp = os.path.join(self.root_dir, self.fnames[index])\r\n>   File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/posixpath.py\", line 94, in join\r\n>     genericpath._check_arg_types('join', a, *p)\r\n>   File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/genericpath.py\", line 153, in _check_arg_types\r\n>     (funcname, s.__class__.__name__)) from None\r\n> TypeError: join() argument must be str or bytes, not 'StringScalar'\r\n\r\n\r\n\r\nMy guess is it's leaving out the string conversion in the latest raw_audio_dataset.py:\r\n\r\nOld:\r\n\r\n> fname = os.path.join(self.root_dir, str(self.fnames[index]))\r\n\r\nNew:\r\n\r\n> path_or_fp = os.path.join(self.root_dir, self.fnames[index])\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3558/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3528", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3528/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3528/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3528/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3528", "id": 874605607, "node_id": "MDU6SXNzdWU4NzQ2MDU2MDc=", "number": 3528, "title": "Getting error during Inference of wav2vec2.0 model", "user": {"login": "MrityunjoyS", "id": 30874320, "node_id": "MDQ6VXNlcjMwODc0MzIw", "avatar_url": "https://avatars.githubusercontent.com/u/30874320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MrityunjoyS", "html_url": "https://github.com/MrityunjoyS", "followers_url": "https://api.github.com/users/MrityunjoyS/followers", "following_url": "https://api.github.com/users/MrityunjoyS/following{/other_user}", "gists_url": "https://api.github.com/users/MrityunjoyS/gists{/gist_id}", "starred_url": "https://api.github.com/users/MrityunjoyS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MrityunjoyS/subscriptions", "organizations_url": "https://api.github.com/users/MrityunjoyS/orgs", "repos_url": "https://api.github.com/users/MrityunjoyS/repos", "events_url": "https://api.github.com/users/MrityunjoyS/events{/privacy}", "received_events_url": "https://api.github.com/users/MrityunjoyS/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-05-03T14:05:47Z", "updated_at": "2021-05-06T04:40:08Z", "closed_at": "2021-05-06T00:36:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I've trained my own model, with my own dataset using wav2vec2.0 . After that I'm trying to do inference using below command -\r\n`python3 examples/speech_recognition/infer.py /path_to_tsv/ --task audio_pretraining --nbest 1 --path /path/to/model/checkpoint_best.pt --gen-subset test --results-path /path/to/save/results/ --w2l-decoder viterbi --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --post-process letter`\r\n\r\nBut getting below error -\r\n\r\n> INFO:__main__:| decoding with criterion ctc\r\n> INFO:__main__:| loading model(s) from /root/model_and_dict/checkpoint_best.pt\r\n> Traceback (most recent call last):\r\n>   File \"examples/speech_recognition/infer.py\", line 427, in <module>\r\n>     cli_main()\r\n>   File \"examples/speech_recognition/infer.py\", line 423, in cli_main\r\n>     main(args)\r\n>   File \"examples/speech_recognition/infer.py\", line 239, in main\r\n>     task.load_dataset(args.gen_subset, task_cfg=saved_cfg.task)\r\n>   File \"/root/fairseq/fairseq/tasks/audio_pretraining.py\", line 202, in load_dataset\r\n>     if task_cfg.binarized_dataset:\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 305, in __getattr__\r\n>     self._format_and_raise(key=key, value=None, cause=e)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n>     type_override=type_override,\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 629, in format_and_raise\r\n>     _raise(ex, cause)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 610, in _raise\r\n>     raise ex  # set end OC_CAUSE=1 for full backtrace\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 303, in __getattr__\r\n>     return self._get_impl(key=key, default_value=DEFAULT_VALUE_MARKER)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 361, in _get_impl\r\n>     node = self._get_node(key=key)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 383, in _get_node\r\n>     self._validate_get(key)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 136, in _validate_get\r\n>     key=key, value=value, cause=ConfigAttributeError(msg)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n>     type_override=type_override,\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n>     _raise(ex, cause)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 610, in _raise\r\n>     raise ex  # set end OC_CAUSE=1 for full backtrace\r\n> omegaconf.errors.ConfigAttributeError: Key 'binarized_dataset' is not in struct\r\n> \tfull_key: task.binarized_dataset\r\n> \treference_type=Any\r\n> \tobject_type=dict\r\n\r\nCan you please have a look and if possible please point out what I might be doing wrong", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3528/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3511", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3511/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3511/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3511/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3511", "id": 867981239, "node_id": "MDU6SXNzdWU4Njc5ODEyMzk=", "number": 3511, "title": "wav2vec2 finetuning is broken due to missing parameters in config", "user": {"login": "harveenchadha", "id": 30959215, "node_id": "MDQ6VXNlcjMwOTU5MjE1", "avatar_url": "https://avatars.githubusercontent.com/u/30959215?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harveenchadha", "html_url": "https://github.com/harveenchadha", "followers_url": "https://api.github.com/users/harveenchadha/followers", "following_url": "https://api.github.com/users/harveenchadha/following{/other_user}", "gists_url": "https://api.github.com/users/harveenchadha/gists{/gist_id}", "starred_url": "https://api.github.com/users/harveenchadha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harveenchadha/subscriptions", "organizations_url": "https://api.github.com/users/harveenchadha/orgs", "repos_url": "https://api.github.com/users/harveenchadha/repos", "events_url": "https://api.github.com/users/harveenchadha/events{/privacy}", "received_events_url": "https://api.github.com/users/harveenchadha/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-04-26T18:06:25Z", "updated_at": "2021-04-26T19:37:11Z", "closed_at": "2021-04-26T19:37:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I am trying to train on new fairseq version where I encountered this error.\r\n\r\nThere are fields that are missing in fairseq config provided for finetuning.\r\n\r\nFor example:\r\n\r\nIn common section of [config](https://github.com/pytorch/fairseq/blob/master/examples/wav2vec/config/finetuning/base_10h.yaml) we have:\r\n\r\n```\r\ncommon:\r\n    fp16: true\r\n    log_format: json\r\n    log_interval: 200\r\n```\r\n\r\nbut when I start finetuning it keeps on saying parameters are missing. So I try to add parameters one by one and the final config for common section looks like:\r\n\r\n```\r\ncommon:\r\n    no_progress_bar: false\r\n    log_interval: 500\r\n    log_format: json\r\n    tensorboard_logdir: \r\n    wandb_project: \r\n    azureml_logging: false\r\n    seed: 2337\r\n    cpu: false\r\n    tpu: false\r\n    bf16: false\r\n    memory_efficient_bf16: false\r\n    fp16: true\r\n    memory_efficient_fp16: false\r\n    fp16_no_flatten_grads: false\r\n    fp16_init_scale: 128\r\n    fp16_scale_window: \r\n    fp16_scale_tolerance: 0.0\r\n    min_loss_scale: 0.0001\r\n    threshold_loss_scale: \r\n    user_dir: \r\n    empty_cache_freq: 0\r\n    all_gather_list_size: 16384\r\n    model_parallel_size: 1\r\n    quantization_config_path: \r\n    profile: false\r\n    reset_logging: false\r\n    suppress_crashes: false\r\n    use_plasma_view: false\r\n    plasma_path: '/tmp/plasma'\r\n```\r\n\r\nI want to understand why is this so. Why does all the unspecified parameters initialize themselves.\r\n\r\nSimilar process I need to do for all the other parts in the config.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3511/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3510", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3510/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3510/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3510/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3510", "id": 867942522, "node_id": "MDU6SXNzdWU4Njc5NDI1MjI=", "number": 3510, "title": "Incorrect bpecodes file included with pre-trained WMT19 EN-RU translation model", "user": {"login": "demelin", "id": 26745767, "node_id": "MDQ6VXNlcjI2NzQ1NzY3", "avatar_url": "https://avatars.githubusercontent.com/u/26745767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/demelin", "html_url": "https://github.com/demelin", "followers_url": "https://api.github.com/users/demelin/followers", "following_url": "https://api.github.com/users/demelin/following{/other_user}", "gists_url": "https://api.github.com/users/demelin/gists{/gist_id}", "starred_url": "https://api.github.com/users/demelin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/demelin/subscriptions", "organizations_url": "https://api.github.com/users/demelin/orgs", "repos_url": "https://api.github.com/users/demelin/repos", "events_url": "https://api.github.com/users/demelin/events{/privacy}", "received_events_url": "https://api.github.com/users/demelin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-04-26T17:20:59Z", "updated_at": "2021-04-29T12:29:18Z", "closed_at": "2021-04-29T12:29:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThe `bpecodes` file included in the tar.gz archive of the pre-trained `transformer.wmt19.en-ru` model seems to be a copy of the one included with the `transformer.wmt19.en-de` model. As such, it segments Russian sentences incorrectly (usually close to producing character-level segmentations). The one provided with the `transformer.wmt19.ru-en` model, on the other hand, appears to be correct. Thanks for looking into this!\r\n\r\n### To Reproduce\r\n\r\nNot applicable.\r\n\r\n#### Code sample\r\n\r\nNot applicable.\r\n\r\n### Environment\r\n\r\nNot applicable.\r\n\r\n### Additional context\r\n\r\nNone.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3510/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3510/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3491", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3491/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3491/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3491/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3491", "id": 861347606, "node_id": "MDU6SXNzdWU4NjEzNDc2MDY=", "number": 3491, "title": "`fairseq-train --help` raises error", "user": {"login": "louismartin", "id": 12654189, "node_id": "MDQ6VXNlcjEyNjU0MTg5", "avatar_url": "https://avatars.githubusercontent.com/u/12654189?v=4", "gravatar_id": "", "url": "https://api.github.com/users/louismartin", "html_url": "https://github.com/louismartin", "followers_url": "https://api.github.com/users/louismartin/followers", "following_url": "https://api.github.com/users/louismartin/following{/other_user}", "gists_url": "https://api.github.com/users/louismartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/louismartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/louismartin/subscriptions", "organizations_url": "https://api.github.com/users/louismartin/orgs", "repos_url": "https://api.github.com/users/louismartin/repos", "events_url": "https://api.github.com/users/louismartin/events{/privacy}", "received_events_url": "https://api.github.com/users/louismartin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-04-19T14:05:32Z", "updated_at": "2021-04-28T12:57:23Z", "closed_at": "2021-04-28T12:57:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "`fairseq-train --help` raises error\r\n\r\nI'm not sure if it's a bug on my install or if someone else can reproduce.\r\nI installed fairseq from source using the latest version as of 19/04/2021.\r\n\r\n```\r\n(dev3.7) ~/tmp/camembert-base$ fairseq-train --help\r\nWARNING:root:Torch AMP is not available on this platform\r\nTraceback (most recent call last):\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/private/home/louismartin/dev/ext/fairseq/fairseq_cli/train.py\", line 478, in cli_main\r\n    args = options.parse_args_and_arch(parser, modify_parser=modify_parser)\r\n  File \"/private/home/louismartin/dev/ext/fairseq/fairseq/options.py\", line 128, in parse_args_and_arch\r\n    args, _ = parser.parse_known_args(input_args)\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 1781, in parse_known_args\r\n    namespace, args = self._parse_known_args(args, namespace)\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 1987, in _parse_known_args\r\n    start_index = consume_optional(start_index)\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 1927, in consume_optional\r\n    take_action(action, args, option_string)\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 1855, in take_action\r\n    action(self, namespace, argument_values, option_string)\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 1037, in __call__\r\n    parser.print_help()\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 2474, in print_help\r\n    self._print_message(self.format_help(), file)\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 2458, in format_help\r\n    return formatter.format_help()\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 284, in format_help\r\n    help = self._root_section.format_help()\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 215, in format_help\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 215, in <listcomp>\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 215, in format_help\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 215, in <listcomp>\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 525, in _format_action\r\n    help_text = self._expand_help(action)\r\n  File \"/private/home/louismartin/miniconda3/envs/dev3.7/lib/python3.7/argparse.py\", line 614, in _expand_help\r\n    return self._get_help_string(action) % params\r\nValueError: unsupported format character 'k' (0x6b) at index 95\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3491/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3491/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3475", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3475/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3475/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3475/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3475", "id": 857066517, "node_id": "MDU6SXNzdWU4NTcwNjY1MTc=", "number": 3475, "title": "[Deleted]", "user": {"login": "eil", "id": 45141520, "node_id": "MDQ6VXNlcjQ1MTQxNTIw", "avatar_url": "https://avatars.githubusercontent.com/u/45141520?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eil", "html_url": "https://github.com/eil", "followers_url": "https://api.github.com/users/eil/followers", "following_url": "https://api.github.com/users/eil/following{/other_user}", "gists_url": "https://api.github.com/users/eil/gists{/gist_id}", "starred_url": "https://api.github.com/users/eil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eil/subscriptions", "organizations_url": "https://api.github.com/users/eil/orgs", "repos_url": "https://api.github.com/users/eil/repos", "events_url": "https://api.github.com/users/eil/events{/privacy}", "received_events_url": "https://api.github.com/users/eil/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-04-13T15:17:58Z", "updated_at": "2023-01-03T14:43:23Z", "closed_at": "2022-04-09T09:21:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "[Deleted]", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3475/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3469", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3469/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3469/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3469/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3469", "id": 856574673, "node_id": "MDU6SXNzdWU4NTY1NzQ2NzM=", "number": 3469, "title": "Non-autoregressive models miss prev_output_tokens argument", "user": {"login": "speedcell4", "id": 3585459, "node_id": "MDQ6VXNlcjM1ODU0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/3585459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/speedcell4", "html_url": "https://github.com/speedcell4", "followers_url": "https://api.github.com/users/speedcell4/followers", "following_url": "https://api.github.com/users/speedcell4/following{/other_user}", "gists_url": "https://api.github.com/users/speedcell4/gists{/gist_id}", "starred_url": "https://api.github.com/users/speedcell4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/speedcell4/subscriptions", "organizations_url": "https://api.github.com/users/speedcell4/orgs", "repos_url": "https://api.github.com/users/speedcell4/repos", "events_url": "https://api.github.com/users/speedcell4/events{/privacy}", "received_events_url": "https://api.github.com/users/speedcell4/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-04-13T05:10:30Z", "updated_at": "2021-04-13T05:14:52Z", "closed_at": "2021-04-13T05:14:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nI got this error when I try to evaluate a trained non-autoregressive MT model.\r\n\r\n```bash\r\n\u279c  datasets fairseq-generate data-bin/wmt14_en_de_distill --path checkpoints/crf-nat/210413-135213-05d984c5/checkpoint1.pt\r\n2021-04-13 14:06:01 | INFO | fairseq_cli.generate | Namespace(all_gather_list_size=16384, batch_size=None, batch_size_valid=None, beam=5, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='cross_entropy', curriculum=0, data='data-bin/wmt14_en_de_distill', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, left_pad_source='True', left_pad_target='False', lenpen=1, lm_path=None, lm_weight=0.0, load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_source_positions=1024, max_target_positions=1024, max_tokens=12000, max_tokens_valid=None, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, no_seed_provided=False, nprocs_per_node=8, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer=None, path='checkpoints/crf-nat/210413-135213-05d984c5/checkpoint1.pt', pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, scoring='bleu', seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, zero_sharding='none')\r\n2021-04-13 14:06:01 | INFO | fairseq.tasks.translation | [en] dictionary: 39840 types\r\n2021-04-13 14:06:01 | INFO | fairseq.tasks.translation | [de] dictionary: 39840 types\r\n2021-04-13 14:06:01 | INFO | fairseq.data.data_utils | loaded 3003 examples from: data-bin/wmt14_en_de_distill/test.en-de.en\r\n2021-04-13 14:06:02 | INFO | fairseq.data.data_utils | loaded 3003 examples from: data-bin/wmt14_en_de_distill/test.en-de.de\r\n2021-04-13 14:06:02 | INFO | fairseq.tasks.translation | data-bin/wmt14_en_de_distill test en-de 3003 examples\r\n2021-04-13 14:06:02 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/crf-nat/210413-135213-05d984c5/checkpoint1.pt\r\nTraceback (most recent call last):\r\n  File \"~/miniconda3/bin/fairseq-generate\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"~/miniconda3/lib/python3.8/site-packages/fairseq_cli/generate.py\", line 379, in cli_main\r\n    main(args)\r\n  File \"~/miniconda3/lib/python3.8/site-packages/fairseq_cli/generate.py\", line 41, in main\r\n    return _main(args, sys.stdout)\r\n  File \"~/miniconda3/lib/python3.8/site-packages/fairseq_cli/generate.py\", line 191, in _main\r\n    hypos = task.inference_step(\r\n  File \"~/miniconda3/lib/python3.8/site-packages/fairseq/tasks/fairseq_task.py\", line 433, in inference_step\r\n    return generator.generate(\r\n  File \"~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"~/miniconda3/lib/python3.8/site-packages/fairseq/sequence_generator.py\", line 177, in generate\r\n    return self._generate(sample, **kwargs)\r\n  File \"~/miniconda3/lib/python3.8/site-packages/fairseq/sequence_generator.py\", line 312, in _generate\r\n    lprobs, avg_attn_scores = self.model.forward_decoder(\r\n  File \"~/miniconda3/lib/python3.8/site-packages/fairseq/sequence_generator.py\", line 824, in forward_decoder\r\n    decoder_out = model.decoder.forward(\r\n  File \"~/miniconda3/lib/python3.8/site-packages/fairseq/models/nat/fairseq_nat_model.py\", line 40, in wrapper\r\n    return func(\r\nTypeError: forward() missing 1 required positional argument: 'prev_output_tokens'\r\n```\r\n\r\nthe training command is as following,\r\n\r\n```bash\r\n$HOME/miniconda3/bin/fairseq-train \\\r\n    $HOME/datasets/data-bin/wmt14_en_de_distill \\\r\n    --save-dir $HOME/datasets/checkpoints/$STUDY/$TRIAL \\\r\n    --ddp-backend=c10d \\\r\n    --task translation_lev \\\r\n    --criterion nat_loss \\\r\n    --arch nacrf_transformer \\\r\n    --noise full_mask \\\r\n    --optimizer adam --adam-betas '(0.9,0.98)' \\\r\n    --lr 0.0005 --lr-scheduler inverse_sqrt \\\r\n    --min-lr '1e-09' --warmup-updates 10000 \\\r\n    --warmup-init-lr '1e-07' --label-smoothing 0.1 \\\r\n    --dropout 0.3 --weight-decay 0.01 \\\r\n    --decoder-learned-pos \\\r\n    --encoder-learned-pos \\\r\n    --pred-length-offset \\\r\n    --length-loss-factor 0.1 \\\r\n    --word-ins-loss-factor 0.5 \\\r\n    --crf-lowrank-approx 32 \\\r\n    --crf-beam-approx 64 \\\r\n    --apply-bert-init \\\r\n    --log-format 'simple' --log-interval 100 \\\r\n    --fixed-validation-seed 7 \\\r\n    --max-tokens 8000 \\\r\n    --save-interval-updates 10000 \\\r\n    --max-update 300000 \\\r\n    --fp16 \\\r\n    --share-all-embeddings &> $HOME/datasets/checkpoints/$STUDY/$TRIAL/log.txt\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.10.2\r\n - PyTorch Version (e.g., 1.0) 1.8.1\r\n - OS (e.g., Linux): Ubuntu\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3469/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3466", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3466/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3466/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3466/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3466", "id": 855811683, "node_id": "MDU6SXNzdWU4NTU4MTE2ODM=", "number": 3466, "title": "Wrong Final Dimension size for wav2vec2 large tpu config", "user": {"login": "harveenchadha", "id": 30959215, "node_id": "MDQ6VXNlcjMwOTU5MjE1", "avatar_url": "https://avatars.githubusercontent.com/u/30959215?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harveenchadha", "html_url": "https://github.com/harveenchadha", "followers_url": "https://api.github.com/users/harveenchadha/followers", "following_url": "https://api.github.com/users/harveenchadha/following{/other_user}", "gists_url": "https://api.github.com/users/harveenchadha/gists{/gist_id}", "starred_url": "https://api.github.com/users/harveenchadha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harveenchadha/subscriptions", "organizations_url": "https://api.github.com/users/harveenchadha/orgs", "repos_url": "https://api.github.com/users/harveenchadha/repos", "events_url": "https://api.github.com/users/harveenchadha/events{/privacy}", "received_events_url": "https://api.github.com/users/harveenchadha/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-04-12T10:15:21Z", "updated_at": "2021-05-04T06:19:15Z", "closed_at": "2021-05-04T06:19:15Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi,\r\n\r\nI was trying to run wav2vec2 on TPU when I found a strange thing:\r\n\r\n<img width=\"280\" alt=\"Screenshot 2021-04-12 at 3 41 35 PM\" src=\"https://user-images.githubusercontent.com/30959215/114378671-92383b80-9ba5-11eb-80d1-711b322d466a.png\">\r\n\r\nIn the config file [here](https://github.com/pytorch/fairseq/blob/master/examples/wav2vec/config/pretraining/wav2vec2_large_librivox_tpu-pod.yaml) The final dimension layer is of size 256 which is not the case with wav2vec2_large.\r\n\r\nThere can be two potential issues:\r\n\r\n1. Either the name of the config is wrong i.e. wav2vec2_large_librivox_tpu-pod should be wav2vec2_base_librivox_tpu-pod\r\n\r\n2. Or the the final dimension layer should be of size 768\r\n\r\nThrough my inspection I think it should be case 1. ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3466/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3454", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3454/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3454/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3454/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3454", "id": 852190002, "node_id": "MDU6SXNzdWU4NTIxOTAwMDI=", "number": 3454, "title": "WMT en-de data preprocessing bug", "user": {"login": "YuxianMeng", "id": 11677047, "node_id": "MDQ6VXNlcjExNjc3MDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/11677047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YuxianMeng", "html_url": "https://github.com/YuxianMeng", "followers_url": "https://api.github.com/users/YuxianMeng/followers", "following_url": "https://api.github.com/users/YuxianMeng/following{/other_user}", "gists_url": "https://api.github.com/users/YuxianMeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/YuxianMeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YuxianMeng/subscriptions", "organizations_url": "https://api.github.com/users/YuxianMeng/orgs", "repos_url": "https://api.github.com/users/YuxianMeng/repos", "events_url": "https://api.github.com/users/YuxianMeng/events{/privacy}", "received_events_url": "https://api.github.com/users/YuxianMeng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-04-07T09:03:59Z", "updated_at": "2021-04-07T09:43:34Z", "closed_at": "2021-04-07T09:43:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, thanks for providing scripts to reproduce wmt en-de translation task!\r\n\r\nI followed [your scripts](https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-wmt14en2de.sh) to preprocess wmt data and train a transformer model. However, during decoding, I found that `\u201e`  does not appear in dictionay, thus being indexed and translated to `unk`. \r\n\r\nI guess the reason is that you normalize punctuations (transform \"\u201e\" to \"&quot\") only in training data, but did not do it in test data. \r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3454/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3451", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3451/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3451/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3451/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3451", "id": 851486506, "node_id": "MDU6SXNzdWU4NTE0ODY1MDY=", "number": 3451, "title": "Wav2vec 2.0 fine-tuning failure", "user": {"login": "petrpavlov", "id": 32638714, "node_id": "MDQ6VXNlcjMyNjM4NzE0", "avatar_url": "https://avatars.githubusercontent.com/u/32638714?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petrpavlov", "html_url": "https://github.com/petrpavlov", "followers_url": "https://api.github.com/users/petrpavlov/followers", "following_url": "https://api.github.com/users/petrpavlov/following{/other_user}", "gists_url": "https://api.github.com/users/petrpavlov/gists{/gist_id}", "starred_url": "https://api.github.com/users/petrpavlov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petrpavlov/subscriptions", "organizations_url": "https://api.github.com/users/petrpavlov/orgs", "repos_url": "https://api.github.com/users/petrpavlov/repos", "events_url": "https://api.github.com/users/petrpavlov/events{/privacy}", "received_events_url": "https://api.github.com/users/petrpavlov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-04-06T14:29:19Z", "updated_at": "2021-06-24T14:07:13Z", "closed_at": "2021-04-06T14:47:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI'm trying to fine-tune the pre-trained wav2vec 2.0 model using `fairseq-hydra-train` (according to [the guide](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#fine-tune-a-pre-trained-model-with-ctc) and getting a RuntimeError which says that DistributedDataParallel wasn't properly initialized. \r\n\r\n\r\n### To Reproduce\r\n\r\n1. Run cmd\r\n\r\n```\r\nfairseq-hydra-train \\\r\n    distributed_training.distributed_port=12345 \\\r\n    task.data=/home/ppavlov/wav2vec \\\r\n    model.w2v_path=/home/ppavlov/wav2vec/wav2vec_small.pt \\\r\n    distributed_training.distributed_world_size=2 \\\r\n    +optimization.update_freq='[12]' \\\r\n    --config-dir fairseq/examples/wav2vec/config/finetuning \\\r\n    --config-name base_1h\r\n```\r\n\r\n2. See error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ppavlov/wav2vec/fairseq/fairseq_cli/hydra_train.py\", line 45, in hydra_main\r\n    distributed_utils.call_main(cfg, pre_main)\r\n  File \"/home/ppavlov/wav2vec/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/ppavlov/wav2vec/fairseq/fairseq_cli/train.py\", line 128, in main\r\n    trainer = Trainer(cfg, task, model, criterion, quantizer)\r\n  File \"/home/ppavlov/wav2vec/fairseq/fairseq/trainer.py\", line 144, in __init__\r\n    if self.data_parallel_rank == 0:\r\n  File \"/home/ppavlov/wav2vec/fairseq/fairseq/trainer.py\", line 177, in data_parallel_rank\r\n    return distributed_utils.get_data_parallel_rank()\r\n  File \"/home/ppavlov/wav2vec/fairseq/fairseq/distributed/utils.py\", line 463, in get_data_parallel_rank\r\n    return get_rank(get_data_parallel_group())\r\n  File \"/home/ppavlov/wav2vec/fairseq/fairseq/distributed/utils.py\", line 405, in get_rank\r\n    return dist.get_rank(group=group)\r\n  File \"/home/ppavlov/wav2vec/venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 688, in get_rank\r\n    default_pg = _get_default_group()\r\n  File \"/home/ppavlov/wav2vec/venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 347, in _get_default_group\r\n    raise RuntimeError(\"Default process group has not been initialized, \"\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version: master\r\n - PyTorch Version: 1.8.1\r\n - OS: Ubuntu 18.04\r\n - How you installed fairseq: pip install --editable .\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 11.2\r\n - GPU models and configuration: 2xTesla T4\r\n\r\n### Working directory content\r\n\r\n- data/  \r\n- dev_other.ltr \r\n- dev_other.tsv  \r\n- dev_other.wrd  \r\n- dict.ltr.txt  \r\n- fairseq/  \r\n- outputs/  \r\n- train.ltr  \r\n- train.tsv  \r\n- train.wrd  \r\n- venv/  \r\n- wav2vec_small.pt\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3451/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3445", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3445/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3445/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3445/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3445", "id": 850207152, "node_id": "MDU6SXNzdWU4NTAyMDcxNTI=", "number": 3445, "title": "Fails to evaluate wait-k model on SimulEval if sequence is shorter than k", "user": {"login": "fury00812", "id": 35480446, "node_id": "MDQ6VXNlcjM1NDgwNDQ2", "avatar_url": "https://avatars.githubusercontent.com/u/35480446?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fury00812", "html_url": "https://github.com/fury00812", "followers_url": "https://api.github.com/users/fury00812/followers", "following_url": "https://api.github.com/users/fury00812/following{/other_user}", "gists_url": "https://api.github.com/users/fury00812/gists{/gist_id}", "starred_url": "https://api.github.com/users/fury00812/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fury00812/subscriptions", "organizations_url": "https://api.github.com/users/fury00812/orgs", "repos_url": "https://api.github.com/users/fury00812/repos", "events_url": "https://api.github.com/users/fury00812/events{/privacy}", "received_events_url": "https://api.github.com/users/fury00812/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-04-05T08:49:23Z", "updated_at": "2021-04-18T08:30:21Z", "closed_at": "2021-04-11T05:27:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've got an error while evaluating the wait-k model following the instruction of [README.md](https://github.com/pytorch/fairseq/blob/master/examples/simultaneous_translation/docs/enja-waitk.md).\r\n\r\n```\r\n2021-04-03 10:36:28 | INFO     | simuleval.scorer | Evaluating on text\r\n2021-04-03 10:36:28 | INFO     | simuleval.scorer | Source: /userdir/iwslt17.test0.en\r\n2021-04-03 10:36:28 | INFO     | simuleval.scorer | Target: /userdir/iwslt17.test0.ja\r\n2021-04-03 10:36:28 | INFO     | simuleval.scorer | Number of sentences: 1549\r\n2021-04-03 10:36:28 | INFO     | simuleval.server | Evaluation Server Started (process id 132622). Listening to port 12321\r\n2021-04-03 10:36:31 | WARNING  | simuleval.scorer | Resetting scorer\r\n2021-04-03 10:36:31 | INFO     | simuleval.cli    | Output dir: /userdir/iwslt17.test0\r\n2021-04-03 10:36:31 | INFO     | simuleval.cli    | Start data writer (process id 132639)\r\n2021-04-03 10:36:31 | INFO     | simuleval.cli    | Evaluating SimulTransTextAgentJA (process id 132556) on instances from 0 to 1548\r\n2021-04-03 10:36:31 | INFO     | fairseq.tasks.translation | [en] dictionary: 16004 types\r\n2021-04-03 10:36:31 | INFO     | fairseq.tasks.translation | [ja] dictionary: 16004 types\r\nTraceback (most recent call last):\r\n  File \"/userdir/.venv/bin/simuleval\", line 11, in <module>\r\n    load_entry_point('simuleval', 'console_scripts', 'simuleval')()\r\n  File \"/userdir/SimulEval/simuleval/cli.py\", line 165, in main\r\n    _main(args.client_only)\r\n  File \"/userdir/SimulEval/simuleval/cli.py\", line 192, in _main\r\n    evaluate(args, client, server_process)\r\n  File \"/userdir/SimulEval/simuleval/cli.py\", line 145, in evaluate\r\n    decode(args, client, result_queue, indices)\r\n  File \"/userdir/SimulEval/simuleval/cli.py\", line 108, in decode\r\n    action = agent.policy(states)\r\n  File \"/userdir/fairseq-v0.10.2/examples/simultaneous_translation/eval/agents/simul_t2t_enja.py\", line 196, in policy\r\n    x, outputs = self.model.decoder.forward(\r\n  File \"/userdir/fairseq-v0.10.2/fairseq/models/transformer.py\", line 817, in forward\r\n    x, extra = self.extract_features(\r\n  File \"/userdir/fairseq-v0.10.2/examples/simultaneous_translation/models/transformer_monotonic_attention.py\", line 219, in extract_features\r\n    x, attn, _ = layer(\r\n  File \"/userdir/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/userdir/fairseq-v0.10.2/examples/simultaneous_translation/modules/monotonic_transformer_layer.py\", line 160, in forward\r\n    x, attn = self.encoder_attn(\r\n  File \"/userdir/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/userdir/fairseq-v0.10.2/examples/simultaneous_translation/modules/monotonic_multihead_attention.py\", line 667, in forward\r\n    alpha = self.expected_alignment_infer(\r\n  File \"/userdir/fairseq-v0.10.2/examples/simultaneous_translation/modules/monotonic_multihead_attention.py\", line 528, in expected_alignment_infer\r\n    assert tgt_len == 1\r\nAssertionError\r\n```\r\n\r\nThis problem occurs when the sequence is shorter than k.\r\n\r\n### Environment\r\n\r\n - fairseq Version: 0.10.2 (`commit 14807a361202ba34dbbd3a533899db57a0ebda19`)\r\n - SimulEval Version: latest (`commit 1753363071f989ea3b79fdf5a21b96089a002f36`)", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3445/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3445/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3429", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3429/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3429/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3429/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3429", "id": 848126194, "node_id": "MDU6SXNzdWU4NDgxMjYxOTQ=", "number": 3429, "title": "Issue loading BART-large model finetuned on XSUM dataset ", "user": {"login": "xieyxclack", "id": 31954383, "node_id": "MDQ6VXNlcjMxOTU0Mzgz", "avatar_url": "https://avatars.githubusercontent.com/u/31954383?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xieyxclack", "html_url": "https://github.com/xieyxclack", "followers_url": "https://api.github.com/users/xieyxclack/followers", "following_url": "https://api.github.com/users/xieyxclack/following{/other_user}", "gists_url": "https://api.github.com/users/xieyxclack/gists{/gist_id}", "starred_url": "https://api.github.com/users/xieyxclack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xieyxclack/subscriptions", "organizations_url": "https://api.github.com/users/xieyxclack/orgs", "repos_url": "https://api.github.com/users/xieyxclack/repos", "events_url": "https://api.github.com/users/xieyxclack/events{/privacy}", "received_events_url": "https://api.github.com/users/xieyxclack/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-04-01T07:29:23Z", "updated_at": "2021-04-08T02:33:09Z", "closed_at": "2021-04-01T11:38:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "I 'm trying to load bart.large.xsum finetuned weights:\r\n\r\nbart = BARTModel.from_pretrained('./bart.large.xsum/', checkpoint_file='model.pt', data_name_or_path='./xsum-bin')\r\nThe checkpoint file is download from https://dl.fbaipublicfiles.com/fairseq/models/bart.large.xsum.tar.gz\r\nAnd the dict.txt file is copy from that of bart.large.cnn\r\n\r\nBut I'm getting the following error :\r\nRuntimeError: Error(s) in loading state_dict for BARTModel: \r\n         size mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([50264, 1024]) from checkpoint, the shape in current model is torch.Size([50265, 1024]).\r\n\tsize mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([50264, 1024]) from checkpoint, the shape in current model is torch.Size([50265, 1024]).\r\n\tsize mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([50264, 1024]) from checkpoint, the shape in current model is torch.Size([50265, 1024]).\r\n\r\n\r\n### Environment\r\nfairseq Version\uff1a 0.10.2\r\nPyTorch Version:   1.4.0\r\nOS: Linux\r\nHow you installed fairseq (pip, source): pip\r\nPython version: 3.6\r\nCUDA/cuDNN version: 10.1", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3429/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3423", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3423/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3423/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3423/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3423", "id": 846494914, "node_id": "MDU6SXNzdWU4NDY0OTQ5MTQ=", "number": 3423, "title": "S2T interactive inference producing an error", "user": {"login": "BadriAhmed", "id": 36994416, "node_id": "MDQ6VXNlcjM2OTk0NDE2", "avatar_url": "https://avatars.githubusercontent.com/u/36994416?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BadriAhmed", "html_url": "https://github.com/BadriAhmed", "followers_url": "https://api.github.com/users/BadriAhmed/followers", "following_url": "https://api.github.com/users/BadriAhmed/following{/other_user}", "gists_url": "https://api.github.com/users/BadriAhmed/gists{/gist_id}", "starred_url": "https://api.github.com/users/BadriAhmed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BadriAhmed/subscriptions", "organizations_url": "https://api.github.com/users/BadriAhmed/orgs", "repos_url": "https://api.github.com/users/BadriAhmed/repos", "events_url": "https://api.github.com/users/BadriAhmed/events{/privacy}", "received_events_url": "https://api.github.com/users/BadriAhmed/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-03-31T11:42:09Z", "updated_at": "2021-11-17T07:27:52Z", "closed_at": "2021-04-02T08:24:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## FairSeq S2T Example: Speech Recognition (ASR) on LibriSpeech\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nAfter downloading an pretrained s2t_transformer model, and running inference using fairseq-interactive, i ran into a NoneType error\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior : \r\n\r\n1. Run on terminal fairseq-interactive FairSeq/lbasr/ --task speech_to_text --path librispeech_transformer_s.pt\r\n2. Enter path\r\n3. Get an error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\nfairseq-interactive FairSeq/lbasr/ --task speech_to_text --path librispeech_transformer_s.pt\r\n in terminal : \r\n2021-03-31 12:38:08 | INFO | fairseq.tasks.speech_to_text | dictionary size (dict.txt): 10,000\r\n2021-03-31 12:38:08 | INFO | fairseq_cli.interactive | loading model(s) from FairSeq/lbasr/librispeech_transformer_s.pt\r\n2021-03-31 12:38:09 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\r\n2021-03-31 12:38:09 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\r\npath/test.flac \r\nTraceback (most recent call last):\r\n  File \"/home/badri/anaconda3/envs/Dev/bin/fairseq-interactive\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/home/badri/anaconda3/envs/Dev/lib/python3.8/site-packages/fairseq_cli/interactive.py\", line 307, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/home/badri/anaconda3/envs/Dev/lib/python3.8/site-packages/fairseq/distributed_utils.py\", line 302, in call_main\r\n    main(args, **kwargs)\r\n  File \"/home/badri/anaconda3/envs/Dev/lib/python3.8/site-packages/fairseq_cli/interactive.py\", line 205, in main\r\n    for batch in make_batches(inputs, args, task, max_positions, encode_fn):\r\n  File \"/home/badri/anaconda3/envs/Dev/lib/python3.8/site-packages/fairseq_cli/interactive.py\", line 75, in make_batches\r\n    tokens = [\r\n  File \"/home/badri/anaconda3/envs/Dev/lib/python3.8/site-packages/fairseq_cli/interactive.py\", line 76, in <listcomp>\r\n    task.source_dictionary.encode_line(\r\nAttributeError: 'NoneType' object has no attribute 'encode_line'\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version ( master):\r\n - PyTorch 1.7.0\r\n - OS : Ubuntu 18.04 lts\r\n - fairseq installed via pip\r\n - Python 3.8:\r\n\r\n\r\nI even tried on google colab, and I got the same error.\r\n\r\nThank you\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3423/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3417", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3417/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3417/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3417/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3417", "id": 844799405, "node_id": "MDU6SXNzdWU4NDQ3OTk0MDU=", "number": 3417, "title": "incorrect dictionary format", "user": {"login": "roboticsai", "id": 19629749, "node_id": "MDQ6VXNlcjE5NjI5NzQ5", "avatar_url": "https://avatars.githubusercontent.com/u/19629749?v=4", "gravatar_id": "", "url": "https://api.github.com/users/roboticsai", "html_url": "https://github.com/roboticsai", "followers_url": "https://api.github.com/users/roboticsai/followers", "following_url": "https://api.github.com/users/roboticsai/following{/other_user}", "gists_url": "https://api.github.com/users/roboticsai/gists{/gist_id}", "starred_url": "https://api.github.com/users/roboticsai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/roboticsai/subscriptions", "organizations_url": "https://api.github.com/users/roboticsai/orgs", "repos_url": "https://api.github.com/users/roboticsai/repos", "events_url": "https://api.github.com/users/roboticsai/events{/privacy}", "received_events_url": "https://api.github.com/users/roboticsai/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-03-30T16:05:24Z", "updated_at": "2021-03-31T09:24:23Z", "closed_at": "2021-03-31T09:24:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n`ValueError: Incorrect dictionary format, expected '<token> <cnt> [flags]'`\r\n\r\n### To Reproduce\r\n```\r\nfairseq-hydra-train \\\r\n    distributed_training.distributed_port=$PORT \\\r\n    task.data=/path/to/data \\\r\n    model.w2v_path=/path/to/model.pt \\\r\n    --config-dir /path/to/fairseq-py/examples/wav2vec/config/finetuning \\\r\n    --config-name base_100h\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nI'm trying to fine tune the wav2vec_large model. But i'm getting the above error. I've downloaded the [vacabulary](https://dl.fbaipublicfiles.com/fairseq/wav2vec/dict.ltr.txt) file in the path.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3417/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3415", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3415/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3415/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3415/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3415", "id": 844262197, "node_id": "MDU6SXNzdWU4NDQyNjIxOTc=", "number": 3415, "title": "Problems with SimulEval after recent update", "user": {"login": "sarapapi", "id": 57095209, "node_id": "MDQ6VXNlcjU3MDk1MjA5", "avatar_url": "https://avatars.githubusercontent.com/u/57095209?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sarapapi", "html_url": "https://github.com/sarapapi", "followers_url": "https://api.github.com/users/sarapapi/followers", "following_url": "https://api.github.com/users/sarapapi/following{/other_user}", "gists_url": "https://api.github.com/users/sarapapi/gists{/gist_id}", "starred_url": "https://api.github.com/users/sarapapi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sarapapi/subscriptions", "organizations_url": "https://api.github.com/users/sarapapi/orgs", "repos_url": "https://api.github.com/users/sarapapi/repos", "events_url": "https://api.github.com/users/sarapapi/events{/privacy}", "received_events_url": "https://api.github.com/users/sarapapi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-03-30T09:09:30Z", "updated_at": "2021-04-09T18:56:31Z", "closed_at": "2021-04-09T18:56:30Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi again,\r\nI've updated my local repository with the actual master branch, when I try to use SimulEval on waitk i got this error:\r\n```\r\n/home/spapi/anaconda3/envs/fairseq/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\r\n  warnings.warn(\r\n/home/spapi/anaconda3/envs/fairseq/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\r\n  return torch._C._cuda_getDeviceCount() > 0\r\n2021-03-30 10:48:15 | INFO     | simuleval.scorer | Evaluating on speech\r\n2021-03-30 10:48:15 | INFO     | simuleval.scorer | Source: /storage/MT/sara/simultaneous/dataset/MuST-Cinema/en-it/wav/amara_splitted/src_audiopath_list.txt\r\n2021-03-30 10:48:15 | INFO     | simuleval.scorer | Target: /storage/MT/sara/simultaneous/dataset/MuST-Cinema/en-it/amara.it\r\n2021-03-30 10:48:15 | INFO     | simuleval.scorer | Number of sentences: 545\r\n2021-03-30 10:48:15 | INFO     | simuleval.server | Evaluation Server Started (process id 1774886). Listening to port 6667\r\n2021-03-30 10:49:15 | WARNING  | simuleval.scorer | Resetting scorer\r\n2021-03-30 10:49:15 | INFO     | simuleval.cli    | Output dir: /storage/MT/sara/simultaneous/training/ST/waitk_tag/\r\n2021-03-30 10:49:15 | INFO     | simuleval.cli    | Evaluating FairseqSimulSTAgent (process id 1774880) on instances from 0 to 544\r\n2021-03-30 10:49:15 | INFO     | simuleval.cli    | Start data writer (process id 1775349)\r\n2021-03-30 10:51:02 | INFO     | fairseq.tasks.speech_to_text | dictionary size (spm_unigram10000_st.txt): 10,000\r\n[W NNPACK.cpp:80] Could not initialize NNPACK! Reason: Unsupported hardware.\r\n/home/spapi/anaconda3/envs/fairseq/lib/python3.8/site-packages/torchaudio/compliance/kaldi.py:574: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\r\n  fft = torch.rfft(strided_input, 1, normalized=False, onesided=True)\r\nTraceback (most recent call last):\r\n  File \"/home/spapi/anaconda3/envs/fairseq/bin/simuleval\", line 33, in <module>\r\n    sys.exit(load_entry_point('simuleval', 'console_scripts', 'simuleval')())\r\n  File \"/home/spapi/fairseq_simul/SimulEval/simuleval/cli.py\", line 165, in main\r\n    _main(args.client_only)\r\n  File \"/home/spapi/fairseq_simul/SimulEval/simuleval/cli.py\", line 192, in _main\r\n    evaluate(args, client, server_process)\r\n  File \"/home/spapi/fairseq_simul/SimulEval/simuleval/cli.py\", line 145, in evaluate\r\n    decode(args, client, result_queue, indices)\r\n  File \"/home/spapi/fairseq_simul/SimulEval/simuleval/cli.py\", line 108, in decode\r\n    action = agent.policy(states)\r\n  File \"/home/spapi/fairseq_simul/fairseq/examples/speech_to_text/simultaneous_translation/agents/fairseq_simul_st_agent.py\", line 337, in policy\r\n    if outputs[\"action\"] == 0:\r\nTypeError: tuple indices must be integers or slices, not str\r\n2021-03-30 10:51:08 | INFO     | wandb.sdk.internal.internal | Internal process exited\r\n```\r\nI think that the problem is that with the recent update the decoder passes a NamedTuple and not a dict anymore.\r\nHow can I solve it? @jmp84 @cndn @sravyapopuri388 \r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3415/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3409", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3409/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3409/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3409/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3409", "id": 843137528, "node_id": "MDU6SXNzdWU4NDMxMzc1Mjg=", "number": 3409, "title": "issue while fine tuning the wav2vec model", "user": {"login": "roboticsai", "id": 19629749, "node_id": "MDQ6VXNlcjE5NjI5NzQ5", "avatar_url": "https://avatars.githubusercontent.com/u/19629749?v=4", "gravatar_id": "", "url": "https://api.github.com/users/roboticsai", "html_url": "https://github.com/roboticsai", "followers_url": "https://api.github.com/users/roboticsai/followers", "following_url": "https://api.github.com/users/roboticsai/following{/other_user}", "gists_url": "https://api.github.com/users/roboticsai/gists{/gist_id}", "starred_url": "https://api.github.com/users/roboticsai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/roboticsai/subscriptions", "organizations_url": "https://api.github.com/users/roboticsai/orgs", "repos_url": "https://api.github.com/users/roboticsai/repos", "events_url": "https://api.github.com/users/roboticsai/events{/privacy}", "received_events_url": "https://api.github.com/users/roboticsai/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-03-29T08:50:47Z", "updated_at": "2022-10-02T09:27:29Z", "closed_at": "2021-04-08T16:15:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nWhen i run the fine tuning script on the wav2vec trained model. I'm getting below error everytime. \r\n```\r\nTraceback (most recent call last):\r\n  File \"fairseq_cli/hydra_train.py\", line 45, in hydra_main\r\n    distributed_utils.call_main(cfg, pre_main)\r\n  File \"/home/robot/airobotics/speech/fairseq/fairseq/distributed/utils.py\", line 366, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/robot/airobotics/speech/fairseq/fairseq_cli/train.py\", line 85, in main\r\n    task.load_dataset(valid_sub_split, combine=False, epoch=1)\r\n  File \"/home/robot/airobotics/speech/fairseq/fairseq/tasks/audio_pretraining.py\", line 206, in load_dataset\r\n    **self._get_mask_precompute_kwargs(task_cfg),\r\n  File \"/home/robot/airobotics/speech/fairseq/fairseq/data/audio/raw_audio_dataset.py\", line 256, in __init__\r\n    with open(manifest_path, \"r\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/robot/dataset/fairseq/libri_speech/audio/dev_other.tsv'\r\n```\r\nI'm following [this](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec) documentation to train and fine tune the wav2vec model on librespeech dataset. \r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\nI have generated the manifest and label data according to the instructions providd in that page. But when i run this command:\r\n`fairseq-hydra-train task.data=/home/robot/dataset/fairseq/libri_speech/audio model.w2v_path=/home/robot/dataset/fairseq/libri_speech/models/wav2vec_small.pt --config-dir config/finetuning/ --config-name base_100h`\r\nI'm getting the above error. \r\n\r\n### Environment\r\nfairseq branch: master\r\nos: ubuntu 20.04\r\ncuda:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Feb_14_21:12:58_PST_2021\r\nCuda compilation tools, release 11.2, V11.2.152\r\nBuild cuda_11.2.r11.2/compiler.29618528_0\r\n\r\n### Additional context\r\n\r\nSince the documentation fore the speech recognition with wav2vec is little complex in the readme gude provided. Lots of developers are facing issue and taking longer time to use this framework. So i think it is better if the documentation guide is improved for better understanding and less error. \r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3409/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3409/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3387", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3387/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3387/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3387/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3387", "id": 838578658, "node_id": "MDU6SXNzdWU4Mzg1Nzg2NTg=", "number": 3387, "title": "error is generating due to valid_wer is not calculating", "user": {"login": "vigneshgig", "id": 34392627, "node_id": "MDQ6VXNlcjM0MzkyNjI3", "avatar_url": "https://avatars.githubusercontent.com/u/34392627?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vigneshgig", "html_url": "https://github.com/vigneshgig", "followers_url": "https://api.github.com/users/vigneshgig/followers", "following_url": "https://api.github.com/users/vigneshgig/following{/other_user}", "gists_url": "https://api.github.com/users/vigneshgig/gists{/gist_id}", "starred_url": "https://api.github.com/users/vigneshgig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vigneshgig/subscriptions", "organizations_url": "https://api.github.com/users/vigneshgig/orgs", "repos_url": "https://api.github.com/users/vigneshgig/repos", "events_url": "https://api.github.com/users/vigneshgig/events{/privacy}", "received_events_url": "https://api.github.com/users/vigneshgig/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-03-23T10:32:45Z", "updated_at": "2021-03-25T09:12:13Z", "closed_at": "2021-03-25T09:12:13Z", "author_association": "NONE", "active_lock_reason": null, "body": " Hi, please help me out with this. I have 4,00,000 of the dataset with maximum of 14 seconds of an audio file, I have given  max_token 240000 and update_frequency 1 due to 8 gb of gpu rtx 4000. but I am getting errors while the validation process.below I attached the log file.  \r\n[hydra_train.log](https://github.com/pytorch/fairseq/files/6188763/hydra_train.log)\r\n\r\nNote: Previously I used 10,000 amounts of the dataset with a maximum of 5 seconds audio file. In that, I didn't get any error during the validation process.\r\n==Thanks\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3387/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3387/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3357", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3357/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3357/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3357/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3357", "id": 831814141, "node_id": "MDU6SXNzdWU4MzE4MTQxNDE=", "number": 3357, "title": "Error in SimulST training using Wait-k on multi GPUs", "user": {"login": "sarapapi", "id": 57095209, "node_id": "MDQ6VXNlcjU3MDk1MjA5", "avatar_url": "https://avatars.githubusercontent.com/u/57095209?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sarapapi", "html_url": "https://github.com/sarapapi", "followers_url": "https://api.github.com/users/sarapapi/followers", "following_url": "https://api.github.com/users/sarapapi/following{/other_user}", "gists_url": "https://api.github.com/users/sarapapi/gists{/gist_id}", "starred_url": "https://api.github.com/users/sarapapi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sarapapi/subscriptions", "organizations_url": "https://api.github.com/users/sarapapi/orgs", "repos_url": "https://api.github.com/users/sarapapi/repos", "events_url": "https://api.github.com/users/sarapapi/events{/privacy}", "received_events_url": "https://api.github.com/users/sarapapi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-03-15T13:31:10Z", "updated_at": "2021-07-09T02:39:36Z", "closed_at": "2021-03-24T16:20:28Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nI'm following the SimulST README, I've downloaded the pre-trained ASR checkpoint that you made available few days ago and I've tried to run both the wait-k with fixed pre-decision module and the MMA examples. While the latter works on 1 or many GPUs, the former only works only on 1 GPU and shows this error on many GPUs (2 in this case):\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/fonorato/anaconda3/envs/myenv/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq_cli/train.py\", line 473, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq/distributed/utils.py\", line 342, in call_main\r\n    torch.multiprocessing.spawn(\r\n  File \"/home/fonorato/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/fonorato/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 188, in start_processes\r\n    while not context.join():\r\n  File \"/home/fonorato/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 150, in join\r\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\r\ntorch.multiprocessing.spawn.ProcessRaisedException:\r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq/trainer.py\", line 750, in train_step\r\n    self._check_grad_norms(grad_norm)\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq/trainer.py\", line 1229, in _check_grad_norms\r\n    raise FloatingPointError(\r\nFloatingPointError: Fatal error: gradients are inconsistent between workers. Try --ddp-backend=legacy_ddp. Or are you mixing up different generation of GPUs in training?\r\n--------------------------------------------------------------------------------\r\ngrad_norm across the workers:\r\nrank   0 = 35.95426559\r\nrank   1 = 36.40999985\r\n\r\n--------------------------------------------------------------------------------\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/fonorato/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq/distributed/utils.py\", line 326, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq_cli/train.py\", line 157, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/home/fonorato/anaconda3/envs/myenv/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq_cli/train.py\", line 267, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/fonorato/anaconda3/envs/myenv/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq/trainer.py\", line 768, in train_step\r\n    self.task.train_step(\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq/tasks/fairseq_task.py\", line 475, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/home/fonorato/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py\", line 79, in forward\r\n    net_output = model(**sample[\"net_input\"])\r\n  File \"/home/fonorato/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/fonorato/fairseqSLT/fairseq/fairseq/distributed/module_proxy_wrapper.py\", line 55, in forward\r\n    return self.module(*args, **kwargs)\r\n  File \"/home/fonorato/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/fonorato/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 692, in forward\r\n    if self.reducer._rebuild_buckets():\r\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\r\n\r\n/home/fonorato/anaconda3/envs/myenv/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```\r\n\r\n### To Reproduce\r\nRun Wait-K with fixed pre-decision module example code in the README file\r\n\r\n\r\n### Environment\r\n - fairseq Version (e.g., 1.0 or master): up-to-date considering master\r\n - PyTorch Version (e.g., 1.0): 1.8.0\r\n - OS (e.g., Linux): Linux\r\n - Python version: 3.8.8\r\n - GPU models and configuration: 1/many GPUs K80 \r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3357/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3357/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3354", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3354/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3354/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3354/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3354", "id": 831154529, "node_id": "MDU6SXNzdWU4MzExNTQ1Mjk=", "number": 3354, "title": "SIGSEGV when training mBART with `translation_multi_simple_epoch` in FSDP", "user": {"login": "thpun", "id": 31913095, "node_id": "MDQ6VXNlcjMxOTEzMDk1", "avatar_url": "https://avatars.githubusercontent.com/u/31913095?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thpun", "html_url": "https://github.com/thpun", "followers_url": "https://api.github.com/users/thpun/followers", "following_url": "https://api.github.com/users/thpun/following{/other_user}", "gists_url": "https://api.github.com/users/thpun/gists{/gist_id}", "starred_url": "https://api.github.com/users/thpun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thpun/subscriptions", "organizations_url": "https://api.github.com/users/thpun/orgs", "repos_url": "https://api.github.com/users/thpun/repos", "events_url": "https://api.github.com/users/thpun/events{/privacy}", "received_events_url": "https://api.github.com/users/thpun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-03-14T13:19:00Z", "updated_at": "2021-04-11T10:27:09Z", "closed_at": "2021-04-11T10:27:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nGot `Exception: process 3 terminated with signal SIGSEGV` when trying to perform multilingual fine-tuning with mBART under `translation_multi_simple_epoch` task in fully sharded data parallel (FSDP).\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd\r\n```bash\r\nlang_pairs=<comma-separated list of lang pairs to be trained>\r\nPREFIX=mbart-fsdp\r\nDATA=/path/to/train/data\r\nlang_list=models/$PREFIX/lang_list\r\nMODEL=models/mbart.cc25/model.pt\r\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 fairseq-train $DATA \\\r\n  --finetune-from-model $MODEL \\\r\n  --encoder-normalize-before --decoder-normalize-before \\\r\n  --arch mbart_large --layernorm-embedding \\\r\n  --task translation_multi_simple_epoch \\\r\n  --sampling-method \"temperature\" \\\r\n  --sampling-temperature 5 \\\r\n  --encoder-langtok \"src\" --decoder-langtok \\\r\n  --lang-dict \"$lang_list\" --lang-pairs \"$lang_pairs\" \\\r\n  --source-dict $DATA/dict.en_XX.txt --target-dict $DATA/dict.en_XX.txt \\\r\n  --checkpoint-activations --fp16 --no-reshard-after-forward \\\r\n  --criterion label_smoothed_cross_entropy --label-smoothing 0.2 \\\r\n  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \\\r\n  --lr-scheduler inverse_sqrt --lr 6e-05 --stop-min-lr -1 --warmup-updates 2000 \\\r\n  --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \\\r\n  --max-tokens 1920 --update-freq 2 --upsample-primary 2 \\\r\n  --save-interval-updates 5000 --keep-interval-updates 10 --keep-best-checkpoints 10 \\\r\n  --patience 5 --max-epoch 150 \\\r\n  --seed 222 --log-format simple --log-interval 10 --ddp-backend fully_sharded \\\r\n  --save-dir models/$PREFIX\r\n```\r\n\r\n2. See error\r\n```\r\n2021-03-14 12:25:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en_XX-zh_CN src_langtok: 250004; tgt_langtok: 250025\r\n2021-03-14 12:25:42 | INFO | fairseq.data.data_utils | loaded 5,983 examples from: path/to/valid.en_XX-zh_CN.en_XX\r\n2021-03-14 12:25:42 | INFO | fairseq.data.data_utils | loaded 5,983 examples from: path/to/valid.en_XX-zh_CN.zh_CN\r\n2021-03-14 12:25:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | path/to valid en_XX-zh_CN 5983 examples\r\n2021-03-14 12:25:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:zh_CN-en_XX src_langtok: 250025; tgt_langtok: 250004\r\n2021-03-14 12:25:42 | INFO | fairseq.data.data_utils | loaded 5,982 examples from: path/to/valid.zh_CN-en_XX.zh_CN\r\n2021-03-14 12:25:42 | INFO | fairseq.data.data_utils | loaded 5,982 examples from: path/to/valid.zh_CN-en_XX.en_XX\r\n2021-03-14 12:25:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | path/to valid zh_CN-en_XX 5982 examples\r\n2021-03-14 12:25:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en_XX-es_XX src_langtok: 250004; tgt_langtok: 250005\r\n2021-03-14 12:25:42 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: path/to/valid.en_XX-es_XX.en_XX\r\n2021-03-14 12:25:42 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: path/to/valid.en_XX-es_XX.es_XX\r\n2021-03-14 12:25:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | path/to valid en_XX-es_XX 3003 examples\r\n2021-03-14 12:25:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es_XX-en_XX src_langtok: 250005; tgt_langtok: 250004\r\n2021-03-14 12:25:42 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: path/to/valid.en_XX-es_XX.es_XX\r\n2021-03-14 12:25:42 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: path/to/valid.en_XX-es_XX.en_XX\r\n2021-03-14 12:25:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | path/to valid es_XX-en_XX 3003 examples\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/workspace/fairseq/fairseq_cli/train.py\", line 477, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/workspace/fairseq/fairseq/distributed/utils.py\", line 349, in call_main\r\n    join=True,\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 108, in join\r\n    (error_index, name)\r\nException: process 3 terminated with signal SIGSEGV\r\n```\r\n\r\n### Expected behavior\r\n\r\nIt is expected that either the training starts properly.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master, commit 252d5a9ae93e68254cfb1896fb5624cf11cda15e\r\n - PyTorch Version (e.g., 1.0) 1.7.0a0+8deb4fe\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n ```bash\r\nconda install gcc_linux-64 gxx_linux-64\r\ngit clone https://github.com/pytorch/fairseq.git\r\ngit clone https://github.com/facebookresearch/fairscale\r\ncd fairseq\r\npip install opencc nni tensorboardX pyarrow deepspeed\r\npip install -U numpy cython\r\npip install --editable .\r\npython setup.py build_ext --inplace\r\ncd ../fairscale\r\npip install -r requirements.txt\r\npip install -e .\r\n```\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: 11.0\r\n - GPU models and configuration: V100\r\n - fairscale version: 0.3.1, commit `82986ca0f74a20e1e20e84161735b4b51c609148`\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3354/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3354/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3337", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3337/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3337/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3337/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3337", "id": 828815712, "node_id": "MDU6SXNzdWU4Mjg4MTU3MTI=", "number": 3337, "title": "speech_recognition/w2l_decoder.py load kenlm core dump", "user": {"login": "Gavin90s", "id": 8350994, "node_id": "MDQ6VXNlcjgzNTA5OTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/8350994?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gavin90s", "html_url": "https://github.com/Gavin90s", "followers_url": "https://api.github.com/users/Gavin90s/followers", "following_url": "https://api.github.com/users/Gavin90s/following{/other_user}", "gists_url": "https://api.github.com/users/Gavin90s/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gavin90s/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gavin90s/subscriptions", "organizations_url": "https://api.github.com/users/Gavin90s/orgs", "repos_url": "https://api.github.com/users/Gavin90s/repos", "events_url": "https://api.github.com/users/Gavin90s/events{/privacy}", "received_events_url": "https://api.github.com/users/Gavin90s/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-03-11T06:34:04Z", "updated_at": "2022-06-08T07:15:31Z", "closed_at": "2021-03-12T09:17:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Missing separate debuginfo for /lib64/libcuda.so.1\r\nTry: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/81/b0b00f7a42f1dfb266364925e18e6524d0a1cf.debug\r\nCore was generated by `python examples/speech_recognition/infer.py manifest/finetune/ --task audio_pre'.\r\nProgram terminated with signal 11, Segmentation fault.\r\n#0  _M_data (this=0x2ac160808) at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/bits/basic_string.h:229\r\n229\t/home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/bits/basic_string.h: No such file or directory.\r\nMissing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.6-13.1.alios7.x86_64 glibc-2.17-222.alios7.1.x86_64 libuuid-2.23.2-43.alios7.2.x86_64\r\n(gdb) bt\r\n#0  _M_data (this=0x2ac160808) at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/bits/basic_string.h:229\r\n#1  _M_is_local (this=0x2ac160808) at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/bits/basic_string.h:222\r\n#2  std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_dispose (this=0x2ac160808)\r\n    at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/bits/basic_string.h:231\r\n#3  0x00007f3f79c44825 in std::_Destroy<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > (__pointer=0x2ac160808) at /usr/local/include/c++/5.2.0/bits/stl_construct.h:93\r\n#4  0x00007f3f79c44583 in std::_Destroy_aux<false>::__destroy<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*> (__first=0x2ac160808, __last=0x0) at /usr/local/include/c++/5.2.0/bits/stl_construct.h:103\r\n#5  0x00007f3f79c44220 in std::_Destroy<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*> (__first=0x2ac160808, __last=0x0) at /usr/local/include/c++/5.2.0/bits/stl_construct.h:126\r\n#6  0x00007f3f79c43d73 in std::_Destroy<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > (__first=0x2ac160808, __last=0x0)\r\n    at /usr/local/include/c++/5.2.0/bits/stl_construct.h:151\r\n#7  0x00007f3f79c438ad in std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::~vector (\r\n    this=0x7ffcd25ff1c8, __in_chrg=<optimized out>) at /usr/local/include/c++/5.2.0/bits/stl_vector.h:424\r\n#8  0x00007f3f79c5753c in lm::ngram::Config::~Config (this=0x7ffcd25ff150, __in_chrg=<optimized out>) at /usr/local/include/kenlm/lm/config.hh:19\r\n#9  0x00007f3f79c56bec in fl::lib::text::KenLM::KenLM (this=0x7f401cd0ed70, path=..., usrTknDict=...) at /disk4/zhuozhu.zz/flashlight/flashlight/lib/text/decoder/lm/KenLM.cpp:23\r\n#10 0x00007f3facb487cc in pybind11::detail::initimpl::construct_or_initialize<fl::lib::text::KenLM, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&, 0> ()\r\n    at /disk4/zhuozhu.zz/flashlight/bindings/python/build/temp.linux-x86_64-3.6/pybind11/src/pybind11/include/pybind11/detail/init.h:61\r\n#11 0x00007f3facb36e9c in void pybind11::detail::initimpl::constructor<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&>::execute<pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>, pybind11::arg, pybind11::arg, 0>(pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>&, pybind11::arg const&, pybind11::arg const&)::{lambda(pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&)#1}::operator()(pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&) const (__closure=0x7f401d4ce468, v_h=..., args#0=..., args#1=...)\r\n    at /disk4/zhuozhu.zz/flashlight/bindings/python/build/temp.linux-x86_64-3.6/pybind11/src/pybind11/include/pybind11/detail/init.h:174\r\n#12 0x00007f3facb7e3c5 in pybind11::detail::argument_loader<pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&>::call_impl<void, void pybind11::detail::initimpl::constructor<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&>::execute<pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>, pybind11::arg, pybind11::arg, 0>(pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>&, pybind11::arg const&, pybind11::arg const&)::{lambda(pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&)#1}&, 0ul, 1ul, 2ul, pybind11::detail::void_type>(void pybind11::detail::initimpl::constructor<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&>::execute<pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>, pybind11::arg, pybind11::arg, 0>(pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>&, pybind11::arg const&, pybind11::arg const&)::{lambda(pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&)#1}&, std::integer_sequence<unsigned long, 0ul, 1ul, 2ul>, pybind11::detail::void_type&&) (this=0x7ffcd25ff3f0, f=...)\r\n    at /disk4/zhuozhu.zz/flashlight/bindings/python/build/temp.linux-x86_64-3.6/pybind11/src/pybind11/include/pybind11/cast.h:1874\r\n#13 0x00007f3facb7856b in _ZNO8pybind116detail15argument_loaderIJRNS0_16value_and_holderERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKN2fl3lib4text10DictionaryEEE4callIvNS0_9void_typeERZNS0_8initimpl11constructorIJSB_SH_EE7executeINS_6class_INSE_5KenLMEJSt10shared_ptrISQ_ENSE_2LMEEEEJNS_3argESV_ELi0EEEvRT_DpRKT0_EUlS3_SB_SH_E_EENSt9enable_ifIXsrSt7is_voidISW_E5valueESK_E4typeEOT1_ (this=0x7ffcd25ff3f0, f=...)\r\n    at /disk4/zhuozhu.zz/flashlight/bindings/python/build/temp.linux-x86_64-3.6/pybind11/src/pybind11/include/pybind11/cast.h:1856\r\n#14 0x00007f3facb6ad1e in void pybind11::cpp_function::initialize<void pybind11::detail::initimpl::constructor<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&>::execute<pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>, pybind11::arg, pybind11::arg, 0>(pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>&, pybind11::arg const&, pybind11::arg const&)::{lambda(pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&)#1}, void, pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor, pybind11::arg, pybind11::arg>(void pybind11::detail::initimpl::constructor<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&>::execute<pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>, pybind11::arg, pybind11::arg, 0>(pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>&, pybind11::arg const&, pybind11::arg const&)::{lambda(pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&)#1}&&, void (*)(pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::detail::is_new_style_constructor const&, pybind11::arg const&, pybind11::arg const&)::{lambda(pybind11::detail::function_call&)#3}::operator()(pybind11::detail::function_call) const (__closure=0x0, call=...)\r\n    at /disk4/zhuozhu.zz/flashlight/bindings/python/build/temp.linux-x86_64-3.6/pybind11/src/pybind11/include/pybind11/pybind11.h:154\r\n#15 0x00007f3facb6af93 in void pybind11::cpp_function::initialize<void pybind11::detail::initimpl::constructor<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&>::execute<pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>, pybind11::arg, pybind11::arg, 0>(pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>&, pybind11::arg const&, pybind11::arg const&)::{lambda(pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&)#1}, void, pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor, pybind11::arg, pybind11::arg>(void pybind11::detail::initimpl::constructor<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&>::execute<pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>, pybind11::arg, pybind11::arg, 0>(pybind11::class_<fl::lib::text::KenLM, std::shared_ptr<fl::lib::text::KenLM>, fl::lib::text::LM>&, pybind11::arg const&, pybind11::arg const&)::{lambda(pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&)#1}&&, void (*)(pybind11::detail::value_and_holder&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, fl::lib::text::Dictionary const&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::detail::is_new_style_constructor const&, pybind11::arg const&, pybind11::arg const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) ()\r\n    at /disk4/zhuozhu.zz/flashlight/bindings/python/build/temp.linux-x86_64-3.6/pybind11/src/pybind11/include/pybind11/pybind11.h:132\r\n#16 0x00007f3facb1f828 in pybind11::cpp_function::dispatcher (self=0x7f3f22fd6ab0, args_in=0x7f3fad082c60, kwargs_in=0x0)\r\n    at /disk4/zhuozhu.zz/flashlight/bindings/python/build/temp.linux-x86_64-3.6/pybind11/src/pybind11/include/pybind11/pybind11.h:627\r\n#17 0x00007f401a7e7a14 in _PyCFunction_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/methodobject.c:231\r\n#18 0x00007f401a7e7e2f in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2313\r\n#19 0x00007f401a7ec873 in _PyObject_Call_Prepend () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2373\r\n#20 0x00007f401a7e781e in PyObject_Call () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2261\r\n#21 0x00007f401a84088b in slot_tp_init () at /tmp/build/80754af9/python_1599604603603/work/Objects/typeobject.c:6420\r\n#22 0x00007f401a86fd97 in type_call () at /tmp/build/80754af9/python_1599604603603/work/Objects/typeobject.c:915\r\n#23 0x00007f401a7e7bfb in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2331\r\n#24 0x00007f401a86fbae in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4875\r\n---Type <return> to continue, or q <return> to quit---\r\n#25 0x00007f401a89225a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335\r\n#26 0x00007f401a869166 in _PyEval_EvalCodeWithName () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166\r\n#27 0x00007f401a86a32c in _PyFunction_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:5084\r\n#28 0x00007f401a7e7ddf in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2310\r\n#29 0x00007f401a7ec873 in _PyObject_Call_Prepend () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2373\r\n#30 0x00007f401a7e781e in PyObject_Call () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2261\r\n#31 0x00007f401a84088b in slot_tp_init () at /tmp/build/80754af9/python_1599604603603/work/Objects/typeobject.c:6420\r\n#32 0x00007f401a86fd97 in type_call () at /tmp/build/80754af9/python_1599604603603/work/Objects/typeobject.c:915\r\n#33 0x00007f401a7e7bfb in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1599604603603/work/Objects/abstract.c:2331\r\n#34 0x00007f401a86fbae in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4875\r\n#35 0x00007f401a89225a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335\r\n#36 0x00007f401a869166 in _PyEval_EvalCodeWithName () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166\r\n#37 0x00007f401a869e51 in fast_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4992\r\n#38 0x00007f401a86fb35 in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4872\r\n#39 0x00007f401a89225a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335\r\n#40 0x00007f401a8692ce in _PyEval_EvalCodeWithName () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166\r\n#41 0x00007f401a869e51 in fast_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4992\r\n#42 0x00007f401a86fb35 in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4872\r\n#43 0x00007f401a89225a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335\r\n#44 0x00007f401a869c1b in _PyFunction_FastCall (globals=<optimized out>, nargs=0, args=<optimized out>, co=<optimized out>) at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4933\r\n#45 fast_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4968\r\n#46 0x00007f401a86fb35 in call_function () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4872\r\n#47 0x00007f401a89225a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:3335\r\n#48 0x00007f401a86a969 in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwstep=2, kwcount=<optimized out>, kwargs=0x0, kwnames=0x0, argcount=0, args=0x0, locals=0x7f401a62c1b0, \r\n    globals=0x7f401a62c1b0, _co=0x7f4012fd8390) at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4166\r\n#49 PyEval_EvalCodeEx () at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:4187\r\n#50 0x00007f401a86b70c in PyEval_EvalCode (co=co@entry=0x7f4012fd8390, globals=globals@entry=0x7f401a62c1b0, locals=locals@entry=0x7f401a62c1b0) at /tmp/build/80754af9/python_1599604603603/work/Python/ceval.c:731\r\n#51 0x00007f401a8eb574 in run_mod () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:1025\r\n#52 0x00007f401a8eb971 in PyRun_FileExFlags () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:978\r\n#53 0x00007f401a8ebb73 in PyRun_SimpleFileExFlags () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:419\r\n#54 0x00007f401a8ebc7d in PyRun_AnyFileExFlags () at /tmp/build/80754af9/python_1599604603603/work/Python/pythonrun.c:81\r\n#55 0x00007f401a8ef663 in run_file (p_cf=0x7ffcd2600d2c, filename=0x7f401ac41440 L\"examples/speech_recognition/infer.py\", fp=0x7f401ac79b10) at /tmp/build/80754af9/python_1599604603603/work/Modules/main.c:340\r\n#56 Py_Main () at /tmp/build/80754af9/python_1599604603603/work/Modules/main.c:811\r\n#57 0x00007f401a7b943e in main () at /tmp/build/80754af9/python_1599604603603/work/Programs/python.c:69\r\n#58 0x00007f4019eea445 in __libc_start_main () from /lib64/libc.so.6\r\n#59 0x00007f401a898d0b in _start () at ../sysdeps/x86_64/elf/start.S:103\r\n\r\ncentos 7 \r\ngcc 5.2\r\npytorch 1.15\r\npython 3.6\r\n\r\nis there anyone can help me ! thanks in advance.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3337/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3337/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3299", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3299/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3299/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3299/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3299", "id": 820840708, "node_id": "MDU6SXNzdWU4MjA4NDA3MDg=", "number": 3299, "title": "Can't run fairseq plugins with multiple GPUs.", "user": {"login": "philip30", "id": 5228140, "node_id": "MDQ6VXNlcjUyMjgxNDA=", "avatar_url": "https://avatars.githubusercontent.com/u/5228140?v=4", "gravatar_id": "", "url": "https://api.github.com/users/philip30", "html_url": "https://github.com/philip30", "followers_url": "https://api.github.com/users/philip30/followers", "following_url": "https://api.github.com/users/philip30/following{/other_user}", "gists_url": "https://api.github.com/users/philip30/gists{/gist_id}", "starred_url": "https://api.github.com/users/philip30/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/philip30/subscriptions", "organizations_url": "https://api.github.com/users/philip30/orgs", "repos_url": "https://api.github.com/users/philip30/repos", "events_url": "https://api.github.com/users/philip30/events{/privacy}", "received_events_url": "https://api.github.com/users/philip30/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-03-03T07:34:17Z", "updated_at": "2023-04-20T05:47:22Z", "closed_at": "2023-04-20T05:47:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nI followed the tutorial on building the plugins. Everything seems fine until I want to try to work with multiple GPUs. I tried few things and made sure that my module is not in sys.path by default. Please advice!\r\n\r\n### To Reproduce\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1 fairseq-train \\\r\n  --user-dir $MY_USER_DIRECTORY \\\r\n  --arch my_arch \\\r\n  --task my_task \\\r\n  --criterion my_loss \\\r\n  --batch-size 32 --num-workers 8 \\ \r\n  --optimizer adam \\\r\n  --lr-scheduler inverse_sqrt --lr 5e-4 --warmup-init-lr 1e-6 --warmup-updates 8000 \\\r\n  --update-freq 1 --log-interval 1\r\n```\r\n\r\n#### Code sample\r\n\r\nIn ```$MY_USER_DIRECTORY/__init__.py```\r\n```\r\nfrom . import models\r\nfrom . import criterions\r\nfrom . import tasks\r\n```\r\n\r\n### Expected behavior\r\n\r\n```MY_USER_DIRECTORY=/home/parthur/tools/bert/spanBERT-doc/spanbert_doc```\r\nThis is the error message. I include the real name of the architecture, criterions, and tasks in the real error message\r\n\r\n```\r\n2021-03-03 07:30:05 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:17810\r\n2021-03-03 07:30:05 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:17810\r\n2021-03-03 07:30:05 | INFO | fairseq.distributed_utils | initialized host krylov-ws-rc-byn6lgn-d0lenoip6yck8kbpftzibivz1ryumld9q6c-9g9tv as rank 1\r\n2021-03-03 07:30:05 | INFO | fairseq.distributed_utils | initialized host krylov-ws-rc-byn6lgn-d0lenoip6yck8kbpftzibivz1ryumld9q6c-9g9tv as rank 0\r\nTraceback (most recent call last):\r\n  File \"/home/parthur/miniconda3/bin/fairseq-train\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/home/parthur/miniconda3/lib/python3.8/site-packages/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/home/parthur/miniconda3/lib/python3.8/site-packages/fairseq/distributed_utils.py\", line 283, in call_main\r\n    torch.multiprocessing.spawn(\r\n  File \"/home/parthur/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/parthur/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/home/parthur/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/parthur/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/parthur/miniconda3/lib/python3.8/site-packages/fairseq/distributed_utils.py\", line 270, in distributed_main\r\n    main(args, **kwargs)\r\n  File \"/home/parthur/miniconda3/lib/python3.8/site-packages/fairseq_cli/train.py\", line 43, in main\r\n    utils.import_user_module(args)\r\n  File \"/home/parthur/miniconda3/lib/python3.8/site-packages/fairseq/utils.py\", line 458, in import_user_module\r\n    raise ImportError(\r\nImportError: Failed to import --user-dir=/home/parthur/tools/bert/spanBERT-doc/spanbert_doc because the corresponding module name (spanbert_doc) is not globally unique. Please rename the directory to something unique and try again.\r\n```\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): conda\r\n - Python version: 3.8.\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration:Tesla V100-SXM2\r\n - Any other relevant information:\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3299/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3299/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3283", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3283/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3283/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3283/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3283", "id": 816996086, "node_id": "MDU6SXNzdWU4MTY5OTYwODY=", "number": 3283, "title": "Backtranslation demo failed", "user": {"login": "novelidea", "id": 8478197, "node_id": "MDQ6VXNlcjg0NzgxOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/8478197?v=4", "gravatar_id": "", "url": "https://api.github.com/users/novelidea", "html_url": "https://github.com/novelidea", "followers_url": "https://api.github.com/users/novelidea/followers", "following_url": "https://api.github.com/users/novelidea/following{/other_user}", "gists_url": "https://api.github.com/users/novelidea/gists{/gist_id}", "starred_url": "https://api.github.com/users/novelidea/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/novelidea/subscriptions", "organizations_url": "https://api.github.com/users/novelidea/orgs", "repos_url": "https://api.github.com/users/novelidea/repos", "events_url": "https://api.github.com/users/novelidea/events{/privacy}", "received_events_url": "https://api.github.com/users/novelidea/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-02-26T03:07:49Z", "updated_at": "2023-04-28T16:36:24Z", "closed_at": "2021-03-12T08:00:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI was following the READ me of [Backtranslation](https://github.com/pytorch/fairseq/tree/v0.10.1/examples/backtranslation) and trying to translate \"hello world\" to German but got errors. I tried both master and v0.10.1.\r\n\r\nHere's the command I used\r\n\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./\r\n\r\npip install subword_nmt sacremoses\r\n\r\nimport torch\r\n\r\n// List available models\r\ntorch.hub.list('pytorch/fairseq')  # [..., 'transformer.wmt18.en-de', ... ]\r\n\r\n// Load the WMT'18 En-De ensemble\r\nen2de_ensemble = torch.hub.load(\r\n    'pytorch/fairseq', 'transformer.wmt18.en-de',\r\n    checkpoint_file='wmt18.model1.pt:wmt18.model2.pt:wmt18.model3.pt:wmt18.model4.pt:wmt18.model5.pt',\r\n    tokenizer='moses', bpe='subword_nmt')\r\n\r\nAnd then I got errors as below\r\n\r\n`Error when composing. Overrides: ['common.no_progress_bar=True', 'common.log_interval=100', \"common.log_format='simple'\", 'common.tensorboard_logdir=null', 'common.wandb_project=null', 'common.azureml_logging=False', 'common.seed=1', 'common.cpu=False', 'common.tpu=False', 'common.bf16=False', 'common.memory_efficient_bf16=False', 'common.fp16=True', 'common.memory_efficient_fp16=False', 'common.fp16_no_flatten_grads=False', 'common.fp16_init_scale=128', 'common.fp16_scale_window=null', 'common.fp16_scale_tolerance=0.0', 'common.min_loss_scale=0.0001', 'common.threshold_loss_scale=null', 'common.user_dir=null', 'common.empty_cache_freq=0', 'common.all_gather_list_size=16384', 'common.model_parallel_size=1', 'common.quantization_config_path=null', 'common.profile=False', 'common.reset_logging=False', 'common.suppress_crashes=False', 'common_eval.path=null', 'common_eval.post_process=null', 'common_eval.quiet=False', \"common_eval.model_overrides='{}'\", 'common_eval.results_path=null', 'distributed_training.distributed_world_size=128', 'distributed_training.distributed_rank=0', \"distributed_training.distributed_backend='nccl'\", \"distributed_training.distributed_init_method='tcp://learnfair0250:12597'\", 'distributed_training.distributed_port=12597', 'distributed_training.device_id=0', 'distributed_training.distributed_no_spawn=False', \"distributed_training.ddp_backend='pytorch_ddp'\", 'distributed_training.bucket_cap_mb=25', 'distributed_training.fix_batches_to_gpus=False', 'distributed_training.find_unused_parameters=False', 'distributed_training.fast_stat_sync=False', 'distributed_training.heartbeat_timeout=-1', 'distributed_training.broadcast_buffers=False', 'distributed_training.slowmo_momentum=null', \"distributed_training.slowmo_algorithm='LocalSGD'\", 'distributed_training.localsgd_frequency=3', 'distributed_training.nprocs_per_node=1', 'distributed_training.pipeline_model_parallel=False', 'distributed_training.pipeline_balance=null', 'distributed_training.pipeline_devices=null', 'distributed_training.pipeline_chunks=0', 'distributed_training.pipeline_encoder_balance=null', 'distributed_training.pipeline_encoder_devices=null', 'distributed_training.pipeline_decoder_balance=null', 'distributed_training.pipeline_decoder_devices=null', \"distributed_training.pipeline_checkpoint='never'\", \"distributed_training.zero_sharding='none'\", 'distributed_training.tpu=True', 'dataset.num_workers=1', 'dataset.skip_invalid_size_inputs_valid_test=False', 'dataset.max_tokens=3584', 'dataset.batch_size=null', 'dataset.required_batch_size_multiple=8', 'dataset.required_seq_len_multiple=1', 'dataset.dataset_impl=null', 'dataset.data_buffer_size=10', \"dataset.train_subset='train'\", \"dataset.valid_subset='valid'\", 'dataset.validate_interval=1', 'dataset.validate_interval_updates=0', 'dataset.validate_after_updates=0', 'dataset.fixed_validation_seed=null', 'dataset.disable_validation=False', \"dataset.max_tokens_valid='${dataset.max_tokens}'\", \"dataset.batch_size_valid='${dataset.batch_size}'\", 'dataset.curriculum=0', \"dataset.gen_subset='test'\", 'dataset.num_shards=1', 'dataset.shard_id=0', 'optimization.max_epoch=0', 'optimization.max_update=150000', 'optimization.stop_time_hours=0.0', 'optimization.clip_norm=0.0', 'optimization.sentence_avg=False', 'optimization.update_freq=[1]', 'optimization.lr=[0.0005]', 'optimization.stop_min_lr=1e-09', 'optimization.use_bmuf=False', \"checkpoint.save_dir='/checkpoint/edunov/20180526/wmt18en2de.wmt18.transformer_vaswani_wmt_en_de_big.bsz3584_lr0.0005_dr0.3_size225668919_sample_attdr0.1_upsample16'\", \"checkpoint.restore_file='checkpoint_last.pt'\", 'checkpoint.finetune_from_model=null', 'checkpoint.reset_dataloader=False', 'checkpoint.reset_lr_scheduler=False', 'checkpoint.reset_meters=False', 'checkpoint.reset_optimizer=False', \"checkpoint.optimizer_overrides='{}'\", 'checkpoint.save_interval=1', 'checkpoint.save_interval_updates=null', 'checkpoint.keep_interval_updates=0', 'checkpoint.keep_last_epochs=-1', 'checkpoint.keep_best_checkpoints=-1', 'checkpoint.no_save=False', 'checkpoint.no_epoch_checkpoints=False', 'checkpoint.no_last_checkpoints=False', 'checkpoint.no_save_optimizer_state=False', \"checkpoint.best_checkpoint_metric='loss'\", 'checkpoint.maximize_best_checkpoint_metric=False', 'checkpoint.patience=-1', \"checkpoint.checkpoint_suffix=''\", 'checkpoint.checkpoint_shard_count=1', 'checkpoint.load_checkpoint_on_all_dp_ranks=False', \"checkpoint.model_parallel_size='${common.model_parallel_size}'\", 'checkpoint.distributed_rank=0', 'bmuf.block_lr=1.0', 'bmuf.block_momentum=0.875', 'bmuf.global_sync_iter=50', 'bmuf.warmup_iterations=500', 'bmuf.use_nbm=False', 'bmuf.average_sync=False', 'bmuf.distributed_world_size=128', 'generation.beam=5', 'generation.nbest=1', 'generation.max_len_a=0.0', 'generation.max_len_b=200', 'generation.min_len=1', 'generation.match_source_len=False', 'generation.unnormalized=False', 'generation.no_early_stop=False', 'generation.no_beamable_mm=False', 'generation.lenpen=1.0', 'generation.unkpen=0.0', 'generation.replace_unk=null', 'generation.sacrebleu=False', 'generation.score_reference=False', 'generation.prefix_size=0', 'generation.no_repeat_ngram_size=0', 'generation.sampling=False', 'generation.sampling_topk=-1', 'generation.sampling_topp=-1.0', 'generation.constraints=null', 'generation.temperature=1.0', 'generation.diverse_beam_groups=-1', 'generation.diverse_beam_strength=0.5', 'generation.diversity_rate=-1.0', 'generation.print_alignment=null', 'generation.print_step=False', 'generation.lm_path=null', 'generation.lm_weight=0.0', 'generation.iter_decode_eos_penalty=0.0', 'generation.iter_decode_max_iter=10', 'generation.iter_decode_force_max_iter=False', 'generation.iter_decode_with_beam=1', 'generation.iter_decode_with_external_reranker=False', 'generation.retain_iter_history=False', 'generation.retain_dropout=False', 'generation.retain_dropout_modules=null', 'generation.decoding_format=null', 'generation.no_seed_provided=False', 'eval_lm.output_word_probs=False', 'eval_lm.output_word_stats=False', 'eval_lm.context_window=0', 'eval_lm.softmax_batch=9223372036854775807', 'interactive.buffer_size=0', \"interactive.input='-'\", 'task=translation', 'task._name=translation', \"task.data='/home/jyt/.cache/torch/pytorch_fairseq/132307b1ed6b7c35ac7cc955bed54ec7fa87b612f07da9e5809fa49daedafb2d.09d4a0e2212ce4f65bf3baef404268c8ae0f4605f6f47246d489b600e0ed1b25'\", \"task.source_lang='en'\", \"task.target_lang='de'\", 'task.load_alignments=False', 'task.left_pad_source=True', 'task.left_pad_target=False', 'task.max_source_positions=1024', 'task.max_target_positions=1024', 'task.upsample_primary=16', 'task.truncate_source=False', 'task.num_batch_buckets=0', \"task.train_subset='train'\", \"task.dataset_impl='${dataset.dataset_impl}'\", \"task.required_seq_len_multiple='${dataset.required_seq_len_multiple}'\", 'task.eval_bleu=False', \"task.eval_bleu_args='{}'\", \"task.eval_bleu_detok='space'\", \"task.eval_bleu_detok_args='{}'\", 'task.eval_tokenized_bleu=False', 'task.eval_bleu_remove_bpe=null', 'task.eval_bleu_print_samples=False', 'criterion=label_smoothed_cross_entropy', 'criterion._name=label_smoothed_cross_entropy', 'criterion.label_smoothing=0.1', 'criterion.report_accuracy=False', 'criterion.ignore_prefix_size=0', 'criterion.sentence_avg=False', 'tokenizer=moses', 'tokenizer._name=moses', \"tokenizer.source_lang='en'\", \"tokenizer.target_lang='de'\", 'tokenizer.moses_no_dash_splits=False', 'tokenizer.moses_no_escape=False', 'bpe=subword_nmt', 'bpe._name=subword_nmt', \"bpe.bpe_codes='/home/jyt/.cache/torch/pytorch_fairseq/132307b1ed6b7c35ac7cc955bed54ec7fa87b612f07da9e5809fa49daedafb2d.09d4a0e2212ce4f65bf3baef404268c8ae0f4605f6f47246d489b600e0ed1b25/code'\", \"bpe.bpe_separator='@@'\", 'optimizer=adam', 'optimizer._name=adam', \"optimizer.adam_betas='(0.9, 0.98)'\", 'optimizer.adam_eps=1e-08', 'optimizer.weight_decay=0.0', 'optimizer.use_old_adam=False', 'optimizer.tpu=True', 'optimizer.lr=[0.0005]', 'lr_scheduler=inverse_sqrt', 'lr_scheduler._name=inverse_sqrt', 'lr_scheduler.warmup_updates=4000', 'lr_scheduler.warmup_init_lr=1e-07', 'lr_scheduler.lr=[0.0005]']\r\nTraceback (most recent call last):\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/hydra/_internal/config_loader_impl.py\", line 513, in _apply_overrides_to_config\r\n    OmegaConf.update(cfg, key, value, merge=True)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/omegaconf/omegaconf.py\", line 613, in update\r\n    root.__setattr__(last_key, value)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 285, in __setattr__\r\n    raise e\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 282, in __setattr__\r\n    self.__set_impl(key, value)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 266, in __set_impl\r\n    self._set_item_impl(key, value)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/omegaconf/basecontainer.py\", line 398, in _set_item_impl\r\n    self._validate_set(key, value)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 143, in _validate_set\r\n    self._validate_set_merge_impl(key, value, is_assign=True)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 159, in _validate_set_merge_impl\r\n    cause=ValidationError(\"child '$FULL_KEY' is not Optional\"),\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.ValidationError: child 'checkpoint.save_interval_updates' is not Optional\r\n\tfull_key: checkpoint.save_interval_updates\r\n\treference_type=CheckpointConfig\r\n\tobject_type=CheckpointConfig\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/torch/hub.py\", line 370, in load\r\n    model = _load_local(repo_or_dir, model, *args, **kwargs)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/torch/hub.py\", line 399, in _load_local\r\n    model = entry(*args, **kwargs)\r\n  File \"/home/jyt/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/fairseq_model.py\", line 262, in from_pretrained\r\n    **kwargs,\r\n  File \"/home/jyt/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 75, in from_pretrained\r\n    arg_overrides=kwargs,\r\n  File \"/home/jyt/.cache/torch/hub/pytorch_fairseq_master/fairseq/checkpoint_utils.py\", line 339, in load_model_ensemble_and_task\r\n    state = load_checkpoint_to_cpu(filename, arg_overrides)\r\n  File \"/home/jyt/.cache/torch/hub/pytorch_fairseq_master/fairseq/checkpoint_utils.py\", line 273, in load_checkpoint_to_cpu\r\n    state = _upgrade_state_dict(state)\r\n  File \"/home/jyt/.cache/torch/hub/pytorch_fairseq_master/fairseq/checkpoint_utils.py\", line 558, in _upgrade_state_dict\r\n    state[\"cfg\"] = convert_namespace_to_omegaconf(state[\"args\"])\r\n  File \"/home/jyt/.cache/torch/hub/pytorch_fairseq_master/fairseq/dataclass/utils.py\", line 353, in convert_namespace_to_omegaconf\r\n    composed_cfg = compose(\"config\", overrides=overrides, strict=False)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/hydra/experimental/compose.py\", line 37, in compose\r\n    with_log_configuration=False,\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/hydra/_internal/hydra.py\", line 512, in compose_config\r\n    from_shell=from_shell,\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/hydra/_internal/config_loader_impl.py\", line 156, in load_configuration\r\n    from_shell=from_shell,\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/hydra/_internal/config_loader_impl.py\", line 277, in _load_configuration\r\n    ConfigLoaderImpl._apply_overrides_to_config(config_overrides, cfg)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/hydra/_internal/config_loader_impl.py\", line 522, in _apply_overrides_to_config\r\n    ) from ex\r\nhydra.errors.ConfigCompositionException: Error merging override checkpoint.save_interval_updates=null`\r\n\r\nI'm wondering if anyone could tell me what's wrong with it. Thanks in advance!\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3283/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3283/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3279", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3279/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3279/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3279/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3279", "id": 816051997, "node_id": "MDU6SXNzdWU4MTYwNTE5OTc=", "number": 3279, "title": "LanguagePairDataset breaks the relation between a given sentence and constraints", "user": {"login": "hiromu", "id": 453187, "node_id": "MDQ6VXNlcjQ1MzE4Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/453187?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hiromu", "html_url": "https://github.com/hiromu", "followers_url": "https://api.github.com/users/hiromu/followers", "following_url": "https://api.github.com/users/hiromu/following{/other_user}", "gists_url": "https://api.github.com/users/hiromu/gists{/gist_id}", "starred_url": "https://api.github.com/users/hiromu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hiromu/subscriptions", "organizations_url": "https://api.github.com/users/hiromu/orgs", "repos_url": "https://api.github.com/users/hiromu/repos", "events_url": "https://api.github.com/users/hiromu/events{/privacy}", "received_events_url": "https://api.github.com/users/hiromu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-02-25T03:36:23Z", "updated_at": "2021-03-01T20:38:11Z", "closed_at": "2021-03-01T20:38:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThe current implementation of LanguagePairDataset breaks the relation between a given sentence and constraints.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd `echo -e \"Ja, wer hat, wenn du willst, G\u00f6tter gebildet, uns zu ihnen erhoben, sie zu uns herniedergebracht, als der Dichter?\\tbard\\nZu vollenden ist nicht die Sache des Sch\u00fclers, es ist genug, wenn er sich \u00fcbt\\tstudent\" | python normalize.py | python tok.py | fairseq-interactive --constraints -s de -t en --beam 10 --batch-size 2 --buffer-size 2 --bpe fastbpe --bpe-codes ../../../models/ende30k.fastbpe.code --path ../../../models/wmt19.de-en.ffn8192.pt ../../../models/`\r\n2. See the following output in which the first constraint \"bard\" is associated with the second sentence and vice versa.\r\n\r\n```\r\nS-0\tJa , wer hat , wenn du will@@ st , G\u00f6@@ tter gebildet , uns zu ihnen erhoben , sie zu uns her@@ nieder@@ gebracht , als der Dich@@ ter ?\r\nW-0\t1.755\tseconds\r\nC-0\tstudent\r\nH-0\t-1.1425577402114868\tYes , who , if you will , has formed go@@ ds , raised us up to them , brought them down to us , but the po@@ et student ?\r\nD-0\t-1.1425577402114868\tYes , who , if you will , has formed gods , raised us up to them , brought them down to us , but the poet student ?\r\nP-0\t-1.8768 -0.2214 -0.4671 -1.2521 -0.2101 -0.3053 -1.2077 -0.1496 -1.8780 -1.4195 -0.4071 -0.1347 -0.3726 -1.1306 -0.1665 -1.4588 -0.2837 -0.1722 -0.2330 -0.2840 -0.1806 -0.1432 -0.2263 -0.1395 -0.7261 -1.4593 -0.3639 -0.4030 -0.1083 -18.7577 -0.2396 -0.1837\r\nS-1\tZu voll@@ enden ist nicht die Sache des Sch@@ \u00fcl@@ ers , es ist genug , wenn er sich \u00fcbt\r\nW-1\t1.755\tseconds\r\nC-1\tb@@ ard\r\nH-1\t-1.9625756740570068\tIt is not up to the b@@ ard to complete , it is enough if he practi@@ ses\r\nD-1\t-1.9625756740570068\tIt is not up to the bard to complete , it is enough if he practises\r\nP-1\t-1.2630 -0.3364 -0.1634 -2.7070 -0.1734 -0.2815 -17.3978 -6.0238 -0.4888 -1.7563 -0.8708 -0.6773 -0.2027 -0.2456 -1.6366 -0.2911 -2.0235 -0.1961 -0.5538\r\n```\r\n### Expected behavior\r\n\r\nEach constraint is associated with the sentence given in the same line, as follows (specifying the batch size as 1 avoids the shuffle problem).\r\n\r\n```\r\nS-0\tJa , wer hat , wenn du will@@ st , G\u00f6@@ tter gebildet , uns zu ihnen erhoben , sie zu uns her@@ nieder@@ gebracht , als der Dich@@ ter ?\r\nW-0\t1.740\tseconds\r\nC-0\tb@@ ard\r\nH-0\t-1.2060465812683105\tYes , who , if you will , formed go@@ ds , raised us up to them , brought them down to us , but the b@@ ard ?\r\nD-0\t-1.2060465812683105\tYes , who , if you will , formed gods , raised us up to them , brought them down to us , but the bard ?\r\nP-0\t-1.8768 -0.2214 -0.4671 -1.2521 -0.2101 -0.3053 -1.2077 -0.1496 -2.2551 -0.5702 -0.1331 -0.3940 -1.0268 -0.1750 -1.4635 -0.2821 -0.1725 -0.2404 -0.3575 -0.1833 -0.1441 -0.2250 -0.1419 -0.7020 -1.5215 -0.3700 -16.8578 -2.7290 -0.3405 -0.2060\r\nS-1\tZu voll@@ enden ist nicht die Sache des Sch@@ \u00fcl@@ ers , es ist genug , wenn er sich \u00fcbt\r\nW-1\t1.740\tseconds\r\nC-1\tstudent\r\nH-1\t-0.8064212203025818\tIt is not up to the student to complete , it is enough if he practi@@ ses\r\nD-1\t-0.8064212203025818\tIt is not up to the student to complete , it is enough if he practises\r\nP-1\t-1.2630 -0.3364 -0.1634 -2.7070 -0.1734 -0.2815 -1.5556 -0.2831 -1.3885 -0.7310 -0.6367 -0.1824 -0.2386 -1.5320 -0.2728 -2.0003 -0.2163 -0.5536\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.7.1\r\n - OS (e.g., Linux): Mac\r\n - How you installed fairseq (`pip`, source): pip install --editable ./ (from the master branch)\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.9.2\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information:", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3279/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3279/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3278", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3278/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3278/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3278/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3278", "id": 815947248, "node_id": "MDU6SXNzdWU4MTU5NDcyNDg=", "number": 3278, "title": "wav2vec2 padding issue", "user": {"login": "jeffxtang", "id": 535090, "node_id": "MDQ6VXNlcjUzNTA5MA==", "avatar_url": "https://avatars.githubusercontent.com/u/535090?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeffxtang", "html_url": "https://github.com/jeffxtang", "followers_url": "https://api.github.com/users/jeffxtang/followers", "following_url": "https://api.github.com/users/jeffxtang/following{/other_user}", "gists_url": "https://api.github.com/users/jeffxtang/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeffxtang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeffxtang/subscriptions", "organizations_url": "https://api.github.com/users/jeffxtang/orgs", "repos_url": "https://api.github.com/users/jeffxtang/repos", "events_url": "https://api.github.com/users/jeffxtang/events{/privacy}", "received_events_url": "https://api.github.com/users/jeffxtang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-02-25T00:06:23Z", "updated_at": "2021-08-20T20:43:10Z", "closed_at": "2021-08-20T20:43:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI'm trying to use the https://github.com/pytorch/fairseq/tree/master/examples/wav2vec model on mobile, as I can trace but not script the model, I tried using padding to allow for different audio inputs. But calling `Wav2Vec2Tokenizer` with `max_length` padding results in bad recognition results if `max_length` gets larger.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nBelow is the code and results - padding a 1.4s wav to 3s or 5s lead to about the same recognition, but to 7s or 9s lead to worse and worse results:\r\n\r\n```\r\nimport soundfile as sf\r\nimport torch\r\nfrom transformers import Wav2Vec2ForMaskedLM, Wav2Vec2Tokenizer\r\n\r\ntokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\r\nmodel = Wav2Vec2ForMaskedLM.from_pretrained(\"facebook/wav2vec2-base-960h\")\r\nmodel.eval()\r\naudio_input, _ = sf.read(\"ionlywishtobealone.wav\")\r\ninput_values = tokenizer(audio_input, return_tensors=\"pt\").input_values\r\nlogits = model(input_values).logits\r\npredicted_ids = torch.argmax(logits, dim=-1)\r\ntranscription = tokenizer.batch_decode(predicted_ids)[0]\r\nprint(transcription)\r\nprint(input_values.shape)\r\n\r\n# pad the 1.42s audio to about 3,5,7,9s:\r\nfor n in [3,5,7,9]:\r\n  input_values = tokenizer(audio_input, return_tensors=\"pt\", padding=\"max_length\", max_length=n*10000).input_values\r\n  logits = model(input_values).logits\r\n  predicted_ids = torch.argmax(logits, dim=-1)\r\n  transcription = tokenizer.batch_decode(predicted_ids)[0]\r\n  print(input_values.shape)\r\n  print(\"padding to {}s: {}\\n\".format(n, transcription))\r\n```\r\n\r\n```\r\nI ONLY WAS TO BE ALONE\r\ntorch.Size([1, 15616])\r\n\r\ntorch.Size([1, 30000])\r\npadding to 3s: I ONLY RIS TO BE ALONE\r\n\r\ntorch.Size([1, 50000])\r\npadding to 5s: I ONLY IS TO BE ALONE\r\n\r\ntorch.Size([1, 70000])\r\npadding to 7s: ONER IS TO BE LON\r\n\r\ntorch.Size([1, 90000])\r\npadding to 9s: ONERISTO BELAN\r\n```\r\n\r\n\r\n### Expected behavior\r\nPadding to longer audio should lead to the same results.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): NA\r\n - PyTorch Version (e.g., 1.0) 1.7 & master\r\n - OS (e.g., Linux): Mac\r\n - How you installed fairseq (`pip`, source): NA\r\n - Build command you used (if compiling from source): NA\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: NA\r\n - GPU models and configuration: NA\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3278/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3278/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3274", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3274/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3274/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3274/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3274", "id": 815392702, "node_id": "MDU6SXNzdWU4MTUzOTI3MDI=", "number": 3274, "title": "Problem with TensorboardX", "user": {"login": "salvacarrion", "id": 6052046, "node_id": "MDQ6VXNlcjYwNTIwNDY=", "avatar_url": "https://avatars.githubusercontent.com/u/6052046?v=4", "gravatar_id": "", "url": "https://api.github.com/users/salvacarrion", "html_url": "https://github.com/salvacarrion", "followers_url": "https://api.github.com/users/salvacarrion/followers", "following_url": "https://api.github.com/users/salvacarrion/following{/other_user}", "gists_url": "https://api.github.com/users/salvacarrion/gists{/gist_id}", "starred_url": "https://api.github.com/users/salvacarrion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/salvacarrion/subscriptions", "organizations_url": "https://api.github.com/users/salvacarrion/orgs", "repos_url": "https://api.github.com/users/salvacarrion/repos", "events_url": "https://api.github.com/users/salvacarrion/events{/privacy}", "received_events_url": "https://api.github.com/users/salvacarrion/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-02-24T11:33:15Z", "updated_at": "2021-12-22T10:52:08Z", "closed_at": "2021-02-24T12:22:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nSeems that there is a problem with tensorboardX and the multi-gpu training.\r\nThe first epoch trains correctly, but when it has to start with the logging or evaluation, I get a TypeError.\r\n\r\nPs.: If I set `CUDA_VISIBLE_DEVICES=0`, fairseq keeps using all GPUs\r\n\r\n### To Reproduce\r\n\r\n**Commands:**\r\n\r\n```\r\n# Train model\r\nfairseq-train \\\r\n    $BASE_PATH/data-bin \\\r\n    --arch transformer \\\r\n    --optimizer adam \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens 4096 \\\r\n    --num-workers\t$(nproc) \\\r\n    --max-epoch\t50 \\\r\n    --seed 1 \\\r\n    --save-dir $BASE_PATH/checkpoints \\\r\n    --log-format simple \\\r\n    --no-epoch-checkpoints \\\r\n    --tensorboard-logdir $BASE_PATH/logdir \\\r\n    --update-freq 8 \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\r\n    --eval-bleu-detok moses \\\r\n    --eval-bleu-remove-bpe \\\r\n    --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\r\n    --clip-norm 1.0 \\\r\n    --lr 1e-3  \\\r\n    --lr-scheduler reduce_lr_on_plateau  \\\r\n    --warmup-updates 4000 \\\r\n    --dropout 0.1 --weight-decay 0.0001 \\\r\n    --patience 5\r\n\r\n```\r\n\r\n#### Output\r\n\r\n\r\n```\r\n2021-02-24 12:04:30 | INFO | fairseq.trainer | loading train data for epoch 1\r\n2021-02-24 12:04:30 | INFO | fairseq.data.data_utils | loaded 575482 examples from: /home/scarrion/datasets/scielo/fairseq/scielo_health_es_en/data-bin/train.es-en.es\r\n2021-02-24 12:04:30 | INFO | fairseq.data.data_utils | loaded 575482 examples from: /home/scarrion/datasets/scielo/fairseq/scielo_health_es_en/data-bin/train.es-en.en\r\n2021-02-24 12:04:30 | INFO | fairseq.tasks.translation | /home/scarrion/datasets/scielo/fairseq/scielo_health_es_en/data-bin train es-en 575482 examples\r\n2021-02-24 12:04:31 | INFO | fairseq.optim.adam | using FusedAdam\r\n2021-02-24 12:04:31 | INFO | fairseq.trainer | begin training epoch 1\r\n/home/scarrion/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/distributed.py:398: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.\r\n  \"The `check_reduction` argument in `DistributedDataParallel` \"\r\n/home/scarrion/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/distributed.py:398: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.\r\n  \"The `check_reduction` argument in `DistributedDataParallel` \"\r\n2021-02-24 12:04:44 | INFO | root | Reducer buckets have been rebuilt in this iteration.\r\n2021-02-24 12:07:44 | INFO | train_inner | epoch 001:    100 / 326 loss=12.985, nll_loss=12.715, ppl=6725, wps=29783.4, ups=0.55, wpb=54148.3, bsz=1758, num_updates=100, lr=2.5e-05, gnorm=2.821, clip=98, train_wall=182, wall=194\r\n2021-02-24 12:10:53 | INFO | train_inner | epoch 001:    200 / 326 loss=10.878, nll_loss=10.342, ppl=1297.6, wps=28988.4, ups=0.53, wpb=54737.5, bsz=1769, num_updates=200, lr=5e-05, gnorm=1.684, clip=96, train_wall=189, wall=382\r\n2021-02-24 12:14:03 | INFO | train_inner | epoch 001:    300 / 326 loss=9.916, nll_loss=9.208, ppl=591.35, wps=28726.6, ups=0.53, wpb=54565.3, bsz=1767.5, num_updates=300, lr=7.5e-05, gnorm=1.819, clip=100, train_wall=190, wall=572\r\n2021-02-24 12:14:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n2021-02-24 12:15:04 | INFO | fairseq.tasks.translation | example hypothesis: Conclusions.\r\n2021-02-24 12:15:04 | INFO | fairseq.tasks.translation | example reference: Conclusions.\r\nException in thread Thread-2:\r\nTraceback (most recent call last):\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/site-packages/tensorboardX/event_file_writer.py\", line 202, in run\r\n    data = self._queue.get(True, queue_wait_duration)\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 108, in get\r\n    res = self._recv_bytes()\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 383, in _recv\r\n    raise EOFError\r\nEOFError\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/scarrion/anaconda3/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/scarrion/packages/fairseq-0.10.2/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/home/scarrion/packages/fairseq-0.10.2/fairseq/distributed_utils.py\", line 286, in call_main\r\n    nprocs=args.distributed_num_procs,\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\r\n    while not context.join():\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/scarrion/packages/fairseq-0.10.2/fairseq/distributed_utils.py\", line 270, in distributed_main\r\n    main(args, **kwargs)\r\n  File \"/home/scarrion/packages/fairseq-0.10.2/fairseq_cli/train.py\", line 125, in main\r\n    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/scarrion/packages/fairseq-0.10.2/fairseq_cli/train.py\", line 223, in train\r\n    args, trainer, task, epoch_itr, valid_subsets, end_of_epoch\r\n  File \"/home/scarrion/packages/fairseq-0.10.2/fairseq_cli/train.py\", line 266, in validate_and_save\r\n    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\r\n  File \"/home/scarrion/packages/fairseq-0.10.2/fairseq_cli/train.py\", line 323, in validate\r\n    trainer.valid_step(sample)\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/scarrion/packages/fairseq-0.10.2/fairseq/trainer.py\", line 764, in valid_step\r\n    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)\r\n  File \"/home/scarrion/packages/fairseq-0.10.2/fairseq/trainer.py\", line 1068, in _reduce_and_log_stats\r\n    self.task.reduce_metrics(logging_outputs, self.get_criterion())\r\n  File \"/home/scarrion/packages/fairseq-0.10.2/fairseq/tasks/translation.py\", line 375, in reduce_metrics\r\n    metrics.log_scalar(\"_bleu_counts\", np.array(counts))\r\n  File \"/home/scarrion/anaconda3/lib/python3.7/site-packages/torch/tensor.py\", line 630, in __array__\r\n    return self.numpy()\r\nTypeError: can't convert cuda:1 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version: 0.10.2\r\n - PyTorch Version: 1.7.0\r\n - tensorboardX version==2.1\r\n - OS: Linux\r\n - How you installed fairseq (`pip`, source): \r\n```\r\n (From releases)\r\npip install --editable ./\r\n```\r\n - Python version: Python 3.7.4\r\n - CUDA/cuDNN version: CUDA Version: 11.0 \r\n - GPU models and configuration: Titan XP\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3274/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3274/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3260", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3260/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3260/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3260/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3260", "id": 812770638, "node_id": "MDU6SXNzdWU4MTI3NzA2Mzg=", "number": 3260, "title": "S2T: Very high wer when using pre-trained model on librispeech", "user": {"login": "yyyyxie", "id": 45864638, "node_id": "MDQ6VXNlcjQ1ODY0NjM4", "avatar_url": "https://avatars.githubusercontent.com/u/45864638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yyyyxie", "html_url": "https://github.com/yyyyxie", "followers_url": "https://api.github.com/users/yyyyxie/followers", "following_url": "https://api.github.com/users/yyyyxie/following{/other_user}", "gists_url": "https://api.github.com/users/yyyyxie/gists{/gist_id}", "starred_url": "https://api.github.com/users/yyyyxie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yyyyxie/subscriptions", "organizations_url": "https://api.github.com/users/yyyyxie/orgs", "repos_url": "https://api.github.com/users/yyyyxie/repos", "events_url": "https://api.github.com/users/yyyyxie/events{/privacy}", "received_events_url": "https://api.github.com/users/yyyyxie/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-02-21T04:40:50Z", "updated_at": "2021-02-21T08:17:41Z", "closed_at": "2021-02-21T08:17:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nI am trying to reproduce the result of using naive transformer to do the s2t on librispeech dataset. But by running the pretrained model, I got very high wer as 101.10 (sould be under 10)\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n1. preprocess the data: \r\npython  examples/speech_to_text/prep_librispeech_data.py  --output-root /data/LibriSpeech --vocab-type unigram --vocab-size 10000\r\nThen I got tsv file for training and test audio features.\r\nAnd I also got spm_unigram10000.model , spm_unigram10000.txt and config.yaml files ready.\r\n\r\n2. Evaluate the pretrained model.\r\nI downloaded the librispeech pretrained model [here](https://github.com/pytorch/fairseq/blob/master/examples/speech_to_text/docs/librispeech_example.md)\r\nand run the cmd: \r\nfairseq-generate /data/LibriSpeech  --config-yaml config.yaml --gen-subset dev-clean --task speech_to_text    --path /home/librispeech_transformer_s.pt --max-tokens 50000 --beam 5 --scoring wer\r\n\r\n3. Result:\r\nI got wer over 30 when using my own spm_unigram10000 from data preprocessing.\r\n And the wer result is more than 100 when I use the spm_unigram10000 files you provided with the pratrained model.\r\nSimilar results happened on transformer_m model as well.\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nCould you check with the spm_unigram10000 files. Do they match with the given pretrained model? Or is there anything I missed? Thanks!\r\n \r\n### Environment\r\n\r\n - fairseq Version : 1.0\r\n - PyTorch Version  1.7.1\r\n - How you installed fairseq (`pip`, source): pip\r\n - Python version: 3.8.3\r\n - CUDA/cuDNN version: 11.0", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3260/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3260/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3259", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3259/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3259/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3259/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3259", "id": 812671150, "node_id": "MDU6SXNzdWU4MTI2NzExNTA=", "number": 3259, "title": "fairseq-preprocess does not work while training custom model", "user": {"login": "akash-isu", "id": 65557029, "node_id": "MDQ6VXNlcjY1NTU3MDI5", "avatar_url": "https://avatars.githubusercontent.com/u/65557029?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akash-isu", "html_url": "https://github.com/akash-isu", "followers_url": "https://api.github.com/users/akash-isu/followers", "following_url": "https://api.github.com/users/akash-isu/following{/other_user}", "gists_url": "https://api.github.com/users/akash-isu/gists{/gist_id}", "starred_url": "https://api.github.com/users/akash-isu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akash-isu/subscriptions", "organizations_url": "https://api.github.com/users/akash-isu/orgs", "repos_url": "https://api.github.com/users/akash-isu/repos", "events_url": "https://api.github.com/users/akash-isu/events{/privacy}", "received_events_url": "https://api.github.com/users/akash-isu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "jmp84", "id": 1144053, "node_id": "MDQ6VXNlcjExNDQwNTM=", "avatar_url": "https://avatars.githubusercontent.com/u/1144053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmp84", "html_url": "https://github.com/jmp84", "followers_url": "https://api.github.com/users/jmp84/followers", "following_url": "https://api.github.com/users/jmp84/following{/other_user}", "gists_url": "https://api.github.com/users/jmp84/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmp84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmp84/subscriptions", "organizations_url": "https://api.github.com/users/jmp84/orgs", "repos_url": "https://api.github.com/users/jmp84/repos", "events_url": "https://api.github.com/users/jmp84/events{/privacy}", "received_events_url": "https://api.github.com/users/jmp84/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jmp84", "id": 1144053, "node_id": "MDQ6VXNlcjExNDQwNTM=", "avatar_url": "https://avatars.githubusercontent.com/u/1144053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmp84", "html_url": "https://github.com/jmp84", "followers_url": "https://api.github.com/users/jmp84/followers", "following_url": "https://api.github.com/users/jmp84/following{/other_user}", "gists_url": "https://api.github.com/users/jmp84/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmp84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmp84/subscriptions", "organizations_url": "https://api.github.com/users/jmp84/orgs", "repos_url": "https://api.github.com/users/jmp84/repos", "events_url": "https://api.github.com/users/jmp84/events{/privacy}", "received_events_url": "https://api.github.com/users/jmp84/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2021-02-20T17:47:57Z", "updated_at": "2021-02-25T07:15:38Z", "closed_at": "2021-02-25T07:15:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nFollowing the tutorial available at https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md to train a custom model using Roberta. Gets stuck with at the preprocess step with a few errors.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nFollow the exact steps in the tutorial\r\nRunning this command throws the error.\r\n\r\nfairseq-preprocess --only-source --srcdict gpt2_bpe/dict.txt --trainpref wikitext-103-raw/wiki.train.bpe --validpref wikitext-103-raw/wiki.valid.bpe --testpref wikitext-103-raw/wiki.test.bpe --destdir data-bin/wikitext-103 --workers 60\r\n\r\n**Stack trace:**\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/fairseq-preprocess\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-preprocess')())\r\n  File \"/usr/local/bin/fairseq-preprocess\", line 25, in importlib_load_entry_point\r\n    return next(matches).load()\r\n  File \"/usr/lib/python3.8/importlib/metadata.py\", line 77, in load\r\n    module = import_module(match.group('module'))\r\n  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/adutta/Documents/git_repo/fairseq/fairseq_cli/preprocess.py\", line 18, in <module>\r\n    from fairseq import options, tasks, utils\r\n  File \"/home/adutta/Documents/git_repo/fairseq/fairseq/__init__.py\", line 32, in <module>\r\n    import fairseq.criterions  # noqa\r\n  File \"/home/adutta/Documents/git_repo/fairseq/fairseq/criterions/__init__.py\", line 36, in <module>\r\n    importlib.import_module(\"fairseq.criterions.\" + file_name)\r\n  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/adutta/Documents/git_repo/fairseq/fairseq/criterions/label_smoothed_cross_entropy_latency_augmented.py\", line 6, in <module>\r\n    from examples.simultaneous_translation.utils.latency import LatencyTraining\r\n  File \"/home/adutta/Documents/git_repo/fairseq/examples/simultaneous_translation/__init__.py\", line 6, in <module>\r\n    from . import criterions, eval, models  # noqa\r\n  File \"/home/adutta/Documents/git_repo/fairseq/examples/simultaneous_translation/models/__init__.py\", line 13, in <module>\r\n    importlib.import_module(\r\n  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/adutta/Documents/git_repo/fairseq/examples/simultaneous_translation/models/transformer_monotonic_attention.py\", line 13, in <module>\r\n    from fairseq.models import (\r\n  File \"/home/adutta/Documents/git_repo/fairseq/fairseq/models/__init__.py\", line 208, in <module>\r\n    module = importlib.import_module(\"fairseq.models.\" + model_name)\r\n  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/adutta/Documents/git_repo/fairseq/fairseq/models/speech_to_text/__init__.py\", line 8, in <module>\r\n    from .convtransformer_simul_trans import *  # noqa\r\n  File \"/home/adutta/Documents/git_repo/fairseq/fairseq/models/speech_to_text/convtransformer_simul_trans.py\", line 8, in <module>\r\n    from examples.simultaneous_translation.models.transformer_monotonic_attention import (\r\nImportError: cannot import name 'TransformerMonotonicDecoder' from partially initialized module 'examples.simultaneous_translation.models.transformer_monotonic_attention' (most likely due to a circular import) (/home/adutta/Documents/git_repo/fairseq/examples/simultaneous_translation/models/transformer_monotonic_attention.py)\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master (latest pull from Github)\r\n - PyTorch Version (e.g., 1.0): 1.7.1\r\n - OS (e.g., Linux): Ubuntu 20.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): git clone https://github.com/pytorch/fairseq; cd fairseq; pip3 install --editable ./\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 11.2\r\n - GPU models and configuration: GeForce RTX 2080", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3259/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3259/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3254", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3254/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3254/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3254/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3254", "id": 811196320, "node_id": "MDU6SXNzdWU4MTExOTYzMjA=", "number": 3254, "title": "Bart CNN example breaks", "user": {"login": "snailrowen1337", "id": 45402632, "node_id": "MDQ6VXNlcjQ1NDAyNjMy", "avatar_url": "https://avatars.githubusercontent.com/u/45402632?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snailrowen1337", "html_url": "https://github.com/snailrowen1337", "followers_url": "https://api.github.com/users/snailrowen1337/followers", "following_url": "https://api.github.com/users/snailrowen1337/following{/other_user}", "gists_url": "https://api.github.com/users/snailrowen1337/gists{/gist_id}", "starred_url": "https://api.github.com/users/snailrowen1337/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snailrowen1337/subscriptions", "organizations_url": "https://api.github.com/users/snailrowen1337/orgs", "repos_url": "https://api.github.com/users/snailrowen1337/repos", "events_url": "https://api.github.com/users/snailrowen1337/events{/privacy}", "received_events_url": "https://api.github.com/users/snailrowen1337/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2021-02-18T15:19:49Z", "updated_at": "2021-02-25T16:02:22Z", "closed_at": "2021-02-25T16:02:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI am reproducing the Bart/CNN results from https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md, but the code crashes during inference.\r\n\r\n### To Reproduce\r\n\r\nFollow the instructions from https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md, exchange the architecture bart_large->bart_base during finetuning, the code then crashes at inference. I put the code mention in the instructions in ```eval_cnn.py```, i.e.\r\n\r\n\r\n```\r\nimport torch\r\nfrom fairseq.models.bart import BARTModel\r\n\r\nbart = BARTModel.from_pretrained(\r\n    'checkpoints/',\r\n    checkpoint_file='checkpoint_best.pt',\r\n    data_name_or_path='cnn_dm-bin'\r\n)\r\n\r\nbart.cuda()\r\nbart.eval()\r\nbart.half()\r\n...\r\n```\r\n\r\nrunning this then yields\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"eval_cnn.py\", line 27, in <module>\r\n    hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\r\n  File \"/home/fairseq/fairseq/hub_utils.py\", line 132, in sample\r\n    batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)\r\n  File \"/home/fairseq/fairseq/models/bart/hub_interface.py\", line 109, in generate\r\n    **kwargs\r\n  File \"/home/fairseq/fairseq/hub_utils.py\", line 174, in generate\r\n    generator, self.models, batch, **inference_step_args\r\n  File \"/home/fairseq/fairseq/tasks/fairseq_task.py\", line 501, in inference_step\r\n    models, sample, prefix_tokens=prefix_tokens, constraints=constraints\r\n  File \"/home/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 26, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/fairseq/fairseq/sequence_generator.py\", line 182, in generate\r\n    return self._generate(sample, **kwargs)\r\n  File \"/home/fairseq/fairseq/sequence_generator.py\", line 353, in _generate\r\n    step, lprobs, scores, tokens, prefix_tokens, beam_size\r\n  File \"/home/fairseq/fairseq/sequence_generator.py\", line 560, in _prefix_tokens\r\n    prefix_lprobs = lprobs.gather(-1, prefix_toks.unsqueeze(-1))\r\nRuntimeError: Size does not match at dimension 0 expected index [128, 1] to be smaller than src [20, 50264] apart from dimension 1\r\n\r\n```\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nIt should not crash.\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0) 1.7\r\n - OS (e.g., Linux): CentOs\r\n - How you installed fairseq (`pip`, source): pip\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: V100\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3254/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3254/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3246", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3246/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3246/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3246/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3246", "id": 808650581, "node_id": "MDU6SXNzdWU4MDg2NTA1ODE=", "number": 3246, "title": "Transformer model is not compatible with Torchscript when \"--no-token-positional-embeddings\" is enabled", "user": {"login": "madelagua", "id": 63397416, "node_id": "MDQ6VXNlcjYzMzk3NDE2", "avatar_url": "https://avatars.githubusercontent.com/u/63397416?v=4", "gravatar_id": "", "url": "https://api.github.com/users/madelagua", "html_url": "https://github.com/madelagua", "followers_url": "https://api.github.com/users/madelagua/followers", "following_url": "https://api.github.com/users/madelagua/following{/other_user}", "gists_url": "https://api.github.com/users/madelagua/gists{/gist_id}", "starred_url": "https://api.github.com/users/madelagua/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/madelagua/subscriptions", "organizations_url": "https://api.github.com/users/madelagua/orgs", "repos_url": "https://api.github.com/users/madelagua/repos", "events_url": "https://api.github.com/users/madelagua/events{/privacy}", "received_events_url": "https://api.github.com/users/madelagua/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-02-15T15:56:20Z", "updated_at": "2021-02-22T22:23:04Z", "closed_at": "2021-02-22T22:23:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIf we try to script a transformer model without positional embeddings (--no-token-positional-embeddings) with torch.jit.script, we get the following error:\r\n\r\n```\r\n\"cannot call a value of type 'None'\"\r\n         File \"fairseq/models/transformer.py\", line 780\r\n               # embed positions\r\n               positions = (\r\n                   self.embed_positions(\r\n                   ~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n                       prev_output_tokens, incremental_state=incremental_state\r\n                   )\r\n```\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. This test should check that use case (leveraging tests/test_export.py classes):\r\n\r\n```\r\n    @unittest.skipIf(\r\n        torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\"\r\n    )\r\n    def test_export_transformer_no_token_pos_emb(self):\r\n        task, parser = get_dummy_task_and_parser()\r\n        TransformerModel.add_args(parser)\r\n        args = parser.parse_args([])\r\n        args.no_token_positional_embeddings = True\r\n        model = TransformerModel.build_model(args, task)\r\n        scripted = torch.jit.script(model)\r\n        _test_save_and_load(scripted)\r\n```\r\n\r\n2. See error (from above)\r\n\r\n### Expected behavior\r\n\r\nNo error should be shown.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.71\r\n - OS (e.g., Linux): centos \r\n - Python version: 3.8.1\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3246/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3246/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3241", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3241/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3241/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3241/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3241", "id": 807563859, "node_id": "MDU6SXNzdWU4MDc1NjM4NTk=", "number": 3241, "title": "Omegaconf MissingMandatoryValue error trying to train a base model Wav2Vec2.0", "user": {"login": "gandroz", "id": 38106881, "node_id": "MDQ6VXNlcjM4MTA2ODgx", "avatar_url": "https://avatars.githubusercontent.com/u/38106881?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gandroz", "html_url": "https://github.com/gandroz", "followers_url": "https://api.github.com/users/gandroz/followers", "following_url": "https://api.github.com/users/gandroz/following{/other_user}", "gists_url": "https://api.github.com/users/gandroz/gists{/gist_id}", "starred_url": "https://api.github.com/users/gandroz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gandroz/subscriptions", "organizations_url": "https://api.github.com/users/gandroz/orgs", "repos_url": "https://api.github.com/users/gandroz/repos", "events_url": "https://api.github.com/users/gandroz/events{/privacy}", "received_events_url": "https://api.github.com/users/gandroz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-02-12T21:39:06Z", "updated_at": "2021-02-12T22:12:52Z", "closed_at": "2021-02-12T22:12:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. After building a manifest from LibriSpeech, I tried training a base model Wav2Vec2.0\r\n2. Use the configuration `wav2vec/config/wav2vec2_base_librispeech.yml` \r\n3. Add task.data directly in the config file instead of passing the argument to the CLI \r\n4. Run command `HYDRA_FULL_ERROR=1 fairseq-hydra-train --config-dir ~/src/fairseq_dev --config-name wav2vec2_base_config` \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\nTraceback (most recent call last):\r\n  File \"/home/guillaume/miniconda/envs/fairseq/bin/fairseq-hydra-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-hydra-train')())\r\n  File \"/home/guillaume/src/fairseq/fairseq_cli/hydra_train.py\", line 91, in cli_main\r\n    hydra_main()\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/hydra/main.py\", line 32, in decorated_main\r\n    _run_hydra(\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/hydra/_internal/utils.py\", line 346, in _run_hydra\r\n    run_and_report(\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/hydra/_internal/utils.py\", line 201, in run_and_report\r\n    raise ex\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/hydra/_internal/utils.py\", line 198, in run_and_report\r\n    return func()\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/hydra/_internal/utils.py\", line 347, in <lambda>\r\n    lambda: hydra.run(\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/hydra/_internal/hydra.py\", line 107, in run\r\n    return run_job(\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/hydra/core/utils.py\", line 127, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"/home/guillaume/src/fairseq/fairseq_cli/hydra_train.py\", line 27, in hydra_main\r\n    add_defaults(cfg)\r\n  File \"/home/guillaume/src/fairseq/fairseq/dataclass/initialize.py\", line 42, in add_defaults\r\n    field_cfg = cfg.get(k)\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 357, in get\r\n    self._format_and_raise(key=key, value=None, cause=e)\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/omegaconf/base.py\", line 95, in _format_and_raise\r\n    format_and_raise(\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/home/guillaume/miniconda/envs/fairseq/lib/python3.8/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.MissingMandatoryValue: Missing mandatory value: model\r\n\tfull_key: model\r\n\treference_type=Optional[FairseqConfig]\r\n\tobject_type=FairseqConfig\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.7\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): pip -e\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 11.0\r\n - GPU models and configuration: P4 \r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3241/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3241/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3227", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3227/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3227/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3227/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3227", "id": 805233388, "node_id": "MDU6SXNzdWU4MDUyMzMzODg=", "number": 3227, "title": "[Wav2Vec2] Possible bug with batched input", "user": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-02-10T06:53:26Z", "updated_at": "2021-02-10T22:04:54Z", "closed_at": "2021-02-10T22:04:54Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen using the Wav2Vec2 model with batched input (meaning that both `source` and `padding_mask` are provided), the input is different for padded input than it would be for the same non-padded input. At the moment this accounts for all models:\r\n\r\n- Wav2Vec 2.0 Base\r\n- Wav2Vec 2.0 Large\r\n- Wav2Vec 2.0 Large (LV-60)*\r\n- Wav2Vec 2.0 Large (LV-60) + Self Training *\r\n\r\nIMO, the Base & Large models cannot really be fixed because the convolutional layers apply group norm over the time dimension meaning that adding 0's as padding tokens necessarily changes the result. See the following code snippet.\r\n\r\nOn the other hand the \"new architectures\" Large Lv-60 + Large Lv-60 + Self Training can be fixed as shown in #3228.\r\n\r\n### To Reproduce\r\n\r\nPlease run the following code snippet to reproduce the error:\r\n\r\n```python\r\nimport fairseq\r\nimport torch\r\n\r\n# get model\r\nwav2vec_path = \"data/wav2vec_small_960h.pt\"\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task(\r\n    [wav2vec_path], arg_overrides={\"data\": \"./data\"}\r\n)\r\nmodel = model[0]\r\nmodel.eval()\r\n\r\n# create single input\r\ninput_wav_0 = torch.randn((1, 2000))\r\ninput_wav_1 = torch.randn((1, 3000))\r\n\r\n# create batched input\r\nbatch_input_wav = torch.zeros((2, 3000))\r\nbatch_input_wav[0, :input_wav_0.shape[-1]] = input_wav_0\r\nbatch_input_wav[1, :input_wav_1.shape[-1]] = input_wav_1\r\n\r\n# create padding mask\r\npadding_mask = torch.zeros((2, 3000), dtype=torch.bool)\r\npadding_mask[0, input_wav_0.shape[-1]:] = True\r\n\r\n# run batch & single\r\noutput = model(source=input_wav_0, padding_mask=None)[\"encoder_out\"]\r\nbatch_output = model(source=batch_input_wav, padding_mask=padding_mask)[\"encoder_out\"]\r\n\r\n# is equal?\r\nprint(\"Is batched forward and simple forward equal?\", torch.allclose(output[:,0], batch_output[:output.shape[0], 0], atol=1e-1))\r\n```\r\n\r\n**Note**: It is assumed that both `https://dl.fbaipublicfiles.com/fairseq/wav2vec/dict.ltr.txt` and `https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small_960h.pt` were downloaded and stored in the folder `data`.\r\n\r\nAlso see [this](https://colab.research.google.com/drive/1rvP8N5WmQ4zReoH4yOAXb-ZjBSfsUZ09?usp=sharing) notebook that can be simply run to reproduce the error.\r\n\r\n### Expected behavior\r\n\r\nBoth the padded and identical non-padded input should be equal in my opinion -> it seems like this is however not possible due to the layer norm / group norm layers in the model.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.7.0+cu101\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): !pip install git+https://github.com/pytorch/fairseq\r\n - Build command you used (if compiling from source): just pip\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: No CUDA\r\n - GPU models and configuration: NO GPU\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3227/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3227/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3225", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3225/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3225/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3225/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3225", "id": 804710596, "node_id": "MDU6SXNzdWU4MDQ3MTA1OTY=", "number": 3225, "title": "Question on how to pass pretrained parameters to a self-defined optimizer", "user": {"login": "leonleeldc", "id": 1965156, "node_id": "MDQ6VXNlcjE5NjUxNTY=", "avatar_url": "https://avatars.githubusercontent.com/u/1965156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leonleeldc", "html_url": "https://github.com/leonleeldc", "followers_url": "https://api.github.com/users/leonleeldc/followers", "following_url": "https://api.github.com/users/leonleeldc/following{/other_user}", "gists_url": "https://api.github.com/users/leonleeldc/gists{/gist_id}", "starred_url": "https://api.github.com/users/leonleeldc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leonleeldc/subscriptions", "organizations_url": "https://api.github.com/users/leonleeldc/orgs", "repos_url": "https://api.github.com/users/leonleeldc/repos", "events_url": "https://api.github.com/users/leonleeldc/events{/privacy}", "received_events_url": "https://api.github.com/users/leonleeldc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2021-02-09T16:30:53Z", "updated_at": "2021-02-15T17:25:50Z", "closed_at": "2021-02-15T17:25:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nHi, group,\r\n\r\nI am new to fairseq. I encounter a problem in deploying it. In my project, I have defined a new optimizer. I know the file should be put into fairseq/optimu/my_optimizer.py.\r\n\r\nBut the problem is that in my_optimizer.py, I have a new paprameter, as,  pretrain_params=None.\r\n\r\nWe need to computer differences between pretrain_params and params.\r\n\r\nMy question is how I can pass pretrain_params to this optimizer. \r\nIn huggingface, I can do the following, \r\noptimizer_grouped_parameters = [\r\n{\r\n\"params\": [p for n, p in model.named_parameters() if\r\nnot any(nd in n for nd in no_decay) and args.model_type in n],\r\n\"weight_decay\": args.weight_decay,\r\n\"anneal_w\": args.recadam_anneal_w,\r\n\"pretrain_params\": [p_p for p_n, p_p in pretrained_model.named_parameters() if\r\nnot any(nd in p_n for nd in no_decay) and args.model_type in p_n]\r\n}, ...,]\r\n\r\noptimizer = my_optimizerr(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon,\r\nanneal_fun=args.recadam_anneal_fun, anneal_k=args.recadam_anneal_k,\r\nanneal_t0=args.recadam_anneal_t0, pretrain_cof=args.recadam_pretrain_cof)\r\n\r\nNot sure where fairseq can do this? I tried  to do this in fairseq/trainer.py. But not clear where we can load the pretrained_checkpoint.pt and pass it where for my_optimizer.py to consume. \r\n\r\nMay you guys help with this? Thanks in advance,\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): I am using the latest whole fairseq source package \r\n - Build command you used (if compiling from source): pip install editable .\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: cuda 9\r\n - GPU models and configuration: v100\r\n - Any other relevant information:  \r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3225/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3225/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3224", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3224/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3224/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3224/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3224", "id": 804395156, "node_id": "MDU6SXNzdWU4MDQzOTUxNTY=", "number": 3224, "title": "Issue of reproducing Must-C result on S2T", "user": {"login": "zjw1990", "id": 19264829, "node_id": "MDQ6VXNlcjE5MjY0ODI5", "avatar_url": "https://avatars.githubusercontent.com/u/19264829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zjw1990", "html_url": "https://github.com/zjw1990", "followers_url": "https://api.github.com/users/zjw1990/followers", "following_url": "https://api.github.com/users/zjw1990/following{/other_user}", "gists_url": "https://api.github.com/users/zjw1990/gists{/gist_id}", "starred_url": "https://api.github.com/users/zjw1990/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zjw1990/subscriptions", "organizations_url": "https://api.github.com/users/zjw1990/orgs", "repos_url": "https://api.github.com/users/zjw1990/repos", "events_url": "https://api.github.com/users/zjw1990/events{/privacy}", "received_events_url": "https://api.github.com/users/zjw1990/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2021-02-09T10:01:03Z", "updated_at": "2021-02-24T11:30:16Z", "closed_at": "2021-02-24T11:30:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n2021-02-09 09:46:37 | INFO | fairseq_cli.generate | loading model(s) from /data/mustc/en-fr/mustc_fr_st_transformer_s.pt\r\n/opt/conda/lib/python3.7/site-packages/hydra/_internal/hydra.py:71: UserWarning: \r\n@hydra.main(strict) flag is deprecated and will removed in the next version.\r\nSee \r\nhttps://hydra.cc/docs/next/upgrades/0.11_to_1.0/strict_mode_flag_deprecated\r\n  warnings.warn(message=msg, category=UserWarning)\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/fairseq-generate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())\r\n  File \"/workspace/fairseq/fairseq_cli/generate.py\", line 389, in cli_main\r\n    main(args)\r\n  File \"/workspace/fairseq/fairseq_cli/generate.py\", line 50, in main\r\n    return _main(cfg, sys.stdout)\r\n  File \"/workspace/fairseq/fairseq_cli/generate.py\", line 103, in _main\r\n    num_shards=cfg.checkpoint.checkpoint_shard_count,\r\n  File \"/workspace/fairseq/fairseq/checkpoint_utils.py\", line 264, in load_model_ensemble\r\n    num_shards,\r\n  File \"/workspace/fairseq/fairseq/checkpoint_utils.py\", line 302, in load_model_ensemble_and_task\r\n    model = task.build_model(cfg.model)\r\n  File \"/workspace/fairseq/fairseq/tasks/speech_to_text.py\", line 109, in build_model\r\n    return super(SpeechToTextTask, self).build_model(args)\r\n  File \"/workspace/fairseq/fairseq/tasks/fairseq_task.py\", line 548, in build_model\r\n    model = models.build_model(args, self)\r\n  File \"/workspace/fairseq/fairseq/models/__init__.py\", line 56, in build_model\r\n    return ARCH_MODEL_REGISTRY[cfg.arch].build_model(cfg, task)\r\n  File \"/workspace/fairseq/fairseq/models/speech_to_text/s2t_transformer.py\", line 237, in build_model\r\n    encoder = cls.build_encoder(args)\r\n  File \"/workspace/fairseq/fairseq/models/speech_to_text/s2t_transformer.py\", line 210, in build_encoder\r\n    component=encoder, checkpoint=args.load_pretrained_encoder_from\r\n  File \"/workspace/fairseq/fairseq/checkpoint_utils.py\", line 576, in load_pretrained_component_from_model\r\n    raise IOError(\"Model file not found: {}\".format(checkpoint))\r\nOSError: Model file not found: /checkpoint/changhan/projects/fairseq_s2t/mustc.task_fr_asr_unigram5000.ls_0.1.arch_s2t_transformer_s.dp_0.1.lr_0.001.wu_10000.ngpu8/avg_last_10_checkpoint.pt\r\nexecute status:1\r\nfailed\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nUsing Fairseq-genertae. The path seems to be fixed in the checkpoint. I used exact the same command lines provided\r\n\r\n#!/bin/bash\r\n\r\nMUSTC_ROOT=/mustc\r\nST_SAVE_DIR=${MUSTC_ROOT}/en-fr\r\nFAIR_ROOT=/fairseq\r\nCHECKPOINT_FILENAME=mustc_fr_st_transformer_s.pt\r\n\r\n#Evaluate st\r\nfairseq-generate ${MUSTC_ROOT}/en-fr --gen-subset tst-COMMON_st --task speech_to_text --max-tokens 50000 --num-workers 0 \\\r\n    --path ${ST_SAVE_DIR}/${CHECKPOINT_FILENAME} --beam 5 --scoring sacrebleu --config-yaml ${MUSTC_ROOT}/en-fr/config_st.yaml\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3224/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3224/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3217", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3217/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3217/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3217/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3217", "id": 801863645, "node_id": "MDU6SXNzdWU4MDE4NjM2NDU=", "number": 3217, "title": "TypeError: can't pickle SwigPyObject objects (MuST-C EN-DE speech translation)", "user": {"login": "smkim0220", "id": 32120006, "node_id": "MDQ6VXNlcjMyMTIwMDA2", "avatar_url": "https://avatars.githubusercontent.com/u/32120006?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smkim0220", "html_url": "https://github.com/smkim0220", "followers_url": "https://api.github.com/users/smkim0220/followers", "following_url": "https://api.github.com/users/smkim0220/following{/other_user}", "gists_url": "https://api.github.com/users/smkim0220/gists{/gist_id}", "starred_url": "https://api.github.com/users/smkim0220/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smkim0220/subscriptions", "organizations_url": "https://api.github.com/users/smkim0220/orgs", "repos_url": "https://api.github.com/users/smkim0220/repos", "events_url": "https://api.github.com/users/smkim0220/events{/privacy}", "received_events_url": "https://api.github.com/users/smkim0220/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2021-02-05T05:39:20Z", "updated_at": "2021-02-09T04:08:18Z", "closed_at": "2021-02-09T04:08:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI followed up the example provided in https://github.com/pytorch/fairseq/blob/master/examples/speech_to_text/docs/mustc_example.md, but when I run fairseq-train I get TypeError: can't pickle SwigPyObject objects.\r\n\r\n### To Reproduce\r\n\r\nRun\r\nMKL_THREADING_LAYER=GNU CUDA_VISIBLE_DEVICES=4,5,6,7 fairseq-train espnet/egs/must_c/st1/dataset/en-de   --config-yaml config_st.yaml --train-subset train_st --valid-subset dev_st --save-dir result/en-de --num-workers 4 --max-tokens 10000 --max-update 100000 --task speech_to_text --criterion label_smoothed_cross_entropy --report-accuracy --arch s2t_transformer_s --optimizer adam --lr 2e-3 --lr-scheduler inverse_sqrt --warmup-updates 10000 --clip-norm 10.0 --seed 1 --update-freq 8 --fp16 \r\n\r\nError\r\n\r\nTraceback (most recent call last):                                                                             \r\n  File \"/home/smkim/anaconda3/envs/test/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/smkim/fairseq/fairseq_cli/train.py\", line 449, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/smkim/fairseq/fairseq/distributed/utils.py\", line 345, in call_main\r\n    join=True,\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\r\n    while not context.join():\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/smkim/fairseq/fairseq/distributed/utils.py\", line 325, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/smkim/fairseq/fairseq_cli/train.py\", line 143, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/smkim/fairseq/fairseq_cli/train.py\", line 239, in train\r\n    for i, samples in enumerate(progress):\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/site-packages/tqdm/std.py\", line 1107, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/smkim/fairseq/fairseq/data/iterators.py\", line 59, in __iter__\r\n    for x in self.iterable:\r\n  File \"/home/smkim/fairseq/fairseq/data/iterators.py\", line 528, in _chunk_iterator\r\n    for x in itr:\r\n  File \"/home/smkim/fairseq/fairseq/data/iterators.py\", line 59, in __iter__\r\n    for x in self.iterable:\r\n  File \"/home/smkim/fairseq/fairseq/data/iterators.py\", line 650, in __next__\r\n    raise item\r\n  File \"/home/smkim/fairseq/fairseq/data/iterators.py\", line 581, in run\r\n    for item in self._source:\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 352, in __iter__\r\n    return self._get_iterator()\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 294, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 801, in __init__\r\n    w.start()\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/multiprocessing/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/home/smkim/anaconda3/envs/test/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nTypeError: can't pickle SwigPyObject objects\r\n\r\nWhen I set --num-workers 0, it works fine but otherwise TypeError: can't pickle SwigPyObject objects appears.\r\nI\u2019m not using the server alone and with --num-workers 0 options, my code occupies all CPU of server.\r\n\r\n#### Code sample\r\nn/a\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): fairseq  1.0.0a0+148327d  /home/smkim/fairseq\r\n - PyTorch Version (e.g., 1.0): 1.7.1\r\n - OS (e.g., Linux): Linux \r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: 10.0.130/7.6.5\r\n - GPU models and configuration: 4*RTX2080\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3217/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3217/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3215", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3215/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3215/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3215/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3215", "id": 801609674, "node_id": "MDU6SXNzdWU4MDE2MDk2NzQ=", "number": 3215, "title": "Exception: process 0 terminated with signal SIGKILL when training with multiple training files (as per backtranslation example)", "user": {"login": "davidepatrucco", "id": 27724701, "node_id": "MDQ6VXNlcjI3NzI0NzAx", "avatar_url": "https://avatars.githubusercontent.com/u/27724701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidepatrucco", "html_url": "https://github.com/davidepatrucco", "followers_url": "https://api.github.com/users/davidepatrucco/followers", "following_url": "https://api.github.com/users/davidepatrucco/following{/other_user}", "gists_url": "https://api.github.com/users/davidepatrucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidepatrucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidepatrucco/subscriptions", "organizations_url": "https://api.github.com/users/davidepatrucco/orgs", "repos_url": "https://api.github.com/users/davidepatrucco/repos", "events_url": "https://api.github.com/users/davidepatrucco/events{/privacy}", "received_events_url": "https://api.github.com/users/davidepatrucco/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-02-04T20:25:03Z", "updated_at": "2021-03-22T20:44:07Z", "closed_at": "2021-03-22T20:44:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI followed up almost literally the example provided in https://github.com/pytorch/fairseq/blob/master/examples/backtranslation/README.md, but when I run fairseq-train I get \r\n\"Exception: process 0 terminated with signal SIGKILL\".\r\n\r\nIt happens if I run fairseq-train with more than one binarized train file (train1.. train2..)\r\nIf I use just one (no train1, train2 etc.), it works.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nI followed the example provided, and now I have multiple binarized training files in my directory:\r\n-rw-r--r-- 1 root root    1151103 Jan 13 10:57 dict.it.txt\r\n-rw-r--r-- 1 root root    1151103 Jan 13 10:57 dict.en.txt\r\n-rw-r--r-- 1 root root 9372868696 Jan 13 11:48 train.it-en.it.bin\r\n-rw-r--r-- 1 root root 1107481034 Jan 13 11:51 train.it-en.it.idx\r\n-rw-r--r-- 1 root root     168026 Jan 13 11:51 valid.it-en.it.idx\r\n-rw-r--r-- 1 root root    1146152 Jan 13 11:51 valid.it-en.it.bin\r\n-rw-r--r-- 1 root root      60026 Jan 13 11:51 test.it-en.it.idx\r\n-rw-r--r-- 1 root root     407816 Jan 13 11:51 test.it-en.it.bin\r\n-rw-r--r-- 1 root root 9040600732 Jan 13 12:43 train.it-en.en.bin\r\n-rw-r--r-- 1 root root 1107481034 Jan 13 12:46 train.it-en.en.idx\r\n-rw-r--r-- 1 root root     168026 Jan 13 12:46 valid.it-en.en.idx\r\n-rw-r--r-- 1 root root    1132744 Jan 13 12:46 valid.it-en.en.bin\r\n-rw-r--r-- 1 root root      60026 Jan 13 12:46 test.it-en.en.idx\r\n-rw-r--r-- 1 root root     398840 Jan 13 12:46 test.it-en.en.bin\r\n-rw-r--r-- 1 root root       2638 Jan 13 12:46 preprocess.log\r\n-rw-r--r-- 1 root root   38020824 Feb  4 19:54 train1.it-en.it.bin\r\n-rw-r--r-- 1 root root    3677738 Feb  4 19:55 train1.it-en.it.idx\r\n-rw-r--r-- 1 root root   41839636 Feb  4 19:55 train1.it-en.en.bin\r\n-rw-r--r-- 1 root root    3677738 Feb  4 19:55 train1.it-en.en.idx \r\n\r\nWhen I run (same command as the example provided except for the language pair)\r\nfairseq-train --fp16 \\\r\n    ./ \\\r\n    --upsample-primary 16 \\\r\n    --source-lang it --target-lang en \\\r\n    --arch transformer_wmt_en_de_big --share-all-embeddings \\\r\n    --dropout 0.3 --weight-decay 0.0 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 0.0007 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --max-tokens 3584 --update-freq 16 \\\r\n    --max-update 100000 \\\r\n    --save-dir $CHECKPOINT_DIR\r\n\r\nI get (full log here...) Exception: process 0 terminated with signal SIGKILL\r\n\r\n2021-02-04 20:05:23 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:17908\r\n2021-02-04 20:05:23 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:17908\r\n2021-02-04 20:05:25 | INFO | fairseq.distributed.utils | initialized host 4d9f00389311 as rank 1\r\n2021-02-04 20:05:25 | INFO | fairseq.distributed.utils | initialized host 4d9f00389311 as rank 0\r\n2021-02-04 20:05:25 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17908', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 2}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3584, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3584, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 100000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0007], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints.it-en.BT', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_wmt_en_de_big', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_wmt_en_de_big', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='./', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0007], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=3584, max_tokens_valid=3584, max_update=100000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints.it-en.BT', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='it', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[16], upsample_primary=16, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, zero_sharding='none'), 'task': {'_name': 'translation', 'data': './', 'source_lang': 'it', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': 16, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0007]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}\r\n2021-02-04 20:05:25 | INFO | fairseq.tasks.translation | [it] dictionary: 84800 types\r\n2021-02-04 20:05:25 | INFO | fairseq.tasks.translation | [en] dictionary: 84800 types\r\n2021-02-04 20:05:25 | INFO | fairseq.data.data_utils | loaded 14,000 examples from: ./valid.it-en.it\r\n2021-02-04 20:05:25 | INFO | fairseq.data.data_utils | loaded 14,000 examples from: ./valid.it-en.en\r\n2021-02-04 20:05:25 | INFO | fairseq.tasks.translation | ./ valid it-en 14000 examples\r\n2021-02-04 20:05:28 | INFO | fairseq_cli.train | TransformerModel(\r\n  (encoder): TransformerEncoder(\r\n    (dropout_module): FairseqDropout()\r\n    (embed_tokens): Embedding(84800, 1024, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (4): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (5): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n  )\r\n  (decoder): TransformerDecoder(\r\n    (dropout_module): FairseqDropout()\r\n    (embed_tokens): Embedding(84800, 1024, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (4): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (5): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n    (output_projection): Linear(in_features=1024, out_features=84800, bias=False)\r\n  )\r\n)\r\n2021-02-04 20:05:28 | INFO | fairseq_cli.train | task: TranslationTask\r\n2021-02-04 20:05:28 | INFO | fairseq_cli.train | model: TransformerModel\r\n2021-02-04 20:05:28 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\r\n2021-02-04 20:05:28 | INFO | fairseq_cli.train | num. model params: 263,192,576 (num. trained: 263,192,576)\r\n2021-02-04 20:05:28 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\r\n2021-02-04 20:05:28 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\r\n2021-02-04 20:05:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************\r\n2021-02-04 20:05:28 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = GeForce RTX 3090                        \r\n2021-02-04 20:05:28 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = GeForce RTX 3090                        \r\n2021-02-04 20:05:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************\r\n2021-02-04 20:05:28 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)\r\n2021-02-04 20:05:28 | INFO | fairseq_cli.train | max tokens per GPU = 3584 and batch size per GPU = None\r\n2021-02-04 20:05:28 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints.it-en.BT/checkpoint_last.pt\r\n2021-02-04 20:05:28 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints.it-en.BT/checkpoint_last.pt\r\n2021-02-04 20:05:28 | INFO | fairseq.trainer | loading train data for epoch 1\r\n2021-02-04 20:05:34 | INFO | fairseq.data.data_utils | loaded 92,290,084 examples from: ./train.it-en.it\r\n2021-02-04 20:05:38 | INFO | fairseq.data.data_utils | loaded 92,290,084 examples from: ./train.it-en.en\r\n2021-02-04 20:05:38 | INFO | fairseq.tasks.translation | ./ train it-en 92290084 examples\r\n2021-02-04 20:05:38 | INFO | fairseq.data.data_utils | loaded 306,476 examples from: ./train1.it-en.it\r\n2021-02-04 20:05:38 | INFO | fairseq.data.data_utils | loaded 306,476 examples from: ./train1.it-en.en\r\n2021-02-04 20:05:38 | INFO | fairseq.tasks.translation | ./ train1 it-en 306476 examples\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/root/fairseq/fairseq_cli/train.py\", line 449, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/root/fairseq/fairseq/distributed/utils.py\", line 346, in call_main\r\n    join=True,\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\r\n    while not context.join():\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 107, in join\r\n    (error_index, name)\r\nException: process 0 terminated with signal SIGKILL\r\n\r\n#### Code sample\r\nn/a\r\n\r\n### Expected behavior\r\n\r\nI believe should work as per the example. BTW if I run with just one set of train datafile (i.e. without train1, train2 etc..) it works!\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.7\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 11.1\r\n - GPU models and configuration: 2x RTX 3090\r\n - Any other relevant information:\r\npip install torch==1.7.0+cu110 torchvision==0.8.1+cu110 torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3215/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3215/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3211", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3211/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3211/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3211/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3211", "id": 801075147, "node_id": "MDU6SXNzdWU4MDEwNzUxNDc=", "number": 3211, "title": "KeyError when reload from checkpoint which has no attribute", "user": {"login": "xuhu357", "id": 7018012, "node_id": "MDQ6VXNlcjcwMTgwMTI=", "avatar_url": "https://avatars.githubusercontent.com/u/7018012?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuhu357", "html_url": "https://github.com/xuhu357", "followers_url": "https://api.github.com/users/xuhu357/followers", "following_url": "https://api.github.com/users/xuhu357/following{/other_user}", "gists_url": "https://api.github.com/users/xuhu357/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuhu357/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuhu357/subscriptions", "organizations_url": "https://api.github.com/users/xuhu357/orgs", "repos_url": "https://api.github.com/users/xuhu357/repos", "events_url": "https://api.github.com/users/xuhu357/events{/privacy}", "received_events_url": "https://api.github.com/users/xuhu357/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-02-04T09:00:48Z", "updated_at": "2021-02-22T22:32:11Z", "closed_at": "2021-02-22T22:32:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen I want to reload the wmt19 ende transformer model for fine-tuning, KeyError occurred~!\r\nIt happens when reload the pre-trained model(wmt19.en-de.ffn8192.pt) which has no attribute \"version\".\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd 'fairseq-train \\\r\n$DATA_BIN_PATH \\\r\n    --arch transformer_vaswani_wmt_en_de_big --share-decoder-input-output-embed \\\r\n    --restore-file checkpoints/wmt19.en-de.ffn8192.pt \\\r\n    --tensorboard-logdir $tflog_dir \\\r\n    --fp16 \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --reset-optimizer \\\r\n    --lr 1e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --dropout 0.3 --weight-decay 0.0001 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens 3072 \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\r\n    --eval-bleu-detok moses \\\r\n    --eval-bleu-remove-bpe \\\r\n    --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\r\n    --keep-last-epochs 5 \\\r\n    --batch-size 32'\r\n2. See error\r\n`Traceback (most recent call last):\r\n  File \"/home/xuhu357/miniconda3/envs/env_fairseq/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/xuhu357/fairseq/fairseq_cli/train.py\", line 449, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/xuhu357/fairseq/fairseq/distributed/utils.py\", line 346, in call_main\r\n    join=True,\r\n  File \"/home/xuhu357/miniconda3/envs/env_fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/xuhu357/miniconda3/envs/env_fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/home/xuhu357/miniconda3/envs/env_fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 3 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/xuhu357/miniconda3/envs/env_fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/xuhu357/fairseq/fairseq/distributed/utils.py\", line 326, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/xuhu357/fairseq/fairseq_cli/train.py\", line 126, in main\r\n    disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n  File \"/home/xuhu357/fairseq/fairseq/checkpoint_utils.py\", line 201, in load_checkpoint\r\n    reset_meters=reset_meters,\r\n  File \"/home/xuhu357/fairseq/fairseq/trainer.py\", line 402, in load_checkpoint\r\n    if itr_state[\"version\"] >= 2 and itr_state[\"iterations_in_epoch\"] == 0:\r\nKeyError: 'version'`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., master 1.0.0a0):\r\n - PyTorch Version (e.g., 1.0): 1.6 cuda 10.1\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: v100\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3211/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3211/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3205", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3205/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3205/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3205/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3205", "id": 800137122, "node_id": "MDU6SXNzdWU4MDAxMzcxMjI=", "number": 3205, "title": "Fairseq-hydra-train: str interpolation key 'hydra.job.name' not found", "user": {"login": "carlosep93", "id": 5550934, "node_id": "MDQ6VXNlcjU1NTA5MzQ=", "avatar_url": "https://avatars.githubusercontent.com/u/5550934?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carlosep93", "html_url": "https://github.com/carlosep93", "followers_url": "https://api.github.com/users/carlosep93/followers", "following_url": "https://api.github.com/users/carlosep93/following{/other_user}", "gists_url": "https://api.github.com/users/carlosep93/gists{/gist_id}", "starred_url": "https://api.github.com/users/carlosep93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carlosep93/subscriptions", "organizations_url": "https://api.github.com/users/carlosep93/orgs", "repos_url": "https://api.github.com/users/carlosep93/repos", "events_url": "https://api.github.com/users/carlosep93/events{/privacy}", "received_events_url": "https://api.github.com/users/carlosep93/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-02-03T09:21:20Z", "updated_at": "2021-02-05T09:26:49Z", "closed_at": "2021-02-05T09:26:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen trying to run any example using fairseq-hydra-train and configurations I get an error stating that the str interpolation key 'hydra.job.name' is not found. \r\n\r\n### To Reproduce\r\n\r\nUsing fairseq from source (3rd February 2021), from master branch. Using Python 3.6 and Pytorch 1.7.1, Hydra 2.5 and Omegaconf 2.0.6\r\n\r\n#### Code sample\r\n\r\nRunning any of the examples [here](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec)\r\n\r\nfairseq-hydra-train \\\r\n    task.data=/path/to/data \\\r\n    --config-dir /path/to/fairseq-py/examples/wav2vec/config/pretraining \\\r\n    --config-name wav2vec2_base_librispeech\r\n\r\n### Expected behavior\r\n\r\nTraceback (most recent call last):                                                                                                          \r\n  File \"/home/carlos/anaconda3/envs/fairseq010/bin/fairseq-hydra-train\", line 33, in <module>                                               \r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-hydra-train')())                                                       \r\n  File \"/home/carlos/Documentos/fairseq-0.10/fairseq/fairseq_cli/hydra_train.py\", line 91, in cli_main                                      \r\n    hydra_main()                                                                                                                            \r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/hydra/main.py\", line 37, in decorated_main                       \r\n    strict=strict,                                                                                                                          \r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/hydra/_internal/utils.py\", line 347, in _run_hydra               \r\n    lambda: hydra.run(                                                                                                                      \r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/hydra/_internal/utils.py\", line 201, in run_and_report           \r\n    raise ex                                                                                                                                \r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/hydra/_internal/utils.py\", line 198, in run_and_report           \r\n    return func()                                                                                                                           \r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/hydra/_internal/utils.py\", line 350, in <lambda>                 \r\n    overrides=args.overrides,                                                                                                               \r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/hydra/_internal/hydra.py\", line 112, in run                      \r\n    configure_logging=with_log_configuration,                                                                                               \r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/hydra/core/utils.py\", line 128, in run_job                       \r\n    ret.return_value = task_function(task_cfg)                                                                                              \r\n  File \"/home/carlos/Documentos/fairseq-0.10/fairseq/fairseq_cli/hydra_train.py\", line 34, in hydra_main                                    \r\n    cfg.job_logging_cfg = OmegaConf.to_container(HydraConfig.get().job_logging, resolve=True)\r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/omegaconf/omegaconf.py\", line 442, in to_container\r\n    return BaseContainer._to_content(cfg, resolve=resolve, enum_to_str=enum_to_str)\r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/omegaconf/basecontainer.py\", line 195, in _to_content\r\n    node, resolve=resolve, enum_to_str=enum_to_str\r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/omegaconf/basecontainer.py\", line 195, in _to_content\r\n    node, resolve=resolve, enum_to_str=enum_to_str\r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/omegaconf/basecontainer.py\", line 189, in _to_content\r\n    throw_on_missing=False, throw_on_resolution_failure=True\r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/omegaconf/base.py\", line 137, in _dereference_node\r\n    throw_on_resolution_failure=throw_on_resolution_failure,\r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/omegaconf/base.py\", line 392, in _resolve_interpolation\r\n    throw_on_resolution_failure=throw_on_resolution_failure,\r\n  File \"/home/carlos/anaconda3/envs/fairseq010/lib/python3.6/site-packages/omegaconf/base.py\", line 328, in _resolve_simple_interpolation\r\n    f\"{inter_type} interpolation key '{inter_key}' not found\"\r\nomegaconf.errors.ConfigKeyError: str interpolation key 'hydra.job.name' not found\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.7.1 \r\n - OS (e.g., Linux): Ubuntu 20.4\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install --editable ./\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 11.2\r\n - GPU models and configuration: gtx 950m \r\n - Any other relevant information:\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3205/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3202", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3202/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3202/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3202/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3202", "id": 799618762, "node_id": "MDU6SXNzdWU3OTk2MTg3NjI=", "number": 3202, "title": "Getting ValueError while running wav2vec2.0 ASR inference with Wav2Vec 2.0 Base (no finetuning split) model", "user": {"login": "tushar-rishav", "id": 7397433, "node_id": "MDQ6VXNlcjczOTc0MzM=", "avatar_url": "https://avatars.githubusercontent.com/u/7397433?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tushar-rishav", "html_url": "https://github.com/tushar-rishav", "followers_url": "https://api.github.com/users/tushar-rishav/followers", "following_url": "https://api.github.com/users/tushar-rishav/following{/other_user}", "gists_url": "https://api.github.com/users/tushar-rishav/gists{/gist_id}", "starred_url": "https://api.github.com/users/tushar-rishav/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tushar-rishav/subscriptions", "organizations_url": "https://api.github.com/users/tushar-rishav/orgs", "repos_url": "https://api.github.com/users/tushar-rishav/repos", "events_url": "https://api.github.com/users/tushar-rishav/events{/privacy}", "received_events_url": "https://api.github.com/users/tushar-rishav/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-02-02T19:43:08Z", "updated_at": "2021-08-12T09:33:31Z", "closed_at": "2021-03-30T04:45:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI am running ASR inference with the latest commit of master (da83e2f3) and [Wav2Vec 2.0 Base (no finetuning split) model](https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt) available under the list of [pre-trained models](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#pre-trained-models).\r\nI am seeing the following error:\r\n\r\n### To Reproduce\r\n\r\n1. Run cmd \r\n```bash\r\n$ export MODEL=/workspace/fairseq-model/wav2vec2.0/wav2vec_small.pt\r\n$ export RESULT_DIR=/workspace/fairseq-results/sclite\r\n$ python3 examples/speech_recognition/infer.py /workspace/fairseq/data/librispeech \r\n--task audio_pretraining \\\r\n--nbest 1 \\\r\n--path $MODEL \\\r\n--gen-subset all \\\r\n--results-path $RESULT_DIR \\\r\n--w2l-decoder viterbi \\\r\n--criterion ctc \\\r\n--labels ltr \\\r\n--max-tokens 3200000 \\\r\n--user-dir examples/speech_recognition \\\r\n--post-process letter\r\n```\r\nThe manifest files\r\n```sh\r\n$ cat /workspace/fairseq/data/librispeech/all*\r\nT O | F A D E | A W A Y | L I K E | M O R N I N G | B E A U T Y | F R O M | H E R | M O R T A L | D A Y | D O W N | B Y | T H E | R I V E R | O F | A D O N A | H E R | S O F T | V O I C E | I S | H E A R D | A N D | T H U S | H E R | G E N T L E | L A M E N T A T I O N | F A L L S | L I K E | M O R N I N G | D E W |\r\n/workspace/data/librispeech/908-157963-0000.wav  201920\r\nTO FADE AWAY LIKE MORNING BEAUTY FROM HER MORTAL DAY DOWN BY THE RIVER OF ADONA HER SOFT VOICE IS HEARD AND THUS HER GENTLE LAMENTATION FALLS LIKE MORNING DEW\r\n```\r\n\r\n2. See error\r\n\r\n```\r\nINFO:__main__:Namespace(all_gather_list_size=16384, autoregressive=False, azureml_logging=False, batch_size=None, batch_size_valid=None, beam=5, beam_size_token=100, beam_threshold=25.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='ctc', curriculum=0, data='/workspace/fairseq/data/librispeech', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, dump_emissions=None, dump_features=None, empty_cache_freq=0, enable_padding=False, eos=2, eval_wer=False, eval_wer_post_process='letter', eval_wer_tokenizer=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='all', heartbeat_timeout=-1, iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, kenlm_model=None, kspmodel=None, labels='ltr', lenpen=1, lexicon=None, lm_path=None, lm_weight=0.0, load_checkpoint_on_all_dp_ranks=False, load_emissions=None, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sample_size=None, max_tokens=3200000, max_tokens_valid=3200000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_sample_size=None, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, normalize=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer=None, optimizer_overrides='{}', pad=1, path='/workspace/fairseq-model/wav2vec2.0/wav2vec_small.pt', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process='letter', prefix_size=0, print_alignment=None, print_step=False, profile=False, quantization_config_path=None, quiet=False, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path='/workspace/fairseq-results/sclite', retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, rnnt_decoding_type='greedy', rnnt_len_penalty=-0.5, sacrebleu=False, sample_rate=16000, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=1, shard_id=0, sil_weight=0.0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, suppress_crashes=False, task='audio_pretraining', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', unit_lm=False, unk=3, unk_weight=-inf, unkpen=0, unnormalized=False, user_dir='examples/speech_recognition', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2l_decoder='viterbi', wandb_project=None, warmup_updates=0, wer_args=None, wer_kenlm_model=None, wer_lexicon=None, wer_lm_weight=2.0, wer_word_score=-1.0, wfstlm=None, word_score=1.0, zero_infinity=False, zero_sharding='none')\r\nINFO:__main__:| decoding with criterion ctc\r\nINFO:__main__:| loading model(s) from /root/host/zxpan/fairseq-model/wav2vec2.0/wav2vec_small.pt\r\nINFO:fairseq.data.audio.raw_audio_dataset:loaded 1, skipped 0 samples\r\nINFO:__main__:| /workspace/fairseq/data/librispeech all 1 examples\r\nTraceback (most recent call last):                                                                                                                                                \r\n  File \"examples/speech_recognition/infer.py\", line 428, in <module>\r\n    cli_main()\r\n  File \"examples/speech_recognition/infer.py\", line 424, in cli_main\r\n    main(args)\r\n  File \"examples/speech_recognition/infer.py\", line 349, in main\r\n    hypos = task.inference_step(generator, models, sample, prefix_tokens)\r\n  File \"/workspace/fairseq/fairseq/tasks/fairseq_task.py\", line 454, in inference_step\r\n    models, sample, prefix_tokens=prefix_tokens, constraints=constraints\r\n  File \"/workspace/fairseq/examples/speech_recognition/w2l_decoder.py\", line 87, in generate\r\n    return self.decode(emissions)\r\n  File \"/workspace/fairseq/examples/speech_recognition/w2l_decoder.py\", line 118, in decode\r\n    B, T, N = emissions.size()\r\nValueError: not enough values to unpack (expected 3, got 2)\r\n```\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\nNA\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI am expecting to see the inference results.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): **master**\r\n - PyTorch Version (e.g., 1.0): **1.7.1**\r\n - OS (e.g., Linux): **Ubuntu 18.04.5 LTS**\r\n - How you installed fairseq (`pip`, source): **source**\r\n - Build command you used (if compiling from source): `git clone https://github.com/pytorch/fairseq && cd fairseq && pip3 install --editable ./`\r\n - Python version: **3.6.9**\r\n - CUDA/cuDNN version: **CUDA version: 10.2.89**, **cuDNN version: 7.6.5.32**\r\n - GPU models and configuration: GeForce RTX 2080 Ti\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nPlease note that `all.tsv` file format above is slightly different from the wav2vec2_manifest version. \r\nI had to make minor modifications to `fairseq/data/audio/raw_audio_dataset.py` to support fine-tuning over multiple audio directories. Unlike valid subsets, I couldn't find a way to provide a comma separated list of training subsets while reading the doc (please let me know if there's a clean approach? :) ) \r\nI am reasonably sure the following change didn't introduce the bug because I am seeing the error after reverting the changes as well.\r\n```diff\r\ndiff --git a/fairseq/data/audio/raw_audio_dataset.py b/fairseq/data/audio/raw_audio_dataset.py\r\nindex ac5acd03..d168ca57 100644\r\n--- a/fairseq/data/audio/raw_audio_dataset.py\r\n+++ b/fairseq/data/audio/raw_audio_dataset.py\r\n@@ -157,7 +157,8 @@ class FileAudioDataset(RawAudioDataset):\r\n \r\n         skipped = 0\r\n         with open(manifest_path, \"r\") as f:\r\n-            self.root_dir = f.readline().strip()\r\n+            ## commented by Tushar to support merged IITM + Alphonso dataset\r\n+            #self.root_dir = f.readline().strip()\r\n             for i, line in enumerate(f):\r\n                 items = line.strip().split(\"\\t\")\r\n                 assert len(items) == 2, line\r\n@@ -173,7 +174,7 @@ class FileAudioDataset(RawAudioDataset):\r\n     def __getitem__(self, index):\r\n         import soundfile as sf\r\n \r\n-        fname = os.path.join(self.root_dir, self.fnames[index])\r\n+        fname = self.fnames[index] #os.path.join(self.root_dir, self.fnames[index])\r\n         wav, curr_sample_rate = sf.read(fname)\r\n         feats = torch.from_numpy(wav).float()\r\n         feats = self.postprocess(feats, curr_sample_rate)\r\n``` ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3202/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3202/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3197", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3197/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3197/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3197/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3197", "id": 799014673, "node_id": "MDU6SXNzdWU3OTkwMTQ2NzM=", "number": 3197, "title": "Null", "user": {"login": "JonnesLin", "id": 45222374, "node_id": "MDQ6VXNlcjQ1MjIyMzc0", "avatar_url": "https://avatars.githubusercontent.com/u/45222374?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JonnesLin", "html_url": "https://github.com/JonnesLin", "followers_url": "https://api.github.com/users/JonnesLin/followers", "following_url": "https://api.github.com/users/JonnesLin/following{/other_user}", "gists_url": "https://api.github.com/users/JonnesLin/gists{/gist_id}", "starred_url": "https://api.github.com/users/JonnesLin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JonnesLin/subscriptions", "organizations_url": "https://api.github.com/users/JonnesLin/orgs", "repos_url": "https://api.github.com/users/JonnesLin/repos", "events_url": "https://api.github.com/users/JonnesLin/events{/privacy}", "received_events_url": "https://api.github.com/users/JonnesLin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-02-02T07:26:37Z", "updated_at": "2021-02-02T10:12:33Z", "closed_at": "2021-02-02T10:12:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "a", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3197/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3197/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3187", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3187/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3187/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3187/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3187", "id": 797628123, "node_id": "MDU6SXNzdWU3OTc2MjgxMjM=", "number": 3187, "title": "AttributeError: module 'signal' has no attribute 'SIGKILL'", "user": {"login": "zzxn", "id": 23329452, "node_id": "MDQ6VXNlcjIzMzI5NDUy", "avatar_url": "https://avatars.githubusercontent.com/u/23329452?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zzxn", "html_url": "https://github.com/zzxn", "followers_url": "https://api.github.com/users/zzxn/followers", "following_url": "https://api.github.com/users/zzxn/following{/other_user}", "gists_url": "https://api.github.com/users/zzxn/gists{/gist_id}", "starred_url": "https://api.github.com/users/zzxn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zzxn/subscriptions", "organizations_url": "https://api.github.com/users/zzxn/orgs", "repos_url": "https://api.github.com/users/zzxn/repos", "events_url": "https://api.github.com/users/zzxn/events{/privacy}", "received_events_url": "https://api.github.com/users/zzxn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-01-31T05:47:39Z", "updated_at": "2021-02-03T20:09:27Z", "closed_at": "2021-02-03T20:09:27Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nOn Windows 10, when import fairseq, a error message shows `AttributeError: module 'signal' has no attribute 'SIGKILL'`.\r\nConsider to change to signal.SIGINT as Windows supports it? After I did this, everything seem OK.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Install fairseq latest from GitHub on Windows\r\n2. Run python code `import fairseq`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n\r\n```shell\r\n$ python\r\nPython 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import fairseq\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"d:\\pycharmprojects\\fairseq\\fairseq\\__init__.py\", line 20, in <module>\r\n    from fairseq.distributed import utils as distributed_utils\r\n  File \"d:\\pycharmprojects\\fairseq\\fairseq\\distributed\\__init__.py\", line 6, in <module>\r\n    from .distributed_timeout_wrapper import DistributedTimeoutWrapper\r\n  File \"d:\\pycharmprojects\\fairseq\\fairseq\\distributed\\distributed_timeout_wrapper.py\", line 17, in <module>\r\n    class DistributedTimeoutWrapper(nn.Module):\r\n  File \"d:\\pycharmprojects\\fairseq\\fairseq\\distributed\\distributed_timeout_wrapper.py\", line 36, in DistributedTimeoutWrapper\r\n    def __init__(self, module: nn.Module, timeout: int, signal=signal.SIGKILL):\r\nAttributeError: module 'signal' has no attribute 'SIGKILL'\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo error.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 1.0.0a0+148327d (latest of 2021-1-31)\r\n - PyTorch Version (e.g., 1.0): 1.7.1\r\n - OS (e.g., Linux): Windows 10\r\n - How you installed fairseq (`pip`, source): `pip`\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: 10.2/7.0\r\n - GPU models and configuration: -\r\n - Any other relevant information: -\r\n\r\n### Additional context\r\n\r\nNone\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3187/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 1, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3187/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3168", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3168/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3168/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3168/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3168", "id": 794395278, "node_id": "MDU6SXNzdWU3OTQzOTUyNzg=", "number": 3168, "title": "Wav2vec2_s2s model fine_tuning error:TypeError: forward() missing 1 required positional argument: 'prev_output_tokens'", "user": {"login": "orangelulu", "id": 38940699, "node_id": "MDQ6VXNlcjM4OTQwNjk5", "avatar_url": "https://avatars.githubusercontent.com/u/38940699?v=4", "gravatar_id": "", "url": "https://api.github.com/users/orangelulu", "html_url": "https://github.com/orangelulu", "followers_url": "https://api.github.com/users/orangelulu/followers", "following_url": "https://api.github.com/users/orangelulu/following{/other_user}", "gists_url": "https://api.github.com/users/orangelulu/gists{/gist_id}", "starred_url": "https://api.github.com/users/orangelulu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/orangelulu/subscriptions", "organizations_url": "https://api.github.com/users/orangelulu/orgs", "repos_url": "https://api.github.com/users/orangelulu/repos", "events_url": "https://api.github.com/users/orangelulu/events{/privacy}", "received_events_url": "https://api.github.com/users/orangelulu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2021-01-26T16:54:07Z", "updated_at": "2022-11-21T17:42:08Z", "closed_at": "2021-02-12T02:07:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '#!/bin/bash\r\nenv HYDRA_FULL_ERROR=1 \\\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 \\\r\npython3 /opt/tiger/fairseq/fairseq_cli/hydra_train.py \\\r\ntask.data=/mnt/bd/liulu-asr-data/mls_spanish \\\r\nmodel.w2v_path=/opt/tiger/fairseq/examples/wav2vec/outputs/xlsr_53_56k.pt \\\r\ndataset.train_subset='train_100h' dataset.valid_subset='valid' \\\r\ncheckpoint.patience=-1 \\\r\ncheckpoint.save_dir='spanish_100h_s2s' \\\r\ncheckpoint.maximize_best_checkpoint_metric=False \\\r\ndistributed_training.distributed_world_size=4 optimization.update_freq='[6]' \\\r\n--config-dir /opt/tiger/fairseq/examples/wav2vec/config/finetuning/ \\\r\n--config-name vox_100h \\'\r\n2. See error\r\n2021-01-27 00:45:20 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15491\r\n2021-01-27 00:45:20 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15491\r\n2021-01-27 00:45:20 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:15491\r\n2021-01-27 00:45:20 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15491\r\n2021-01-27 00:45:27 | INFO | fairseq.distributed_utils | initialized host n147-194-033 as rank 1\r\n2021-01-27 00:45:27 | INFO | fairseq.distributed_utils | initialized host n147-194-033 as rank 0\r\n2021-01-27 00:45:27 | INFO | fairseq.distributed_utils | initialized host n147-194-033 as rank 2\r\n2021-01-27 00:45:27 | INFO | fairseq.distributed_utils | initialized host n147-194-033 as rank 3\r\n2021-01-27 00:45:27 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': True}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15491', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 4}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_100h', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 32, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [6], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'spanish_100h_s2s', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_seq2seq', 'w2v_path': '/opt/tiger/fairseq/examples/wav2vec/outputs/xlsr_53_56k.pt', 'apply_mask': True, 'mask_prob': 0.5, 'mask_channel_prob': 0.5, 'mask_channel_length': 64, 'layerdrop': 0.1, 'activation_dropout': 0.1, 'feature_grad_mult': 0.0, 'freeze_finetune_updates': 10000}, 'task': {'_name': 'audio_pretraining', 'data': '/mnt/bd/liulu-asr-data/mls_spanish', 'normalize': True, 'labels': 'ltr'}, 'criterion': {'_name': 'cross_entropy'}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08}, 'lr_scheduler': {'_name': 'tri_stage', 'phase_ratio': [0.1, 0.4, 0.5], 'final_lr_scale': 0.05}, 'scoring': None, 'bpe': None, 'tokenizer': None}\r\n2021-01-27 00:45:27 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 2408, skipped 0 samples\r\n2021-01-27 00:45:38 | INFO | fairseq_cli.train | task: AudioPretrainingTask\r\n2021-01-27 00:45:38 | INFO | fairseq_cli.train | model: Wav2Vec2Seq2SeqModel\r\n2021-01-27 00:45:38 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\r\n2021-01-27 00:45:38 | INFO | fairseq_cli.train | num. model params: 372207744 (num. trained: 372207744)\r\n2021-01-27 00:45:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************\r\n2021-01-27 00:45:39 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.719 GB ; name = Tesla V100-SXM2-32GB                    \r\n2021-01-27 00:45:39 | INFO | fairseq.utils | rank   1: capabilities =  7.0  ; total memory = 31.719 GB ; name = Tesla V100-SXM2-32GB                    \r\n2021-01-27 00:45:39 | INFO | fairseq.utils | rank   2: capabilities =  7.0  ; total memory = 31.719 GB ; name = Tesla V100-SXM2-32GB                    \r\n2021-01-27 00:45:39 | INFO | fairseq.utils | rank   3: capabilities =  7.0  ; total memory = 31.719 GB ; name = Tesla V100-SXM2-32GB                    \r\n2021-01-27 00:45:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************\r\n2021-01-27 00:45:39 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)\r\n2021-01-27 00:45:39 | INFO | fairseq_cli.train | max tokens per GPU = None and batch size per GPU = 32\r\n2021-01-27 00:45:39 | INFO | fairseq.trainer | no existing checkpoint found spanish_100h_s2s/checkpoint_last.pt\r\n2021-01-27 00:45:39 | INFO | fairseq.trainer | loading train data for epoch 1\r\n2021-01-27 00:45:39 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 24103, skipped 0 samples\r\n2021-01-27 00:45:40 | INFO | fairseq.trainer | begin training epoch 1\r\nTraceback (most recent call last):\r\n  File \"/opt/tiger/fairseq/fairseq_cli/hydra_train.py\", line 70, in <module>\r\n    cli_main()\r\n  File \"/opt/tiger/fairseq/fairseq_cli/hydra_train.py\", line 66, in cli_main\r\n    hydra_main()\r\n  File \"/usr/local/lib/python3.7/dist-packages/hydra/main.py\", line 37, in decorated_main\r\n    strict=strict,\r\n  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 347, in _run_hydra\r\n    lambda: hydra.run(\r\n  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 201, in run_and_report\r\n    raise ex\r\n  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 198, in run_and_report\r\n    return func()\r\n  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 350, in <lambda>\r\n    overrides=args.overrides,\r\n  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/hydra.py\", line 112, in run\r\n    configure_logging=with_log_configuration,\r\n  File \"/usr/local/lib/python3.7/dist-packages/hydra/core/utils.py\", line 125, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"/opt/tiger/fairseq/fairseq_cli/hydra_train.py\", line 38, in hydra_main\r\n    distributed_utils.call_main(cfg, pre_main)\r\n  File \"/opt/tiger/fairseq/fairseq/distributed_utils.py\", line 320, in call_main\r\n    cfg.distributed_training.distributed_world_size,\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\r\n    while not context.join():\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/opt/tiger/fairseq/fairseq/distributed_utils.py\", line 302, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/opt/tiger/fairseq/fairseq_cli/train.py\", line 138, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/opt/tiger/fairseq/fairseq_cli/train.py\", line 235, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/opt/tiger/fairseq/fairseq/trainer.py\", line 536, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/opt/tiger/fairseq/fairseq/tasks/fairseq_task.py\", line 428, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/tiger/fairseq/fairseq/criterions/cross_entropy.py\", line 35, in forward\r\n    net_output = model(**sample[\"net_input\"])\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/tiger/fairseq/fairseq/legacy_distributed_data_parallel.py\", line 83, in forward\r\n    return self.module(*inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/tiger/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 249, in forward\r\n    decoder_out = self.decoder(encoder_out=encoder_out, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\nTypeError: forward() missing 1 required positional argument: 'prev_output_tokens'\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.0\r\n - OS (e.g., Linux):Linux\r\n - How you installed fairseq (`pip`, source):pip\r\n - Build command you used (if compiling from source): pip install -e .\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 11.0\r\n - GPU models and configuration: Tesla V100 8 GPU\r\n - Any other relevant information: no\r\n\r\n### Additional context\r\nno\r\n<!-- Add any other context about the problem here. -->", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3168/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3168/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3166", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3166/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3166/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3166/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3166", "id": 793837010, "node_id": "MDU6SXNzdWU3OTM4MzcwMTA=", "number": 3166, "title": "distributed training attribute does not exist in Namespace", "user": {"login": "jtorrev", "id": 17103624, "node_id": "MDQ6VXNlcjE3MTAzNjI0", "avatar_url": "https://avatars.githubusercontent.com/u/17103624?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jtorrev", "html_url": "https://github.com/jtorrev", "followers_url": "https://api.github.com/users/jtorrev/followers", "following_url": "https://api.github.com/users/jtorrev/following{/other_user}", "gists_url": "https://api.github.com/users/jtorrev/gists{/gist_id}", "starred_url": "https://api.github.com/users/jtorrev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jtorrev/subscriptions", "organizations_url": "https://api.github.com/users/jtorrev/orgs", "repos_url": "https://api.github.com/users/jtorrev/repos", "events_url": "https://api.github.com/users/jtorrev/events{/privacy}", "received_events_url": "https://api.github.com/users/jtorrev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-01-26T01:04:22Z", "updated_at": "2021-01-26T01:38:33Z", "closed_at": "2021-01-26T01:38:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- Namespace does not contain distributed_training attribute -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n\r\n1.langs=ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN\r\n2. python3.7 ../../fairseq_cli/interactive.py  data-bin/en-es --path models/enes/checkpoint_best.pt  --task translation_from_pretrained_bart -t es_XX -s en_XX --langs $langs --batch-size 32  --bpe 'sentencepiece' --sentencepiece-model models/enes/sentence.bpe.model --input tests/en.txt \r\n2. See error\r\nTraceback (most recent call last):\r\n  File \"../../fairseq_cli/interactive.py\", line 328, in <module>\r\n    cli_main()\r\n  File \"../../fairseq_cli/interactive.py\", line 321, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/home/gpu24/work/fairseq/fairseq/distributed_utils.py\", line 287, in call_main\r\n    infer_init_method(cfg.distributed_training)\r\nAttributeError: 'Namespace' object has no attribute 'distributed_training'\r\n\r\n### Expected behavior\r\n\r\nIt should translate the sentences\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Additional context\r\nNamespace, in the following line clearly does not have the required attribute\r\nNamespace(all_gather_list_size=16384, batch_size=32, batch_size_valid=32, beam=5, best_checkpoint_metric='loss', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, buffer_size=0, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='cross_entropy', curriculum=0, data='data-bin/en-es', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eos=2, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', input='tests/en.txt', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', left_pad_source='True', left_pad_target='False', lenpen=1, lm_path=None, lm_weight=0.0, load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer=None, optimizer_overrides='{}', pad=1, path='models/enes/checkpoint_best.pt', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, prefix_size=0, prepend_bos=False, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=1, sentencepiece_model='models/enes/sentence.bpe.model', shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en_XX', target_lang='es_XX', task='translation_from_pretrained_bart', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, zero_sharding='none')\r\nSO, I'm missing something or there is an issue here?\r\n\r\nThanks", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3166/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3166/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3158", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3158/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3158/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3158/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3158", "id": 792211739, "node_id": "MDU6SXNzdWU3OTIyMTE3Mzk=", "number": 3158, "title": "Hydra error when loading pretrained BART XSum model", "user": {"login": "keyonvafa", "id": 10766891, "node_id": "MDQ6VXNlcjEwNzY2ODkx", "avatar_url": "https://avatars.githubusercontent.com/u/10766891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keyonvafa", "html_url": "https://github.com/keyonvafa", "followers_url": "https://api.github.com/users/keyonvafa/followers", "following_url": "https://api.github.com/users/keyonvafa/following{/other_user}", "gists_url": "https://api.github.com/users/keyonvafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/keyonvafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keyonvafa/subscriptions", "organizations_url": "https://api.github.com/users/keyonvafa/orgs", "repos_url": "https://api.github.com/users/keyonvafa/repos", "events_url": "https://api.github.com/users/keyonvafa/events{/privacy}", "received_events_url": "https://api.github.com/users/keyonvafa/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-01-22T18:18:42Z", "updated_at": "2021-02-03T19:56:58Z", "closed_at": "2021-02-03T19:56:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen I load a pretrained BART XSum model with \r\n```\r\nbart = BARTModel.from_pretrained('checkpoints/bart.large.xsum', checkpoint_file='model.pt')\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\nhydra.errors.ConfigCompositionException: Error merging override task.eval_bleu_args=null\r\n```\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Download BART XSum pretrained model\r\n```\r\nwget https://dl.fbaipublicfiles.com/fairseq/models/bart.large.xsum.tar.gz\r\ntar -xzvf bart.large.xsum.tar.gz\r\n```\r\n2. Load model in Python\r\n```\r\nimport torch\r\nfrom fairseq.models.bart import BARTModel\r\nbart = BARTModel.from_pretrained('checkpoints/bart.large.xsum', checkpoint_file='model.pt')\r\n```\r\nI get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/env/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 513, in _apply_overrides_to_config\r\n    OmegaConf.update(cfg, key, value, merge=True)\r\n  File \"/env/lib/python3.8/site-packages/omegaconf/omegaconf.py\", line 613, in update\r\n    root.__setattr__(last_key, value)\r\n  File \"/env/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 285, in __setattr__\r\n    raise e\r\n  File \"/env/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 282, in __setattr__\r\n    self.__set_impl(key, value)\r\n  File \"/env/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 266, in __set_impl\r\n    self._set_item_impl(key, value)\r\n  File \"/env/lib/python3.8/site-packages/omegaconf/basecontainer.py\", line 398, in _set_item_impl\r\n    self._validate_set(key, value)\r\n  File \"/env/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 143, in _validate_set\r\n    self._validate_set_merge_impl(key, value, is_assign=True)\r\n  File \"/env/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 156, in _validate_set_merge_impl\r\n    self._format_and_raise(\r\n  File \"/env/lib/python3.8/site-packages/omegaconf/base.py\", line 95, in _format_and_raise\r\n    format_and_raise(\r\n  File \"/env/lib/python3.8/site-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/env/lib/python3.8/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.ValidationError: child 'task.eval_bleu_args' is not Optional\r\n        full_key: task.eval_bleu_args\r\n        reference_type=Optional[TranslationConfig]\r\n        object_type=TranslationConfig\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/proj/fairseq/fairseq/models/bart/model.py\", line 128, in from_pretrained\r\n    x = hub_utils.from_pretrained(\r\n  File \"/proj/fairseq/fairseq/hub_utils.py\", line 74, in from_pretrained\r\n    models, args, task = checkpoint_utils.load_model_ensemble_and_task(\r\n  File \"/proj/fairseq/fairseq/checkpoint_utils.py\", line 339, in load_model_ensemble_and_task\r\n    state = load_checkpoint_to_cpu(filename, arg_overrides)\r\n  File \"/proj/fairseq/fairseq/checkpoint_utils.py\", line 273, in load_checkpoint_to_cpu\r\n    state = _upgrade_state_dict(state)\r\n  File \"/proj/fairseq/fairseq/checkpoint_utils.py\", line 550, in _upgrade_state_dict\r\n    state[\"cfg\"] = convert_namespace_to_omegaconf(state[\"args\"])\r\n  File \"/proj/fairseq/fairseq/dataclass/utils.py\", line 353, in convert_namespace_to_omegaconf\r\n    composed_cfg = compose(\"config\", overrides=overrides, strict=False)\r\n  File \"/env/lib/python3.8/site-packages/hydra/experimental/compose.py\", line 31, in compose\r\n    cfg = gh.hydra.compose_config(\r\n  File \"/env/lib/python3.8/site-packages/hydra/_internal/hydra.py\", line 507, in compose_config\r\n    cfg = self.config_loader.load_configuration(\r\n  File \"/env/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 151, in load_configuration\r\n    return self._load_configuration(\r\n  File \"/env/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 277, in _load_configuration\r\n    ConfigLoaderImpl._apply_overrides_to_config(config_overrides, cfg)\r\n  File \"/env/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 520, in _apply_overrides_to_config\r\n    raise ConfigCompositionException(\r\nhydra.errors.ConfigCompositionException: Error merging override task.eval_bleu_args=null\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen I try the above with `bart.large` (not finetuned) or `bart.large.cnn` it works.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): '1.0.0a0+cfbf0dd'\r\n - PyTorch Version (e.g., 1.0): 1.4\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source): git clone + pip install\r\n - Python version: Python 3.8.5\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: Titan V\r\n - Any other relevant information:\r\n\r\nThank you!", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3158/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3147", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3147/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3147/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3147/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3147", "id": 789671277, "node_id": "MDU6SXNzdWU3ODk2NzEyNzc=", "number": 3147, "title": "m2m_100 model mismatched parameters", "user": {"login": "jjangsangy", "id": 3851682, "node_id": "MDQ6VXNlcjM4NTE2ODI=", "avatar_url": "https://avatars.githubusercontent.com/u/3851682?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjangsangy", "html_url": "https://github.com/jjangsangy", "followers_url": "https://api.github.com/users/jjangsangy/followers", "following_url": "https://api.github.com/users/jjangsangy/following{/other_user}", "gists_url": "https://api.github.com/users/jjangsangy/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjangsangy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjangsangy/subscriptions", "organizations_url": "https://api.github.com/users/jjangsangy/orgs", "repos_url": "https://api.github.com/users/jjangsangy/repos", "events_url": "https://api.github.com/users/jjangsangy/events{/privacy}", "received_events_url": "https://api.github.com/users/jjangsangy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-01-20T06:12:35Z", "updated_at": "2021-01-26T13:13:45Z", "closed_at": "2021-01-26T13:13:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nRunning steps to generate on 418M and 1.2B models have mismatched parameters.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/fairseq-generate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())\r\n  File \"/content/fairseq/fairseq_cli/generate.py\", line 404, in cli_main\r\n    main(args)\r\n  File \"/content/fairseq/fairseq_cli/generate.py\", line 49, in main\r\n    return _main(cfg, sys.stdout)\r\n  File \"/content/fairseq/fairseq_cli/generate.py\", line 102, in _main\r\n    num_shards=cfg.checkpoint.checkpoint_shard_count,\r\n  File \"/content/fairseq/fairseq/checkpoint_utils.py\", line 304, in load_model_ensemble\r\n    state,\r\n  File \"/content/fairseq/fairseq/checkpoint_utils.py\", line 355, in load_model_ensemble_and_task\r\n    model.load_state_dict(state[\"model\"], strict=strict, model_cfg=cfg.model)\r\n  File \"/content/fairseq/fairseq/models/fairseq_model.py\", line 115, in load_state_dict\r\n    return super().load_state_dict(new_state_dict, strict)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1052, in load_state_dict\r\n    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\nRuntimeError: Error(s) in loading state_dict for TransformerModel:\r\n        size mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([128112, 1024]) from checkpoint, the shape in current model is torch.Size([128006, 1024]).\r\n        size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([128112, 1024]) from checkpoint, the shape in current model is torch.Size([128006, 1024]).\r\n        size mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([128112, 1024]) from checkpoint, the shape in current model is torch.Size([128006, 1024]).\r\n```\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n#### Code sample\r\n```sh\r\nwget https://dl.fbaipublicfiles.com/m2m_100/418M_last_checkpoint.pt\r\n\r\nsacrebleu --echo src -l de-fr -t wmt19 | head -n 20 > raw_input.de-fr.de\r\nsacrebleu --echo ref -l de-fr -t wmt19 | head -n 20 > raw_input.de-fr.fr\r\n\r\nwget https://dl.fbaipublicfiles.com/m2m_100/spm.128k.model\r\nfor lang in de fr ; do\r\n    python scripts/spm_encode.py \\\r\n        --model spm.128k.model \\\r\n        --output_format=piece \\\r\n        --inputs=raw_input.de-fr.${lang} \\\r\n        --outputs=spm.de-fr.${lang}\r\ndone\r\n\r\nwget https://dl.fbaipublicfiles.com/m2m_100/data_dict.128k.txt\r\nfairseq-preprocess \\\r\n    --source-lang de --target-lang fr \\\r\n    --testpref spm.de-fr \\\r\n    --thresholdsrc 0 --thresholdtgt 0 \\\r\n    --destdir data_bin \\\r\n    --srcdict data_dict.128k.txt --tgtdict data_dict.128k.txt\r\n\r\nfairseq-generate data_bin \\\r\n  --batch-size 32 \\\r\n  --path 418M_last_checkpoint.pt \\\r\n  -s de \\\r\n  -t fr \\\r\n  --remove-bpe 'sentencepiece' \\\r\n  --beam 5 \\\r\n  --task translation_multi_simple_epoch \\\r\n  --lang-pairs de-fr,fr-de \\\r\n  --decoder-langtok \\\r\n  --encoder-langtok src \\\r\n  --gen-subset test\r\n```\r\n\r\n### Environment\r\n\r\nOn Colab GPU instance with P100 GPU\r\n\r\n - fairseq Version: Master\r\n - PyTorch Version: 1.7.0+cu101\r\n - OS: Linux\r\n - How you installed fairseq: pip --editable\r\n - Build command you used (if compiling from source):\r\n```sh\r\n  git clone https://github.com/pytorch/fairseq.git\r\n  ./fairseq/examples/m2m_100/install_dependecies.sh\r\n  pip install -e ./fairseq sentencepiece\r\n```\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 10.1\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3147/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3135", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3135/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3135/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3135/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3135", "id": 787082320, "node_id": "MDU6SXNzdWU3ODcwODIzMjA=", "number": 3135, "title": "Option `--scoring sacrebleu` gives same result as without in generate.py", "user": {"login": "munael", "id": 5769148, "node_id": "MDQ6VXNlcjU3NjkxNDg=", "avatar_url": "https://avatars.githubusercontent.com/u/5769148?v=4", "gravatar_id": "", "url": "https://api.github.com/users/munael", "html_url": "https://github.com/munael", "followers_url": "https://api.github.com/users/munael/followers", "following_url": "https://api.github.com/users/munael/following{/other_user}", "gists_url": "https://api.github.com/users/munael/gists{/gist_id}", "starred_url": "https://api.github.com/users/munael/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/munael/subscriptions", "organizations_url": "https://api.github.com/users/munael/orgs", "repos_url": "https://api.github.com/users/munael/repos", "events_url": "https://api.github.com/users/munael/events{/privacy}", "received_events_url": "https://api.github.com/users/munael/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-01-15T17:58:17Z", "updated_at": "2021-01-17T16:37:06Z", "closed_at": "2021-01-17T16:37:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Fetch and build `master`.\r\n2. Prepare any generation command on a pretrained MT model for some WMT language that are known to work fine with `13a` tokenization (used by SacreBLEU).\r\n3. Run one with no `--scoring` option specified, and once with `--scoring sacrebleu`.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n`bleu` and `sacrebleu` usually get different results on known WMT models. E.g. ( https://github.com/pytorch/fairseq/blob/master/examples/wmt19/README.md#pre-trained-models )\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.7\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `python setup.py build_ext --inplace; pip install . --user`\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: 8*V100\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3135/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3126", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3126/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3126/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3126/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3126", "id": 783047698, "node_id": "MDU6SXNzdWU3ODMwNDc2OTg=", "number": 3126, "title": "AttributeError: 'Namespace' object has no attribute 'sacrebleu_char_level'", "user": {"login": "rizwan09", "id": 6012792, "node_id": "MDQ6VXNlcjYwMTI3OTI=", "avatar_url": "https://avatars.githubusercontent.com/u/6012792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rizwan09", "html_url": "https://github.com/rizwan09", "followers_url": "https://api.github.com/users/rizwan09/followers", "following_url": "https://api.github.com/users/rizwan09/following{/other_user}", "gists_url": "https://api.github.com/users/rizwan09/gists{/gist_id}", "starred_url": "https://api.github.com/users/rizwan09/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rizwan09/subscriptions", "organizations_url": "https://api.github.com/users/rizwan09/orgs", "repos_url": "https://api.github.com/users/rizwan09/repos", "events_url": "https://api.github.com/users/rizwan09/events{/privacy}", "received_events_url": "https://api.github.com/users/rizwan09/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-01-11T04:05:02Z", "updated_at": "2021-03-10T23:36:08Z", "closed_at": "2021-01-21T23:25:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- AttributeError: 'Namespace' object has no attribute 'sacrebleu_char_level'. Perhaps the eroor is https://github.com/pytorch/fairseq/blob/v0.10.2/fairseq/scoring/bleu.py#L40 -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd \r\nFILE_PREF=${SAVE_DIR}/output;\r\nRESULT_FILE=${SAVE_DIR}/result.txt;\r\nGOUND_TRUTH_PATH=\"${path_2_data}/test.target\";\r\n\r\nfairseq-generate \"${path_2_data}/data-bin\" $USER_DIR \\\r\n    --fp16 \\\r\n    --path ${SAVE_DIR}/checkpoint_best.pt \\\r\n    --task translation_in_same_language \\\r\n    --langs $langs \\\r\n    --gen-subset test \\\r\n    --source-lang en_XX \\\r\n    --target-lang $LANG \\\r\n    --sacrebleu \\\r\n    --remove-bpe 'sentencepiece'\\\r\n    --batch-size 64 \\\r\n    --beam 5 > ${FILE_PREF};\r\n\r\n2. See error\r\n\r\n\r\n/home/rizwan/fairseq-0.10.2/fairseq_cli/generate.py:172: UserWarning: --sacrebleu is deprecated. Please use --scoring sacrebleu instead.\r\n  scorer = scoring.build_scorer(args, tgt_dict)\r\nTraceback (most recent call last):\r\n  File \"/home/rizwan/anaconda3_2020/envs/fairseq/bin/fairseq-generate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())\r\n  File \"/home/rizwan/fairseq-0.10.2/fairseq_cli/generate.py\", line 379, in cli_main\r\n    main(args)\r\n  File \"/home/rizwan/fairseq-0.10.2/fairseq_cli/generate.py\", line 41, in main\r\n    return _main(args, sys.stdout)\r\n  File \"/home/rizwan/fairseq-0.10.2/fairseq_cli/generate.py\", line 172, in _main\r\n    scorer = scoring.build_scorer(args, tgt_dict)\r\n  File \"/home/rizwan/fairseq-0.10.2/fairseq/scoring/__init__.py\", line 54, in build_scorer\r\n    return _build_scorer(args)\r\n  File \"/home/rizwan/fairseq-0.10.2/fairseq/registry.py\", line 54, in build_x\r\n    return builder(args, *extra_args, **extra_kwargs)\r\n  File \"/home/rizwan/fairseq-0.10.2/fairseq/scoring/bleu.py\", line 41, in __init__\r\n    character_tokenization=self.args.sacrebleu_char_level,\r\nAttributeError: 'Namespace' object has no attribute 'sacrebleu_char_level'\r\n\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (0.10.2):\r\n - PyTorch Version (1.7.1)\r\n - OS (Linux):\r\n - How you installed fairseq (source):\r\n - Build command you used (pip install -e .):\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: \r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3126/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3126/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3099", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3099/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3099/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3099/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3099", "id": 778790637, "node_id": "MDU6SXNzdWU3Nzg3OTA2Mzc=", "number": 3099, "title": "En-Ja, En-Lv data missing for Speech-to-text CoVoST2 experiment", "user": {"login": "beomseok-lee", "id": 13184386, "node_id": "MDQ6VXNlcjEzMTg0Mzg2", "avatar_url": "https://avatars.githubusercontent.com/u/13184386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/beomseok-lee", "html_url": "https://github.com/beomseok-lee", "followers_url": "https://api.github.com/users/beomseok-lee/followers", "following_url": "https://api.github.com/users/beomseok-lee/following{/other_user}", "gists_url": "https://api.github.com/users/beomseok-lee/gists{/gist_id}", "starred_url": "https://api.github.com/users/beomseok-lee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/beomseok-lee/subscriptions", "organizations_url": "https://api.github.com/users/beomseok-lee/orgs", "repos_url": "https://api.github.com/users/beomseok-lee/repos", "events_url": "https://api.github.com/users/beomseok-lee/events{/privacy}", "received_events_url": "https://api.github.com/users/beomseok-lee/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-01-05T09:19:25Z", "updated_at": "2023-04-18T09:05:47Z", "closed_at": "2021-01-07T01:53:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nEn-Ja, En-Lv data missing for Speech-to-text CoVoST2 dataset,\r\npossibly file link is broken or wrong tar files (empty inside) are uploaded.\r\nAll different EN_XX languages except for Ja, Lv are good.\r\n\r\n<img width=\"397\" alt=\"image\" src=\"https://user-images.githubusercontent.com/13184386/103627785-6e030800-4f81-11eb-987e-f734e52746da.png\">\r\n\r\n[examples/speech_to_text/prep_covost_data.py#L56](https://github.com/pytorch/fairseq/blob/336942734c85791a90baa373c212d27e7c722662/examples/speech_to_text/prep_covost_data.py#L56)\r\n\r\n<img width=\"528\" alt=\"image\" src=\"https://user-images.githubusercontent.com/13184386/103627893-8ffc8a80-4f81-11eb-8ddd-fdd174600a30.png\">\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Download En-Ja, En-Lv data for Speech-to-text CoVoST2 experiment with following link\r\n2. For En-Ja [https://dl.fbaipublicfiles.com/covost/covost_v2.en_ja.tsv.tar.gz](https://dl.fbaipublicfiles.com/covost/covost_v2.en_ja.tsv.tar.gz)\r\n3. For En-Lv [https://dl.fbaipublicfiles.com/covost/covost_v2.en_lv.tsv.tar.gz](https://dl.fbaipublicfiles.com/covost/covost_v2.en_lv.tsv.tar.gz)\r\n4. Untar and check 'covost_v2.en_ja.tsv', 'covost_v2.en_lv.tsv' files\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\nEn-Ja, En-Lv tsv files are not supposed to be empty.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): https://github.com/pytorch/fairseq/tree/336942734c85791a90baa373c212d27e7c722662\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3099/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3097", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3097/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3097/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3097/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3097", "id": 778711194, "node_id": "MDU6SXNzdWU3Nzg3MTExOTQ=", "number": 3097, "title": "register_model_architecture not working in version 0.10.1", "user": {"login": "voidmagic", "id": 10617485, "node_id": "MDQ6VXNlcjEwNjE3NDg1", "avatar_url": "https://avatars.githubusercontent.com/u/10617485?v=4", "gravatar_id": "", "url": "https://api.github.com/users/voidmagic", "html_url": "https://github.com/voidmagic", "followers_url": "https://api.github.com/users/voidmagic/followers", "following_url": "https://api.github.com/users/voidmagic/following{/other_user}", "gists_url": "https://api.github.com/users/voidmagic/gists{/gist_id}", "starred_url": "https://api.github.com/users/voidmagic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/voidmagic/subscriptions", "organizations_url": "https://api.github.com/users/voidmagic/orgs", "repos_url": "https://api.github.com/users/voidmagic/repos", "events_url": "https://api.github.com/users/voidmagic/events{/privacy}", "received_events_url": "https://api.github.com/users/voidmagic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 2634521869, "node_id": "MDU6TGFiZWwyNjM0NTIxODY5", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/0.10.2", "name": "0.10.2", "color": "8ECF2D", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-01-05T08:08:28Z", "updated_at": "2022-04-29T09:10:00Z", "closed_at": "2021-01-05T20:27:15Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nUsing register_model_architecture to register new hyper parameter set failed. The hyper parameter is overrided by default settings.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd 'fairseq-train --task language_modeling data-bin --arch transformer_lm_big --batch-size 1 --optimizer adam'\r\n2. See error\r\nThere are 6 layers in the model, which should be 12 as defined in  `transformer_lm_big `. https://github.com/pytorch/fairseq/blob/v0.10.1/fairseq/models/transformer_lm.py#L311\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.10.1\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3097/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3095", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3095/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3095/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3095/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3095", "id": 778388625, "node_id": "MDU6SXNzdWU3NzgzODg2MjU=", "number": 3095, "title": "speech-to-text example 2 computes BLEU on BPEs, not detokenised text", "user": {"login": "bhaddow", "id": 992795, "node_id": "MDQ6VXNlcjk5Mjc5NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/992795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bhaddow", "html_url": "https://github.com/bhaddow", "followers_url": "https://api.github.com/users/bhaddow/followers", "following_url": "https://api.github.com/users/bhaddow/following{/other_user}", "gists_url": "https://api.github.com/users/bhaddow/gists{/gist_id}", "starred_url": "https://api.github.com/users/bhaddow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bhaddow/subscriptions", "organizations_url": "https://api.github.com/users/bhaddow/orgs", "repos_url": "https://api.github.com/users/bhaddow/repos", "events_url": "https://api.github.com/users/bhaddow/events{/privacy}", "received_events_url": "https://api.github.com/users/bhaddow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737617054, "node_id": "MDU6TGFiZWwxNzM3NjE3MDU0", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/documentation", "name": "documentation", "color": "fff2a3", "default": true, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2021-01-04T21:44:04Z", "updated_at": "2021-01-06T08:38:33Z", "closed_at": "2021-01-06T05:05:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIn speech-to-text example 2, the fairseq-generate command provided for ST (speech translation) runs sacrebleu. However it does not undo pre-processing, so bleu is calculated on text that has been split into BPEs. This gives results that are not comparable with any other published results, and is poor experimental procedure. Possibly the BLEU scores in the table in the README have been calculated in this way, and so should be updated.\r\n\r\n### To Reproduce\r\nFollow example 2\r\n 1. Prepare must-c data\r\n 2. Train ASR\r\n 3. Train ST\r\n 4. Run the provided generate command with the trained model.\r\n\r\n\r\n#### Code sample\r\n\r\n\r\n### Expected behavior\r\n\r\nI should be able to reproduce the expected bleu scores. In fact my bleu score is lower, but if I extract the translations, detokenise and run sacrebleu, then I get an even lower bleu score. By extracting the BPEs, and running  sacrebleu on them, I can verify that fairseq is using this approach.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master: master\r\n - PyTorch Version (e.g., 1.0): 1.7.1\r\n - OS (e.g., Linux): Ubuntu\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install --editable ./\r\n - Python version: 3.7.9\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: GTX 1080\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3095/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3095/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3093", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3093/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3093/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3093/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3093", "id": 778075717, "node_id": "MDU6SXNzdWU3NzgwNzU3MTc=", "number": 3093, "title": "RuntimeError: Missing dependencies: hydra-core on google colab", "user": {"login": "teralab", "id": 8972005, "node_id": "MDQ6VXNlcjg5NzIwMDU=", "avatar_url": "https://avatars.githubusercontent.com/u/8972005?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teralab", "html_url": "https://github.com/teralab", "followers_url": "https://api.github.com/users/teralab/followers", "following_url": "https://api.github.com/users/teralab/following{/other_user}", "gists_url": "https://api.github.com/users/teralab/gists{/gist_id}", "starred_url": "https://api.github.com/users/teralab/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teralab/subscriptions", "organizations_url": "https://api.github.com/users/teralab/orgs", "repos_url": "https://api.github.com/users/teralab/repos", "events_url": "https://api.github.com/users/teralab/events{/privacy}", "received_events_url": "https://api.github.com/users/teralab/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-01-04T12:56:08Z", "updated_at": "2021-01-05T19:28:15Z", "closed_at": "2021-01-05T19:28:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "HI\r\ndo you know why when i try to use this python code i've got following issue under Google Colab ?\r\n\r\nimport torch\r\ncamembert = torch.hub.load('pytorch/fairseq', 'camembert')\r\ncamembert.eval()  # disable dropout (or leave in train mode to finetune)\r\n\r\nRuntimeError: Missing dependencies: hydra-core. \r\n\r\ni installed this module with pip but always same error.\r\n\r\nThanks for your help\r\n\r\n@louismartin", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3093/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3087", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3087/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3087/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3087/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3087", "id": 777358457, "node_id": "MDU6SXNzdWU3NzczNTg0NTc=", "number": 3087, "title": "ModuleNotFoundError: No module named 'examples.speech_to_text'", "user": {"login": "pyyush", "id": 49844424, "node_id": "MDQ6VXNlcjQ5ODQ0NDI0", "avatar_url": "https://avatars.githubusercontent.com/u/49844424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pyyush", "html_url": "https://github.com/pyyush", "followers_url": "https://api.github.com/users/pyyush/followers", "following_url": "https://api.github.com/users/pyyush/following{/other_user}", "gists_url": "https://api.github.com/users/pyyush/gists{/gist_id}", "starred_url": "https://api.github.com/users/pyyush/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pyyush/subscriptions", "organizations_url": "https://api.github.com/users/pyyush/orgs", "repos_url": "https://api.github.com/users/pyyush/repos", "events_url": "https://api.github.com/users/pyyush/events{/privacy}", "received_events_url": "https://api.github.com/users/pyyush/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-01-01T21:47:12Z", "updated_at": "2021-01-05T19:16:50Z", "closed_at": "2021-01-05T19:16:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nSuccessfully installed fairseq but got ModuleNotFoundError: No module named 'examples.speech_to_text'\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. git clone https://github.com/pytorch/fairseq\r\n2. cd fairseq\r\n3. rm pyproject.toml https://github.com/pytorch/fairseq/issues/1977#issuecomment-612148593\r\n4. pip install --editable .\r\nDefaulting to user installation because normal site-packages is not writeable\r\nObtaining file:///N/slate/piyush/fairseq\r\nRequirement already satisfied: cffi in /geode2/soft/hps/rhel7/deeplearning/Python-3.8.5/lib/python3.8/site-packages (from fairseq==1.0.0a0+01fcec5) (1.14.1)\r\nRequirement already satisfied: cython in /geode2/home/u070/piyush/Carbonate/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+01fcec5) (0.29.21)\r\nRequirement already satisfied: hydra-core<1.1 in /geode2/home/u070/piyush/Carbonate/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+01fcec5) (1.0.4)\r\nRequirement already satisfied: omegaconf<2.1 in /geode2/home/u070/piyush/Carbonate/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+01fcec5) (2.0.5)\r\nRequirement already satisfied: regex in /geode2/soft/hps/rhel7/deeplearning/Python-3.8.5/lib/python3.8/site-packages (from fairseq==1.0.0a0+01fcec5) (2020.7.14)\r\nRequirement already satisfied: sacrebleu>=1.4.12 in /geode2/home/u070/piyush/Carbonate/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+01fcec5) (1.4.13)\r\nRequirement already satisfied: torch in /geode2/soft/hps/rhel7/deeplearning/Python-3.8.5/lib/python3.8/site-packages (from fairseq==1.0.0a0+01fcec5) (1.6.0)\r\nRequirement already satisfied: tqdm in /geode2/soft/hps/rhel7/deeplearning/Python-3.8.5/lib/python3.8/site-packages (from fairseq==1.0.0a0+01fcec5) (4.48.0)\r\nRequirement already satisfied: numpy in /geode2/soft/hps/rhel7/deeplearning/Python-3.8.5/lib/python3.8/site-packages (from fairseq==1.0.0a0+01fcec5) (1.18.5)\r\nRequirement already satisfied: pycparser in /geode2/soft/hps/rhel7/deeplearning/Python-3.8.5/lib/python3.8/site-packages (from cffi->fairseq==1.0.0a0+01fcec5) (2.20)\r\nRequirement already satisfied: antlr4-python3-runtime==4.8 in /geode2/home/u070/piyush/Carbonate/.local/lib/python3.8/site-packages (from hydra-core<1.1->fairseq==1.0.0a0+01fcec5) (4.8)\r\nRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /geode2/home/u070/piyush/Carbonate/.local/lib/python3.8/site-packages (from hydra-core<1.1->fairseq==1.0.0a0+01fcec5) (3.3.0)\r\nRequirement already satisfied: typing-extensions in /geode2/home/u070/piyush/Carbonate/.local/lib/python3.8/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+01fcec5) (3.7.4.2)\r\nRequirement already satisfied: PyYAML>=5.1.* in /geode2/soft/hps/rhel7/deeplearning/Python-3.8.5/lib/python3.8/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+01fcec5) (5.3.1)\r\nRequirement already satisfied: portalocker in /geode2/home/u070/piyush/Carbonate/.local/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+01fcec5) (2.0.0)\r\nRequirement already satisfied: future in /geode2/soft/hps/rhel7/deeplearning/Python-3.8.5/lib/python3.8/site-packages (from torch->fairseq==1.0.0a0+01fcec5) (0.18.2)\r\nInstalling collected packages: fairseq\r\n  Running setup.py develop for fairseq\r\nSuccessfully installed fairseq\r\n\r\n5. python examples/speech_to_text/prep_covost_data.py -d /N/slate/piyush/fairseq/CoVoST -s zh-CN -t en --vocab-type unigram --vocab-size 300\r\nTraceback (most recent call last):\r\n  File \"examples/speech_to_text/prep_covost_data.py\", line 18, in <module>\r\n    from examples.speech_to_text.data_utils import (\r\nModuleNotFoundError: No module named 'examples.speech_to_text'", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3087/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3087/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3076", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3076/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3076/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3076/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3076", "id": 775416464, "node_id": "MDU6SXNzdWU3NzU0MTY0NjQ=", "number": 3076, "title": "KeyError: 'translation_moe'", "user": {"login": "RomiBed", "id": 44785043, "node_id": "MDQ6VXNlcjQ0Nzg1MDQz", "avatar_url": "https://avatars.githubusercontent.com/u/44785043?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RomiBed", "html_url": "https://github.com/RomiBed", "followers_url": "https://api.github.com/users/RomiBed/followers", "following_url": "https://api.github.com/users/RomiBed/following{/other_user}", "gists_url": "https://api.github.com/users/RomiBed/gists{/gist_id}", "starred_url": "https://api.github.com/users/RomiBed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RomiBed/subscriptions", "organizations_url": "https://api.github.com/users/RomiBed/orgs", "repos_url": "https://api.github.com/users/RomiBed/repos", "events_url": "https://api.github.com/users/RomiBed/events{/privacy}", "received_events_url": "https://api.github.com/users/RomiBed/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-12-28T14:03:10Z", "updated_at": "2020-12-28T14:13:30Z", "closed_at": "2020-12-28T14:13:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "paraphrase.py is giving error for uploading the fr2en model using task translation_moe. Can you please help  ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3076/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3074", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3074/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3074/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3074/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3074", "id": 775373996, "node_id": "MDU6SXNzdWU3NzUzNzM5OTY=", "number": 3074, "title": "close", "user": {"login": "indifferen", "id": 32159593, "node_id": "MDQ6VXNlcjMyMTU5NTkz", "avatar_url": "https://avatars.githubusercontent.com/u/32159593?v=4", "gravatar_id": "", "url": "https://api.github.com/users/indifferen", "html_url": "https://github.com/indifferen", "followers_url": "https://api.github.com/users/indifferen/followers", "following_url": "https://api.github.com/users/indifferen/following{/other_user}", "gists_url": "https://api.github.com/users/indifferen/gists{/gist_id}", "starred_url": "https://api.github.com/users/indifferen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/indifferen/subscriptions", "organizations_url": "https://api.github.com/users/indifferen/orgs", "repos_url": "https://api.github.com/users/indifferen/repos", "events_url": "https://api.github.com/users/indifferen/events{/privacy}", "received_events_url": "https://api.github.com/users/indifferen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-12-28T12:22:48Z", "updated_at": "2021-01-01T12:58:51Z", "closed_at": "2021-01-01T12:58:51Z", "author_association": "NONE", "active_lock_reason": null, "body": ".", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3074/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3053", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3053/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3053/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3053/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3053", "id": 771812811, "node_id": "MDU6SXNzdWU3NzE4MTI4MTE=", "number": 3053, "title": "ModuleNotFoundError: No module named 'fairseq.data.token_block_utils_fast' while using torch.hub", "user": {"login": "wonkeelee", "id": 33458788, "node_id": "MDQ6VXNlcjMzNDU4Nzg4", "avatar_url": "https://avatars.githubusercontent.com/u/33458788?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wonkeelee", "html_url": "https://github.com/wonkeelee", "followers_url": "https://api.github.com/users/wonkeelee/followers", "following_url": "https://api.github.com/users/wonkeelee/following{/other_user}", "gists_url": "https://api.github.com/users/wonkeelee/gists{/gist_id}", "starred_url": "https://api.github.com/users/wonkeelee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wonkeelee/subscriptions", "organizations_url": "https://api.github.com/users/wonkeelee/orgs", "repos_url": "https://api.github.com/users/wonkeelee/repos", "events_url": "https://api.github.com/users/wonkeelee/events{/privacy}", "received_events_url": "https://api.github.com/users/wonkeelee/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-12-21T03:48:42Z", "updated_at": "2021-01-28T13:12:29Z", "closed_at": "2020-12-22T19:46:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nHi, I'm trying to follow the tutorial of the backtranslation (fairseq/examples/backtranslation) to reproduce their results. Unfortunately, when I run the code `torch.hub.list(..)`, it failed with errors. \r\nIn my guess, this error seems to be almost the same as #1376.\r\n\r\nHere is a part of the errors:\r\n\r\n```\r\n/usr/include/c++/7/bits/basic_string.tcc:1067:16: error: cannot call member function \u2018void std::basic_string<_CharT, _Traits, _Alloc>::_Rep::_M_set_sharable() [with _CharT = char16_t; _Traits = std::char_traits<char16_t>; _Alloc = std::allocator<char16_t>]\u2019 without object\r\n       __p->_M_set_sharable();\r\n       ~~~~~~~~~^~\r\n/usr/include/c++/7/bits/basic_string.tcc: In instantiation of \u2018static std::basic_string<_CharT, _Traits, _Alloc>::_Rep* std::basic_string<_CharT, _Traits, _Alloc>::_Rep::_S_create(std::basic_string<_CharT, _Traits, _Alloc>::size_type, std::basic_string<_CharT, _Traits, _Alloc>::size_type, const _Alloc&) [with _CharT = char32_t; _Traits = std::char_traits<char32_t>; _Alloc = std::allocator<char32_t>; std::basic_string<_CharT, _Traits, _Alloc>::size_type = long unsigned int]\u2019:\r\n/usr/include/c++/7/bits/basic_string.tcc:578:28:   required from \u2018static _CharT* std::basic_string<_CharT, _Traits, _Alloc>::_S_construct(_InIterator, _InIterator, const _Alloc&, std::forward_iterator_tag) [with _FwdIterator = const char32_t*; _CharT = char32_t; _Traits = std::char_traits<char32_t>; _Alloc = std::allocator<char32_t>]\u2019\r\n/usr/include/c++/7/bits/basic_string.h:5042:20:   required from \u2018static _CharT* std::basic_string<_CharT, _Traits, _Alloc>::_S_construct_aux(_InIterator, _InIterator, const _Alloc&, std::__false_type) [with _InIterator = const char32_t*; _CharT = char32_t; _Traits = std::char_traits<char32_t>; _Alloc = std::allocator<char32_t>]\u2019\r\n/usr/include/c++/7/bits/basic_string.h:5063:24:   required from \u2018static _CharT* std::basic_string<_CharT, _Traits, _Alloc>::_S_construct(_InIterator, _InIterator, const _Alloc&) [with _InIterator = const char32_t*; _CharT = char32_t; _Traits = std::char_traits<char32_t>; _Alloc = std::allocator<char32_t>]\u2019\r\n/usr/include/c++/7/bits/basic_string.tcc:656:134:   required from \u2018std::basic_string<_CharT, _Traits, _Alloc>::basic_string(const _CharT*, std::basic_string<_CharT, _Traits, _Alloc>::size_type, const _Alloc&) [with _CharT = char32_t; _Traits = std::char_traits<char32_t>; _Alloc = std::allocator<char32_t>; std::basic_string<_CharT, _Traits, _Alloc>::size_type = long unsigned int]\u2019\r\n/usr/include/c++/7/bits/basic_string.h:6693:95:   required from here\r\n/usr/include/c++/7/bits/basic_string.tcc:1067:16: error: cannot call member function \u2018void std::basic_string<_CharT, _Traits, _Alloc>::_Rep::_M_set_sharable() [with _CharT = char32_t; _Traits = std::char_traits<char32_t>; _Alloc = std::allocator<char32_t>]\u2019 without object\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home3/wklee/.cache/torch/hub/pytorch_fairseq_master/hubconf.py\", line 45, in <module>\r\n    import fairseq.data.token_block_utils_fast  # noqa\r\nModuleNotFoundError: No module named 'fairseq.data.token_block_utils_fast'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home3/wklee/.pyenv/versions/3.7.9/lib/python3.7/distutils/unixccompiler.py\", line 118, in _compile\r\n    extra_postargs)\r\n  File \"/home3/wklee/.pyenv/versions/3.7.9/lib/python3.7/distutils/ccompiler.py\", line 910, in spawn\r\n    spawn(cmd, dry_run=self.dry_run)\r\n  File \"/home3/wklee/.pyenv/versions/3.7.9/lib/python3.7/distutils/spawn.py\", line 36, in spawn\r\n    _spawn_posix(cmd, search_path, dry_run=dry_run)\r\n  File \"/home3/wklee/.pyenv/versions/3.7.9/lib/python3.7/distutils/spawn.py\", line 159, in _spawn_posix\r\n    % (cmd, exit_status))\r\ndistutils.errors.DistutilsExecError: command '/usr/local/cuda-10.1/bin/nvcc' failed with exit status 1\r\n\r\n```\r\n\r\n\r\nEnvironment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.7 and also 1.6 (**I tried it on two environments**)\r\n - OS (e.g., Linux): linux 18.04\r\n - How you installed fairseq (`pip`, source): pip install --editable ./\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.9\r\n - CUDA/cuDNN version: CUDA 11.0 (for torch==1.7) and CUDA 10.1 (for torch==1.6) (**I tried it on two environments**)\r\n - GPU models and configuration: RTX3090 for torch==1.7 and RTX2080 super for torch==1.6 (**I tried it on two environments**)\r\n\r\n\r\nThank you, in advance!!.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3053/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3053/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3052", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3052/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3052/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3052/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3052", "id": 771793872, "node_id": "MDU6SXNzdWU3NzE3OTM4NzI=", "number": 3052, "title": "Logging to STDERR", "user": {"login": "voidmagic", "id": 10617485, "node_id": "MDQ6VXNlcjEwNjE3NDg1", "avatar_url": "https://avatars.githubusercontent.com/u/10617485?v=4", "gravatar_id": "", "url": "https://api.github.com/users/voidmagic", "html_url": "https://github.com/voidmagic", "followers_url": "https://api.github.com/users/voidmagic/followers", "following_url": "https://api.github.com/users/voidmagic/following{/other_user}", "gists_url": "https://api.github.com/users/voidmagic/gists{/gist_id}", "starred_url": "https://api.github.com/users/voidmagic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/voidmagic/subscriptions", "organizations_url": "https://api.github.com/users/voidmagic/orgs", "repos_url": "https://api.github.com/users/voidmagic/repos", "events_url": "https://api.github.com/users/voidmagic/events{/privacy}", "received_events_url": "https://api.github.com/users/voidmagic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 2634521869, "node_id": "MDU6TGFiZWwyNjM0NTIxODY5", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/0.10.2", "name": "0.10.2", "color": "8ECF2D", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-12-21T02:51:00Z", "updated_at": "2021-01-05T20:27:12Z", "closed_at": "2021-01-05T20:27:12Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n\r\nThe output stream should be STDOUT by default, as shown in `train.py`:\r\n```\r\nlogging.basicConfig(\r\n    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\r\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\r\n    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\r\n    stream=sys.stdout,\r\n)\r\n```\r\n\r\nHowever after `import fairseq`, the default output stream becomes STDERR. I cannot figure out why.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd `fairseq-train ... > output.txt`\r\n2. See error `cat output.txt` with empty content.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\nA simple way to reproduce:\r\n\r\n`log_test1.py`:\r\n```\r\nimport logging, sys\r\nlogging.basicConfig(level='INFO', stream=sys.stdout)\r\nlogging.info('hello')\r\n```\r\nRun `python log_test1.py > output.txt`, the content in `output.txt` is `INFO:root:hello`.\r\n\r\n`log_test2.py`:\r\n```\r\nimport logging, sys, fairseq\r\nlogging.basicConfig(level='INFO', stream=sys.stdout)\r\nlogging.info('hello')\r\n```\r\nRun `python log_test1.py > output.txt`, the content in `output.txt` is empty while the stderr prints `2020-12-21 10:48:39 | INFO | root | hello`.\r\n\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nRedirect all loggings to STDOUT so that we can distinguish infos and errors with different logging files.\r\n\r\n### Environment\r\n\r\n - fairseq Version (0.10.1):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3052/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3042", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3042/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3042/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3042/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3042", "id": 768917672, "node_id": "MDU6SXNzdWU3Njg5MTc2NzI=", "number": 3042, "title": "Inconsistency in the load of the Wav2vec model", "user": {"login": "Edresson", "id": 28763586, "node_id": "MDQ6VXNlcjI4NzYzNTg2", "avatar_url": "https://avatars.githubusercontent.com/u/28763586?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Edresson", "html_url": "https://github.com/Edresson", "followers_url": "https://api.github.com/users/Edresson/followers", "following_url": "https://api.github.com/users/Edresson/following{/other_user}", "gists_url": "https://api.github.com/users/Edresson/gists{/gist_id}", "starred_url": "https://api.github.com/users/Edresson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Edresson/subscriptions", "organizations_url": "https://api.github.com/users/Edresson/orgs", "repos_url": "https://api.github.com/users/Edresson/repos", "events_url": "https://api.github.com/users/Edresson/events{/privacy}", "received_events_url": "https://api.github.com/users/Edresson/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 2634592838, "node_id": "MDU6TGFiZWwyNjM0NTkyODM4", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/wav2vec", "name": "wav2vec", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2020-12-16T14:12:28Z", "updated_at": "2021-02-12T00:56:49Z", "closed_at": "2021-02-12T00:56:49Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nThe Commit [032a404d389307a0e8f7dd2a0d501c78afa78f39](https://github.com/pytorch/fairseq/commit/032a404d389307a0e8f7dd2a0d501c78afa78f39) broke the load structure of the wav2vec multilingual model (trained before this commit).\r\n\r\nBefore the commit the models load normally occur after the following error occurs during the inference (more specifically during the model load):\r\n\r\n> INFO:__main__:| decoding with criterion ctc\r\nINFO:__main__:| loading model(s) from wav2vec/checkpoint_best_multilingual.pt\r\nTraceback (most recent call last):\r\n  File \"infer_multi.py\", line 764, in <module>\r\n    cli_main()\r\n  File \"infer_multi.py\", line 760, in cli_main\r\n    main(args)\r\n  File \"infer_multi.py\", line 238, in main\r\n    state=model_state,\r\n  File \"/root/fairseq/fairseq/checkpoint_utils.py\", line 269, in load_model_ensemble\r\n    state,\r\n  File \"/root/fairseq/fairseq/checkpoint_utils.py\", line 318, in load_model_ensemble_and_task\r\n    model = task.build_model(cfg.model)\r\n  File \"/root/fairseq/fairseq/tasks/audio_pretraining.py\", line 201, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/root/fairseq/fairseq/tasks/fairseq_task.py\", line 282, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/root/fairseq/fairseq/models/__init__.py\", line 96, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/root/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 147, in build_model\r\n    w2v_encoder = Wav2VecEncoder(cfg, task.target_dictionary)\r\n  File \"/root/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 298, in __init__\r\n    task = tasks.setup_task(w2v_args.task)\r\n  File \"/root/fairseq/fairseq/tasks/__init__.py\", line 39, in setup_task\r\n    cfg = merge_with_parent(dc(), cfg)\r\n  File \"/root/fairseq/fairseq/dataclass/utils.py\", line 448, in merge_with_parent\r\n    merged_cfg = OmegaConf.merge(dc, cfg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/omegaconf.py\", line 321, in merge\r\n    target.merge_with(*others[1:])\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/basecontainer.py\", line 331, in merge_with\r\n    self._format_and_raise(key=None, value=None, cause=e)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 629, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/basecontainer.py\", line 329, in merge_with\r\n    self._merge_with(*others)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/basecontainer.py\", line 347, in _merge_with\r\n    BaseContainer._map_merge(self, other)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/basecontainer.py\", line 294, in _map_merge\r\n    dest_node._merge_with(src_value)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/basecontainer.py\", line 347, in _merge_with\r\n    BaseContainer._map_merge(self, other)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/basecontainer.py\", line 305, in _map_merge\r\n    dest._format_and_raise(key=key, value=src_value, cause=e)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.ValidationError: Invalid value 'False', expected one of [hard, soft]\r\n\tfull_key: eval_wer_config.print_alignment\r\n\treference_type=GenerationConfig\r\n\tobject_type=GenerationConfig\r\n\r\n\r\nI believe it is interesting to maintain compatibility with old models.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3042/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3026", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3026/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3026/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3026/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3026", "id": 762712119, "node_id": "MDU6SXNzdWU3NjI3MTIxMTk=", "number": 3026, "title": "can't open input file `CoVoST/fr/raw/clips/common_voice_fr_19528232.mp3'", "user": {"login": "pyyush", "id": 49844424, "node_id": "MDQ6VXNlcjQ5ODQ0NDI0", "avatar_url": "https://avatars.githubusercontent.com/u/49844424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pyyush", "html_url": "https://github.com/pyyush", "followers_url": "https://api.github.com/users/pyyush/followers", "following_url": "https://api.github.com/users/pyyush/following{/other_user}", "gists_url": "https://api.github.com/users/pyyush/gists{/gist_id}", "starred_url": "https://api.github.com/users/pyyush/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pyyush/subscriptions", "organizations_url": "https://api.github.com/users/pyyush/orgs", "repos_url": "https://api.github.com/users/pyyush/repos", "events_url": "https://api.github.com/users/pyyush/events{/privacy}", "received_events_url": "https://api.github.com/users/pyyush/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-12-11T18:12:27Z", "updated_at": "2021-01-07T02:49:04Z", "closed_at": "2021-01-07T02:49:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n### To Reproduce\r\n\r\n1. Run cmd 'python3 examples/speech_to_text/prep_covost_data.py --data-root CoVoST --vocab-type char --src-lang fr --tgt-lang en'\r\n2. See error - formats: can't open input file `CoVoST/fr/raw/clips/common_voice_fr_19528232.mp3': \r\n 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                         | 168928/207374 [1:04:51<14:45, 43.41it/s]\r\nTraceback (most recent call last):\r\n  File \"examples/speech_to_text/prep_covost_data.py\", line 290, in <module>\r\n    main()\r\n  File \"examples/speech_to_text/prep_covost_data.py\", line 286, in main\r\n    process(args)\r\n  File \"examples/speech_to_text/prep_covost_data.py\", line 222, in process\r\n    for waveform, sample_rate, _, _, _, utt_id in tqdm(dataset):\r\n  File \"/usr/local/lib/python3.7/site-packages/tqdm/std.py\", line 1129, in __iter__\r\n    for obj in iterable:\r\n  File \"examples/speech_to_text/prep_covost_data.py\", line 201, in __getitem__\r\n    waveform, sample_rate = torchaudio.load(path)\r\n  File \"/usr/local/lib/python3.7/site-packages/torchaudio/backend/sox_backend.py\", line 56, in load\r\n    filetype\r\nRuntimeError: Error opening audio file", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3026/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3026/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3017", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3017/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3017/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3017/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3017", "id": 761143134, "node_id": "MDU6SXNzdWU3NjExNDMxMzQ=", "number": 3017, "title": "Bug (and solution) for `import_user_module`", "user": {"login": "davidecaroselli", "id": 1674891, "node_id": "MDQ6VXNlcjE2NzQ4OTE=", "avatar_url": "https://avatars.githubusercontent.com/u/1674891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidecaroselli", "html_url": "https://github.com/davidecaroselli", "followers_url": "https://api.github.com/users/davidecaroselli/followers", "following_url": "https://api.github.com/users/davidecaroselli/following{/other_user}", "gists_url": "https://api.github.com/users/davidecaroselli/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidecaroselli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidecaroselli/subscriptions", "organizations_url": "https://api.github.com/users/davidecaroselli/orgs", "repos_url": "https://api.github.com/users/davidecaroselli/repos", "events_url": "https://api.github.com/users/davidecaroselli/events{/privacy}", "received_events_url": "https://api.github.com/users/davidecaroselli/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-12-10T11:08:19Z", "updated_at": "2020-12-12T02:55:53Z", "closed_at": "2020-12-12T02:55:53Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nMy original implementation of the function `utils.import_user_module` supported external user modules wrapped in zip/jar files (python natively support modules in zip/jar files).\r\n\r\nThe new piece of code which checks for file existence breaks this functionality.\r\nA trivial fix can solve the problem and I have already implemented it: I'm sending a PR right after this bug report.\r\n\r\n### To Reproduce\r\n\r\nJust wrap a sample module in a zip file and then use the option:\r\n```\r\n--user-dir /home/ubuntu/my-custom-model.zip/myexample\r\n```\r\nYou will get the following error message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/venv/bin/fairseq-preprocess\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/fairseq_cli/preprocess.py\", line 392, in cli_main\r\n    parser = options.get_preprocessing_parser()\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/fairseq/options.py\", line 28, in get_preprocessing_parser\r\n    parser = get_parser(\"Preprocessing\", default_task)\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/fairseq/options.py\", line 210, in get_parser\r\n    utils.import_user_module(usr_args)\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/fairseq/utils.py\", line 447, in import_user_module\r\n    raise FileNotFoundError(module_path)\r\nFileNotFoundError: /home/ubuntu/my-custom-model.zip/myexample\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3017/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3017/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3008", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3008/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3008/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3008/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3008", "id": 760038064, "node_id": "MDU6SXNzdWU3NjAwMzgwNjQ=", "number": 3008, "title": "An error occurs when finetune wav2vec 2.0 with hydra-train", "user": {"login": "monhoney", "id": 1555360, "node_id": "MDQ6VXNlcjE1NTUzNjA=", "avatar_url": "https://avatars.githubusercontent.com/u/1555360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/monhoney", "html_url": "https://github.com/monhoney", "followers_url": "https://api.github.com/users/monhoney/followers", "following_url": "https://api.github.com/users/monhoney/following{/other_user}", "gists_url": "https://api.github.com/users/monhoney/gists{/gist_id}", "starred_url": "https://api.github.com/users/monhoney/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/monhoney/subscriptions", "organizations_url": "https://api.github.com/users/monhoney/orgs", "repos_url": "https://api.github.com/users/monhoney/repos", "events_url": "https://api.github.com/users/monhoney/events{/privacy}", "received_events_url": "https://api.github.com/users/monhoney/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-12-09T06:18:54Z", "updated_at": "2022-07-12T17:05:16Z", "closed_at": "2020-12-09T12:30:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nI am trying to finetuning using the wav2vec 2.0 base pretrained model. \r\nI downloaded the wav2vec_small_100h.pt model and changed the name to wav2vec_small.pt.\r\nI ran finetune according to the manual, but the following error occurs:\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd \r\n```\r\nfairseq-hydra-train \\\r\n    distributed_training.distributed_port=22000 \\\r\n    task.data=/work/volumes/manifest \\\r\n    model.w2v_path=/work/volumes/pretrained_model/wav2vec_small.pt \\\r\n    --config-dir /work/fairseq/examples/wav2vec/config/finetuning \\\r\n    --config-name base_100h\r\n```\r\n\r\n2. See error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/fairseq-hydra-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-hydra-train')())\r\n  File \"/work/fairseq/fairseq_cli/hydra_train.py\", line 66, in cli_main\r\n    hydra_main()\r\n  File \"/opt/conda/lib/python3.6/site-packages/hydra/main.py\", line 37, in decorated_main\r\n    strict=strict,\r\n  File \"/opt/conda/lib/python3.6/site-packages/hydra/_internal/utils.py\", line 347, in _run_hydra\r\n    lambda: hydra.run(\r\n  File \"/opt/conda/lib/python3.6/site-packages/hydra/_internal/utils.py\", line 201, in run_and_report\r\n    raise ex\r\n  File \"/opt/conda/lib/python3.6/site-packages/hydra/_internal/utils.py\", line 198, in run_and_report\r\n    return func()\r\n  File \"/opt/conda/lib/python3.6/site-packages/hydra/_internal/utils.py\", line 350, in <lambda>\r\n    overrides=args.overrides,\r\n  File \"/opt/conda/lib/python3.6/site-packages/hydra/_internal/hydra.py\", line 112, in run\r\n    configure_logging=with_log_configuration,\r\n  File \"/opt/conda/lib/python3.6/site-packages/hydra/core/utils.py\", line 125, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"/work/fairseq/fairseq_cli/hydra_train.py\", line 38, in hydra_main\r\n    distributed_utils.call_main(cfg, pre_main)\r\n  File \"/work/fairseq/fairseq/distributed_utils.py\", line 334, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/work/fairseq/fairseq_cli/train.py\", line 74, in main\r\n    model = task.build_model(cfg.model)\r\n  File \"/work/fairseq/fairseq/tasks/audio_pretraining.py\", line 201, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/work/fairseq/fairseq/tasks/fairseq_task.py\", line 282, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/work/fairseq/fairseq/models/__init__.py\", line 86, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/work/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 147, in build_model\r\n    w2v_encoder = Wav2VecEncoder(cfg, task.target_dictionary)\r\n  File \"/work/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 304, in __init__\r\n    model.remove_pretraining_modules()\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 779, in __getattr__\r\n    type(self).__name__, name))\r\ntorch.nn.modules.module.ModuleAttributeError: 'Wav2VecCtc' object has no attribute 'remove_pretraining_modules'\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip install --editable ./\r\n - Build command you used (if compiling from source):\r\n - Python version: \r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3008/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3008/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3005", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3005/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3005/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3005/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/3005", "id": 759654090, "node_id": "MDU6SXNzdWU3NTk2NTQwOTA=", "number": 3005, "title": "Errors when fine-tune Wav2Vec 2.0 Base Model. Config mismatch", "user": {"login": "mailong25", "id": 12481660, "node_id": "MDQ6VXNlcjEyNDgxNjYw", "avatar_url": "https://avatars.githubusercontent.com/u/12481660?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mailong25", "html_url": "https://github.com/mailong25", "followers_url": "https://api.github.com/users/mailong25/followers", "following_url": "https://api.github.com/users/mailong25/following{/other_user}", "gists_url": "https://api.github.com/users/mailong25/gists{/gist_id}", "starred_url": "https://api.github.com/users/mailong25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mailong25/subscriptions", "organizations_url": "https://api.github.com/users/mailong25/orgs", "repos_url": "https://api.github.com/users/mailong25/repos", "events_url": "https://api.github.com/users/mailong25/events{/privacy}", "received_events_url": "https://api.github.com/users/mailong25/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2020-12-08T17:53:52Z", "updated_at": "2021-02-03T20:19:18Z", "closed_at": "2021-02-03T20:19:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nomegaconf.errors.ConfigAttributeError: Key 'normalize' is not in struct\r\n\r\n### To Reproduce\r\n\r\n1. Run cmd '....'\r\n\r\nfairseq-hydra-train task.data=/mnt/disks2/wav2vec/dataset distributed_training.distributed_world_size=2 +optimization.update_freq='[12]' model.w2v_path=/mnt/disks2/wav2vec/wav2vec_small_vie.pt dataset.num_workers=8 dataset.valid_subset=valid --config-dir /mnt/disks2/wav2vec/fairseq/examples/wav2vec/config/finetuning --config-name base_100h\r\n\r\n2. See error\r\n\r\n2020-12-08 17:44:50 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15186\r\n2020-12-08 17:44:50 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15186\r\n2020-12-08 17:44:51 | INFO | fairseq.distributed_utils | initialized host longsg-vm.c.vnlp-tts.internal as rank 1\r\n2020-12-08 17:44:51 | INFO | fairseq.distributed_utils | initialized host longsg-vm.c.vnlp-tts.internal as rank 0\r\n2020-12-08 17:44:56 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'tensorboard_logdir': None, 'wandb_project': None, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': True}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15186', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 2}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3200000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3200000, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [12], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/mnt/disks2/wav2vec/wav2vec_small_vie.pt', 'apply_mask': True, 'mask_prob': 0.65, 'mask_channel_prob': 0.5, 'mask_channel_length': 64, 'layerdrop': 0.1, 'activation_dropout': 0.1, 'feature_grad_mult': 0.0, 'freeze_finetune_updates': 0}, 'task': {'_name': 'audio_pretraining', 'data': '/mnt/disks2/wav2vec/dataset', 'normalize': False, 'labels': 'ltr'}, 'criterion': {'_name': 'ctc', 'zero_infinity': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08}, 'lr_scheduler': {'_name': 'tri_stage', 'phase_ratio': [0.1, 0.4, 0.5], 'final_lr_scale': 0.05}, 'scoring': None, 'bpe': None, 'tokenizer': None}\r\n2020-12-08 17:44:56 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 688, skipped 0 samples\r\n2020-12-08 17:44:56 | INFO | wandb.sdk.internal.internal | Internal process exited\r\nTraceback (most recent call last):\r\n  File \"/mnt/disks2/wav2vec/fairseq/fairseq_cli/hydra_train.py\", line 38, in hydra_main\r\n    distributed_utils.call_main(cfg, pre_main)\r\n  File \"/mnt/disks2/wav2vec/fairseq/fairseq/distributed_utils.py\", line 320, in call_main\r\n    cfg.distributed_training.distributed_world_size,\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/mnt/disks2/wav2vec/fairseq/fairseq/distributed_utils.py\", line 302, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/mnt/disks2/wav2vec/fairseq/fairseq_cli/train.py\", line 74, in main\r\n    model = task.build_model(cfg.model)\r\n  File \"/mnt/disks2/wav2vec/fairseq/fairseq/tasks/audio_pretraining.py\", line 201, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/mnt/disks2/wav2vec/fairseq/fairseq/tasks/fairseq_task.py\", line 282, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/mnt/disks2/wav2vec/fairseq/fairseq/models/__init__.py\", line 96, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/mnt/disks2/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 147, in build_model\r\n    w2v_encoder = Wav2VecEncoder(cfg, task.target_dictionary)\r\n  File \"/mnt/disks2/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 292, in __init__\r\n    assert cfg.normalize == w2v_args.task.normalize, (\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 297, in __getattr__\r\n    self._format_and_raise(key=key, value=None, cause=e)\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/_utils.py\", line 629, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 295, in __getattr__\r\n    return self._get_impl(key=key, default_value=DEFAULT_VALUE_MARKER)\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 353, in _get_impl\r\n    node = self._get_node(key=key)\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 375, in _get_node\r\n    self._validate_get(key)\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 128, in _validate_get\r\n    key=key, value=value, cause=ConfigAttributeError(msg)\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/mnt/disks2/miniconda3/lib/python3.7/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.ConfigAttributeError: Key 'normalize' is not in struct\r\n\tfull_key: task.normalize\r\n\treference_type=Any\r\n\tobject_type=dict\r\n\r\n### Environment\r\n\r\n - fairseq Version (1.0.0a0+4817a91):\r\n - PyTorch Version (1.6)\r\n - OS ( Linux):\r\n - How you installed fairseq (pip install --editable ./):\r\n - Build command you used (if compiling from source):\r\n - Python version:3.7.6\r\n - CUDA/cuDNN version:10.1\r\n - GPU models and configuration:Tesla V100\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3005/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2983", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2983/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2983/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2983/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2983", "id": 755673248, "node_id": "MDU6SXNzdWU3NTU2NzMyNDg=", "number": 2983, "title": "TypeError: tuple indices must be integers or slices, not str when running encoder_out[\"encoder_out\"]", "user": {"login": "ZLKong", "id": 28882362, "node_id": "MDQ6VXNlcjI4ODgyMzYy", "avatar_url": "https://avatars.githubusercontent.com/u/28882362?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZLKong", "html_url": "https://github.com/ZLKong", "followers_url": "https://api.github.com/users/ZLKong/followers", "following_url": "https://api.github.com/users/ZLKong/following{/other_user}", "gists_url": "https://api.github.com/users/ZLKong/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZLKong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZLKong/subscriptions", "organizations_url": "https://api.github.com/users/ZLKong/orgs", "repos_url": "https://api.github.com/users/ZLKong/repos", "events_url": "https://api.github.com/users/ZLKong/events{/privacy}", "received_events_url": "https://api.github.com/users/ZLKong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-12-02T23:00:01Z", "updated_at": "2020-12-15T16:37:36Z", "closed_at": "2020-12-15T16:37:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "I ran command in Speech Recognition (ASR) on LibriSpeech\r\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_to_text#training \r\n`CUDA_VISIBLE_DEVICES=1 fairseq-train /data/ --train-subset train-clean-100 --valid-subset dev-clean --save-dir /data/ --num-workers 4 --max-tokens 40000 --task speech_to_text --criterion label_smoothed_cross_entropy --max-update 300000 --arch s2t_transformer_s --optimizer adam --lr 2e-3 --lr-scheduler inverse_sqrt --warmup-updates 10000 --clip-norm 10.0 --seed 1 --update-freq 8`\r\n\r\nand got the tuple indices error: `TypeError: tuple indices must be integers or slices, not str . ` \r\n\r\nIt has something to do with the `encoder_out[\"encoder_out\"] ` in transformer.py \r\n\r\nhttps://github.com/pytorch/fairseq/blob/0db28cdd0e50cad9c36e5e47ffceff40beaf6f60/fairseq/models/transformer.py#L807-L810\r\n\r\nYou can just `print(encoder_out[\"encoder_out\"])`or  `print(len(encoder_out[\"encoder_out\"]))` and get the same error.\r\n\r\n**Full Traceback** \r\n```\r\nTraceback (most recent call last):\r\nFile \"/home/zhk20002/anaconda2/envs/ASR/bin/fairseq-train\", line 33, in\r\nsys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\ndistributed_utils.call_main(cfg, main)\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq/distributed_utils.py\", line 334, in call_main\r\nmain(cfg, **kwargs)\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq_cli/train.py\", line 130, in main\r\nvalid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\nFile \"/home/zhk20002/anaconda2/envs/ASR/lib/python3.6/contextlib.py\", line 52, in inner\r\nreturn func(*args, **kwds)\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq_cli/train.py\", line 219, in train\r\nlog_output = trainer.train_step(samples)\r\nFile \"/home/zhk20002/anaconda2/envs/ASR/lib/python3.6/contextlib.py\", line 52, in inner\r\nreturn func(*args, **kwds)\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq/trainer.py\", line 537, in train_step\r\nignore_grad=is_dummy_batch,\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq/tasks/fairseq_task.py\", line 428, in train_step\r\nloss, sample_size, logging_output = criterion(model, sample)\r\nFile \"/home/zhk20002/anaconda2/envs/ASR/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in call\r\nresult = self.forward(*input, **kwargs)\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py\", line 69, in forward\r\nnet_output = model(**sample[\"net_input\"])\r\nFile \"/home/zhk20002/anaconda2/envs/ASR/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in call\r\nresult = self.forward(*input, **kwargs)\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq/models/speech_to_text/s2t_transformer.py\", line 258, in forward\r\nprev_output_tokens=prev_output_tokens, encoder_out=encoder_out\r\nFile \"/home/zhk20002/anaconda2/envs/ASR/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in call\r\nresult = self.forward(*input, **kwargs)\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq/models/transformer.py\", line 699, in forward\r\nalignment_heads=alignment_heads,\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq/models/speech_to_text/s2t_transformer.py\", line 385, in extract_features\r\nalignment_heads,\r\nFile \"/home/zhk20002/SAM/fairseq/fairseq/models/transformer.py\", line 810, in extract_features_scriptable\r\nif (encoder_out is not None and len(encoder_out[\"encoder_out\"]) > 0)\r\nTypeError: tuple indices must be integers or slices, not str\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2983/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2973", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2973/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2973/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2973/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2973", "id": 753556017, "node_id": "MDU6SXNzdWU3NTM1NTYwMTc=", "number": 2973, "title": "Wav2vec checkpoint and validation", "user": {"login": "haythemdhieb", "id": 44007597, "node_id": "MDQ6VXNlcjQ0MDA3NTk3", "avatar_url": "https://avatars.githubusercontent.com/u/44007597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/haythemdhieb", "html_url": "https://github.com/haythemdhieb", "followers_url": "https://api.github.com/users/haythemdhieb/followers", "following_url": "https://api.github.com/users/haythemdhieb/following{/other_user}", "gists_url": "https://api.github.com/users/haythemdhieb/gists{/gist_id}", "starred_url": "https://api.github.com/users/haythemdhieb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/haythemdhieb/subscriptions", "organizations_url": "https://api.github.com/users/haythemdhieb/orgs", "repos_url": "https://api.github.com/users/haythemdhieb/repos", "events_url": "https://api.github.com/users/haythemdhieb/events{/privacy}", "received_events_url": "https://api.github.com/users/haythemdhieb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-11-30T15:07:13Z", "updated_at": "2020-11-30T15:24:32Z", "closed_at": "2020-11-30T15:24:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nMy model (Wav2vecCtc) is simply not loading  the validation data and is not checkpointing\r\n![Screenshot from 2020-11-30 16-04-31](https://user-images.githubusercontent.com/44007597/100626209-cfeaa500-3325-11eb-9a2d-5265381af655.png)\r\n\r\n#### Code sample\r\n`\r\npython finetune_wav2vec.py task.data=/home/haythem/Desktop/Work/NLP/arabic-assistant/wav2vec/data model.w2v_path=/home/haythem/Downloads/wav2vec_small.pt        --config-name base_1h\r\n\r\n`\r\nI tried to modify the config file but still not working:\r\n\r\ncommon:\r\n  fp16: true\r\n  log_format: json\r\n  log_interval: 200\r\n\r\ncheckpoint:\r\n  save_interval: 1000\r\n  save_interval_updates: 50\r\n  keep_interval_updates: 1\r\n  no_epoch_checkpoints: True\r\n  save_dir: wav2vec/checkpoint/\r\n  best_checkpoint_metric: wer\r\n\r\ntask:\r\n  _name: audio_pretraining\r\n  data: ???\r\n  normalize: false\r\n  labels: ltr\r\n\r\ndataset:\r\n  num_workers: 1\r\n  max_tokens: 3200000\r\n  batch_size: 1\r\n  skip_invalid_size_inputs_valid_test: False\r\n  validate_after_updates: 10\r\n  validate_interval: 1000\r\n  \r\n\r\ndistributed_training:\r\n  ddp_backend: no_c10d\r\n  distributed_world_size: 1\r\n\r\ncriterion:\r\n  _name: ctc\r\n  zero_infinity: true\r\n\r\noptimization:\r\n  max_epoch: 4\r\n  max_update: 13000\r\n  lr: [0.00005]", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2973/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2971", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2971/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2971/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2971/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2971", "id": 753477876, "node_id": "MDU6SXNzdWU3NTM0Nzc4NzY=", "number": 2971, "title": "Errors while replicating Example 3: ST on CoVoST", "user": {"login": "cromz22", "id": 42872165, "node_id": "MDQ6VXNlcjQyODcyMTY1", "avatar_url": "https://avatars.githubusercontent.com/u/42872165?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cromz22", "html_url": "https://github.com/cromz22", "followers_url": "https://api.github.com/users/cromz22/followers", "following_url": "https://api.github.com/users/cromz22/following{/other_user}", "gists_url": "https://api.github.com/users/cromz22/gists{/gist_id}", "starred_url": "https://api.github.com/users/cromz22/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cromz22/subscriptions", "organizations_url": "https://api.github.com/users/cromz22/orgs", "repos_url": "https://api.github.com/users/cromz22/repos", "events_url": "https://api.github.com/users/cromz22/events{/privacy}", "received_events_url": "https://api.github.com/users/cromz22/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 22, "created_at": "2020-11-30T13:27:06Z", "updated_at": "2021-01-07T08:45:30Z", "closed_at": "2021-01-07T02:22:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nHi, thank you for providing such a nice toolkit. I'm currently working on a research about speech translation, and tried to replicate your experiment. I followed the instructions [here](https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text#example-3-st-on-covost), but could not reproduce your results.\r\n\r\nThere are mainly 4 problems:\r\n- Bug 1: Some python packages are missing\r\n- Bug 2: Some CoVoST2 audio files are empty\r\n- Bug 3: Path in config file is incorrect\r\n- Bug 4: Tuple indices must be integers\r\n\r\nDetailed error messages are at the bottom of this report.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n(command line options are same as provided ones, except that I didn't set `--num-workers` and `--update-freq` when executing `fairseq-train`)\r\n\r\n1. Install fairseq following [instruction on Github](https://github.com/pytorch/fairseq#requirements-and-installation).\r\n2. Run `examples/speech_to_text/prep_covost_data.py`.\r\n3. You will see Bug 1.\r\n4. Install pandas, torchaudio, and sentencepiece using pip.\r\n5. Run `examples/speech_to_text/prep_covost_data.py` again.\r\n6. You will see Bug 2 in some languages (at least I encountered this problem for En, De and Es).\r\n7. Find and remove empty audio files from `<covost root>/<lang>/raw/clips`; also remove corresponding tsv rows from tsv files.\r\n8. Run `examples/speech_to_text/prep_covost_data.py` again. It will finish successfully.\r\n9. Run `fairseq-train` command:\r\n```\r\nfairseq-train ${COVOST_ROOT} --train-subset train_asr_<lang> ...\r\n```\r\n10. You will see Bug 3.\r\n11. Copy or rename config file:\r\n```\r\ncp config_asr_<lang>.yaml config.yaml\r\n```\r\n12. Change audio_root in config.yaml line 1:\r\n\r\nchange\r\n`audio_root: <path to covost root>/<lang>`\r\nto\r\n`audio_root: <path to covost root>`\r\n\r\n13. Run `fairseq-train` command:\r\n```\r\nfairseq-train ${COVOST_ROOT}/<lang> --train-subset train_asr_<lang> ...\r\n```\r\n\r\n14. You will see Bug 4.\r\n15. Change `fairseq/fairseq/models/transformer.py` line 809 - 817 to:\r\n```\r\n    encoder_out[0]\r\n    if (encoder_out is not None and len(encoder_out[0]) > 0)\r\n    else None,\r\n    encoder_out[1]\r\n    if (\r\n        encoder_out is not None\r\n    )\r\n    else None,\r\n```\r\n16. Run `fairseq-train` command again, training will start successfully.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n### Expected behavior\r\n\r\nAll commands finishes without any error.\r\n\r\n### Environment\r\n\r\n - fairseq version: Installed via following commands on Nov. 27, 2020 (commit [dea66cc](https://github.com/pytorch/fairseq/commit/dea66cc294a18dd4d9e59aa0af8d51f951e83884))\r\n```\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./\r\n```\r\n - PyTorch version: 1.7.0\r\n - OS: Ubuntu 20.04\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: 10.2/8.0.3\r\n - GPU models and configuration: GeForce GTX 1080 Ti x 8\r\n\r\n### Error messages\r\n<details>\r\n<summary>Error messages for Bug 1</summary>\r\n\r\n<div>\r\n\r\n```\r\nTraceback (most recent call last):\r\n    File \"fairseq/examples/speech_to_text/prep_covost_data.py\", line 16, in <module>\r\n      import pandas as pd\r\nModuleNotFoundError: No module named 'pandas'\r\n```\r\n```\r\nTraceback (most recent call last):\r\n    File \"fairseq/examples/speech_to_text/prep_covost_data.py\", line 17, in <module>\r\n    import torchaudio\r\nModuleNotFoundError: No module named 'torchaudio'\r\n```\r\n```\r\n...\r\nTraceback (most recent call last):\r\n    File \"fairseq/examples/speech_to_text/prep_covost_data.py\", line 18, in <module>\r\n    from examples.speech_to_text.data_utils import (\r\n    File \"<path to fairseq>/fairseq/examples/speech_to_text/data_utils.py\", line 18, in <module>\r\n      import sentencepiece as sp\r\nModuleNotFoundError: No module named 'sentencepiece'\r\n```\r\n</div>\r\n</details>\r\n\r\n<details>\r\n<summary>Error messages for Bug 2</summary>\r\n<div>\r\n\r\n```\r\nFetching split train...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.80G/5.80G [10:37<00:00, 9.76MB/s]\r\n\r\n\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.05M/3.05M [00:00<00:00, 3.66MB/s]\r\nExtracting log mel filter bank features...\r\n  0%|                                  | 0/79015 [00:00<?, ?it/s]<path to venv>/venv/lib/python3.8/site-packages/torchaudio/compliance/kaldi.py:574: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\r\n    fft = torch.rfft(strided_input, 1, normalized=False, onesided=True)\r\n 37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                      | 29137/79015 [1:21:38<1:52:09,  7.41it/s]\r\n formats: can't open input file `<path to covost root>/es/raw/clips/common_voice_es_19499893.mp3':\r\n 37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                       | 29138/79015 [1:21:38<2:19:45,  5.95it/s]\r\n  Traceback (most recent call last):\r\n    File \"fairseq/examples/speech_to_text/prep_covost_data.py\", line 294, in <module>\r\n    main()\r\n    File \"fairseq/examples/speech_to_text/prep_covost_data.py\", line 290, in main\r\n      process(args)\r\n    File \"fairseq/examples/speech_to_text/prep_covost_data.py\", line 222, in process\r\n    for waveform, sample_rate, _, _, _, utt_id in tqdm(dataset):\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/tqdm/std.py\", line 1193, in __iter__\r\n      for obj in iterable:\r\n    File \"fairseq/examples/speech_to_text/prep_covost_data.py\", line 201, in __getitem__\r\n    waveform, sample_rate = torchaudio.load(path)\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torchaudio/backend/sox_backend.py\", line 48, in load\r\n      sample_rate = _torchaudio.read_audio_file(\r\n  RuntimeError: Error opening audio file\r\n```\r\n</div>\r\n</details>\r\n\r\n<details>\r\n<summary>Error messages for Bug 3</summary>\r\n<div>\r\n\r\n```\r\n  2020-11-30 14:36:44 | INFO | fairseq.data.audio.speech_to_text_dataset | Cannot find <path to covost root>/config.yaml\r\n  Traceback (most recent call last):\r\n    File \"<path to venv>/venv/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n    File \"<path to fairseq>/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\n      distributed_utils.call_main(cfg, main)\r\n    File \"<path to fairseq>/fairseq/fairseq/distributed_utils.py\", line 313, in call_main\r\n    torch.multiprocessing.spawn(\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\r\n      return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\r\n    while not context.join():\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n      raise Exception(msg)\r\n  Exception:\r\n\r\n  -- Process 3 terminated with the following error:\r\n  Traceback (most recent call last):\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n    File \"<path to fairseq>/fairseq/fairseq/distributed_utils.py\", line 300, in distributed_main\r\n      main(cfg, **kwargs)\r\n    File \"<path to fairseq>/fairseq/fairseq_cli/train.py\", line 66, in main\r\n    task = tasks.setup_task(cfg.task)\r\n    File \"<path to fairseq>/fairseq/fairseq/tasks/__init__.py\", line 44, in setup_task\r\n      return task.setup_task(cfg, **kwargs)\r\n    File \"<path to fairseq>/fairseq/fairseq/tasks/speech_to_text.py\", line 58, in setup_task\r\n    raise FileNotFoundError(f\"Dict not found: {dict_path}\")\r\nFileNotFoundError: Dict not found: <path to covost root>/dict.txt\r\n```\r\n\r\n</div>\r\n</details>\r\n\r\n<details>\r\n<summary>Error messages for Bug 4</summary>\r\n<div>\r\n\r\n```\r\n...\r\n  warnings.warn(\r\n  epoch 001:   0%|                                                                         | 0/2 [00:00<?, ?it/s]2020-11-30 14:40:54 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboardX\r\n  2020-11-30 14:40:54 | INFO | fairseq.trainer | begin training epoch 1\r\n  Traceback (most recent call last):\r\n    File \"<path to venv>/venv/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n    File \"<path to fairseq>/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\n      distributed_utils.call_main(cfg, main)\r\n    File \"<path to fairseq>/fairseq/fairseq/distributed_utils.py\", line 313, in call_main\r\n    torch.multiprocessing.spawn(\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\r\n      return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\r\n    while not context.join():\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n      raise Exception(msg)\r\n  Exception:\r\n\r\n  -- Process 7 terminated with the following error:\r\n  Traceback (most recent call last):\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n    File \"<path to fairseq>/fairseq/fairseq/distributed_utils.py\", line 300, in distributed_main\r\n      main(cfg, **kwargs)\r\n    File \"<path to fairseq>/fairseq/fairseq_cli/train.py\", line 130, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n    File \"/usr/lib/python3.8/contextlib.py\", line 75, in inner\r\n      return func(*args, **kwds)\r\n    File \"<path to fairseq>/fairseq/fairseq_cli/train.py\", line 219, in train\r\n    log_output = trainer.train_step(samples)\r\n    File \"/usr/lib/python3.8/contextlib.py\", line 75, in inner\r\n      return func(*args, **kwds)\r\n    File \"<path to fairseq>/fairseq/fairseq/trainer.py\", line 540, in train_step\r\n    loss, sample_size_i, logging_output = self.task.train_step(\r\n    File \"<path to fairseq>/fairseq/fairseq/tasks/fairseq_task.py\", line 428, in train_step\r\n      loss, sample_size, logging_output = criterion(model, sample)\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n    File \"<path to fairseq>/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py\", line 69, in forward\r\n      net_output = model(**sample[\"net_input\"])\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 619, in forward\r\n      output = self.module(*inputs[0], **kwargs[0])\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n    File \"<path to fairseq>/fairseq/fairseq/models/speech_to_text/s2t_transformer.py\", line 259, in forward\r\n      decoder_out = self.decoder(\r\n    File \"<path to venv>/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n    File \"<path to fairseq>/fairseq/fairseq/models/transformer.py\", line 693, in forward\r\n      x, extra = self.extract_features(\r\n    File \"<path to fairseq>/fairseq/fairseq/models/speech_to_text/s2t_transformer.py\", line 381, in extract_features\r\n    x, _ = self.extract_features_scriptable(\r\n    File \"<path to fairseq>/fairseq/fairseq/models/transformer.py\", line 810, in extract_features_scriptable\r\n      if (encoder_out is not None and len(encoder_out[\"encoder_out\"]) > 0)\r\n  TypeError: tuple indices must be integers or slices, not str\r\n\r\n  /usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown\r\n    warnings.warn('resource_tracker: There appear to be %d '\r\n```\r\n\r\n</div>\r\n</details>", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2971/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2971/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2961", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2961/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2961/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2961/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2961", "id": 752341808, "node_id": "MDU6SXNzdWU3NTIzNDE4MDg=", "number": 2961, "title": "Generation args not loaded in GeneratorHubInterface since hydra migration", "user": {"login": "louismartin", "id": 12654189, "node_id": "MDQ6VXNlcjEyNjU0MTg5", "avatar_url": "https://avatars.githubusercontent.com/u/12654189?v=4", "gravatar_id": "", "url": "https://api.github.com/users/louismartin", "html_url": "https://github.com/louismartin", "followers_url": "https://api.github.com/users/louismartin/followers", "following_url": "https://api.github.com/users/louismartin/following{/other_user}", "gists_url": "https://api.github.com/users/louismartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/louismartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/louismartin/subscriptions", "organizations_url": "https://api.github.com/users/louismartin/orgs", "repos_url": "https://api.github.com/users/louismartin/repos", "events_url": "https://api.github.com/users/louismartin/events{/privacy}", "received_events_url": "https://api.github.com/users/louismartin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-11-27T15:54:43Z", "updated_at": "2020-12-02T01:45:40Z", "closed_at": "2020-12-02T01:45:40Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When loading the bart pretrained model using the hub interface, it seems that the generation args are not correctly loaded because of the new nested configuration variable `cfg`.\r\n\r\nCurrently generation args are loaded with `gen_args = copy.copy(self.cfg)`, but generation args are actually stored in `self.cfg.generation`.\r\n\r\nhttps://github.com/pytorch/fairseq/blob/dea66cc294a18dd4d9e59aa0af8d51f951e83884/fairseq/hub_utils.py#L160\r\n\r\n\r\nFor instance when setting the beam size with `gen_args.beam = beam` [here](https://github.com/pytorch/fairseq/blob/dea66cc294a18dd4d9e59aa0af8d51f951e83884/fairseq/hub_utils.py#L162), it will result in `self.cfg` having two beam values, one at `self.cfg.beam` and one at `self.cfg.generation.beam`. I guess all arguments in `self.cfg.generation` won't be used at all.\r\n\r\nIs this intended @alexeib ?", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2961/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2960", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2960/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2960/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2960/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2960", "id": 752324310, "node_id": "MDU6SXNzdWU3NTIzMjQzMTA=", "number": 2960, "title": "omegaconf.errors.ConfigAttributeError: Key 'min_lr' not in 'CosineLRScheduleConfig'", "user": {"login": "olesyaksyon", "id": 16019681, "node_id": "MDQ6VXNlcjE2MDE5Njgx", "avatar_url": "https://avatars.githubusercontent.com/u/16019681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/olesyaksyon", "html_url": "https://github.com/olesyaksyon", "followers_url": "https://api.github.com/users/olesyaksyon/followers", "following_url": "https://api.github.com/users/olesyaksyon/following{/other_user}", "gists_url": "https://api.github.com/users/olesyaksyon/gists{/gist_id}", "starred_url": "https://api.github.com/users/olesyaksyon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/olesyaksyon/subscriptions", "organizations_url": "https://api.github.com/users/olesyaksyon/orgs", "repos_url": "https://api.github.com/users/olesyaksyon/repos", "events_url": "https://api.github.com/users/olesyaksyon/events{/privacy}", "received_events_url": "https://api.github.com/users/olesyaksyon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-11-27T15:22:50Z", "updated_at": "2021-01-05T18:33:30Z", "closed_at": "2021-01-05T18:33:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I've created manifest and I'm trying to train wav2vec model from scratch by following command:\r\n\r\npython train.py path/to/manifest --save-dir models/ --num-workers 1 --fp16 --max-update 400000 \\\r\n--save-interval 1 --no-epoch-checkpoints --arch wav2vec --task audio_pretraining --lr 1e-06 --min-lr 1e-09 \\\r\n--optimizer adam --min-lr 1e-09 --max-lr 1e-05 --lr-scheduler cosine \\\r\n--conv-feature-layers \"[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1), (512, 1, 1)]\" \\\r\n--conv-aggregator-layers \"[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]\" \\\r\n--vq-type gumbel --vq-groups 2 --vq-depth 2 \\\r\n--combine-groups --vq-vars 320 --vq-temp \"(2,0.5,0.999995)\" --prediction-steps 12 --warmup-updates 1000 \\\r\n--warmup-init-lr 1e-07 --criterion wav2vec --num-negatives 10 --max-sample-size 150000 \\\r\n--max-tokens 300000 --cross-sample-negatives 0 --update-freq 1 --seed 2 --skip-invalid-size-inputs-valid-test\r\n\r\nBut after running I get error:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 14, in <module>\r\n    cli_main()\r\n  File \"/home/olesya/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/olesya/fairseq/fairseq/distributed_utils.py\", line 334, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/olesya/fairseq/fairseq_cli/train.py\", line 121, in main\r\n    disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n  File \"/home/olesya/fairseq/fairseq/checkpoint_utils.py\", line 219, in load_checkpoint\r\n    trainer.lr_step(epoch_itr.epoch)\r\n  File \"/home/olesya/fairseq/fairseq/trainer.py\", line 837, in lr_step\r\n    self.lr_scheduler.step(epoch, val_loss)\r\n  File \"/home/olesya/fairseq/fairseq/trainer.py\", line 200, in lr_scheduler\r\n    self._build_optimizer()  # this will initialize self._lr_scheduler\r\n  File \"/home/olesya/fairseq/fairseq/trainer.py\", line 254, in _build_optimizer\r\n    self.optimizer,\r\n  File \"/home/olesya/fairseq/fairseq/optim/lr_scheduler/__init__.py\", line 29, in build_lr_scheduler\r\n    return build_lr_scheduler_(cfg, optimizer)\r\n  File \"/home/olesya/fairseq/fairseq/registry.py\", line 61, in build_x\r\n    return builder(cfg, *extra_args, **extra_kwargs)\r\n  File \"/home/olesya/fairseq/fairseq/optim/lr_scheduler/cosine_lr_scheduler.py\", line 84, in __init__\r\n    self.min_lr = cfg.min_lr if cfg.min_lr > 0.0 else 0.0\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 297, in __getattr__\r\n    self._format_and_raise(key=key, value=None, cause=e)\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/_utils.py\", line 629, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 295, in __getattr__\r\n    return self._get_impl(key=key, default_value=DEFAULT_VALUE_MARKER)\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 353, in _get_impl\r\n    node = self._get_node(key=key)\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 375, in _get_node\r\n    self._validate_get(key)\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 128, in _validate_get\r\n    key=key, value=value, cause=ConfigAttributeError(msg)\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/opt/conda/lib/python3.7/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.ConfigAttributeError: Key 'min_lr' not in 'CosineLRScheduleConfig'\r\n\tfull_key: min_lr\r\n\treference_type=Any\r\n\tobject_type=CosineLRScheduleConfig\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2960/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2960/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2959", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2959/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2959/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2959/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2959", "id": 752096568, "node_id": "MDU6SXNzdWU3NTIwOTY1Njg=", "number": 2959, "title": "error when loading wav2vector model", "user": {"login": "wl-junlin", "id": 73527369, "node_id": "MDQ6VXNlcjczNTI3MzY5", "avatar_url": "https://avatars.githubusercontent.com/u/73527369?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wl-junlin", "html_url": "https://github.com/wl-junlin", "followers_url": "https://api.github.com/users/wl-junlin/followers", "following_url": "https://api.github.com/users/wl-junlin/following{/other_user}", "gists_url": "https://api.github.com/users/wl-junlin/gists{/gist_id}", "starred_url": "https://api.github.com/users/wl-junlin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wl-junlin/subscriptions", "organizations_url": "https://api.github.com/users/wl-junlin/orgs", "repos_url": "https://api.github.com/users/wl-junlin/repos", "events_url": "https://api.github.com/users/wl-junlin/events{/privacy}", "received_events_url": "https://api.github.com/users/wl-junlin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-11-27T09:08:40Z", "updated_at": "2020-12-07T06:51:49Z", "closed_at": "2020-12-07T06:51:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\njust run the give example of wav2vector, this error is caused by recent updats\r\n2. See error\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-11-7d1e8947ae92> in <module>\r\n      3 \r\n      4 cp = torch.load('wav2vec_large.pt')\r\n----> 5 model = Wav2VecModel.build_model(cp['args'], task=None)\r\n      6 model.load_state_dict(cp['model'])\r\n      7 model.eval()\r\n\r\n~/fairseq/fairseq/models/wav2vec/wav2vec.py in build_model(cls, cfg, task)\r\n    166         \"\"\"Build a new model instance.\"\"\"\r\n    167 \r\n--> 168         model = Wav2VecModel(cfg)\r\n    169         logger.info(model)\r\n    170         return model\r\n\r\n~/fairseq/fairseq/models/wav2vec/wav2vec.py in __init__(self, cfg)\r\n    176         offset = cfg.offset\r\n    177 \r\n--> 178         if cfg.activation == \"relu\":\r\n    179             activation = nn.ReLU()\r\n    180         elif cfg.activation == \"gelu\":\r\n\r\nAttributeError: 'Namespace' object has no attribute 'activation'\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n---------------------------------------------------------------------------\r\n\r\n\r\n#### Code sample\r\n---------------------------------------------------------------------------\r\n\r\nimport torch\r\nfrom fairseq.models.wav2vec import Wav2VecModel\r\n\r\ncp = torch.load('/path/to/wav2vec.pt')\r\nmodel = Wav2VecModel.build_model(cp['args'], task=None)\r\nmodel.load_state_dict(cp['model'])\r\nmodel.eval()\r\n\r\nwav_input_16khz = torch.randn(1,10000)\r\nz = model.feature_extractor(wav_input_16khz)\r\nc = model.feature_aggregator(z)\r\n\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n---------------------------------------------------------------------------\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (0.1.0):\r\n - PyTorch Version (1.7.0)\r\n - OS (ubuntu18.04):\r\n - How you installed fairseq (git clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2959/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2934", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2934/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2934/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2934/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2934", "id": 748361044, "node_id": "MDU6SXNzdWU3NDgzNjEwNDQ=", "number": 2934, "title": "BART model does NOT work properly when trained from scratch", "user": {"login": "mcao516", "id": 24154312, "node_id": "MDQ6VXNlcjI0MTU0MzEy", "avatar_url": "https://avatars.githubusercontent.com/u/24154312?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcao516", "html_url": "https://github.com/mcao516", "followers_url": "https://api.github.com/users/mcao516/followers", "following_url": "https://api.github.com/users/mcao516/following{/other_user}", "gists_url": "https://api.github.com/users/mcao516/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcao516/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcao516/subscriptions", "organizations_url": "https://api.github.com/users/mcao516/orgs", "repos_url": "https://api.github.com/users/mcao516/repos", "events_url": "https://api.github.com/users/mcao516/events{/privacy}", "received_events_url": "https://api.github.com/users/mcao516/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 16, "created_at": "2020-11-22T22:03:23Z", "updated_at": "2022-10-26T15:26:54Z", "closed_at": "2020-12-19T18:28:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI trained a BART model from scratch (without the \"--restore-file $PATH\" argument) for the summarization task. During inference, the decoding seems wrong. Here are some output samples from the model:\r\n\r\n```\r\ns in Wales have been warned to be \"inadequate\" by the Welsh Government over the next five years.\r\ns of a man who died after being hit by a car have been named by police.\r\ning the murder of a man who was found dead at a house in County Antrim has been jailed for life.\r\n Ched Evans has told a court that she would have to be a woman accused of raping a woman in the UK.\r\n Glamorgan has signed a new two-year contract with the Premier League club.\r\n```\r\n\r\nThe beginning of each output sentence seems incomplete. Note that when the model is fine-tuned on the same dataset, everything is fine (with \"--restore-file $PATH\").\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Train a BART model on the summarization dataset (XSum/CNNDM) from scratch.\r\n```\r\nTOTAL_NUM_UPDATES=15000\r\nWARMUP_UPDATES=500      \r\nLR=3e-05\r\nMAX_TOKENS=2048\r\nUPDATE_FREQ=2\r\nSAVE_DIR=checkpoints/\r\n\r\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 fairseq-train $DATA_PATH \\\r\n    --save-dir $SAVE_DIR \\\r\n    --max-tokens $MAX_TOKENS \\\r\n    --task translation \\\r\n    --source-lang source --target-lang target \\\r\n    --truncate-source \\\r\n    --layernorm-embedding \\\r\n    --share-all-embeddings \\\r\n    --share-decoder-input-output-embed \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --required-batch-size-multiple 1 \\\r\n    --arch bart_large \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --label-smoothing 0.1 \\\r\n    --dropout 0.1 --attention-dropout 0.1 \\\r\n    --weight-decay 0.01 --optimizer adam --adam-betas \"(0.9, 0.999)\" --adam-eps 1e-08 \\\r\n    --clip-norm 0.1 \\\r\n    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\r\n    --fp16 --update-freq $UPDATE_FREQ \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --find-unused-parameters;\r\n```\r\n2. Inference with the official code.\r\n```\r\nbart = BARTModel.from_pretrained(\r\n    args.checkpoint_path,\r\n    checkpoint_file=\"checkpoint_best.pt\",\r\n    data_name_or_path=args.data_path\r\n)\r\n\r\nbart.cuda()\r\nbart.eval()\r\nbart.half()\r\ncount = 1\r\nbsz = 32\r\nwith open('test.source') as source, open('test.hypo', 'w') as fout:\r\n    sline = source.readline().strip()\r\n    slines = [sline]\r\n    for sline in source:\r\n        if count % bsz == 0:\r\n            with torch.no_grad():\r\n                hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\r\n\r\n            for hypothesis in hypotheses_batch:\r\n                fout.write(hypothesis + '\\n')\r\n                fout.flush()\r\n            slines = []\r\n\r\n        slines.append(sline.strip())\r\n        count += 1\r\n    if slines != []:\r\n        hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\r\n        for hypothesis in hypotheses_batch:\r\n            fout.write(hypothesis + '\\n')\r\n            fout.flush()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nDuring inference, the decoding goes wrong and the decoded sentences are not completed. \r\n\r\n### Environment\r\n\r\n - fairseq Version: 0.9.0\r\n - PyTorch Version: 1.5.0\r\n - OS: Linux\r\n - How you installed fairseq:  `pip`, source\r\n - Build command you used (if compiling from source): pip install --editable ./\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: V100\r\n - Any other relevant information: None\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2934/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2934/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2929", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2929/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2929/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2929/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2929", "id": 747909151, "node_id": "MDU6SXNzdWU3NDc5MDkxNTE=", "number": 2929, "title": "AttributeError: 'Wav2VecCtc' object has no attribute 'remove_pretraining_modules'", "user": {"login": "xwuShirley", "id": 37637998, "node_id": "MDQ6VXNlcjM3NjM3OTk4", "avatar_url": "https://avatars.githubusercontent.com/u/37637998?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xwuShirley", "html_url": "https://github.com/xwuShirley", "followers_url": "https://api.github.com/users/xwuShirley/followers", "following_url": "https://api.github.com/users/xwuShirley/following{/other_user}", "gists_url": "https://api.github.com/users/xwuShirley/gists{/gist_id}", "starred_url": "https://api.github.com/users/xwuShirley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xwuShirley/subscriptions", "organizations_url": "https://api.github.com/users/xwuShirley/orgs", "repos_url": "https://api.github.com/users/xwuShirley/repos", "events_url": "https://api.github.com/users/xwuShirley/events{/privacy}", "received_events_url": "https://api.github.com/users/xwuShirley/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2020-11-21T03:01:03Z", "updated_at": "2021-04-13T09:10:32Z", "closed_at": "2020-11-22T04:26:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Dear authors of wav2vec2,\r\n\r\nThank you for the great work and for open-source the code and model.\r\n\r\nI have question regarding to the  fine-tuning the wav2v model code with my own dataset.  I followed exactly what it said:\r\n$ fairseq-hydra-train \\\r\n    distributed_training.distributed_port=$PORT \\\r\n    task.data=/path/to/data \\\r\n    model.w2v_path=/path/to/model.pt \\\r\n    --config-path /path/to/fairseq-py/examples/wav2vec/config/finetuning \\\r\n    --config-name base_100h\r\n\r\nI have successfully  run the code with  path/to/model.pt  to be **Wav2Vec 2.0 Base | No finetuning** (the first model in the table) and **Wav2Vec 2.0 Large (LV-60) | No finetuning** (the 9th-row model in the table) \r\n\r\nHowever, I could not run it with any other models.  it returns the following error.  Looks like those already finetuned models have no \"remove_pretraining_modules\". I am not sure how to fix it.  It would be great if you have any hints. \r\n\r\nThank you very much for your help.\r\nBest,\r\nShirley \r\n\r\n```\r\n\r\n[2020-11-20 18:18:32,657][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 2748, skipped 0 samples\r\nTraceback (most recent call last):\r\n  File \u201c/home/fairseq/fairseq_cli/hydra_train.py\", line 35, in hydra_main\r\n    distributed_utils.call_main(cfg, pre_main)\r\n  File \u201c/home/fairseq/fairseq/distributed_utils.py\", line 334, in call_main\r\n    main(cfg, **kwargs)\r\n  File \u201c/home/fairseq/fairseq_cli/train.py\", line 74, in main\r\n    model = task.build_model(cfg.model)\r\n  File \u201c/home/fairseq/fairseq/tasks/audio_pretraining.py\", line 200, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \u201c/home/fairseq/fairseq/tasks/fairseq_task.py\", line 282, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \u201c/home/fairseq/fairseq/models/__init__.py\", line 86, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \u201c/home/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 147, in build_model\r\n    w2v_encoder = Wav2VecEncoder(cfg, task.target_dictionary)\r\n  File \u201c/home/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 304, in __init__\r\n    model.remove_pretraining_modules()\r\n  File \"/root/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 594, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: 'Wav2VecCtc' object has no attribute 'remove_pretraining_modules'\r\n\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2929/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2924", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2924/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2924/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2924/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2924", "id": 747006108, "node_id": "MDU6SXNzdWU3NDcwMDYxMDg=", "number": 2924, "title": "Distributed training error [invalid usage] when using torch.distributed.launch for fairseq 0.10.0 and master branch", "user": {"login": "yuyan2do", "id": 5111542, "node_id": "MDQ6VXNlcjUxMTE1NDI=", "avatar_url": "https://avatars.githubusercontent.com/u/5111542?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuyan2do", "html_url": "https://github.com/yuyan2do", "followers_url": "https://api.github.com/users/yuyan2do/followers", "following_url": "https://api.github.com/users/yuyan2do/following{/other_user}", "gists_url": "https://api.github.com/users/yuyan2do/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuyan2do/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuyan2do/subscriptions", "organizations_url": "https://api.github.com/users/yuyan2do/orgs", "repos_url": "https://api.github.com/users/yuyan2do/repos", "events_url": "https://api.github.com/users/yuyan2do/events{/privacy}", "received_events_url": "https://api.github.com/users/yuyan2do/received_events", "type": "User", "site_admin": true}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-11-19T23:30:54Z", "updated_at": "2020-11-21T12:19:20Z", "closed_at": "2020-11-21T00:21:18Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nRun torch.distributed.launch under 1 machine with 4 gpus. I tried to run it in several machines, but found this error can reproduce even in single machine. \r\n```\r\npython -m torch.distributed.launch --nproc_per_node=${NPROC_PER_NODE} \\\r\n    --nnodes=${NNODE} --node_rank=${NODE_RANK} --master_addr=${MASTER_ADDR} \\\r\n    --master_port=${MASTER_PORT} \\\r\n/opt/conda/bin/fairseq-train ... --distributed-no-spawn --ddp-backend no_c10d\r\n```\r\n\r\n### Environment\r\nfairseq 0.9.0: works well\r\nfairseq 1.0.0: **fairseq-train: error: unrecognized arguments: --local_rank=0**\r\nfairseq-master: **RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:514, invalid usage, NCCL version 2.6.3**\r\n\r\n#### --- more error message log when run in master branch ---\r\n\r\nTraceback (most recent call last):\r\nTraceback (most recent call last):\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/fairseq-train\", line 33, in <module>\r\n  File \"/opt/conda/bin/fairseq-train\", line 33, in <module>\r\n  File \"/opt/conda/bin/fairseq-train\", line 33, in <module>\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"./git/tag/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"./git/tag/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\n  File \"./git/tag/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"./git/tag/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n    distributed_utils.call_main(cfg, main)\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 322, in call_main\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 322, in call_main\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 322, in call_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 322, in call_main\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 294, in distributed_main\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 294, in distributed_main\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 294, in distributed_main\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 294, in distributed_main\r\n    cfg.distributed_training.distributed_rank = distributed_init(cfg)\r\n    cfg.distributed_training.distributed_rank = distributed_init(cfg)\r\n    cfg.distributed_training.distributed_rank = distributed_init(cfg)\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 248, in distributed_init\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 248, in distributed_init\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 248, in distributed_init\r\n    cfg.distributed_training.distributed_rank = distributed_init(cfg)\r\n  File \"./git/tag/fairseq/fairseq/distributed_utils.py\", line 248, in distributed_init\r\n    dist.all_reduce(torch.zeros(1).cuda())\r\n    dist.all_reduce(torch.zeros(1).cuda())\r\n    dist.all_reduce(torch.zeros(1).cuda())\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 898, in all_reduce\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 898, in all_reduce\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 898, in all_reduce\r\n    dist.all_reduce(torch.zeros(1).cuda())\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 898, in all_reduce\r\n    work = _default_pg.allreduce([tensor], opts)\r\n    work = _default_pg.allreduce([tensor], opts)\r\n    work = _default_pg.allreduce([tensor], opts)\r\nRuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:514, invalid usage, NCCL version 2.6.3\r\nRuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:514, invalid usage, NCCL version 2.6.3\r\n    work = _default_pg.allreduce([tensor], opts)\r\nRuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:514, invalid usage, NCCL version 2.6.3\r\nRuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:514, invalid usage, NCCL version 2.6.3\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 263, in <module>\r\n    main()\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py\", line 259, in main\r\n    cmd=cmd)\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2924/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2913", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2913/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2913/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2913/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2913", "id": 745724215, "node_id": "MDU6SXNzdWU3NDU3MjQyMTU=", "number": 2913, "title": "wav2vec2.0 arg_overrides does not apply when loading a model that has been finetuned in unuspervised manner.", "user": {"login": "kwasnydam", "id": 32868095, "node_id": "MDQ6VXNlcjMyODY4MDk1", "avatar_url": "https://avatars.githubusercontent.com/u/32868095?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kwasnydam", "html_url": "https://github.com/kwasnydam", "followers_url": "https://api.github.com/users/kwasnydam/followers", "following_url": "https://api.github.com/users/kwasnydam/following{/other_user}", "gists_url": "https://api.github.com/users/kwasnydam/gists{/gist_id}", "starred_url": "https://api.github.com/users/kwasnydam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kwasnydam/subscriptions", "organizations_url": "https://api.github.com/users/kwasnydam/orgs", "repos_url": "https://api.github.com/users/kwasnydam/repos", "events_url": "https://api.github.com/users/kwasnydam/events{/privacy}", "received_events_url": "https://api.github.com/users/kwasnydam/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-11-18T14:46:55Z", "updated_at": "2020-11-23T13:55:53Z", "closed_at": "2020-11-23T13:55:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIn `fairseq.models.wav2vec2_asr.Wav2VecEncoder` there is a piece of code responsible for overwriting some of the model's parameters from the checkpoint:\r\n```\r\narg_overrides = {\r\n            \"dropout\": args.dropout,\r\n            \"activation_dropout\": args.activation_dropout,\r\n            \"dropout_input\": args.dropout_input,\r\n            \"attention_dropout\": args.attention_dropout,\r\n            \"mask_length\": args.mask_length,\r\n            \"mask_prob\": args.mask_prob,\r\n            \"mask_selection\": args.mask_selection,\r\n            \"mask_other\": args.mask_other,\r\n            \"no_mask_overlap\": args.no_mask_overlap,\r\n            \"mask_channel_length\": args.mask_channel_length,\r\n            \"mask_channel_prob\": args.mask_channel_prob,\r\n            \"mask_channel_selection\": args.mask_channel_selection,\r\n            \"mask_channel_other\": args.mask_channel_other,\r\n            \"no_mask_channel_overlap\": args.no_mask_channel_overlap,\r\n            \"encoder_layerdrop\": args.layerdrop,\r\n            \"feature_grad_mult\": args.feature_grad_mult,\r\n        }\r\n\r\nif getattr(args, \"w2v_args\", None) is None:\r\n            print(\"w2v_args is not None\")\r\n            state = checkpoint_utils.load_checkpoint_to_cpu(\r\n                args.w2v_path, arg_overrides\r\n            )\r\n            for key, value in arg_overrides.items():\r\n                setattr(state[\"cfg\"].model, key, value)\r\n            w2v_args = state.get(\"args\", None) or state[\"cfg\"].model\r\n            print(w2v_args)\r\n        else:\r\n            print(\"w2v_args is None\")\r\n            state = None\r\n            w2v_args = args.w2v_args\r\n```\r\n\r\nwhen the model from [here](https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt) is used for ctc fine-tuning, everything works fine and the parameters are correctly overridden, as it contains the `args` key in the loaded `state`:\r\n```\r\nif \"args\" in state and state[\"args\"] is not None and arg_overrides is not None:\r\n        args = state[\"args\"]\r\n        for arg_name, arg_val in arg_overrides.\r\n            setattr(args, arg_name, arg_val)\r\n```\r\n\r\nhowever, when you finetune the model in unsupervised manner, instead of `args` key, a `cfg` of type DictConf is saved in the checkpoint and this piece of logic is used:\r\n```\r\nif \"cfg\" in state and state[\"cfg\"] is not None and arg_overrides is not None:\r\n        overwrite_args_by_name(state[\"cfg\"], arg_overrides)\r\n```\r\n\r\nthe function `overwrite_args_by_name` looks like this\r\n```\r\ndef overwrite_args_by_name(cfg: DictConfig, overrides: Dict[str, any]):\r\n    # this will be deprecated when we get rid of argparse and model_overrides logic\r\n\r\n    with open_dict(cfg):\r\n        for k in cfg.keys():\r\n            if isinstance(cfg[k], DictConfig):\r\n                overwrite_args_by_name(cfg[k], overrides)\r\n            elif k in overrides:\r\n                cfg[k] = overrides[k]\r\n```\r\n\r\nso the idea here is correct, because we should recursively look for the relevant keys to overwrite. The problem is that the nested `state['cfg']` in the unsupervised finetuning checkpoint is ill-defined. Especially, the `model` field which is the most relevant, is not of type `DictConfig` but of `argparse.Namespace`. So according to the logic above, the fields that are of such type won't get overridden. The effect is that the ctc finetuning code ignores all parameters that the user specified on cli and uses those of the base model that were specified for the purpose of unsupervised training.\r\n\r\n### To Reproduce\r\n1. Finetune the  [wav2vec_small](https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt) in unsupervised manner using the command specified in readme (command slightly modified) to get a checkpoint:\r\n```\r\npython train.py --distributed-world-size 2 \"${data_dir}\" \\\r\n--save-dir \"${save_model}\" --fp16 --num-workers 6 --task audio_pretraining --criterion wav2vec --arch wav2vec2 \\\r\n--log-keys '[\"prob_perplexity\",\"code_perplexity\",\"temp\"]' --quantize-targets --extractor-mode default \\\r\n--conv-feature-layers '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2' --final-dim 256 --latent-vars 320 \\\r\n--latent-groups 2 --latent-temp '(2,0.5,0.999995)' --infonce --optimizer adam \\\r\n--adam-betas '(0.9,0.98)' --adam-eps 1e-06 --lr-scheduler polynomial_decay --total-num-update 100000 \\\r\n--lr 0.0005 --warmup-updates 8000 --mask-length 10 --mask-prob 0.65 --mask-selection static --mask-other 0 \\\r\n--encoder-layerdrop 0.05 --dropout-input 0.1 --dropout-features 0.1 --feature-grad-mult 0.1 \\\r\n--loss-weights '[0.1, 10]' --conv-pos 128 --conv-pos-groups 16 --num-negatives 100 --cross-sample-negatives 0 \\\r\n--max-sample-size 250000 --min-sample-size 32000 --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n--max-tokens 640000 --max-update 100000 --skip-invalid-size-inputs-valid-test --ddp-backend no_c10d --update-freq 32 \\\r\n--finetune-from-model \"${base_model}\"\r\n```\r\n2. ctc finetune the resulting model according to the command in [readme](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#fine-tune-a-pre-trained-model-with-ctc)\r\n\r\n3. print the argument you want to overwrite in wav2vec2_asr.py:\r\n```\r\nclass Wav2VecEncoder(FairseqEncoder):\r\n    def __init__(self, args, tgt_dict=None):\r\n        ...\r\n        print(f\"attention_dropout args: {args.attention_dropout}\")\r\n        print(f\"attention_dropout w2v_args: {w2v_args.attention_dropout}\")\r\n        print(f\"attention_dropout cfg: {state['cfg'].model.attention_dropout}\")\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nexpected:\r\n```\r\nattention_dropout args: 0.0\r\nattention_dropout w2v_args: 0.0\r\nattention_dropout cfg: 0.0\r\n```\r\nactual \r\n```\r\nattention_dropout args: 0.0\r\nattention_dropout w2v_args: 0.1\r\nattention_dropout cfg: 0.1\r\n```\r\n### Temporary solution\r\nset relevant fields in state[\"cfg\"].model manually in wav2vec2_asr.py, line 330 \r\n```\r\n state = checkpoint_utils.load_checkpoint_to_cpu(\r\n                args.w2v_path, arg_overrides\r\n            )\r\n            for key, value in arg_overrides.items():\r\n                setattr(state[\"cfg\"].model, key, value)\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master, commit 1bc83c703ad70d7f62c1e54b197e29b95d07b1f0\r\n - PyTorch Version (e.g., 1.0): 1.7.0\r\n - OS (e.g., Linux): ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): source\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2913/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2913/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2902", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2902/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2902/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2902/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2902", "id": 744519089, "node_id": "MDU6SXNzdWU3NDQ1MTkwODk=", "number": 2902, "title": "Unable to install fairseq-0.10.0 via downloaded/pulled source code", "user": {"login": "DaHaiHuha", "id": 38548370, "node_id": "MDQ6VXNlcjM4NTQ4Mzcw", "avatar_url": "https://avatars.githubusercontent.com/u/38548370?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DaHaiHuha", "html_url": "https://github.com/DaHaiHuha", "followers_url": "https://api.github.com/users/DaHaiHuha/followers", "following_url": "https://api.github.com/users/DaHaiHuha/following{/other_user}", "gists_url": "https://api.github.com/users/DaHaiHuha/gists{/gist_id}", "starred_url": "https://api.github.com/users/DaHaiHuha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DaHaiHuha/subscriptions", "organizations_url": "https://api.github.com/users/DaHaiHuha/orgs", "repos_url": "https://api.github.com/users/DaHaiHuha/repos", "events_url": "https://api.github.com/users/DaHaiHuha/events{/privacy}", "received_events_url": "https://api.github.com/users/DaHaiHuha/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-11-17T08:23:05Z", "updated_at": "2020-11-22T14:21:00Z", "closed_at": "2020-11-22T14:21:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nerror: package directory 'fairseq/model_parallel/megatron/mpu' does not exist\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n```\r\n$ pip install --editable ./\r\nLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\nObtaining file:///[***]/workspace/fairseq-0.10.0\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Installing backend dependencies ... done\r\n    Preparing wheel metadata ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: python [***]/python3.7/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpa_9w5hkn\r\n         cwd: [***]/fairseq-0.10.0\r\n    Complete output (9 lines):\r\n    running dist_info\r\n    creating /tmp/pip-modern-metadata-hrku8ega/fairseq.egg-info\r\n    writing /tmp/pip-modern-metadata-hrku8ega/fairseq.egg-info/PKG-INFO\r\n    writing dependency_links to /tmp/pip-modern-metadata-hrku8ega/fairseq.egg-info/dependency_links.txt\r\n    writing entry points to /tmp/pip-modern-metadata-hrku8ega/fairseq.egg-info/entry_points.txt\r\n    writing requirements to /tmp/pip-modern-metadata-hrku8ega/fairseq.egg-info/requires.txt\r\n    writing top-level names to /tmp/pip-modern-metadata-hrku8ega/fairseq.egg-info/top_level.txt\r\n    writing manifest file '/tmp/pip-modern-metadata-hrku8ega/fairseq.egg-info/SOURCES.txt'\r\n    error: package directory 'fairseq/model_parallel/megatron/mpu' does not exist\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: python [***]/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpa_9w5hkn Check the logs for full command output.\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):0.10.0\r\n - PyTorch Version (e.g., 1.0) 1.7.0\r\n - OS (e.g., Linux): Ubuntu18.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install --editable ./\r\n - Python version: 3.7.9\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2902/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2897", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2897/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2897/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2897/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2897", "id": 743679491, "node_id": "MDU6SXNzdWU3NDM2Nzk0OTE=", "number": 2897, "title": "Hydra error since commit 4ea1c1ee", "user": {"login": "alealv", "id": 9395091, "node_id": "MDQ6VXNlcjkzOTUwOTE=", "avatar_url": "https://avatars.githubusercontent.com/u/9395091?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alealv", "html_url": "https://github.com/alealv", "followers_url": "https://api.github.com/users/alealv/followers", "following_url": "https://api.github.com/users/alealv/following{/other_user}", "gists_url": "https://api.github.com/users/alealv/gists{/gist_id}", "starred_url": "https://api.github.com/users/alealv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alealv/subscriptions", "organizations_url": "https://api.github.com/users/alealv/orgs", "repos_url": "https://api.github.com/users/alealv/repos", "events_url": "https://api.github.com/users/alealv/events{/privacy}", "received_events_url": "https://api.github.com/users/alealv/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2020-11-16T09:37:46Z", "updated_at": "2020-11-23T18:50:38Z", "closed_at": "2020-11-16T19:09:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nCommit 4ea1c1ee introduce this ~~potential~~ bug:\r\n\r\n```sh\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 14, in <module>\r\n    cli_main()\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py\", line 313, in call_main\r\n    torch.multiprocessing.spawn(\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\r\n    while not context.join():\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 3 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 513, in _apply_overrides_to_config\r\n    OmegaConf.update(cfg, key, value, merge=True)\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/omegaconf/omegaconf.py\", line 613, in update\r\n    root.__setattr__(last_key, value)\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/omegaconf/dictconfig.py\", line 278, in __setattr__\r\n    self._format_and_raise(key=key, value=value, cause=e)\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/omegaconf/base.py\", line 95, in _format_and_raise\r\n    format_and_raise(\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.ValidationError: Invalid value assigned : str is not a subclass of ListConfig or list.\r\n\tfull_key: model.latent_temp\r\n\treference_type=Optional[Wav2Vec2Config]\r\n\tobject_type=Wav2Vec2Config\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py\", line 300, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq_cli/train.py\", line 74, in main\r\n    model = task.build_model(cfg.model)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/tasks/audio_pretraining.py\", line 197, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/tasks/fairseq_task.py\", line 272, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/models/__init__.py\", line 86, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 168, in build_model\r\n    w2v_encoder = Wav2VecEncoder(args, task.target_dictionary)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 330, in __init__\r\n    state = checkpoint_utils.load_checkpoint_to_cpu(\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/checkpoint_utils.py\", line 237, in load_checkpoint_to_cpu\r\n    state = _upgrade_state_dict(state)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/checkpoint_utils.py\", line 458, in _upgrade_state_dict\r\n    state[\"cfg\"] = convert_namespace_to_omegaconf(state[\"args\"])\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py\", line 325, in convert_namespace_to_omegaconf\r\n    composed_cfg = compose(\"config\", overrides=overrides, strict=False)\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/experimental/compose.py\", line 31, in compose\r\n    cfg = gh.hydra.compose_config(\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/hydra.py\", line 507, in compose_config\r\n    cfg = self.config_loader.load_configuration(\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 151, in load_configuration\r\n    return self._load_configuration(\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 277, in _load_configuration\r\n    ConfigLoaderImpl._apply_overrides_to_config(config_overrides, cfg)\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\", line 520, in _apply_overrides_to_config\r\n    raise ConfigCompositionException(\r\nhydra.errors.ConfigCompositionException: Error merging override model.latent_temp='(2.0,0.1,0.999995)'\r\n```\r\n\r\n\r\nThen, commit 6815772 changed it to: \r\n```sh\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 14, in <module>\r\n    cli_main()\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py\", line 313, in call_main\r\n    torch.multiprocessing.spawn(\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\r\n    while not context.join():\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec_training/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py\", line 300, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq_cli/train.py\", line 74, in main\r\n    model = task.build_model(cfg.model)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/tasks/audio_pretraining.py\", line 198, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/tasks/fairseq_task.py\", line 272, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/models/__init__.py\", line 85, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 168, in build_model\r\n    w2v_encoder = Wav2VecEncoder(args, task.target_dictionary)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 330, in __init__\r\n    state = checkpoint_utils.load_checkpoint_to_cpu(\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/checkpoint_utils.py\", line 237, in load_checkpoint_to_cpu\r\n    state = _upgrade_state_dict(state)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/checkpoint_utils.py\", line 458, in _upgrade_state_dict\r\n    state[\"cfg\"] = convert_namespace_to_omegaconf(state[\"args\"])\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py\", line 318, in convert_namespace_to_omegaconf\r\n    overrides, deletes = override_module_args(args)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py\", line 306, in override_module_args\r\n    overrides.extend(_override_attr(\"model\", dc, args))\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/dataclass/utils.py\", line 213, in _override_attr\r\n    and not issubclass(field_type, Enum)  # not choices enum\r\nTypeError: issubclass() arg 1 must be a class\r\n```\r\n\r\nBut commit a66cc28b restored the previous\r\n\r\n\r\n### To Reproduce\r\n\r\n```\r\npython -W ignore train.py /mnt/data/ale/manifest_ml2 \\\r\n--save-dir ~/trainings \\\r\n--wer-args '(\"/mnt/data/ale/kenlm-models/openwebtext/v5/4-gram-265M-25-10-2020-pruned-300K-1.bin\",\"/mnt/data/ale/manifest_ml2/dev.lex\",2,-1)' \\\r\n--post-process letter  \\\r\n--valid-subset dev \\\r\n--best-checkpoint-metric wer  \\\r\n--num-workers 10 \\\r\n--max-update 80000  \\\r\n--sentence-avg  \\\r\n--task audio_pretraining  \\\r\n--arch wav2vec_ctc  \\\r\n--w2v-path /mnt/data/ale/models/wav2vec_vox_new.pt \\\r\n--labels ltr  \\\r\n--apply-mask  \\\r\n--mask-selection static  \\\r\n--mask-other 0  \\\r\n--mask-length 10  \\\r\n--mask-prob 0.5  \\\r\n--layerdrop 0.1 \\\r\n--mask-channel-selection static  \\\r\n--mask-channel-other 0  \\\r\n--mask-channel-length 64  \\\r\n--mask-channel-prob 0.5  \\\r\n--zero-infinity \\\r\n--feature-grad-mult 0.0  \\\r\n--freeze-finetune-updates 10000  \\\r\n--validate-after-updates 10000  \\\r\n--optimizer adam \\\r\n--adam-betas '(0.9, 0.98)'  \\\r\n--adam-eps 1e-08  \\\r\n--lr 2e-05  \\\r\n--lr-scheduler tri_stage  \\\r\n--warmup-steps 8000  \\\r\n--hold-steps 32000 \\\r\n--decay-steps 40000  \\\r\n--final-lr-scale 0.05  \\\r\n--final-dropout 0.0  \\\r\n--dropout 0.0  \\\r\n--activation-dropout 0.1  \\\r\n--criterion ctc \\\r\n--attention-dropout 0.0  \\\r\n--seed 2337  \\\r\n--log-format json  \\\r\n--log-interval 500  \\\r\n--ddp-backend no_c10d \\\r\n--normalize \\\r\n--batch-size 10 \\\r\n--distributed-world-size 4 \\\r\n--fp16\r\n```\r\n\r\n### Environment\r\n\r\n- fairseq Version (e.g., 1.0 or master): master\r\n- PyTorch Version (e.g., 1.0): 1.7+cuda11\r\n- OS (e.g., Linux): Ubuntu 20.04\r\n- How you installed fairseq (pip, source): pip\r\n- Build command you used (if compiling from source):\r\n- Python version: v3.8\r\n- CUDA/cuDNN version: CUDA v11.1 cuDNN v8\r\n- GPU models and configuration: RTX 2080 Ti\r\n- Any other relevant information:", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2897/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2897/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2888", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2888/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2888/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2888/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2888", "id": 741785909, "node_id": "MDU6SXNzdWU3NDE3ODU5MDk=", "number": 2888, "title": "Error in generating summaries with BART fine-tuned on CNN/DM", "user": {"login": "manikbhandari", "id": 17298127, "node_id": "MDQ6VXNlcjE3Mjk4MTI3", "avatar_url": "https://avatars.githubusercontent.com/u/17298127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manikbhandari", "html_url": "https://github.com/manikbhandari", "followers_url": "https://api.github.com/users/manikbhandari/followers", "following_url": "https://api.github.com/users/manikbhandari/following{/other_user}", "gists_url": "https://api.github.com/users/manikbhandari/gists{/gist_id}", "starred_url": "https://api.github.com/users/manikbhandari/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manikbhandari/subscriptions", "organizations_url": "https://api.github.com/users/manikbhandari/orgs", "repos_url": "https://api.github.com/users/manikbhandari/repos", "events_url": "https://api.github.com/users/manikbhandari/events{/privacy}", "received_events_url": "https://api.github.com/users/manikbhandari/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-11-12T17:25:49Z", "updated_at": "2020-12-04T08:59:24Z", "closed_at": "2020-11-12T17:34:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI was following this [tutorial](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md) to finetune BART on the CNN/DM dataset. After finetuning, I get an error while generating the summaries (full stack trace below).\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the following python code after downloading the data shared in the above mentioned tutorial.\r\n```python\r\nimport torch\r\nfrom fairseq.models.bart import BARTModel\r\n\r\nbart = BARTModel.from_pretrained(\r\n    'checkpoints/',\r\n    checkpoint_file='checkpoint_best.pt',\r\n    data_name_or_path='cnn_dm-bin'\r\n)\r\n\r\nbart.cuda()\r\nbart.eval()\r\nbart.half()\r\ncount = 1\r\nbsz = 32\r\nwith open('cnn_dm/test.source') as source, open('cnn_dm/test.hypo', 'w') as fout:\r\n    sline = source.readline().strip()\r\n    slines = [sline]\r\n    for sline in source:\r\n        if count % bsz == 0:\r\n            with torch.no_grad():\r\n                hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\r\n\r\n            for hypothesis in hypotheses_batch:\r\n                fout.write(hypothesis + '\\n')\r\n                fout.flush()\r\n            slines = []\r\n\r\n        slines.append(sline.strip())\r\n        count += 1\r\n    if slines != []:\r\n        hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\r\n        for hypothesis in hypotheses_batch:\r\n            fout.write(hypothesis + '\\n')\r\n            fout.flush()\r\n```\r\n2. See error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"generate.py\", line 67, in <module>\r\n    main()\r\n  File \"generate.py\", line 26, in main\r\n    hypotheses_batch = bart.sample(slines,\r\n  File \"/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/hub_utils.py\", line 130, in sample\r\n    batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)\r\n  File \"/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/models/bart/hub_interface.py\", line 105, in generate\r\n    return super().generate(\r\n  File \"/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/hub_utils.py\", line 177, in generate\r\n    translations = self.task.inference_step(\r\n  File \"/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/tasks/fairseq_task.py\", line 434, in inference_step\r\n    return generator.generate(\r\n  File \"/projects/tir4/users/mbhandar2/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/sequence_generator.py\", line 177, in generate\r\n    return self._generate(sample, **kwargs)\r\n  File \"/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/sequence_generator.py\", line 342, in _generate\r\n    lprobs, tokens, scores = self._prefix_tokens(\r\n  File \"/projects/tir4/users/mbhandar2/misc/fairseq/fairseq/sequence_generator.py\", line 548, in _prefix_tokens\r\n    prefix_lprobs = lprobs.gather(-1, prefix_toks.unsqueeze(-1))\r\nRuntimeError: Size does not match at dimension 0 expected index [128, 1] to be smaller than src [20, 50264] apart from dimension 1\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version: master\r\n - PyTorch Version: 1.6.0\r\n - OS: Linux\r\n - How you installed fairseq: source\r\n - Build command you used: pip install --editable ./\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: Tesla V100", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2888/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2888/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2857", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2857/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2857/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2857/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2857", "id": 737179645, "node_id": "MDU6SXNzdWU3MzcxNzk2NDU=", "number": 2857, "title": "Variable `logging_output` is referenced before assignment", "user": {"login": "alealv", "id": 9395091, "node_id": "MDQ6VXNlcjkzOTUwOTE=", "avatar_url": "https://avatars.githubusercontent.com/u/9395091?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alealv", "html_url": "https://github.com/alealv", "followers_url": "https://api.github.com/users/alealv/followers", "following_url": "https://api.github.com/users/alealv/following{/other_user}", "gists_url": "https://api.github.com/users/alealv/gists{/gist_id}", "starred_url": "https://api.github.com/users/alealv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alealv/subscriptions", "organizations_url": "https://api.github.com/users/alealv/orgs", "repos_url": "https://api.github.com/users/alealv/repos", "events_url": "https://api.github.com/users/alealv/events{/privacy}", "received_events_url": "https://api.github.com/users/alealv/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-11-05T18:47:34Z", "updated_at": "2020-11-09T09:56:17Z", "closed_at": "2020-11-05T23:37:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "### \ud83d\udc1b Bug\r\n\r\nAfter an OOM error in my GPUs I got a: _Variable `logging_output` is referenced before assignment_\r\n\r\n```sh\r\n2020-11-05 19:30:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\r\n2020-11-05 19:30:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 12, in <module>\r\n    main()\r\n  File \"train.py\", line 8, in main\r\n    cli_main()\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq_cli/train.py\", line 392, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py\", line 305, in call_main\r\n    torch.multiprocessing.spawn(\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec-training-XQtg4Z6z-py3.8/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec-training-XQtg4Z6z-py3.8/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\r\n    while not context.join():\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec-training-XQtg4Z6z-py3.8/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/aalvarez/.virtualenvs/wav2vec-training-XQtg4Z6z-py3.8/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/distributed_utils.py\", line 292, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq_cli/train.py\", line 130, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/usr/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq_cli/train.py\", line 219, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/usr/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/aalvarez/Projects/fairseq/fairseq/trainer.py\", line 761, in train_step\r\n    return logging_output\r\nUnboundLocalError: local variable 'logging_output' referenced before assignment\r\n\r\n/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 32 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```\r\n\r\n### Environment\r\n\r\n- fairseq Version (e.g., 1.0 or master): master\r\n- PyTorch Version (e.g., 1.0): 1.7\r\n- OS (e.g., Linux): Linux\r\n- How you installed fairseq (pip, source): source\r\n- Build command you used (if compiling from source): clone git repository\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 11.1\r\n- GPU models and configuration: 2 x GeForce RTX 2080 Ti\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2857/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2855", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2855/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2855/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2855/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2855", "id": 737147190, "node_id": "MDU6SXNzdWU3MzcxNDcxOTA=", "number": 2855, "title": "fairseq-score --sacrebleu returns object instead of score", "user": {"login": "lorelupo", "id": 26467076, "node_id": "MDQ6VXNlcjI2NDY3MDc2", "avatar_url": "https://avatars.githubusercontent.com/u/26467076?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lorelupo", "html_url": "https://github.com/lorelupo", "followers_url": "https://api.github.com/users/lorelupo/followers", "following_url": "https://api.github.com/users/lorelupo/following{/other_user}", "gists_url": "https://api.github.com/users/lorelupo/gists{/gist_id}", "starred_url": "https://api.github.com/users/lorelupo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lorelupo/subscriptions", "organizations_url": "https://api.github.com/users/lorelupo/orgs", "repos_url": "https://api.github.com/users/lorelupo/repos", "events_url": "https://api.github.com/users/lorelupo/events{/privacy}", "received_events_url": "https://api.github.com/users/lorelupo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-11-05T17:56:20Z", "updated_at": "2020-11-06T18:31:13Z", "closed_at": "2020-11-06T18:31:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nfairseq-score returns a python object instead of a score when setting the flag `--sacrebleu`.\r\nTo fix this, please modify\r\n\r\nhttps://github.com/pytorch/fairseq/blob/f57b14893837716bdaab4cb9a1430b19d4a6ccf7/fairseq_cli/score.py#L61\r\n\r\nto `print(sacrebleu.corpus_bleu(fdsys, [fdref]).format())`\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd 'fairseq-score --sys gen.sys --ref gen.out --sacrebleu` with whatever `gen.sys` and `gen.out` files.\r\n\r\n#### Code sample\r\n\r\n`fairseq-score --sys $sdir/logs/test.detok.sys --ref $sdir/logs/test.detok.ref --sacrebleu` returns\r\n```\r\nNamespace(ignore_case=False, order=4, ref='checkpoints/wmt14/transfo_base/logs/test.detok.ref', sacrebleu=True, sentence_bleu=False, sys='checkpoints/wmt14/transfo_base/logs/test.detok.sys')\r\n<sacrebleu.metrics.bleu.BLEUScore object at 0x7f0b57584af0>\r\n```\r\n\r\n### Expected behavior\r\n\r\nBLEU score printed.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2855/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2855/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2851", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2851/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2851/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2851/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2851", "id": 736764651, "node_id": "MDU6SXNzdWU3MzY3NjQ2NTE=", "number": 2851, "title": "Getting Attribute error while inference in wav2vec2.0 model.", "user": {"login": "MrityunjoyS", "id": 30874320, "node_id": "MDQ6VXNlcjMwODc0MzIw", "avatar_url": "https://avatars.githubusercontent.com/u/30874320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MrityunjoyS", "html_url": "https://github.com/MrityunjoyS", "followers_url": "https://api.github.com/users/MrityunjoyS/followers", "following_url": "https://api.github.com/users/MrityunjoyS/following{/other_user}", "gists_url": "https://api.github.com/users/MrityunjoyS/gists{/gist_id}", "starred_url": "https://api.github.com/users/MrityunjoyS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MrityunjoyS/subscriptions", "organizations_url": "https://api.github.com/users/MrityunjoyS/orgs", "repos_url": "https://api.github.com/users/MrityunjoyS/repos", "events_url": "https://api.github.com/users/MrityunjoyS/events{/privacy}", "received_events_url": "https://api.github.com/users/MrityunjoyS/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-11-05T09:28:09Z", "updated_at": "2021-03-01T17:18:37Z", "closed_at": "2021-02-12T20:41:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've finetuned the wav2vece2.0 model using my own dataset. But while I'm trying to infer, I'm getting error.\r\nCode I'm using -->\r\n```\r\npython3 examples/speech_recognition/infer.py /path/audio_file/manifest/save_dir/ --task audio_pretraining --nbest 1 \\\r\n    --path /path/audio_file/manifest/save_dir/checkpoints1/checkpoint_best.pt --gen-subset valid --results-path /path/audio_file/manifest/save_dir/ \\\r\n    --w2l-decoder fairseqlm --lexicon /path/audio_file/lexicon.lst --lm-model /path/audio_file/lm_librispeech_word_transformer.pt \\\r\n    --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --max-tokens 4000000 --labels ltr\r\n```\r\n\r\nError I'm getting -->\r\n\r\n> INFO:__main__:| loading model(s) from /path/audio_file/manifest/save_dir/checkpoints1/checkpoint_best.pt\r\n> Traceback (most recent call last):\r\n>   File \"examples/speech_recognition/infer.py\", line 464, in <module>\r\n>     cli_main()\r\n>   File \"examples/speech_recognition/infer.py\", line 460, in cli_main\r\n>     main(args)\r\n>   File \"examples/speech_recognition/infer.py\", line 285, in main\r\n>     model_state=model_state,\r\n>   File \"examples/speech_recognition/infer.py\", line 209, in load_models_and_criterions\r\n>     model = task.build_model(args)\r\n>   File \"/path/fairseq/fairseq/tasks/fairseq_task.py\", line 548, in build_model\r\n>     model = models.build_model(args, self)\r\n>   File \"/path/fairseq/fairseq/models/__init__.py\", line 56, in build_model\r\n>     return ARCH_MODEL_REGISTRY[cfg.arch].build_model(cfg, task)\r\n> AttributeError: 'NoneType' object has no attribute 'arch'\r\n> \r\n\r\nCan you please suggest like what I'm doing wrong? Thanks in advance", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2851/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2851/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2849", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2849/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2849/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2849/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2849", "id": 736690883, "node_id": "MDU6SXNzdWU3MzY2OTA4ODM=", "number": 2849, "title": "Is this a bug in the implementation of  the \"early stop\" of  Lev-T's decoder ?", "user": {"login": "CheungZeeCn", "id": 2025362, "node_id": "MDQ6VXNlcjIwMjUzNjI=", "avatar_url": "https://avatars.githubusercontent.com/u/2025362?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CheungZeeCn", "html_url": "https://github.com/CheungZeeCn", "followers_url": "https://api.github.com/users/CheungZeeCn/followers", "following_url": "https://api.github.com/users/CheungZeeCn/following{/other_user}", "gists_url": "https://api.github.com/users/CheungZeeCn/gists{/gist_id}", "starred_url": "https://api.github.com/users/CheungZeeCn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CheungZeeCn/subscriptions", "organizations_url": "https://api.github.com/users/CheungZeeCn/orgs", "repos_url": "https://api.github.com/users/CheungZeeCn/repos", "events_url": "https://api.github.com/users/CheungZeeCn/events{/privacy}", "received_events_url": "https://api.github.com/users/CheungZeeCn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-11-05T07:38:18Z", "updated_at": "2020-11-08T15:54:14Z", "closed_at": "2020-11-08T15:54:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nthe paper says: \r\n\r\n![image](https://user-images.githubusercontent.com/2025362/98210196-21984d80-1f7b-11eb-9d66-8adf855a790c.png)\r\n\r\n\r\n\r\nLet's say  self.early_exit[1] == 2 and layers_msk is not None,  the input will go through the first 2 layers of self.layers_msk  without sharing the modules of the fisrt 2 layers self.layers.\r\nrefer to the code: fairseq/models/nat/levenshtein_transformer.py,\r\n```  \r\n        # in extract_features(self, prev_output_tokens, encoder_out=None, early_exit=None, layers=None, **unused)\r\n        # input will go through the first early_exit layer's if layers is not None\r\n        for _, layer in enumerate(layers[: early_exit]):\r\n            x, attn, _ = layer(\r\n                x,\r\n                encoder_out.encoder_out if encoder_out is not None else None,\r\n                encoder_out.encoder_padding_mask if encoder_out is not None else None,\r\n                self_attn_mask=None,\r\n                self_attn_padding_mask=decoder_padding_mask,\r\n            )\r\n            inner_states.append(x)\r\n\r\n        # but here, if self.early_exit[1] == 2 and layers_msk is not None, the input will not go through the first 2 layers of self.layers. \r\n        def forward_mask_ins(self, normalize, encoder_out, prev_output_tokens, **unused):\r\n            features, extra = self.extract_features(\r\n                prev_output_tokens, encoder_out=encoder_out, early_exit=self.early_exit[1], layers=self.layers_msk, **unused\r\n            )\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2849/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2849/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2847", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2847/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2847/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2847/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2847", "id": 736411602, "node_id": "MDU6SXNzdWU3MzY0MTE2MDI=", "number": 2847, "title": "Sentencepiece is broken by migration to hydra", "user": {"login": "erip", "id": 2348806, "node_id": "MDQ6VXNlcjIzNDg4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/2348806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erip", "html_url": "https://github.com/erip", "followers_url": "https://api.github.com/users/erip/followers", "following_url": "https://api.github.com/users/erip/following{/other_user}", "gists_url": "https://api.github.com/users/erip/gists{/gist_id}", "starred_url": "https://api.github.com/users/erip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erip/subscriptions", "organizations_url": "https://api.github.com/users/erip/orgs", "repos_url": "https://api.github.com/users/erip/repos", "events_url": "https://api.github.com/users/erip/events{/privacy}", "received_events_url": "https://api.github.com/users/erip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-11-04T20:44:39Z", "updated_at": "2020-11-06T18:31:13Z", "closed_at": "2020-11-06T18:31:13Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. \r\n\r\n```sh\r\nfairseq-train \\\r\n    data-bin \\\r\n    --arch transformer --share-decoder-input-output-embed \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --dropout 0.3 --weight-decay 0.0001 \\\r\n    --decoder-attention-heads 2 \\\r\n    --encoder-attention-heads 2 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens 4000 \\\r\n    --max-epoch 10\r\n```\r\n2. \r\n```sh\r\nmkdir model\r\n\r\ncp checkpoints/checkpoint_best.pt model/\r\ncp sentencepiece* model/\r\ncp data-bin/dict* model/\r\n```\r\n3.\r\n\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nfrom fairseq.models.transformer import TransformerModel\r\n\r\nif __name__ == \"__main__\":\r\n    model = TransformerModel.from_pretrained(\"model\", \"checkpoint_best.pt\", \"model\", bpe=\"sentencepiece\", sentencepiece_model=\"model/sentencepiece.bpe.model\")\r\n    print(model.translate(\"abcd\"))\r\n```\r\n\r\n4. \r\n\r\n```log\r\nTraceback (most recent call last):\r\n  File \"test_inference.py\", line 6, in <module>\r\n    model = TransformerModel.from_pretrained(\"model\", \"checkpoint_best.pt\", \"model\", bpe=\"sentencepiece\", sentencepiece_model=\"model/sentencepiece.bpe.model\")\r\n  File \"/Users/erip/Code/fairseq/fairseq/models/fairseq_model.py\", line 280, in from_pretrained\r\n    return hub_utils.GeneratorHubInterface(x[\"args\"], x[\"task\"], x[\"models\"])\r\n  File \"/Users/erip/Code/fairseq/fairseq/hub_utils.py\", line 106, in __init__\r\n    self.bpe = encoders.build_bpe(cfg.bpe)\r\n  File \"/Users/erip/Code/fairseq/fairseq/registry.py\", line 52, in build_x\r\n    return builder(cfg, *extra_args, **extra_kwargs)\r\n  File \"/Users/erip/Code/fairseq/fairseq/data/encoders/sentencepiece_bpe.py\", line 23, in __init__\r\n    sentencepiece_model = file_utils.cached_path(cfg.sentencepiece_model)\r\n  File \"/Users/erip/Code/fairseq/fairseq/file_utils.py\", line 166, in cached_path\r\n    raise EnvironmentError(\"file {} not found\".format(url_or_filename))\r\nOSError: file ??? not found\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n\r\nReproducer coming soon.\r\n\r\n### Expected behavior\r\n\r\nIt shouldn't crash. \ud83d\ude04 \r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.7.0\r\n - OS (e.g., Linux): all\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install -e .`\r\n - Python version: 3.6.x\r\n - CUDA/cuDNN version: n/a\r\n - GPU models and configuration: n/a\r\n - Any other relevant information: n/a\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2847/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2847/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2841", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2841/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2841/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2841/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2841", "id": 735603379, "node_id": "MDU6SXNzdWU3MzU2MDMzNzk=", "number": 2841, "title": "Problems translating and generating", "user": {"login": "Balidajr14", "id": 34488333, "node_id": "MDQ6VXNlcjM0NDg4MzMz", "avatar_url": "https://avatars.githubusercontent.com/u/34488333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Balidajr14", "html_url": "https://github.com/Balidajr14", "followers_url": "https://api.github.com/users/Balidajr14/followers", "following_url": "https://api.github.com/users/Balidajr14/following{/other_user}", "gists_url": "https://api.github.com/users/Balidajr14/gists{/gist_id}", "starred_url": "https://api.github.com/users/Balidajr14/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Balidajr14/subscriptions", "organizations_url": "https://api.github.com/users/Balidajr14/orgs", "repos_url": "https://api.github.com/users/Balidajr14/repos", "events_url": "https://api.github.com/users/Balidajr14/events{/privacy}", "received_events_url": "https://api.github.com/users/Balidajr14/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-11-03T20:09:24Z", "updated_at": "2020-11-06T18:31:14Z", "closed_at": "2020-11-06T18:31:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI've got an error on the parameters while running both `fairseq-generate`and the translate function after load a pre-trained model.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nWhen I run:\r\n\r\nself.ja2en = TransformerModel.from_pretrained(\r\n          './checkpoints',\r\n          checkpoint_file='checkpoint_best.pt',\r\n          bpe='subword_nmt',\r\n          bpe_codes='checkpoints/codes.32000.bpe.ja'\r\n        )\r\n\r\nja2en.translate(\"Some text\")\r\n\r\nand when I run:\r\n\r\nfairseq-generate $DATADIR --path $TRAIN/checkpoint_best.pt  \\\r\n     --beam 5 --nbest 2 --batch-size 512\r\n\r\nI get this error:\r\n\r\nTraceback (most recent call last):\r\n  File \"trans_ex.py\", line 28, in <module>\r\n    result = ft.translate(texts)\r\n  File \"/root/ja2en/own_dataset/translator.py\", line 33, in translate\r\n    self.__class__.instance = MyFairseqTranslator()\r\n  File \"/root/ja2en/own_dataset/translator.py\", line 45, in __init__\r\n    bpe_codes='checkpoints/codes.32000.bpe.ja'\r\n  File \"/root/fairseq/fairseq/models/fairseq_model.py\", line 280, in from_pretrained\r\n    **kwargs,\r\n  File \"/root/fairseq/fairseq/hub_utils.py\", line 75, in from_pretrained\r\n    arg_overrides=kwargs,\r\n  File \"/root/fairseq/fairseq/checkpoint_utils.py\", line 292, in load_model_ensemble_and_task\r\n    state = load_checkpoint_to_cpu(filename, arg_overrides)\r\n  File \"/root/fairseq/fairseq/checkpoint_utils.py\", line 242, in load_checkpoint_to_cpu\r\n    overwrite_args_by_name(state[\"cfg\"], arg_overrides)\r\n  File \"/root/fairseq/fairseq/dataclass/utils.py\", line 360, in overwrite_args_by_name\r\n    overwrite_args_by_name(cfg[k], overrides)\r\n  File \"/root/fairseq/fairseq/dataclass/utils.py\", line 359, in overwrite_args_by_name\r\n    if isinstance(cfg[k], DictConfig):\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 313, in __getitem__\r\n    self._format_and_raise(key=key, value=None, cause=e)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.MissingMandatoryValue: Missing mandatory value: bpe.bpe_codes\r\n\tfull_key: bpe.bpe_codes\r\n\treference_type=Any\r\n\tobject_type=dict\r\n\r\n### Expected behavior\r\n\r\nThis two commands worked for me before pulling the master on fairseq and I haven't change anything.\r\n\r\nThank you!\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2841/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2841/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2837", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2837/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2837/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2837/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2837", "id": 735264101, "node_id": "MDU6SXNzdWU3MzUyNjQxMDE=", "number": 2837, "title": "validation on \"valid\" subset results in TypeError: forward() missing 1 required positional argument: 'prev_output_tokens'", "user": {"login": "davidepatrucco", "id": 27724701, "node_id": "MDQ6VXNlcjI3NzI0NzAx", "avatar_url": "https://avatars.githubusercontent.com/u/27724701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidepatrucco", "html_url": "https://github.com/davidepatrucco", "followers_url": "https://api.github.com/users/davidepatrucco/followers", "following_url": "https://api.github.com/users/davidepatrucco/following{/other_user}", "gists_url": "https://api.github.com/users/davidepatrucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidepatrucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidepatrucco/subscriptions", "organizations_url": "https://api.github.com/users/davidepatrucco/orgs", "repos_url": "https://api.github.com/users/davidepatrucco/repos", "events_url": "https://api.github.com/users/davidepatrucco/events{/privacy}", "received_events_url": "https://api.github.com/users/davidepatrucco/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-11-03T12:04:18Z", "updated_at": "2022-06-04T04:50:43Z", "closed_at": "2020-11-10T12:41:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen training a model, at validation time (before checkpoint saving) an error is raised:\r\n\r\nTypeError: forward() missing 1 required positional argument: 'prev_output_tokens'\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nfairseq-train      ./data-bin      \\\r\n--source-lang it --target-lang en     \\\r\n--arch transformer_wmt_en_de_big_t2t  \\\r\n--share-all-embeddings     --dropout 0.3 --weight-decay 0.0     --criterion label_smoothed_cross_entropy  \\\r\n--label-smoothing 0.1     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0  --lr 0.001 --lr-scheduler inverse_sqrt \\\r\n--warmup-updates 2000     --max-tokens 1000 --update-freq 1   --save-interval-updates 2\r\n2020-11-03 12:56:19 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='./data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=0, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.001], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=1000, max_tokens_valid=1000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=2, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='it', stop_time_hours=0, target_lang='en', task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=2000, weight_decay=0.0, zero_sharding='none')\r\n2020-11-03 12:56:19 | INFO | fairseq.tasks.translation | [it] dictionary: 45000 types\r\n2020-11-03 12:56:19 | INFO | fairseq.tasks.translation | [en] dictionary: 45000 types\r\n2020-11-03 12:56:19 | INFO | fairseq.data.data_utils | loaded 51975 examples from: ./data-bin/valid.it-en.it\r\n2020-11-03 12:56:19 | INFO | fairseq.tasks.translation | ./data-bin valid it-en 51975 examples\r\n2020-11-03 12:56:21 | INFO | fairseq_cli.train | TransformerModel(\r\n  (encoder): TransformerEncoder(\r\n    (dropout_module): FairseqDropout()\r\n    (embed_tokens): Embedding(45000, 1024, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (4): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (5): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (dropout_module): FairseqDropout()\r\n        (activation_dropout_module): FairseqDropout()\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (decoder): TransformerDecoder(\r\n    (dropout_module): FairseqDropout()\r\n    (embed_tokens): Embedding(45000, 1024, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (4): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (5): TransformerDecoderLayer(\r\n        (dropout_module): FairseqDropout()\r\n        (self_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (activation_dropout_module): FairseqDropout()\r\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (dropout_module): FairseqDropout()\r\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n    (output_projection): Linear(in_features=1024, out_features=45000, bias=False)\r\n  )\r\n)\r\n2020-11-03 12:56:21 | INFO | fairseq_cli.train | task: translation (TranslationTask)\r\n2020-11-03 12:56:21 | INFO | fairseq_cli.train | model: transformer_wmt_en_de_big_t2t (TransformerModel)\r\n2020-11-03 12:56:21 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\r\n2020-11-03 12:56:21 | INFO | fairseq_cli.train | num. model params: 222441472 (num. trained: 222441472)\r\n2020-11-03 12:56:21 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\r\n2020-11-03 12:56:21 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\r\n2020-11-03 12:56:21 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\r\n2020-11-03 12:56:21 | INFO | fairseq_cli.train | max tokens per GPU = 1000 and max sentences per GPU = None\r\n2020-11-03 12:56:21 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\r\n2020-11-03 12:56:21 | INFO | fairseq.trainer | loading train data for epoch 1\r\n2020-11-03 12:56:21 | INFO | fairseq.data.data_utils | loaded 42827970 examples from: ./data-bin/train.it-en.it\r\n2020-11-03 12:56:22 | INFO | fairseq.data.data_utils | loaded 42827970 examples from: ./data-bin/train.it-en.en\r\n2020-11-03 12:56:22 | INFO | fairseq.tasks.translation | ./data-bin train it-en 42827970 examples\r\nepoch 001:   0%|                                                                                                                                                      | 0/865646 [00:00<?, ?it/s]2020-11-03 12:57:14 | INFO | fairseq.trainer | begin training epoch 1\r\nepoch 001:   0%|                                                                                                                                         | 1/865646 [00:06<1458:25:24,  6.07s/it]2020-11-03 12:57:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\nTraceback (most recent call last):                                                                                                                                                               \r\n  File \"/Users/davide/Projects/TAMP/_tamp_/bin/fairseq-train\", line 33, in <module>                                                                                                              \r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/Users/davide/Projects/TAMP/fairseq/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/Users/davide/Projects/TAMP/fairseq/fairseq/distributed_utils.py\", line 268, in call_main\r\n    main(args, **kwargs)\r\n  File \"/Users/davide/Projects/TAMP/fairseq/fairseq_cli/train.py\", line 125, in main\r\n    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/Users/davide/Projects/TAMP/fairseq/fairseq_cli/train.py\", line 223, in train\r\n    args, trainer, task, epoch_itr, valid_subsets, end_of_epoch\r\n  File \"/Users/davide/Projects/TAMP/fairseq/fairseq_cli/train.py\", line 266, in validate_and_save\r\n    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\r\n  File \"/Users/davide/Projects/TAMP/fairseq/fairseq_cli/train.py\", line 323, in validate\r\n    trainer.valid_step(sample)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/Users/davide/Projects/TAMP/fairseq/fairseq/trainer.py\", line 681, in valid_step\r\n    sample, self.model, self.criterion\r\n  File \"/Users/davide/Projects/TAMP/fairseq/fairseq/tasks/translation.py\", line 300, in valid_step\r\n    loss, sample_size, logging_output = super().valid_step(sample, model, criterion)\r\n  File \"/Users/davide/Projects/TAMP/fairseq/fairseq/tasks/fairseq_task.py\", line 425, in valid_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/Users/davide/Projects/TAMP/_tamp_/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/Users/davide/Projects/TAMP/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py\", line 64, in forward\r\n    net_output = model(**sample['net_input'])\r\n  File \"/Users/davide/Projects/TAMP/_tamp_/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\nTypeError: forward() missing 1 required positional argument: 'prev_output_tokens'\r\n\r\n(note: I put a low value for --save-interval-updates to replicate the issue without waiting for a whole epoch to complete)\r\n\r\n#### Code sample\r\n\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master branch\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./\r\n\r\n - Python version: 3.7.9\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2837/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2827", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2827/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2827/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2827/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2827", "id": 733278978, "node_id": "MDU6SXNzdWU3MzMyNzg5Nzg=", "number": 2827, "title": "\"speech_recognition\" Invalid choice error", "user": {"login": "mironnn", "id": 9884126, "node_id": "MDQ6VXNlcjk4ODQxMjY=", "avatar_url": "https://avatars.githubusercontent.com/u/9884126?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mironnn", "html_url": "https://github.com/mironnn", "followers_url": "https://api.github.com/users/mironnn/followers", "following_url": "https://api.github.com/users/mironnn/following{/other_user}", "gists_url": "https://api.github.com/users/mironnn/gists{/gist_id}", "starred_url": "https://api.github.com/users/mironnn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mironnn/subscriptions", "organizations_url": "https://api.github.com/users/mironnn/orgs", "repos_url": "https://api.github.com/users/mironnn/repos", "events_url": "https://api.github.com/users/mironnn/events{/privacy}", "received_events_url": "https://api.github.com/users/mironnn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}, {"id": 2634594675, "node_id": "MDU6TGFiZWwyNjM0NTk0Njc1", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/speech", "name": "speech", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kahne", "id": 3947493, "node_id": "MDQ6VXNlcjM5NDc0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/3947493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kahne", "html_url": "https://github.com/kahne", "followers_url": "https://api.github.com/users/kahne/followers", "following_url": "https://api.github.com/users/kahne/following{/other_user}", "gists_url": "https://api.github.com/users/kahne/gists{/gist_id}", "starred_url": "https://api.github.com/users/kahne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kahne/subscriptions", "organizations_url": "https://api.github.com/users/kahne/orgs", "repos_url": "https://api.github.com/users/kahne/repos", "events_url": "https://api.github.com/users/kahne/events{/privacy}", "received_events_url": "https://api.github.com/users/kahne/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-10-30T15:29:24Z", "updated_at": "2021-01-07T02:50:04Z", "closed_at": "2021-01-07T02:50:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI'm trying to run a speech recognition example from [speech recognition example](https://github.com/pytorch/fairseq/tree/master/examples/speech_recognition) but getting the error `infer.py: error: argument --task: invalid choice: 'speech_recognition'`.\r\n\r\n### To Reproduce\r\n\r\nI'm using Dockerfile to run fairseq with wav2letter python bindings.\r\n\r\nDockerfile content:\r\n\r\n```\r\nFROM wav2letter/wav2letter:cpu-latest\r\n\r\nENV USE_CUDA=0\r\nENV KENLM_ROOT_DIR=/root/kenlm\r\n\r\n# will use Intel MKL for featurization but this may cause dynamic loading conflicts.\r\n# ENV USE_MKL=1\r\n\r\nENV LD_LIBRARY_PATH=/opt/intel/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64:$LD_IBRARY_PATH\r\nWORKDIR /root/wav2letter/bindings/python\r\n\r\nRUN pip install --upgrade pip && pip install soundfile packaging && pip install -e .\r\n\r\nWORKDIR /root\r\nRUN git clone https://github.com/pytorch/fairseq.git\r\nRUN mkdir data\r\n\r\nWORKDIR /root/fairseq\r\nRUN pip install --editable ./ && python examples/speech_recognition/infer.py --help\r\n\r\n```\r\n\r\n- docker build -t wav2vec2 -f Dockerfile_w2l .\r\n- docker run -it wav2vec2 bash\r\n- (inside docker run to get error with invalid input with task 'speech_recognition') python3 examples/speech_recognition/infer.py /tmp --task speech_recognition\r\n\r\noutput:\r\n```\r\nroot@d5b5fe56d655:~/fairseq# python3 examples/speech_recognition/infer.py /tmp --task speech_recognition\r\n/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\r\n  return torch._C._cuda_getDeviceCount() > 0\r\nusage: infer.py [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\r\n                [--log-format {json,none,simple,tqdm}]\r\n                [--tensorboard-logdir TENSORBOARD_LOGDIR] [--seed SEED]\r\n                [--cpu] [--tpu] [--bf16] [--memory-efficient-bf16] [--fp16]\r\n                [--memory-efficient-fp16] [--fp16-no-flatten-grads]\r\n                [--fp16-init-scale FP16_INIT_SCALE]\r\n                [--fp16-scale-window FP16_SCALE_WINDOW]\r\n                [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\r\n                [--min-loss-scale MIN_LOSS_SCALE]\r\n                [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\r\n                [--user-dir USER_DIR] [--empty-cache-freq EMPTY_CACHE_FREQ]\r\n                [--all-gather-list-size ALL_GATHER_LIST_SIZE]\r\n                [--model-parallel-size MODEL_PARALLEL_SIZE]\r\n                [--quantization-config-path QUANTIZATION_CONFIG_PATH]\r\n                [--profile] [--tokenizer {nltk,moses,space}]\r\n                [--bpe {bytes,subword_nmt,gpt2,hf_byte_bpe,characters,bert,byte_bpe,sentencepiece,fastbpe}]\r\n                [--criterion {nat_loss,label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,composite_loss,legacy_masked_lm_loss,cross_entropy,wav2vec,masked_lm,ctc,adaptive_loss,sentence_ranking,sentence_prediction,vocab_parallel_cross_entropy}]\r\n                [--optimizer {adadelta,adafactor,adam,sgd,lamb,nag,adamax,adagrad}]\r\n                [--lr-scheduler {reduce_lr_on_plateau,fixed,inverse_sqrt,tri_stage,cosine,triangular,polynomial_decay}]\r\n                [--scoring {sacrebleu,bleu,chrf,wer}] [--task TASK]\r\n                [--num-workers NUM_WORKERS]\r\n                [--skip-invalid-size-inputs-valid-test]\r\n                [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\r\n                [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\r\n                [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\r\n                [--dataset-impl {raw,lazy,cached,mmap,fasta}]\r\n                [--data-buffer-size DATA_BUFFER_SIZE]\r\n                [--train-subset TRAIN_SUBSET] [--valid-subset VALID_SUBSET]\r\n                [--validate-interval VALIDATE_INTERVAL]\r\n                [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\r\n                [--validate-after-updates VALIDATE_AFTER_UPDATES]\r\n                [--fixed-validation-seed FIXED_VALIDATION_SEED]\r\n                [--disable-validation] [--max-tokens-valid MAX_TOKENS_VALID]\r\n                [--batch-size-valid BATCH_SIZE_VALID]\r\n                [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]\r\n                [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\r\n                [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\r\n                [--distributed-rank DISTRIBUTED_RANK]\r\n                [--distributed-backend DISTRIBUTED_BACKEND]\r\n                [--distributed-init-method DISTRIBUTED_INIT_METHOD]\r\n                [--distributed-port DISTRIBUTED_PORT] [--device-id DEVICE_ID]\r\n                [--local-rank LOCAL_RANK] [--distributed-no-spawn]\r\n                [--ddp-backend {c10d,no_c10d}] [--bucket-cap-mb BUCKET_CAP_MB]\r\n                [--fix-batches-to-gpus] [--find-unused-parameters]\r\n                [--fast-stat-sync] [--broadcast-buffers]\r\n                [--distributed-wrapper {DDP,SlowMo}]\r\n                [--slowmo-momentum SLOWMO_MOMENTUM]\r\n                [--slowmo-algorithm SLOWMO_ALGORITHM]\r\n                [--localsgd-frequency LOCALSGD_FREQUENCY]\r\n                [--nprocs-per-node NPROCS_PER_NODE]\r\n                [--pipeline-model-parallel]\r\n                [--pipeline-balance PIPELINE_BALANCE]\r\n                [--pipeline-devices PIPELINE_DEVICES]\r\n                [--pipeline-chunks PIPELINE_CHUNKS]\r\n                [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\r\n                [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\r\n                [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\r\n                [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\r\n                [--pipeline-checkpoint {always,never,except_last}]\r\n                [--zero-sharding {none,os}] [--path PATH]\r\n                [--post-process [POST_PROCESS]] [--quiet]\r\n                [--model-overrides MODEL_OVERRIDES]\r\n                [--results-path RESULTS_PATH] [--beam BEAM] [--nbest NBEST]\r\n                [--max-len-a MAX_LEN_A] [--max-len-b MAX_LEN_B]\r\n                [--min-len MIN_LEN] [--match-source-len] [--unnormalized]\r\n                [--no-early-stop] [--no-beamable-mm] [--lenpen LENPEN]\r\n                [--unkpen UNKPEN] [--replace-unk [REPLACE_UNK]] [--sacrebleu]\r\n                [--score-reference] [--prefix-size PREFIX_SIZE]\r\n                [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE] [--sampling]\r\n                [--sampling-topk SAMPLING_TOPK]\r\n                [--sampling-topp SAMPLING_TOPP]\r\n                [--constraints [{ordered,unordered}]]\r\n                [--temperature TEMPERATURE]\r\n                [--diverse-beam-groups DIVERSE_BEAM_GROUPS]\r\n                [--diverse-beam-strength DIVERSE_BEAM_STRENGTH]\r\n                [--diversity-rate DIVERSITY_RATE] [--print-alignment]\r\n                [--print-step] [--lm-path LM_PATH] [--lm-weight LM_WEIGHT]\r\n                [--iter-decode-eos-penalty ITER_DECODE_EOS_PENALTY]\r\n                [--iter-decode-max-iter ITER_DECODE_MAX_ITER]\r\n                [--iter-decode-force-max-iter]\r\n                [--iter-decode-with-beam ITER_DECODE_WITH_BEAM]\r\n                [--iter-decode-with-external-reranker] [--retain-iter-history]\r\n                [--retain-dropout]\r\n                [--retain-dropout-modules RETAIN_DROPOUT_MODULES]\r\n                [--decoding-format {unigram,ensemble,vote,dp,bs}]\r\n                [--no-seed-provided] [--save-dir SAVE_DIR]\r\n                [--restore-file RESTORE_FILE]\r\n                [--finetune-from-model FINETUNE_FROM_MODEL]\r\n                [--reset-dataloader] [--reset-lr-scheduler] [--reset-meters]\r\n                [--reset-optimizer]\r\n                [--optimizer-overrides OPTIMIZER_OVERRIDES]\r\n                [--save-interval SAVE_INTERVAL]\r\n                [--save-interval-updates SAVE_INTERVAL_UPDATES]\r\n                [--keep-interval-updates KEEP_INTERVAL_UPDATES]\r\n                [--keep-last-epochs KEEP_LAST_EPOCHS]\r\n                [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS] [--no-save]\r\n                [--no-epoch-checkpoints] [--no-last-checkpoints]\r\n                [--no-save-optimizer-state]\r\n                [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\r\n                [--maximize-best-checkpoint-metric] [--patience PATIENCE]\r\n                [--checkpoint-suffix CHECKPOINT_SUFFIX]\r\n                [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\r\n                [--kspmodel KSPMODEL] [--wfstlm WFSTLM]\r\n                [--rnnt_decoding_type RNNT_DECODING_TYPE]\r\n                [--rnnt_len_penalty RNNT_LEN_PENALTY]\r\n                [--w2l-decoder {viterbi,kenlm,fairseqlm}] [--lexicon LEXICON]\r\n                [--unit-lm] [--kenlm-model KENLM_MODEL]\r\n                [--beam-threshold BEAM_THRESHOLD]\r\n                [--beam-size-token BEAM_SIZE_TOKEN] [--word-score WORD_SCORE]\r\n                [--unk-weight UNK_WEIGHT] [--sil-weight SIL_WEIGHT]\r\n                [--dump-emissions DUMP_EMISSIONS]\r\n                [--dump-features DUMP_FEATURES]\r\n                [--load-emissions LOAD_EMISSIONS]\r\ninfer.py: error: argument --task: invalid choice: 'speech_recognition' (choose from 'cross_lingual_lm', 'translation', 'translation_from_pretrained_bart', 'multilingual_masked_lm', 'denoising', 'multilingual_denoising', 'translation_lev', 'multilingual_translation', 'translation_multi_simple_epoch', 'legacy_masked_lm', 'speech_to_text', 'audio_pretraining', 'semisupervised_translation', 'masked_lm', 'translation_from_pretrained_xlm', 'language_modeling', 'sentence_ranking', 'sentence_prediction', 'dummy_lm', 'dummy_masked_lm', 'dummy_mt')\r\n\r\n```\r\n\r\n### Environment\r\n\r\nDockerfile see above", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2827/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2822", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2822/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2822/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2822/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2822", "id": 732447986, "node_id": "MDU6SXNzdWU3MzI0NDc5ODY=", "number": 2822, "title": "invalid choice: 'speech_recognition'", "user": {"login": "mironnn", "id": 9884126, "node_id": "MDQ6VXNlcjk4ODQxMjY=", "avatar_url": "https://avatars.githubusercontent.com/u/9884126?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mironnn", "html_url": "https://github.com/mironnn", "followers_url": "https://api.github.com/users/mironnn/followers", "following_url": "https://api.github.com/users/mironnn/following{/other_user}", "gists_url": "https://api.github.com/users/mironnn/gists{/gist_id}", "starred_url": "https://api.github.com/users/mironnn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mironnn/subscriptions", "organizations_url": "https://api.github.com/users/mironnn/orgs", "repos_url": "https://api.github.com/users/mironnn/repos", "events_url": "https://api.github.com/users/mironnn/events{/privacy}", "received_events_url": "https://api.github.com/users/mironnn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-10-29T16:04:08Z", "updated_at": "2020-10-30T15:30:42Z", "closed_at": "2020-10-29T16:10:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to run a speech recognition example from [speech recognition example](https://github.com/pytorch/fairseq/tree/master/examples/speech_recognition).\r\n\r\nBut I'm getting an error:\r\ninfer.py: error: argument --task: invalid choice: 'speech_recognition'", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2822/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2822/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2811", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2811/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2811/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2811/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2811", "id": 731535670, "node_id": "MDU6SXNzdWU3MzE1MzU2NzA=", "number": 2811, "title": "Got \"'NoneType' object has no attribute 'zero_'\" when running with zero sharding", "user": {"login": "thpun", "id": 31913095, "node_id": "MDQ6VXNlcjMxOTEzMDk1", "avatar_url": "https://avatars.githubusercontent.com/u/31913095?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thpun", "html_url": "https://github.com/thpun", "followers_url": "https://api.github.com/users/thpun/followers", "following_url": "https://api.github.com/users/thpun/following{/other_user}", "gists_url": "https://api.github.com/users/thpun/gists{/gist_id}", "starred_url": "https://api.github.com/users/thpun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thpun/subscriptions", "organizations_url": "https://api.github.com/users/thpun/orgs", "repos_url": "https://api.github.com/users/thpun/repos", "events_url": "https://api.github.com/users/thpun/events{/privacy}", "received_events_url": "https://api.github.com/users/thpun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-10-28T15:13:41Z", "updated_at": "2020-11-04T04:45:20Z", "closed_at": "2020-11-04T04:45:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nFailed to train with `--zero-sharding os`. Got the captioned error once the training really starts.\r\nI'm doing multilingual fine-tuning in mBART.\r\n\r\n### To Reproduce\r\n\r\n1. Run cmd\r\n```bash\r\nPREFIX=mbart-mlft\r\nlang_pairs=...\r\nDATA=train-data/\r\nlang_list=models/$PREFIX/lang_list\r\nMODEL=models/$PREFIX/model.pt\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-train $DATA \\\r\n  --finetune-from-model $MODEL \\\r\n  --encoder-normalize-before --decoder-normalize-before \\\r\n  --arch mbart_large --layernorm-embedding \\\r\n  --task translation_multi_simple_epoch \\\r\n  --sampling-method \"temperature\" \\\r\n  --sampling-temperature 5 \\\r\n  --encoder-langtok \"src\" --decoder-langtok \\\r\n  --lang-dict \"$lang_list\" --lang-pairs \"$lang_pairs\" \\\r\n  --fp16-no-flatten-grads --zero-sharding os \\\r\n  --criterion label_smoothed_cross_entropy --label-smoothing 0.2 \\\r\n  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \\\r\n  --lr-scheduler inverse_sqrt --lr 6e-05 --min-lr -1 --warmup-updates 2500 \\\r\n  --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \\\r\n  --max-tokens 1536 --update-freq 16 --upsample-primary 3 \\\r\n  --save-interval-updates 5000 --no-epoch-checkpoints \\\r\n  --seed 222 --log-format simple --log-interval 10 --ddp-backend no_c10d \\\r\n  --fp16 --max-update 250000 --save-dir models/$PREFIX\r\n```\r\n2. See error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/workspace/fairseq/fairseq/distributed_utils.py\", line 283, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/workspace/fairseq/fairseq_cli/train.py\", line 124, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/opt/conda/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/workspace/fairseq/fairseq_cli/train.py\", line 202, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/opt/conda/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/workspace/fairseq/fairseq/trainer.py\", line 459, in train_step\r\n    self.zero_grad()\r\n  File \"/workspace/fairseq/fairseq/trainer.py\", line 783, in zero_grad\r\n    self.optimizer.zero_grad()\r\n  File \"/workspace/fairseq/fairseq/optim/fp16_optimizer.py\", line 218, in zero_grad\r\n    p32.grad.zero_()\r\nAttributeError: 'NoneType' object has no attribute 'zero_'\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): Master (commit 1bc83c703ad70d7f62c1e54b197e29b95d07b1f0)\r\n- fairscale Version: 0.0.3\r\n - PyTorch Version (e.g., 1.0) `1.7.0a0+8deb4fe`\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install --editable .`\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: 11.0\r\n - GPU models and configuration: 4x V100 in one machine\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2811/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2811/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2807", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2807/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2807/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2807/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2807", "id": 730517430, "node_id": "MDU6SXNzdWU3MzA1MTc0MzA=", "number": 2807, "title": "Wav2Vec2.0 state of saved model missing in checkpoint after pre-training and code breaks in fine-tuning.", "user": {"login": "amant555", "id": 49357028, "node_id": "MDQ6VXNlcjQ5MzU3MDI4", "avatar_url": "https://avatars.githubusercontent.com/u/49357028?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amant555", "html_url": "https://github.com/amant555", "followers_url": "https://api.github.com/users/amant555/followers", "following_url": "https://api.github.com/users/amant555/following{/other_user}", "gists_url": "https://api.github.com/users/amant555/gists{/gist_id}", "starred_url": "https://api.github.com/users/amant555/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amant555/subscriptions", "organizations_url": "https://api.github.com/users/amant555/orgs", "repos_url": "https://api.github.com/users/amant555/repos", "events_url": "https://api.github.com/users/amant555/events{/privacy}", "received_events_url": "https://api.github.com/users/amant555/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2020-10-27T14:43:56Z", "updated_at": "2021-01-06T02:32:09Z", "closed_at": "2020-10-29T00:18:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI have pre-trained the model and when I used the best checkpoint to fine-tune it. I received the given error.\r\n```\r\n\"/home/aman_tiwari/wav2vec2/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 337, in __init__\r\n    args.normalize == w2v_args.normalize\r\nAttributeError: 'NoneType' object has no attribute 'normalize'\r\n```\r\nAfter further investigating the code in [wav2vec2_asr.py](https://github.com/pytorch/fairseq/blob/master/fairseq/models/wav2vec/wav2vec2_asr.py), I found that the state is being fetched from args in saved checkpoint. Which has none value. And its same for all the checkpoints that are generated. \r\nI have also analysed the checkpoint given in Readme on wav2vec page. The args values in that are properly populated. \r\nAlso, there is an additional key value (cfg) in newly generated checkpoints.\r\n\r\nCode start working again once I change the state that is being fetched from state[\"args\"] to state[\"cfg\"][\"model\"] in  [wav2vec2_asr.py](https://github.com/pytorch/fairseq/blob/master/fairseq/models/wav2vec/wav2vec2_asr.py)\r\n\r\n\r\n### Complete error traceback\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 14, in <module>\r\n    cli_main()\r\n  File \"/home/aman_tiwari/wav2vec2/fairseq/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/aman_tiwari/wav2vec2/fairseq/fairseq/distributed_utils.py\", line 317, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/aman_tiwari/wav2vec2/fairseq/fairseq_cli/train.py\", line 74, in main\r\n    model = task.build_model(cfg.model)\r\n  File \"/home/aman_tiwari/wav2vec2/fairseq/fairseq/tasks/fairseq_task.py\", line 548, in build_model\r\n    model = models.build_model(args, self)\r\n  File \"/home/aman_tiwari/wav2vec2/fairseq/fairseq/models/__init__.py\", line 56, in build_model\r\n    return ARCH_MODEL_REGISTRY[cfg.arch].build_model(cfg, task)\r\n  File \"/home/aman_tiwari/wav2vec2/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 166, in build_model\r\n    w2v_encoder = Wav2VecEncoder(args, task.target_dictionary)\r\n  File \"/home/aman_tiwari/wav2vec2/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 337, in __init__\r\n    args.normalize == w2v_args.normalize\r\nAttributeError: 'NoneType' object has no attribute 'normalize'\r\n```\r\n\r\n### Command used for FINE-TUNING\r\n```\r\npython train.py --distributed-world-size 8 --distributed-port -1 /home/aman_tiwari/wav2vec2/fairseq/prep_scripts --save-dir /home/aman_tiwari/wav2vec2/fairseq/finetune_checkpoints --fp16 \\\r\n\t--wer-args '(\"/home/aman_tiwari/wav2vec2/fairseq/prep_scripts/lm_3.binary\",\"/home/aman_tiwari/wav2vec2/fairseq/prep_scripts/lexicon.lst\",2,-1)' \\\r\n\t--post-process letter --valid-subset valid --no-epoch-checkpoints --best-checkpoint-metric wer --num-workers 12 \\\r\n\t--max-update 80000 --sentence-avg --task audio_pretraining --arch wav2vec_ctc --w2v-path /home/aman_tiwari/wav2vec2/fairseq/pretrainning_checkpoint/checkpoint_best.pt \\\r\n\t--labels ltr --apply-mask --mask-selection static --mask-other 0 --mask-length 10 --mask-prob 0.5 --layerdrop 0.1 \\\r\n\t--mask-channel-selection static --mask-channel-other 0 --mask-channel-length 64 --mask-channel-prob 0.5 --zero-infinity \\\r\n\t--feature-grad-mult 0.0 --freeze-finetune-updates 10000 --validate-after-updates 10000 --optimizer adam \\\r\n\t--adam-betas '(0.9, 0.98)' --adam-eps 1e-08 --lr 2e-05 --lr-scheduler tri_stage --warmup-steps 8000 --hold-steps 32000 \\\r\n\t--decay-steps 40000 --final-lr-scale 0.05 --final-dropout 0.0 --dropout 0.0 --activation-dropout 0.1 --criterion ctc \\\r\n\t--attention-dropout 0.0 --max-tokens 1280000 --seed 2337 --log-format json --log-interval 500 --ddp-backend no_c10d\r\n\r\n```\r\n### Command used for PRE-TRAINING\r\n```\r\npython train.py --distributed-world-size 8 --distributed-port -1 /home/aman_tiwari/manifests_temp/ \\\r\n--save-dir /home/aman_tiwari/checkpoints/ --fp16 --device-id 0 --num-workers 32 --task audio_pretraining --criterion wav2vec --arch wav2vec2 \\\r\n--log-keys '[\"prob_perplexity\",\"code_perplexity\",\"temp\"]' --quantize-targets --extractor-mode default \\\r\n--conv-feature-layers '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2' --final-dim 256 --latent-vars 320 \\\r\n--latent-groups 2 --latent-temp '(2,0.5,0.999995)' --infonce --optimizer adam \\\r\n--adam-betas '(0.9,0.98)' --adam-eps 1e-06 --lr-scheduler polynomial_decay --total-num-update 400000 \\\r\n--lr 0.00005 --warmup-updates 32000 --mask-length 10 --mask-prob 0.65 --mask-selection static --mask-other 0 \\\r\n--encoder-layerdrop 0.05 --dropout-input 0.1 --dropout-features 0.1 --feature-grad-mult 0.1 \\\r\n--loss-weights '[0.1, 10]' --conv-pos 128 --conv-pos-groups 16 --num-negatives 100 --cross-sample-negatives 0 \\\r\n--max-sample-size 250000 --min-sample-size 32000 --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n--max-tokens 1400000 --max-update 400000 --skip-invalid-size-inputs-valid-test --ddp-backend no_c10d  --tensorboard-logdir /home/aman_tiwari/tensorboard\r\n```\r\n### Questions\r\n\r\n1. Is it okay to utilise the state[\"cfg\"][\"model\"] to fetch model state during fine-tuning?\r\n2. If not, What are the steps that I can use to resolve this?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2807/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2800", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2800/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2800/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2800/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2800", "id": 729910841, "node_id": "MDU6SXNzdWU3Mjk5MTA4NDE=", "number": 2800, "title": "Error when loading finetuned BART", "user": {"login": "mukhal", "id": 5109053, "node_id": "MDQ6VXNlcjUxMDkwNTM=", "avatar_url": "https://avatars.githubusercontent.com/u/5109053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mukhal", "html_url": "https://github.com/mukhal", "followers_url": "https://api.github.com/users/mukhal/followers", "following_url": "https://api.github.com/users/mukhal/following{/other_user}", "gists_url": "https://api.github.com/users/mukhal/gists{/gist_id}", "starred_url": "https://api.github.com/users/mukhal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mukhal/subscriptions", "organizations_url": "https://api.github.com/users/mukhal/orgs", "repos_url": "https://api.github.com/users/mukhal/repos", "events_url": "https://api.github.com/users/mukhal/events{/privacy}", "received_events_url": "https://api.github.com/users/mukhal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-10-26T21:10:55Z", "updated_at": "2020-10-27T02:06:23Z", "closed_at": "2020-10-27T02:06:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI finetuned BART on my own data following the scripts [here](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md)\r\n\r\nNow when I try to load the model using \r\n```\r\n>>> bart = BARTModel.from_pretrained('trained_models/my_bart_large', checkpoint_file='checkpoint_last.pt')\r\nloading archive file trained_models/my_bart_large\r\n```\r\nI get the following error \r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/khalfad/fairseq-0.9.0/fairseq/models/bart/model.py\", line 104, in from_pretrained\r\n    **kwargs,\r\n  File \"/home/ubuntu/khalfad/fairseq-0.9.0/fairseq/hub_utils.py\", line 68, in from_pretrained\r\n    arg_overrides=kwargs,\r\n  File \"/home/ubuntu/khalfad/fairseq-0.9.0/fairseq/checkpoint_utils.py\", line 190, in load_model_ensemble_and_task\r\n    state = load_checkpoint_to_cpu(filename, arg_overrides)\r\n  File \"/home/ubuntu/khalfad/fairseq-0.9.0/fairseq/checkpoint_utils.py\", line 165, in load_checkpoint_to_cpu\r\n    setattr(args, arg_name, arg_val)\r\nAttributeError: 'NoneType' object has no attribute 'bpe'\r\n```\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version: 0.9\r\n - PyTorch Version: 1.6\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2800/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2799", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2799/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2799/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2799/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2799", "id": 729647652, "node_id": "MDU6SXNzdWU3Mjk2NDc2NTI=", "number": 2799, "title": "train.py checkpoint args are None", "user": {"login": "turian", "id": 65918, "node_id": "MDQ6VXNlcjY1OTE4", "avatar_url": "https://avatars.githubusercontent.com/u/65918?v=4", "gravatar_id": "", "url": "https://api.github.com/users/turian", "html_url": "https://github.com/turian", "followers_url": "https://api.github.com/users/turian/followers", "following_url": "https://api.github.com/users/turian/following{/other_user}", "gists_url": "https://api.github.com/users/turian/gists{/gist_id}", "starred_url": "https://api.github.com/users/turian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/turian/subscriptions", "organizations_url": "https://api.github.com/users/turian/orgs", "repos_url": "https://api.github.com/users/turian/repos", "events_url": "https://api.github.com/users/turian/events{/privacy}", "received_events_url": "https://api.github.com/users/turian/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-10-26T15:08:16Z", "updated_at": "2020-10-26T23:31:35Z", "closed_at": "2020-10-26T23:31:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI am trying to pretrain and featurize a small dataset of percussive sounds, >= 1024 samples.\r\nFollowing the README to [train a new model with the CLI tools](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#training-a-new-model-with-the-cli-tools-1), `wav2vec_featurize.py` cannot read the checkpoint generated. This is because `checkpoint[\"args\"]` is None.\r\n\r\n### To Reproduce\r\n\r\nYou can try in colab here:\r\nhttps://colab.research.google.com/drive/1AP66z1JXCJlU92aWUkPeA-8oqLJDk6D8?usp=sharing\r\n\r\nIt gets this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"examples/wav2vec/wav2vec_featurize.py\", line 238, in <module>\r\n    use_feat=args.use_feat,\r\n  File \"examples/wav2vec/wav2vec_featurize.py\", line 140, in __init__\r\n    self.model = Prediction(self.model_fname, gpu)\r\n  File \"examples/wav2vec/wav2vec_featurize.py\", line 87, in __init__\r\n    self.model = PretrainedWav2VecModel(fname).cuda(gpu)\r\n  File \"examples/wav2vec/wav2vec_featurize.py\", line 40, in __init__\r\n    model = Wav2VecModel.build_model(self.args, None)\r\n  File \"/content/fairseq/fairseq/models/wav2vec/wav2vec.py\", line 214, in build_model\r\n    base_wav2vec_architecture(args)\r\n  File \"/content/fairseq/fairseq/models/wav2vec/wav2vec.py\", line 690, in base_wav2vec_architecture\r\n    args.conv_feature_layers = getattr(args, \"conv_feature_layers\", conv_feature_layers)\r\nAttributeError: 'NoneType' object has no attribute 'conv_feature_layers'\r\n```\r\n\r\nIf I `torch.load` the checkpoint, it looks like it has a `model` but `args` is `None`.\r\n\r\n#### Code sample\r\n\r\n```\r\n!git clone https://github.com/pytorch/fairseq\r\n# %cd fairseq\r\n\r\n!pip3 install torch soundfile\r\n!pip install -q --editable ./\r\n\r\n# https://zenodo.org/record/3665275\r\n!wget -c https://zenodo.org/record/3665275/files/one_shot_percussive_sounds.zip\r\n!unzip -o one_shot_percussive_sounds.zip\r\n\r\n# Remove files under 1024 samples\r\nimport glob, os, soundfile\r\nfor f in glob.glob(\"one_shot_percussive_sounds/*/*wav\"):\r\n  x, sr = soundfile.read(f)\r\n  if x.shape[0] < 1024:\r\n    os.remove(f)\r\n#    print(\"Removing\", f)\r\n\r\n!mkdir tmpmanifest tmpmodel ; python examples/wav2vec/wav2vec_manifest.py one_shot_percussive_sounds/1/ --dest tmpmanifest --ext wav\r\n\r\n!python train.py tmpmanifest --save-dir tmpmodel --num-workers 6 --fp16 --max-update 10 --save-interval 1 --no-epoch-checkpoints --arch wav2vec --task audio_pretraining --lr 1e-06 --min-lr 1e-09 --optimizer adam --max-lr 0.005 --lr-scheduler cosine --conv-feature-layers '[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]' --conv-aggregator-layers '[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]' --skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion wav2vec --num-negatives 10 --max-sample-size 150000 --max-tokens 1500000 --skip-invalid-size-inputs-valid-test\r\n\r\n!mkdir tmpoutput\r\n!python examples/wav2vec/wav2vec_featurize.py --input one_shot_percussive_sounds/ --output tmpoutput \\\r\n--model tmpmodel/checkpoint_last.pt --split train valid test\r\n```\r\n\r\n### Expected behavior\r\n\r\n`train.py` should generate a checkpoint that contains the arguments (`checkpoint[\"args\"]` shouldn't be `None`). Then, `wav2vec_featurize` can correctly featurize the input corpus.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.6.0+cu101\r\n - OS (e.g., Linux): Ubuntu 18.04.5 LTS\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install -q --editable ./\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: Tesla P100\r\n - Any other relevant information: Google Colab", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2799/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2799/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2792", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2792/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2792/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2792/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2792", "id": 729076363, "node_id": "MDU6SXNzdWU3MjkwNzYzNjM=", "number": 2792, "title": "Wav2Vec 2.0 Large (LV-60) + Self Training *\t960 hours link \"AccessDenied\"", "user": {"login": "khursani8", "id": 5404903, "node_id": "MDQ6VXNlcjU0MDQ5MDM=", "avatar_url": "https://avatars.githubusercontent.com/u/5404903?v=4", "gravatar_id": "", "url": "https://api.github.com/users/khursani8", "html_url": "https://github.com/khursani8", "followers_url": "https://api.github.com/users/khursani8/followers", "following_url": "https://api.github.com/users/khursani8/following{/other_user}", "gists_url": "https://api.github.com/users/khursani8/gists{/gist_id}", "starred_url": "https://api.github.com/users/khursani8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/khursani8/subscriptions", "organizations_url": "https://api.github.com/users/khursani8/orgs", "repos_url": "https://api.github.com/users/khursani8/repos", "events_url": "https://api.github.com/users/khursani8/events{/privacy}", "received_events_url": "https://api.github.com/users/khursani8/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-10-25T18:00:02Z", "updated_at": "2020-10-26T21:28:12Z", "closed_at": "2020-10-26T21:28:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Cannot access this link\r\nhttps://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec2_vox_960h_pl.pt", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2792/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2792/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2783", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2783/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2783/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2783/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2783", "id": 728463612, "node_id": "MDU6SXNzdWU3Mjg0NjM2MTI=", "number": 2783, "title": "fairseq-generate errors, incorrectly saying I didn't specify the sentencepiece_model", "user": {"login": "mawright", "id": 2333858, "node_id": "MDQ6VXNlcjIzMzM4NTg=", "avatar_url": "https://avatars.githubusercontent.com/u/2333858?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mawright", "html_url": "https://github.com/mawright", "followers_url": "https://api.github.com/users/mawright/followers", "following_url": "https://api.github.com/users/mawright/following{/other_user}", "gists_url": "https://api.github.com/users/mawright/gists{/gist_id}", "starred_url": "https://api.github.com/users/mawright/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mawright/subscriptions", "organizations_url": "https://api.github.com/users/mawright/orgs", "repos_url": "https://api.github.com/users/mawright/repos", "events_url": "https://api.github.com/users/mawright/events{/privacy}", "received_events_url": "https://api.github.com/users/mawright/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2020-10-23T19:16:38Z", "updated_at": "2020-11-06T18:31:14Z", "closed_at": "2020-11-06T18:31:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI have a model that uses Sentencepiece but the new argument parsing system seems to not be able to find the value for sentencepiece-model from the command line.\r\n\r\n### To Reproduce\r\n\r\nRunning the command\r\n```\r\nfairseq-generate --user-dir=\"/data/mwright/attn_approx/\" \\\r\n        --task translation -s en -t fr --gen-subset test \\\r\n        --beam=4 --lenpen=0.6 \\\r\n        --bpe=sentencepiece \\\r\n        --sentencepiece-model /work/mwright/test/raw_data/wmt14/en-fr/sentencepiece/sentencepiece.bpe.model \\\r\n        --sacrebleu \\\r\n        --path $save_dir/checkpoint_best.pt \\\r\n        $data_dir &> $save_dir/results.txt\r\n```\r\n\r\ngave me\r\n\r\n```\r\n2020-10-23 12:05:08 | INFO | fairseq_cli.generate | {'common': {'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'memory_efficient_bf16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': '/data/mwright/attn_approx/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False}, 'distributed_training': {'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'local_rank': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'broadcast_buffers': False, 'distributed_wrapper': 'DDP', 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'distributed_world_size': 1, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False}, 'dataset': {'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'fixed_validation_seed': None, 'disable_validation': False, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'max_tokens_valid': None, 'batch_size_valid': None, 'required_seq_len_multiple': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0}, 'optimization': {'max_epoch': 0, 'max_update': 0, 'clip_norm': 25.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'min_lr': -1.0, 'use_bmuf': False, 'stop_time_hours': 0}, 'checkpoint': {'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'finetune_from_model': None, 'checkpoint_shard_count': 1, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'block_lr': 1, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'task': Namespace(_name='translation', all_gather_list_size=16384, batch_size=None, batch_size_valid=None, beam=4, best_checkpoint_metric='loss', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='cross_entropy', curriculum=0, data='/work/mwright/test/data/wmt14/en-fr/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eos=2, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, left_pad_source='True', left_pad_target='False', lenpen=0.6, lm_path=None, lm_weight=0.0, load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer=None, optimizer_overrides='{}', pad=1, path='./checkpoint_best.pt', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process=None, prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=1, sentencepiece_model='/work/mwright/test/raw_data/wmt14/en-fr/sentencepiece/sentencepiece.bpe.model', shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', target_lang='fr', task='translation', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='/data/mwright/attn_approx/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, zero_sharding='none'), 'criterion': {'sentence_avg': False, '_name': 'cross_entropy'}, 'common_eval': {'path': './checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'generation': {'beam': 4, 'nbest': 1, 'max_len_a': 0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 0.6, 'unkpen': 0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': False, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'buffer_size': 0, 'input': '-'}, 'bpe': {'_name': 'sentencepiece', 'sentencepiece_model': '/work/mwright/test/raw_data/wmt14/en-fr/sentencepiece/sentencepiece.bpe.model'}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'tokenizer': None, 'optimizer': None, 'lr_scheduler': Namespace(_name='fixed', all_gather_list_size=16384, batch_size=None, batch_size_valid=None, beam=4, best_checkpoint_metric='loss', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='cross_entropy', curriculum=0, data='/work/mwright/test/data/wmt14/en-fr/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eos=2, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, left_pad_source='True', left_pad_target='False', lenpen=0.6, lm_path=None, lm_weight=0.0, load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer=None, optimizer_overrides='{}', pad=1, path='./checkpoint_best.pt', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process=None, prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=1, sentencepiece_model='/work/mwright/test/raw_data/wmt14/en-fr/sentencepiece/sentencepiece.bpe.model', shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', target_lang='fr', task='translation', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='/data/mwright/attn_approx/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, zero_sharding='none'), 'model': None}\r\n2020-10-23 12:05:09 | INFO | fairseq.tasks.translation | [en] dictionary: 32400 types\r\n2020-10-23 12:05:09 | INFO | fairseq.tasks.translation | [fr] dictionary: 32400 types\r\n2020-10-23 12:05:09 | INFO | fairseq.data.data_utils | loaded 3003 examples from: /work/mwright/test/data/wmt14/en-fr/test.en-fr.en\r\n2020-10-23 12:05:09 | INFO | fairseq.data.data_utils | loaded 3003 examples from: /work/mwright/test/data/wmt14/en-fr/test.en-fr.fr\r\n2020-10-23 12:05:09 | INFO | fairseq.tasks.translation | /work/mwright/test/data/wmt14/en-fr/ test en-fr 3003 examples\r\n2020-10-23 12:05:09 | INFO | fairseq_cli.generate | loading model(s) from ./checkpoint_best.pt\r\nTraceback (most recent call last):\r\n  File \"/data/mwright/anaconda3/envs/cpu/bin/fairseq-generate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())\r\n  File \"/work/mwright/fairseq/fairseq_cli/generate.py\", line 392, in cli_main\r\n    main(args)\r\n  File \"/work/mwright/fairseq/fairseq_cli/generate.py\", line 53, in main\r\n    return _main(cfg, sys.stdout)\r\n  File \"/work/mwright/fairseq/fairseq_cli/generate.py\", line 106, in _main\r\n    num_shards=cfg.checkpoint.checkpoint_shard_count,\r\n  File \"/work/mwright/fairseq/fairseq/checkpoint_utils.py\", line 264, in load_model_ensemble\r\n    num_shards,\r\n  File \"/work/mwright/fairseq/fairseq/checkpoint_utils.py\", line 288, in load_model_ensemble_and_task\r\n    state = load_checkpoint_to_cpu(filename, arg_overrides)\r\n  File \"/work/mwright/fairseq/fairseq/checkpoint_utils.py\", line 238, in load_checkpoint_to_cpu\r\n    overwrite_args_by_name(state[\"cfg\"], arg_overrides)\r\n  File \"/work/mwright/fairseq/fairseq/dataclass/utils.py\", line 355, in overwrite_args_by_name\r\n    overwrite_args_by_name(cfg[k], overrides)\r\n  File \"/work/mwright/fairseq/fairseq/dataclass/utils.py\", line 354, in overwrite_args_by_name\r\n    if isinstance(cfg[k], DictConfig):\r\n  File \"/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/dictconfig.py\", line 313, in __getitem__\r\n    self._format_and_raise(key=key, value=None, cause=e)\r\n  File \"/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/data/mwright/anaconda3/envs/cpu/lib/python3.7/site-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.MissingMandatoryValue: Missing mandatory value: bpe.sentencepiece_model\r\n\tfull_key: bpe.sentencepiece_model\r\n\treference_type=Any\r\n\tobject_type=dict\r\n```\r\n\r\nIn the log above when the arguments are logged I see a dict\r\n`'bpe': {'_name': 'sentencepiece', 'sentencepiece_model': '/work/mwright/test/raw_data/wmt14/en-fr/sentencepiece/sentencepiece.bpe.model'}` but it does not seem to be found according to the error.\r\n\r\n#### Code sample\r\nN/A\r\n\r\n### Expected behavior\r\n\r\nfairseq-generate generates translations.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 2409d5a36e074fe237a734fa2053867fe62b5e01\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - OS (e.g., Linux): Ubuntu Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install -e .\r\n - Python version: Anaconda 3.7.9\r\n - CUDA/cuDNN version: N/A, this is CPU\r\n - GPU models and configuration: None\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2783/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2783/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2776", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2776/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2776/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2776/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2776", "id": 727172677, "node_id": "MDU6SXNzdWU3MjcxNzI2Nzc=", "number": 2776, "title": "I just tried to run exactly the same command on the latest version of fairseq and I don't see the issue. Could be fixed with #2761", "user": {"login": "davidepatrucco", "id": 27724701, "node_id": "MDQ6VXNlcjI3NzI0NzAx", "avatar_url": "https://avatars.githubusercontent.com/u/27724701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidepatrucco", "html_url": "https://github.com/davidepatrucco", "followers_url": "https://api.github.com/users/davidepatrucco/followers", "following_url": "https://api.github.com/users/davidepatrucco/following{/other_user}", "gists_url": "https://api.github.com/users/davidepatrucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidepatrucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidepatrucco/subscriptions", "organizations_url": "https://api.github.com/users/davidepatrucco/orgs", "repos_url": "https://api.github.com/users/davidepatrucco/repos", "events_url": "https://api.github.com/users/davidepatrucco/events{/privacy}", "received_events_url": "https://api.github.com/users/davidepatrucco/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-10-22T08:34:26Z", "updated_at": "2020-10-23T02:05:12Z", "closed_at": "2020-10-23T02:05:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "I just tried to run exactly the same command on the latest version of fairseq and I don't see the issue. Could be fixed with #2761 \r\nIf you still see the issue, can you please get exact version of your fairseq and re-open this ticket: \r\ngit log -n 1\r\n\r\n_Originally posted by @edunov in https://github.com/pytorch/fairseq/issues/2764#issuecomment-713920425_", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2776/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2776/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2769", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2769/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2769/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2769/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2769", "id": 726449727, "node_id": "MDU6SXNzdWU3MjY0NDk3Mjc=", "number": 2769, "title": "game#4", "user": {"login": "BhavyaRangray", "id": 72501879, "node_id": "MDQ6VXNlcjcyNTAxODc5", "avatar_url": "https://avatars.githubusercontent.com/u/72501879?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BhavyaRangray", "html_url": "https://github.com/BhavyaRangray", "followers_url": "https://api.github.com/users/BhavyaRangray/followers", "following_url": "https://api.github.com/users/BhavyaRangray/following{/other_user}", "gists_url": "https://api.github.com/users/BhavyaRangray/gists{/gist_id}", "starred_url": "https://api.github.com/users/BhavyaRangray/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BhavyaRangray/subscriptions", "organizations_url": "https://api.github.com/users/BhavyaRangray/orgs", "repos_url": "https://api.github.com/users/BhavyaRangray/repos", "events_url": "https://api.github.com/users/BhavyaRangray/events{/privacy}", "received_events_url": "https://api.github.com/users/BhavyaRangray/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-10-21T12:49:27Z", "updated_at": "2020-10-21T12:50:20Z", "closed_at": "2020-10-21T12:50:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2769/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2769/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2764", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2764/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2764/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2764/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2764", "id": 726310387, "node_id": "MDU6SXNzdWU3MjYzMTAzODc=", "number": 2764, "title": "Error: \"argparse.ArgumentError: argument -s/--source-lang: conflicting option string: --source-lang\" after a clean install", "user": {"login": "davidepatrucco", "id": 27724701, "node_id": "MDQ6VXNlcjI3NzI0NzAx", "avatar_url": "https://avatars.githubusercontent.com/u/27724701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidepatrucco", "html_url": "https://github.com/davidepatrucco", "followers_url": "https://api.github.com/users/davidepatrucco/followers", "following_url": "https://api.github.com/users/davidepatrucco/following{/other_user}", "gists_url": "https://api.github.com/users/davidepatrucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidepatrucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidepatrucco/subscriptions", "organizations_url": "https://api.github.com/users/davidepatrucco/orgs", "repos_url": "https://api.github.com/users/davidepatrucco/repos", "events_url": "https://api.github.com/users/davidepatrucco/events{/privacy}", "received_events_url": "https://api.github.com/users/davidepatrucco/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-10-21T09:36:09Z", "updated_at": "2021-04-28T19:41:07Z", "closed_at": "2020-10-21T22:43:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nexactly the same as #2761 but with a different error message, this time:\r\n\r\n(tamp_env) davide@davides-MBP fairseq % MODEL_DIR=wmt14.en-fr.fconv-py\r\nfairseq-interactive \\\r\n    --path $MODEL_DIR/model.pt $MODEL_DIR \\\r\n    --beam 5 --source-lang en --target-lang fr \\\r\n    --tokenizer moses \\\r\n    --bpe subword_nmt --bpe-codes $MODEL_DIR/bpecodes\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/davide/Projects/tamp/tamp_env/bin/fairseq-interactive\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-interactive')())\r\n  File \"/Users/davide/Projects/tamp/fairseq/fairseq_cli/interactive.py\", line 320, in cli_main\r\n    args = options.parse_args_and_arch(parser)\r\n  File \"/Users/davide/Projects/tamp/fairseq/fairseq/options.py\", line 159, in parse_args_and_arch\r\n    TASK_REGISTRY[args.task].add_args(parser)\r\n  File \"/Users/davide/Projects/tamp/fairseq/fairseq/tasks/translation.py\", line 195, in add_args\r\n    help='source language')\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1373, in add_argument\r\n    return self._add_action(action)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1736, in _add_action\r\n    self._optionals._add_action(action)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1577, in _add_action\r\n    action = super(_ArgumentGroup, self)._add_action(action)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1387, in _add_action\r\n    self._check_conflict(action)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1526, in _check_conflict\r\n    conflict_handler(action, confl_optionals)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1535, in _handle_conflict_error\r\n    raise ArgumentError(action, message % conflict_string)\r\nargparse.ArgumentError: argument -s/--source-lang: conflicting option string: --source-lang\r\n\r\n\r\n### To Reproduce\r\n\r\nSame as #2761, \r\n\r\n1) done a fresh install as per the instruction in https://fairseq.readthedocs.io/en/latest/getting_started.html\r\n\r\n2) downloaded the wmt14 model\r\ncurl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2 | tar xvjf -\r\n\r\n3)launched fairseq-interactive\r\n\r\nMODEL_DIR=wmt14.en-fr.fconv-py\r\nfairseq-interactive\r\n--path $MODEL_DIR/model.pt $MODEL_DIR\r\n--beam 5 --source-lang en --target-lang fr\r\n--tokenizer moses\r\n--bpe subword_nmt --bpe-codes $MODEL_DIR/bpecodes\r\n\r\nresult:\r\n\r\nargparse.ArgumentError: argument -s/--source-lang: conflicting option string: --source-lang\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0)1.6\r\n - OS (e.g., Linux):max os\r\n - How you installed fairseq (`pip`, source): as per instructions\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.9\r\n - CUDA/cuDNN version: n/a\r\n - GPU models and configuration: n/a\r\n - Any other relevant information: n/a\r\n\r\n### Additional context\r\n\r\nFull error stack:\r\nMODEL_DIR=wmt14.en-fr.fconv-py\r\nfairseq-interactive \\\r\n    --path $MODEL_DIR/model.pt $MODEL_DIR \\\r\n    --beam 5 --source-lang en --target-lang fr \\\r\n    --tokenizer moses \\\r\n    --bpe subword_nmt --bpe-codes $MODEL_DIR/bpecodes\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/davide/Projects/tamp/tamp_env/bin/fairseq-interactive\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-interactive')())\r\n  File \"/Users/davide/Projects/tamp/fairseq/fairseq_cli/interactive.py\", line 320, in cli_main\r\n    args = options.parse_args_and_arch(parser)\r\n  File \"/Users/davide/Projects/tamp/fairseq/fairseq/options.py\", line 159, in parse_args_and_arch\r\n    TASK_REGISTRY[args.task].add_args(parser)\r\n  File \"/Users/davide/Projects/tamp/fairseq/fairseq/tasks/translation.py\", line 195, in add_args\r\n    help='source language')\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1373, in add_argument\r\n    return self._add_action(action)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1736, in _add_action\r\n    self._optionals._add_action(action)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1577, in _add_action\r\n    action = super(_ArgumentGroup, self)._add_action(action)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1387, in _add_action\r\n    self._check_conflict(action)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1526, in _check_conflict\r\n    conflict_handler(action, confl_optionals)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py\", line 1535, in _handle_conflict_error\r\n    raise ArgumentError(action, message % conflict_string)\r\nargparse.ArgumentError: argument -s/--source-lang: conflicting option string: --source-lang", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2764/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2764/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2761", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2761/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2761/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2761/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2761", "id": 725741887, "node_id": "MDU6SXNzdWU3MjU3NDE4ODc=", "number": 2761, "title": "\"unrecognized arguments: --bpe-codes\" after a clean install", "user": {"login": "davidepatrucco", "id": 27724701, "node_id": "MDQ6VXNlcjI3NzI0NzAx", "avatar_url": "https://avatars.githubusercontent.com/u/27724701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidepatrucco", "html_url": "https://github.com/davidepatrucco", "followers_url": "https://api.github.com/users/davidepatrucco/followers", "following_url": "https://api.github.com/users/davidepatrucco/following{/other_user}", "gists_url": "https://api.github.com/users/davidepatrucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidepatrucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidepatrucco/subscriptions", "organizations_url": "https://api.github.com/users/davidepatrucco/orgs", "repos_url": "https://api.github.com/users/davidepatrucco/repos", "events_url": "https://api.github.com/users/davidepatrucco/events{/privacy}", "received_events_url": "https://api.github.com/users/davidepatrucco/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-10-20T16:07:12Z", "updated_at": "2020-10-20T20:44:52Z", "closed_at": "2020-10-20T20:44:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI just followed the instruction in https://fairseq.readthedocs.io/en/latest/getting_started.html\r\n\r\n1) downloaded the wmt14 model\r\ncurl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2 | tar xvjf -\r\n\r\n2) launched fairseq-interactive\r\n\r\nMODEL_DIR=wmt14.en-fr.fconv-py\r\nfairseq-interactive \\\r\n    --path $MODEL_DIR/model.pt $MODEL_DIR \\\r\n    --beam 5 --source-lang en --target-lang fr \\\r\n    --tokenizer moses \\\r\n    --bpe subword_nmt --bpe-codes $MODEL_DIR/bpecodes\r\n\r\nresult: \r\nfairseq-interactive: error: unrecognized arguments: --bpe-codes wmt14.en-fr.fconv-py/bpecodes\r\n\r\n### To Reproduce\r\n\r\n1. As above. Just followed the tutorial, this is the outcome:\r\n\r\nMODEL_DIR=wmt14.en-fr.fconv-py\r\nfairseq-interactive \\\r\n    --path $MODEL_DIR/model.pt $MODEL_DIR \\\r\n    --beam 5 --source-lang en --target-lang fr \\\r\n    --tokenizer moses \\\r\n    --bpe subword_nmt --bpe-codes $MODEL_DIR/bpecodes\r\nusage: fairseq-interactive [-h] [--no-progress-bar]\r\n                           [--log-interval LOG_INTERVAL]\r\n                           [--log-format {json,none,simple,tqdm}]\r\n                           [--tensorboard-logdir TENSORBOARD_LOGDIR]\r\n                           [--seed SEED] [--cpu] [--tpu] [--bf16]\r\n                           [--memory-efficient-bf16] [--fp16]\r\n                           [--memory-efficient-fp16] [--fp16-no-flatten-grads]\r\n                           [--fp16-init-scale FP16_INIT_SCALE]\r\n                           [--fp16-scale-window FP16_SCALE_WINDOW]\r\n                           [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\r\n                           [--min-loss-scale MIN_LOSS_SCALE]\r\n                           [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\r\n                           [--user-dir USER_DIR]\r\n                           [--empty-cache-freq EMPTY_CACHE_FREQ]\r\n                           [--all-gather-list-size ALL_GATHER_LIST_SIZE]\r\n                           [--model-parallel-size MODEL_PARALLEL_SIZE]\r\n                           [--quantization-config-path QUANTIZATION_CONFIG_PATH]\r\n                           [--profile]\r\n                           [--criterion {cross_entropy,ctc,adaptive_loss,wav2vec,legacy_masked_lm_loss,nat_loss,label_smoothed_cross_entropy,composite_loss,sentence_prediction,label_smoothed_cross_entropy_with_alignment,masked_lm,sentence_ranking,vocab_parallel_cross_entropy}]\r\n                           [--tokenizer {nltk,space,moses}]\r\n                           [--bpe {sentencepiece,fastbpe,gpt2,subword_nmt,hf_byte_bpe,bert,byte_bpe,characters,bytes}]\r\n                           [--optimizer {nag,adafactor,sgd,adamax,adagrad,adam,lamb,adadelta}]\r\n                           [--lr-scheduler {fixed,reduce_lr_on_plateau,polynomial_decay,inverse_sqrt,tri_stage,cosine,triangular}]\r\n                           [--scoring {sacrebleu,bleu,wer,chrf}] [--task TASK]\r\n                           [--num-workers NUM_WORKERS]\r\n                           [--skip-invalid-size-inputs-valid-test]\r\n                           [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\r\n                           [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\r\n                           [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\r\n                           [--dataset-impl {raw,lazy,cached,mmap,fasta}]\r\n                           [--data-buffer-size DATA_BUFFER_SIZE]\r\n                           [--train-subset TRAIN_SUBSET]\r\n                           [--valid-subset VALID_SUBSET]\r\n                           [--validate-interval VALIDATE_INTERVAL]\r\n                           [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\r\n                           [--validate-after-updates VALIDATE_AFTER_UPDATES]\r\n                           [--fixed-validation-seed FIXED_VALIDATION_SEED]\r\n                           [--disable-validation]\r\n                           [--max-tokens-valid MAX_TOKENS_VALID]\r\n                           [--batch-size-valid BATCH_SIZE_VALID]\r\n                           [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]\r\n                           [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\r\n                           [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\r\n                           [--distributed-rank DISTRIBUTED_RANK]\r\n                           [--distributed-backend DISTRIBUTED_BACKEND]\r\n                           [--distributed-init-method DISTRIBUTED_INIT_METHOD]\r\n                           [--distributed-port DISTRIBUTED_PORT]\r\n                           [--device-id DEVICE_ID] [--local-rank LOCAL_RANK]\r\n                           [--distributed-no-spawn]\r\n                           [--ddp-backend {c10d,no_c10d}]\r\n                           [--bucket-cap-mb BUCKET_CAP_MB]\r\n                           [--fix-batches-to-gpus] [--find-unused-parameters]\r\n                           [--fast-stat-sync] [--broadcast-buffers]\r\n                           [--distributed-wrapper {DDP,SlowMo}]\r\n                           [--slowmo-momentum SLOWMO_MOMENTUM]\r\n                           [--slowmo-algorithm SLOWMO_ALGORITHM]\r\n                           [--localsgd-frequency LOCALSGD_FREQUENCY]\r\n                           [--nprocs-per-node NPROCS_PER_NODE]\r\n                           [--pipeline-model-parallel]\r\n                           [--pipeline-balance PIPELINE_BALANCE]\r\n                           [--pipeline-devices PIPELINE_DEVICES]\r\n                           [--pipeline-chunks PIPELINE_CHUNKS]\r\n                           [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\r\n                           [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\r\n                           [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\r\n                           [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\r\n                           [--pipeline-checkpoint {always,never,except_last}]\r\n                           [--zero-sharding {none,os}] [--path PATH]\r\n                           [--remove-bpe [REMOVE_BPE]] [--quiet]\r\n                           [--model-overrides MODEL_OVERRIDES]\r\n                           [--results-path RESULTS_PATH] [--beam BEAM]\r\n                           [--nbest NBEST] [--max-len-a MAX_LEN_A]\r\n                           [--max-len-b MAX_LEN_B] [--min-len MIN_LEN]\r\n                           [--match-source-len] [--unnormalized]\r\n                           [--no-early-stop] [--no-beamable-mm]\r\n                           [--lenpen LENPEN] [--unkpen UNKPEN]\r\n                           [--replace-unk [REPLACE_UNK]] [--sacrebleu]\r\n                           [--score-reference] [--prefix-size PREFIX_SIZE]\r\n                           [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]\r\n                           [--sampling] [--sampling-topk SAMPLING_TOPK]\r\n                           [--sampling-topp SAMPLING_TOPP]\r\n                           [--constraints [{ordered,unordered}]]\r\n                           [--temperature TEMPERATURE]\r\n                           [--diverse-beam-groups DIVERSE_BEAM_GROUPS]\r\n                           [--diverse-beam-strength DIVERSE_BEAM_STRENGTH]\r\n                           [--diversity-rate DIVERSITY_RATE]\r\n                           [--print-alignment] [--print-step]\r\n                           [--lm-path LM_PATH] [--lm-weight LM_WEIGHT]\r\n                           [--iter-decode-eos-penalty ITER_DECODE_EOS_PENALTY]\r\n                           [--iter-decode-max-iter ITER_DECODE_MAX_ITER]\r\n                           [--iter-decode-force-max-iter]\r\n                           [--iter-decode-with-beam ITER_DECODE_WITH_BEAM]\r\n                           [--iter-decode-with-external-reranker]\r\n                           [--retain-iter-history] [--retain-dropout]\r\n                           [--retain-dropout-modules RETAIN_DROPOUT_MODULES]\r\n                           [--decoding-format {unigram,ensemble,vote,dp,bs}]\r\n                           [--no-seed-provided] [--save-dir SAVE_DIR]\r\n                           [--restore-file RESTORE_FILE]\r\n                           [--finetune-from-model FINETUNE_FROM_MODEL]\r\n                           [--reset-dataloader] [--reset-lr-scheduler]\r\n                           [--reset-meters] [--reset-optimizer]\r\n                           [--optimizer-overrides OPTIMIZER_OVERRIDES]\r\n                           [--save-interval SAVE_INTERVAL]\r\n                           [--save-interval-updates SAVE_INTERVAL_UPDATES]\r\n                           [--keep-interval-updates KEEP_INTERVAL_UPDATES]\r\n                           [--keep-last-epochs KEEP_LAST_EPOCHS]\r\n                           [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]\r\n                           [--no-save] [--no-epoch-checkpoints]\r\n                           [--no-last-checkpoints] [--no-save-optimizer-state]\r\n                           [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\r\n                           [--maximize-best-checkpoint-metric]\r\n                           [--patience PATIENCE]\r\n                           [--checkpoint-suffix CHECKPOINT_SUFFIX]\r\n                           [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\r\n                           [--buffer-size BUFFER_SIZE] [--input INPUT]\r\n                           [--force-anneal N] [--lr-shrink LS]\r\n                           [--warmup-updates N] [-s SRC] [-t TARGET]\r\n                           [--load-alignments] [--left-pad-source BOOL]\r\n                           [--left-pad-target BOOL] [--max-source-positions N]\r\n                           [--max-target-positions N]\r\n                           [--upsample-primary UPSAMPLE_PRIMARY]\r\n                           [--truncate-source] [--num-batch-buckets N]\r\n                           [--eval-bleu] [--eval-bleu-detok EVAL_BLEU_DETOK]\r\n                           [--eval-bleu-detok-args JSON]\r\n                           [--eval-tokenized-bleu]\r\n                           [--eval-bleu-remove-bpe [EVAL_BLEU_REMOVE_BPE]]\r\n                           [--eval-bleu-args JSON] [--eval-bleu-print-samples]\r\n                           data\r\nfairseq-interactive: error: unrecognized arguments: --bpe-codes wmt14.en-fr.fconv-py/bpecodes\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): obtained by git clone https://github.com/pytorch/fairseq\r\n - PyTorch Version (e.g., 1.0: 1.6.0\r\n - OS (e.g., Linux):Mac OS\r\n - How you installed fairseq (`pip`, source): see below\r\n - Build command you used (if compiling from source):\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\nCFLAGS=\"-stdlib=libc++\" pip install --editable .\r\n - Python version: 3.7.9\r\n - CUDA/cuDNN version: no\r\n - GPU models and configuration: no\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2761/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2761/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2756", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2756/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2756/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2756/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2756", "id": 725041254, "node_id": "MDU6SXNzdWU3MjUwNDEyNTQ=", "number": 2756, "title": "Error while listing available models", "user": {"login": "Worm4047", "id": 13596110, "node_id": "MDQ6VXNlcjEzNTk2MTEw", "avatar_url": "https://avatars.githubusercontent.com/u/13596110?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Worm4047", "html_url": "https://github.com/Worm4047", "followers_url": "https://api.github.com/users/Worm4047/followers", "following_url": "https://api.github.com/users/Worm4047/following{/other_user}", "gists_url": "https://api.github.com/users/Worm4047/gists{/gist_id}", "starred_url": "https://api.github.com/users/Worm4047/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Worm4047/subscriptions", "organizations_url": "https://api.github.com/users/Worm4047/orgs", "repos_url": "https://api.github.com/users/Worm4047/repos", "events_url": "https://api.github.com/users/Worm4047/events{/privacy}", "received_events_url": "https://api.github.com/users/Worm4047/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-10-19T23:09:45Z", "updated_at": "2020-10-20T22:47:02Z", "closed_at": "2020-10-20T22:47:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI'm trying to list available models and getting an error while doing that. This error wasn't there till yesterday but for some reason is showing today after updating the module.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I'm following the guide mentioned here - https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md\r\n2. Same error occurs on the google collab provided here - https://pytorch.org/hub/pytorch_fairseq_translation/\r\n3. See error\r\n\r\n`setuptools.sandbox.SandboxViolation: SandboxViolation: symlink('../config', 'fairseq/config') {}\r\n\r\nThe package setup script has attempted to modify files on your system\r\nthat are not within the EasyInstall build area, and has been aborted.\r\n\r\nThis package cannot be safely installed by EasyInstall, and may not\r\nsupport alternate installation locations even if you run its setup\r\nscript by hand.  Please inform the package's author and the EasyInstall\r\nmaintainers to find out if a fix or workaround is available.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:`\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2756/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2756/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2749", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2749/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2749/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2749/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2749", "id": 724063714, "node_id": "MDU6SXNzdWU3MjQwNjM3MTQ=", "number": 2749, "title": "`SequenceGenerator` is somewhat flaky on all platforms in `TestTranslation::test_multilingual_translation_latent_depth`", "user": {"login": "erip", "id": 2348806, "node_id": "MDQ6VXNlcjIzNDg4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/2348806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erip", "html_url": "https://github.com/erip", "followers_url": "https://api.github.com/users/erip/followers", "following_url": "https://api.github.com/users/erip/following{/other_user}", "gists_url": "https://api.github.com/users/erip/gists{/gist_id}", "starred_url": "https://api.github.com/users/erip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erip/subscriptions", "organizations_url": "https://api.github.com/users/erip/orgs", "repos_url": "https://api.github.com/users/erip/repos", "events_url": "https://api.github.com/users/erip/events{/privacy}", "received_events_url": "https://api.github.com/users/erip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-10-18T18:05:56Z", "updated_at": "2020-10-20T11:30:29Z", "closed_at": "2020-10-20T11:30:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nSometimes [this](https://github.com/pytorch/fairseq/blob/master/fairseq/sequence_generator.py#L416) assertion is triggered in the test TestTranslation::test_multilingual_translation_latent_depth. The reason seems to be related to the fact that [this condition](https://github.com/pytorch/fairseq/blob/master/fairseq/sequence_generator.py#L391) is sometimes not met, meaning that the num remaining sentences is never decremented. \r\n\r\nThis is likely due to some beam search non-determinism.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Clone and install\r\n2. Install pytest\r\n3. run `pytest tests/test_binaries.py::TestTranslation::test_multilingual_translation_latent_depth` several times until observe failure\r\n\r\n### Expected behavior\r\n\r\nThe test should not be flaky. \r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.6.0\r\n - OS (e.g., Linux): OS X, Windows\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): normal install commands\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: n/a\r\n - GPU models and configuration: n/a\r\n - Any other relevant information: n/a", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2749/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2746", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2746/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2746/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2746/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2746", "id": 723777632, "node_id": "MDU6SXNzdWU3MjM3Nzc2MzI=", "number": 2746, "title": "SentencePrediction undefined var causes `TypeError: join() argument must be str or bytes, not 'type'`", "user": {"login": "erip", "id": 2348806, "node_id": "MDQ6VXNlcjIzNDg4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/2348806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erip", "html_url": "https://github.com/erip", "followers_url": "https://api.github.com/users/erip/followers", "following_url": "https://api.github.com/users/erip/following{/other_user}", "gists_url": "https://api.github.com/users/erip/gists{/gist_id}", "starred_url": "https://api.github.com/users/erip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erip/subscriptions", "organizations_url": "https://api.github.com/users/erip/orgs", "repos_url": "https://api.github.com/users/erip/repos", "events_url": "https://api.github.com/users/erip/events{/privacy}", "received_events_url": "https://api.github.com/users/erip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 679100395, "node_id": "MDU6TGFiZWw2NzkxMDAzOTU=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/help%20wanted", "name": "help wanted", "color": "128A0C", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-10-17T15:01:47Z", "updated_at": "2020-10-27T18:26:22Z", "closed_at": "2020-10-27T18:26:22Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n`type` does not refer to a python name [here](https://github.com/pytorch/fairseq/blob/9e4088bc3d2630d5a4285138662fed4426190e73/fairseq/tasks/sentence_prediction.py#L132) and thus causes an error because it falls back to `__builtins__.type`\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. See [here](https://github.com/pytorch/fairseq/runs/1268652670)\r\n\r\n\r\n### Expected behavior\r\n\r\nThis should not happen. \ud83d\ude04 \r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) n/a\r\n - OS (e.g., Linux): all\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): n/a\r\n - Python version: n/a\r\n - CUDA/cuDNN version: n/a\r\n - GPU models and configuration: n/a\r\n - Any other relevant information: n/a\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2746/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2746/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2744", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2744/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2744/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2744/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2744", "id": 723512502, "node_id": "MDU6SXNzdWU3MjM1MTI1MDI=", "number": 2744, "title": "Wav2vec2.0 libri_labels.py not working correctly on windows", "user": {"login": "phantomcoder1996", "id": 19509842, "node_id": "MDQ6VXNlcjE5NTA5ODQy", "avatar_url": "https://avatars.githubusercontent.com/u/19509842?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phantomcoder1996", "html_url": "https://github.com/phantomcoder1996", "followers_url": "https://api.github.com/users/phantomcoder1996/followers", "following_url": "https://api.github.com/users/phantomcoder1996/following{/other_user}", "gists_url": "https://api.github.com/users/phantomcoder1996/gists{/gist_id}", "starred_url": "https://api.github.com/users/phantomcoder1996/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phantomcoder1996/subscriptions", "organizations_url": "https://api.github.com/users/phantomcoder1996/orgs", "repos_url": "https://api.github.com/users/phantomcoder1996/repos", "events_url": "https://api.github.com/users/phantomcoder1996/events{/privacy}", "received_events_url": "https://api.github.com/users/phantomcoder1996/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 679100395, "node_id": "MDU6TGFiZWw2NzkxMDAzOTU=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/help%20wanted", "name": "help wanted", "color": "128A0C", "default": true, "description": null}, {"id": 1697206034, "node_id": "MDU6TGFiZWwxNjk3MjA2MDM0", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/windows", "name": "windows", "color": "0000ff", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-10-16T20:28:36Z", "updated_at": "2020-10-18T17:47:37Z", "closed_at": "2020-10-18T17:47:37Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen running the script fairseq/examples/wav2vec/libri_labels.py  to generate librispeech labels on windows , I get the error \r\n```\r\nTraceback (most recent call last):\r\n  File \"libri_labels.py\", line 56, in <module>\r\n    main()\r\n  File \"libri_labels.py\", line 37, in main\r\n    trans_path = f\"{parts[-2]}-{parts[-1]}.trans.txt\"\r\nIndexError: list index out of range\r\n\r\n```\r\n\r\n### To Reproduce\r\n1- python wav2vec_manifest.py  LibriSpeech\\train --dest LibriSpeech\\train --ext flac\r\n2- python libri_labels.py LibriSpeech\\train\\train.tsv --output-dir LibriSpeech\\train --output-name train\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\ngenerate train.ltr and train.wrd in the output-dir LibriSpeech\\train\r\n\r\n### Environment\r\n\r\n - fairseq Version (master):\r\n - OS (windows):\r\n\r\n\r\n### Additional context\r\n\r\nThe problem is because the script relies on the linux path separator \"/\" \r\nI have created a PR to fix this issue\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2744/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2744/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2725", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2725/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2725/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2725/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2725", "id": 719628568, "node_id": "MDU6SXNzdWU3MTk2Mjg1Njg=", "number": 2725, "title": "Error(s) in loading state_dict for Wav2VecModel:", "user": {"login": "turian", "id": 65918, "node_id": "MDQ6VXNlcjY1OTE4", "avatar_url": "https://avatars.githubusercontent.com/u/65918?v=4", "gravatar_id": "", "url": "https://api.github.com/users/turian", "html_url": "https://github.com/turian", "followers_url": "https://api.github.com/users/turian/followers", "following_url": "https://api.github.com/users/turian/following{/other_user}", "gists_url": "https://api.github.com/users/turian/gists{/gist_id}", "starred_url": "https://api.github.com/users/turian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/turian/subscriptions", "organizations_url": "https://api.github.com/users/turian/orgs", "repos_url": "https://api.github.com/users/turian/repos", "events_url": "https://api.github.com/users/turian/events{/privacy}", "received_events_url": "https://api.github.com/users/turian/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-10-12T20:01:51Z", "updated_at": "2020-12-07T20:35:54Z", "closed_at": "2020-11-30T07:05:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI receive message 'Error(s) in loading state_dict for Wav2VecModel:' with wav2vec_small.pt\r\n\r\nI also tried wav2vec_big_960h.pt and got a similar error.\r\n\r\nI have searched issues but don't find any info.\r\n\r\n### To Reproduce\r\n\r\nI follow the example code [here](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec)\r\n\r\n```\r\nwget -c https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt\r\n```\r\n\r\n```\r\nimport torch\r\nfrom fairseq.models.wav2vec import Wav2VecModel\r\n\r\ncp = torch.load('wav2vec_small.pt')\r\nmodel = Wav2VecModel.build_model(cp['args'], task=None)\r\nmodel.load_state_dict(cp['model'])\r\nmodel.eval()\r\n```\r\n\r\nHere is my stack trace\r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-9-8b7e23d59ca9> in <module>()\r\n      4 cp = torch.load('wav2vec_small.pt')\r\n      5 model = Wav2VecModel.build_model(cp['args'], task=None)\r\n----> 6 model.load_state_dict(cp['model'])\r\n      7 model.eval()\r\n\r\n1 frames\r\n/content/fairseq/fairseq/models/fairseq_model.py in load_state_dict(self, state_dict, strict, args)\r\n     94         self.upgrade_state_dict(state_dict)\r\n     95         new_state_dict = prune_state_dict(state_dict, args)\r\n---> 96         return super().load_state_dict(new_state_dict, strict)\r\n     97 \r\n     98     def upgrade_state_dict(self, state_dict):\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict)\r\n   1043         if len(error_msgs) > 0:\r\n   1044             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n-> 1045                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\n   1046         return _IncompatibleKeys(missing_keys, unexpected_keys)\r\n   1047 \r\n\r\nRuntimeError: Error(s) in loading state_dict for Wav2VecModel:\r\n\tMissing key(s) in state_dict: \"feature_extractor.conv_layers.1.2.weight\", \"feature_extractor.conv_layers.1.2.bias\", \"feature_extractor.conv_layers.2.2.weight\", \"feature_extractor.conv_layers.2.2.bias\", \"feature_extractor.conv_layers.3.2.weight\", \"feature_extractor.conv_layers.3.2.bias\", \"feature_extractor.conv_layers.4.2.weight\", \"feature_extractor.conv_layers.4.2.bias\", \"feature_extractor.conv_layers.5.2.weight\", \"feature_extractor.conv_layers.5.2.bias\", \"feature_extractor.conv_layers.6.2.weight\", \"feature_extractor.conv_layers.6.2.bias\", \"feature_aggregator.conv_layers.0.1.weight\", \"feature_aggregator.conv_layers.0.1.bias\", \"feature_aggregator.conv_layers.0.3.weight\", \"feature_aggregator.conv_layers.0.3.bias\", \"feature_aggregator.conv_layers.1.1.weight\", \"feature_aggregator.conv_layers.1.1.bias\", \"feature_aggregator.conv_layers.1.3.weight\", \"feature_aggregator.conv_layers.1.3.bias\", \"feature_aggregator.conv_layers.2.1.weight\", \"feature_aggregator.conv_layers.2.1.bias\", \"feature_aggregator.conv_layers.2.3.weight\", \"feature_aggregator.conv_layers.2.3.bias\", \"feature_aggregator.conv_layers.3.1.weight\", \"feature_aggregator.conv_layers.3.1.bias\", \"feature_aggregator.conv_layers.3.3.weight\", \"feature_aggregator.conv_layers.3.3.bias\", \"feature_aggregator.conv_layers.4.1.weight\", \"feature_aggregator.conv_layers.4.1.bias\", \"feature_aggregator.conv_layers.4.3.weight\", \"feature_aggregator.conv_layers.4.3.bias\", \"feature_aggregator.conv_layers.5.1.weight\", \"feature_aggregator.conv_...\r\n\tUnexpected key(s) in state_dict: \"encoder.pos_conv.0.bias\", \"encoder.pos_conv.0.weight_g\", \"encoder.pos_conv.0.weight_v\", \"encoder.layers.0.self_attn.k_proj.weight\", \"encoder.layers.0.self_attn.k_proj.bias\", \"encoder.layers.0.self_attn.v_proj.weight\", \"encoder.layers.0.self_attn.v_proj.bias\", \"encoder.layers.0.self_attn.q_proj.weight\", \"encoder.layers.0.self_attn.q_proj.bias\", \"encoder.layers.0.self_attn.out_proj.weight\", \"encoder.layers.0.self_attn.out_proj.bias\", \"encoder.layers.0.self_attn_layer_norm.weight\", \"encoder.layers.0.self_attn_layer_norm.bias\", \"encoder.layers.0.fc1.weight\", \"encoder.layers.0.fc1.bias\", \"encoder.layers.0.fc2.weight\", \"encoder.layers.0.fc2.bias\", \"encoder.layers.0.final_layer_norm.weight\", \"encoder.layers.0.final_layer_norm.bias\", \"encoder.layers.1.self_attn.k_proj.weight\", \"encoder.layers.1.self_attn.k_proj.bias\", \"encoder.layers.1.self_attn.v_proj.weight\", \"encoder.layers.1.self_attn.v_proj.bias\", \"encoder.layers.1.self_attn.q_proj.weight\", \"encoder.layers.1.self_attn.q_proj.bias\", \"encoder.layers.1.self_attn.out_proj.weight\", \"encoder.layers.1.self_attn.out_proj.bias\", \"encoder.layers.1.self_attn_layer_norm.weight\", \"encoder.layers.1.self_attn_layer_norm.bias\", \"encoder.layers.1.fc1.weight\", \"encoder.layers.1.fc1.bias\", \"encoder.layers.1.fc2.weight\", \"encoder.layers.1.fc2.bias\", \"encoder.layers.1.final_layer_norm.weight\", \"encoder.layers.1.final_layer_norm.bias\", \"encoder.layers.2.self_attn.k_proj.weight\", \"encoder.layers.2.self_attn.k_proj...\r\n```\r\n\r\n\r\n#### Code sample\r\n\r\nSee above\r\n\r\n\r\n### Expected behavior\r\n\r\nModel should load cleanly.\r\n\r\n### Environment\r\n\r\nGoogle Colab\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.6.0+cu101\r\n - OS (e.g., Linux): Google Colab (Linux)\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install --editable ./\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: Unknown\r\n - GPU models and configuration: Unknown\r\n - Any other relevant information:", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2725/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2724", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2724/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2724/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2724/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2724", "id": 719548460, "node_id": "MDU6SXNzdWU3MTk1NDg0NjA=", "number": 2724, "title": "Cache loading fails when there is no internet connection", "user": {"login": "psorianom", "id": 1085210, "node_id": "MDQ6VXNlcjEwODUyMTA=", "avatar_url": "https://avatars.githubusercontent.com/u/1085210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/psorianom", "html_url": "https://github.com/psorianom", "followers_url": "https://api.github.com/users/psorianom/followers", "following_url": "https://api.github.com/users/psorianom/following{/other_user}", "gists_url": "https://api.github.com/users/psorianom/gists{/gist_id}", "starred_url": "https://api.github.com/users/psorianom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/psorianom/subscriptions", "organizations_url": "https://api.github.com/users/psorianom/orgs", "repos_url": "https://api.github.com/users/psorianom/repos", "events_url": "https://api.github.com/users/psorianom/events{/privacy}", "received_events_url": "https://api.github.com/users/psorianom/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 679100395, "node_id": "MDU6TGFiZWw2NzkxMDAzOTU=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/help%20wanted", "name": "help wanted", "color": "128A0C", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-10-12T17:42:49Z", "updated_at": "2020-12-22T09:30:51Z", "closed_at": "2020-10-22T13:27:38Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen getting resources from cache (with `get_from_cache` in [`file_utils.py`](https://github.com/pytorch/fairseq/blob/60442af216d551e4afc9d4fab1c056c1051725cc/fairseq/file_utils.py#L243)), and not having an internet connection, the launched process fails (after several retrials in [`request_wrap_timeout`](https://github.com/pytorch/fairseq/blob/60442af216d551e4afc9d4fab1c056c1051725cc/fairseq/file_utils.py#L216)) with a `RuntimeError` that is not handled in the caller function ([`get_from_cache`](https://github.com/pytorch/fairseq/blob/60442af216d551e4afc9d4fab1c056c1051725cc/fairseq/file_utils.py#L267)). Instead, an `EnvironmentError` exception is handled and thus the `RuntimeError`exception ends the process, **even if the searched file is already cached**. \r\n\r\n\r\n### To Reproduce\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n```python\r\nfrom fairseq.file_utils import get_from_cache\r\nurl = \"https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\"\r\n\r\npath = get_from_cache(url)\r\nprint(f\"This is the path I got: {path}\")\r\n```\r\n\r\n```bash\r\nFile \"/home/user/fairseq/fairseq/file_utils.py\", line 225, in request_wrap_timeout\r\n    raise RuntimeError(f\"Unable to fetch file {url}\")\r\nRuntimeError: Unable to fetch file https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\r\n```\r\n\r\n### Expected behavior\r\nThe process should not end if the file is already cached. When catching the exception, the value of `etag` should be set to `None` and finally the cached file path should be found.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n```bash\r\nThis is the path I got: /home/user/.cache/torch/pytorch_fairseq/e2aab4d600e7568c2d88fc7732130ccc815ea84ec63906cb0913c7a3a4906a2e.\r\n0f323dfaed92d080380e63f0291d0f31adfa8c61a62cbcb3cb8114f061be27f7\r\n\r\n```\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version : master\r\n - PyTorch Version : 1.6.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): `git clone` + `pip install -e .`\r\n\r\n - Python version: 3.7.9\r\n - CPU\r\n\r\n\r\n### Additional context\r\n\r\nA proper solution would entail not needing an internet connection to check if we have the file cached in disk. I guess this has been already pondered and the current solution must be the best.\r\n\r\nA simple way to address the returned exception, `RuntimeError`, would be to handle it *in lieu* of the handled `EnvironmentError`, as it stands. This is proposed in a [branch of mine](https://github.com/pytorch/fairseq/compare/master...psorianom:request_wrap_exception).\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2724/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2714", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2714/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2714/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2714/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2714", "id": 718200191, "node_id": "MDU6SXNzdWU3MTgyMDAxOTE=", "number": 2714, "title": "Mismatch between actual and expected iterable length. ", "user": {"login": "guolinke", "id": 16040950, "node_id": "MDQ6VXNlcjE2MDQwOTUw", "avatar_url": "https://avatars.githubusercontent.com/u/16040950?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guolinke", "html_url": "https://github.com/guolinke", "followers_url": "https://api.github.com/users/guolinke/followers", "following_url": "https://api.github.com/users/guolinke/following{/other_user}", "gists_url": "https://api.github.com/users/guolinke/gists{/gist_id}", "starred_url": "https://api.github.com/users/guolinke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guolinke/subscriptions", "organizations_url": "https://api.github.com/users/guolinke/orgs", "repos_url": "https://api.github.com/users/guolinke/repos", "events_url": "https://api.github.com/users/guolinke/events{/privacy}", "received_events_url": "https://api.github.com/users/guolinke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-10-09T14:37:07Z", "updated_at": "2020-10-10T02:18:54Z", "closed_at": "2020-10-10T02:18:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n```\r\nFile \"/tmp/workspace/code/fairseq/data/iterators.py\", line 63, in __iter__\r\n'Mismatch between actual and expected iterable length. '\r\nRuntimeError: Mismatch between actual and expected iterable length. Please report this to the fairseq developers.\r\n```\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.6\r\n - OS (e.g., Linux): ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2714/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2714/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2711", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2711/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2711/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2711/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2711", "id": 717881471, "node_id": "MDU6SXNzdWU3MTc4ODE0NzE=", "number": 2711, "title": "AttributeError: 'SpeechRecognitionEspressoTask' object has no attribute 'feat_dim'", "user": {"login": "krishnanpooja", "id": 8016149, "node_id": "MDQ6VXNlcjgwMTYxNDk=", "avatar_url": "https://avatars.githubusercontent.com/u/8016149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krishnanpooja", "html_url": "https://github.com/krishnanpooja", "followers_url": "https://api.github.com/users/krishnanpooja/followers", "following_url": "https://api.github.com/users/krishnanpooja/following{/other_user}", "gists_url": "https://api.github.com/users/krishnanpooja/gists{/gist_id}", "starred_url": "https://api.github.com/users/krishnanpooja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krishnanpooja/subscriptions", "organizations_url": "https://api.github.com/users/krishnanpooja/orgs", "repos_url": "https://api.github.com/users/krishnanpooja/repos", "events_url": "https://api.github.com/users/krishnanpooja/events{/privacy}", "received_events_url": "https://api.github.com/users/krishnanpooja/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-10-09T06:11:50Z", "updated_at": "2020-10-11T04:19:34Z", "closed_at": "2020-10-11T04:19:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThe SpeechRecognitionEspressoTask has no attribute 'feat_dim'. Issue seen when trying to evaluate the model. \r\n\r\n\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Try to import pretrained model using checkpoint file.\r\n2. See error\r\n\r\n```\r\nmodel = models.build_model(args, self)\r\nFile \"git/espresso/fairseq/models/init.py\", line 48, in build_model\r\nreturn ARCH_MODEL_REGISTRY[args.arch].build_model(args, task)\r\nFile \"git/espresso/espresso/models/speech_transformer.py\", line 128, in build_model\r\nlogger.info(\"input feature dimension: {}, channels: {}\".format(task.feat_dim, task.feat_in_channels))\r\nAttributeError: 'SpeechRecognitionEspressoTask' object has no attribute 'feat_dim'\r\n```\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\nNo error\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0) 1.6.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2711/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2711/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2707", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2707/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2707/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2707/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2707", "id": 716834043, "node_id": "MDU6SXNzdWU3MTY4MzQwNDM=", "number": 2707, "title": "translation_moe train fails with multi GPU", "user": {"login": "ykim362", "id": 22177353, "node_id": "MDQ6VXNlcjIyMTc3MzUz", "avatar_url": "https://avatars.githubusercontent.com/u/22177353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ykim362", "html_url": "https://github.com/ykim362", "followers_url": "https://api.github.com/users/ykim362/followers", "following_url": "https://api.github.com/users/ykim362/following{/other_user}", "gists_url": "https://api.github.com/users/ykim362/gists{/gist_id}", "starred_url": "https://api.github.com/users/ykim362/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ykim362/subscriptions", "organizations_url": "https://api.github.com/users/ykim362/orgs", "repos_url": "https://api.github.com/users/ykim362/repos", "events_url": "https://api.github.com/users/ykim362/events{/privacy}", "received_events_url": "https://api.github.com/users/ykim362/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-10-07T20:35:46Z", "updated_at": "2020-10-13T15:11:17Z", "closed_at": "2020-10-13T15:11:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\ntranslation_moe training fails with an error below.\r\n\r\n### To Reproduce\r\n\r\nI followed the steps in README.\r\n\r\n1. bash prepare-wmt14en2de.sh\r\n2. TEXT=examples/translation/wmt17_en_de\r\nfairseq-preprocess \\\r\n    --source-lang en --target-lang de \\\r\n    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\r\n    --destdir data-bin/wmt17_en_de --thresholdtgt 0 --thresholdsrc 0 \\\r\n    --workers 20 --joined-dictionary\r\n\r\n3. fairseq-train --ddp-backend='no_c10d' \\\r\n    data-bin/wmt17_en_de \\\r\n    --max-update 100000 \\\r\n    --task translation_moe --user-dir examples/translation_moe/src \\\r\n    --method hMoElp --mean-pool-gating-network \\\r\n    --num-experts 3 \\\r\n    --arch transformer_wmt_en_de --share-all-embeddings \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \\\r\n    --lr 0.0007 --min-lr 1e-09 \\\r\n    --dropout 0.1 --weight-decay 0.0 --criterion cross_entropy \\\r\n    --max-tokens 3584\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\nError occurs in command number 3.\r\n\r\n```\r\n-- Process 7 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/youki/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/distributed_utils.py\", line 228, in distributed_main\r\n    main(args, **kwargs)\r\n  File \"/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq_cli/train.py\", line 125, in main\r\n    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\r\n  File \"/home/youki/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq_cli/train.py\", line 208, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/youki/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/trainer.py\", line 532, in train_step\r\n    logging_outputs, sample_size, ooms, train_time, ignore=is_dummy_batch,\r\n  File \"/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/trainer.py\", line 874, in _aggregate_logging_outputs\r\n    logging_outputs, *extra_stats_to_sum, ignore=ignore\r\n  File \"/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/trainer.py\", line 937, in _fast_stat_sync_sum\r\n    group=self.data_parallel_process_group\r\n  File \"/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/distributed_utils.py\", line 383, in all_reduce_dict\r\n    cpu_data = _all_reduce_dict(cpu_data)\r\n  File \"/home/youki/.local/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq/distributed_utils.py\", line 379, in _all_reduce_dict\r\n    buf = torch.stack(list(data.values())).to(device=device)\r\nRuntimeError: stack expects each tensor to be equal size, but got [] at entry 0 and [3] at entry 7\r\n\r\n```\r\n\r\n\r\n#### Code sample\r\n\r\nNo change in the code. I just ran the command in README.\r\n\r\n### Expected behavior\r\nTraining starts.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): current master\r\n - PyTorch Version (e.g., 1.0) Pytorch 1.6\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed fairseq (`pip`, source): source (python setup.py install --user)\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: 8 Titan x\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2707/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2707/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2705", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2705/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2705/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2705/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2705", "id": 716325305, "node_id": "MDU6SXNzdWU3MTYzMjUzMDU=", "number": 2705, "title": "The registries update forces the \"--remove-bpe\" option to require an argument", "user": {"login": "DoubleVII", "id": 33449816, "node_id": "MDQ6VXNlcjMzNDQ5ODE2", "avatar_url": "https://avatars.githubusercontent.com/u/33449816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DoubleVII", "html_url": "https://github.com/DoubleVII", "followers_url": "https://api.github.com/users/DoubleVII/followers", "following_url": "https://api.github.com/users/DoubleVII/following{/other_user}", "gists_url": "https://api.github.com/users/DoubleVII/gists{/gist_id}", "starred_url": "https://api.github.com/users/DoubleVII/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DoubleVII/subscriptions", "organizations_url": "https://api.github.com/users/DoubleVII/orgs", "repos_url": "https://api.github.com/users/DoubleVII/repos", "events_url": "https://api.github.com/users/DoubleVII/events{/privacy}", "received_events_url": "https://api.github.com/users/DoubleVII/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-10-07T08:43:05Z", "updated_at": "2020-10-17T16:39:07Z", "closed_at": "2020-10-17T16:39:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nAfter the commit [update registries](https://github.com/pytorch/fairseq/commit/5e82514d687289a73a6dec33b555217acd97cb0d), the option \"--remove-bpe\" needs an argument to declare the BPE decoder, which can be omitted according to the document. The [translation examples](https://github.com/pytorch/fairseq/tree/master/examples/translation) also gives the omitted format.\r\n\r\nFurther more, giving the argument 'subword_nmt', which is used to apply BPE at the preprocessing stage of the translation examples, the BLEU socre is still higher than the ture value. I have tried a couple of possible candidates, but they don't work.\r\n\r\n### To Reproduce\r\n\r\n1. Run the given cmd of the translation examples\r\n```\r\nfairseq-generate data-bin/iwslt14.tokenized.de-en \\\r\n    --path checkpoints/checkpoint_best.pt \\\r\n    --batch-size 128 --beam 5 --remove-bpe\r\n```\r\n2. See error\r\n```\r\nusage: fairseq-generate [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\r\n                        [--log-format {json,none,simple,tqdm}]\r\n                        [--tensorboard-logdir TENSORBOARD_LOGDIR]\r\n                        [--seed SEED] [--cpu] [--tpu] [--bf16]\r\n                        [--memory-efficient-bf16] [--fp16]\r\n                        [--memory-efficient-fp16] [--fp16-no-flatten-grads]\r\n                        [--fp16-init-scale FP16_INIT_SCALE]\r\n                        [--fp16-scale-window FP16_SCALE_WINDOW]\r\n                        [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\r\n                        [--min-loss-scale MIN_LOSS_SCALE]\r\n                        [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\r\n                        [--user-dir USER_DIR]\r\n                        [--empty-cache-freq EMPTY_CACHE_FREQ]\r\n                        [--all-gather-list-size ALL_GATHER_LIST_SIZE]\r\n                        [--model-parallel-size MODEL_PARALLEL_SIZE]\r\n                        [--checkpoint-suffix CHECKPOINT_SUFFIX]\r\n                        [--quantization-config-path QUANTIZATION_CONFIG_PATH]\r\n                        [--profile]\r\n                        [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,example_criterion,label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,legacy_masked_lm_loss,masked_lm,nat_loss,sentence_prediction,sentence_ranking,wav2vec,vocab_parallel_cross_entropy}]\r\n                        [--tokenizer {moses,nltk,space}]\r\n                        [--bpe {bytes,byte_bpe,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\r\n                        [--optimizer {adadelta,adafactor,adagrad,adam,adamax,lamb,nag,sgd}]\r\n                        [--lr-scheduler {cosine,fixed,inverse_sqrt,polynomial_decay,reduce_lr_on_plateau,triangular,tri_stage}]\r\n                        [--scoring {sacrebleu,bleu,wer}] [--task TASK]\r\n                        [--num-workers NUM_WORKERS]\r\n                        [--skip-invalid-size-inputs-valid-test]\r\n                        [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\r\n                        [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\r\n                        [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\r\n                        [--dataset-impl {raw,lazy,cached,mmap,fasta}]\r\n                        [--data-buffer-size DATA_BUFFER_SIZE]\r\n                        [--train-subset TRAIN_SUBSET]\r\n                        [--valid-subset VALID_SUBSET]\r\n                        [--validate-interval VALIDATE_INTERVAL]\r\n                        [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\r\n                        [--validate-after-updates VALIDATE_AFTER_UPDATES]\r\n                        [--fixed-validation-seed FIXED_VALIDATION_SEED]\r\n                        [--disable-validation]\r\n                        [--max-tokens-valid MAX_TOKENS_VALID]\r\n                        [--batch-size-valid BATCH_SIZE_VALID]\r\n                        [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]\r\n                        [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\r\n                        [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\r\n                        [--distributed-rank DISTRIBUTED_RANK]\r\n                        [--distributed-backend DISTRIBUTED_BACKEND]\r\n                        [--distributed-init-method DISTRIBUTED_INIT_METHOD]\r\n                        [--distributed-port DISTRIBUTED_PORT]\r\n                        [--device-id DEVICE_ID] [--local-rank LOCAL_RANK]\r\n                        [--distributed-no-spawn]\r\n                        [--ddp-backend {c10d,no_c10d}]\r\n                        [--bucket-cap-mb BUCKET_CAP_MB]\r\n                        [--fix-batches-to-gpus] [--find-unused-parameters]\r\n                        [--fast-stat-sync] [--broadcast-buffers]\r\n                        [--distributed-wrapper {DDP,SlowMo}]\r\n                        [--slowmo-momentum SLOWMO_MOMENTUM]\r\n                        [--slowmo-algorithm SLOWMO_ALGORITHM]\r\n                        [--localsgd-frequency LOCALSGD_FREQUENCY]\r\n                        [--nprocs-per-node NPROCS_PER_NODE]\r\n                        [--pipeline-model-parallel]\r\n                        [--pipeline-balance PIPELINE_BALANCE]\r\n                        [--pipeline-devices PIPELINE_DEVICES]\r\n                        [--pipeline-chunks PIPELINE_CHUNKS]\r\n                        [--pipeline-checkpoint {always,never,except_last}]\r\n                        [--zero-sharding {none,os}] [--path PATH]\r\n                        [--remove-bpe REMOVE_BPE] [--quiet]\r\n                        [--model-overrides MODEL_OVERRIDES]\r\n                        [--results-path RESULTS_PATH] [--beam N] [--nbest N]\r\n                        [--max-len-a N] [--max-len-b N] [--min-len N]\r\n                        [--match-source-len] [--no-early-stop]\r\n                        [--unnormalized] [--no-beamable-mm] [--lenpen LENPEN]\r\n                        [--unkpen UNKPEN] [--replace-unk [REPLACE_UNK]]\r\n                        [--sacrebleu] [--score-reference] [--prefix-size PS]\r\n                        [--no-repeat-ngram-size N] [--sampling]\r\n                        [--sampling-topk PS] [--sampling-topp PS]\r\n                        [--constraints [{ordered,unordered}]]\r\n                        [--temperature N] [--diverse-beam-groups N]\r\n                        [--diverse-beam-strength N] [--diversity-rate N]\r\n                        [--print-alignment] [--print-step]\r\n                        [--iter-decode-eos-penalty N]\r\n                        [--iter-decode-max-iter N]\r\n                        [--iter-decode-force-max-iter]\r\n                        [--iter-decode-with-beam N]\r\n                        [--iter-decode-with-external-reranker]\r\n                        [--retain-iter-history] [--retain-dropout]\r\n                        [--retain-dropout-modules RETAIN_DROPOUT_MODULES [RETAIN_DROPOUT_MODULES ...]]\r\n                        [--decoding-format {unigram,ensemble,vote,dp,bs}]\r\nfairseq-generate: error: argument --remove-bpe: expected one argument\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo error when the argument is omitted, or correct BLEU socre when the argument is specified.\r\n\r\n### Environment\r\n\r\n - fairseq Version: master\r\n - PyTorch Version: 1.6.0\r\n - OS : Linux or Windows\r\n - How you installed fairseq: source\r\n - Build command you used (if compiling from source): ```pip install --editable ./```\r\n - Python version: 3.6.9\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2705/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2695", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2695/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2695/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2695/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2695", "id": 715015339, "node_id": "MDU6SXNzdWU3MTUwMTUzMzk=", "number": 2695, "title": "Error when running fairseq-generate with both 'score-reference' and 'print-alignment'", "user": {"login": "hlncrg", "id": 4844689, "node_id": "MDQ6VXNlcjQ4NDQ2ODk=", "avatar_url": "https://avatars.githubusercontent.com/u/4844689?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hlncrg", "html_url": "https://github.com/hlncrg", "followers_url": "https://api.github.com/users/hlncrg/followers", "following_url": "https://api.github.com/users/hlncrg/following{/other_user}", "gists_url": "https://api.github.com/users/hlncrg/gists{/gist_id}", "starred_url": "https://api.github.com/users/hlncrg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hlncrg/subscriptions", "organizations_url": "https://api.github.com/users/hlncrg/orgs", "repos_url": "https://api.github.com/users/hlncrg/repos", "events_url": "https://api.github.com/users/hlncrg/events{/privacy}", "received_events_url": "https://api.github.com/users/hlncrg/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-10-05T17:10:02Z", "updated_at": "2020-10-17T16:40:06Z", "closed_at": "2020-10-17T16:40:06Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI am running the flag --print-alignment and --score-reference together and getting an error.\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nI am following the example 'Jointly Learning to Align and Translate with Transformer Models':\r\nhttps://github.com/pytorch/fairseq/tree/master/examples/joint_alignment_translation\r\n\r\nI am running the command to generate answers on a test set:\r\nfairseq-generate \\\r\n    binarized --gen-subset test --print-alignment \\\r\n    --source-lang en --target-lang de \\\r\n    --path checkpoints/checkpoint_best.pt --beam 5 --nbest 1 \\\r\n   --score-reference\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\nThe error that I get is the following:\r\n\r\nraceback (most recent call last):\r\nFile \"/usr/local/bin/fairseq-generate\", line 11, in\r\nload_entry_point('fairseq', 'console_scripts', 'fairseq-generate')()\r\nFile \"/home/hecraig/fairseq/fairseq_cli/generate.py\", line 286, in cli_main\r\nmain(args)\r\nFile \"/home/hecraig/fairseq/fairseq_cli/generate.py\", line 38, in main\r\nreturn _main(args, sys.stdout)\r\nFile \"/home/hecraig/fairseq/fairseq_cli/generate.py\", line 233, in _main\r\n' '.join(['{}-{}'.format(src_idx, tgt_idx) for src_idx, tgt_idx in alignment])\r\nTypeError: 'NoneType' object is not iterable\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nIf I understand correctly, I would expect the output to be the alignments given the source and target sentences. \r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.6\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Python version: 3.6\r\n\r\n\r\n### Additional context\r\n\r\nThe command works if either 'score-reference' or 'print-alignment' are not included.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2695/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2695/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2694", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2694/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2694/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2694/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2694", "id": 714783256, "node_id": "MDU6SXNzdWU3MTQ3ODMyNTY=", "number": 2694, "title": "Errors while evaluating CTC wav2vec model", "user": {"login": "kamakshi-malhotra", "id": 60693761, "node_id": "MDQ6VXNlcjYwNjkzNzYx", "avatar_url": "https://avatars.githubusercontent.com/u/60693761?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kamakshi-malhotra", "html_url": "https://github.com/kamakshi-malhotra", "followers_url": "https://api.github.com/users/kamakshi-malhotra/followers", "following_url": "https://api.github.com/users/kamakshi-malhotra/following{/other_user}", "gists_url": "https://api.github.com/users/kamakshi-malhotra/gists{/gist_id}", "starred_url": "https://api.github.com/users/kamakshi-malhotra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kamakshi-malhotra/subscriptions", "organizations_url": "https://api.github.com/users/kamakshi-malhotra/orgs", "repos_url": "https://api.github.com/users/kamakshi-malhotra/repos", "events_url": "https://api.github.com/users/kamakshi-malhotra/events{/privacy}", "received_events_url": "https://api.github.com/users/kamakshi-malhotra/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-10-05T12:10:04Z", "updated_at": "2021-04-09T07:35:35Z", "closed_at": "2020-10-07T04:56:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### Command used\r\npython examples/speech_recognition/infer.py ./manifest --task audio_pretraining --nbest 1 --path ./wav2vec_small_100h.pt --gen-subset dev_other --results-path ./ --w2l-decoder fairseqlm --lm-model ./lm_librispeech_word_transformer.pt --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --post-process letter\r\n\r\nWhen I use this command I got error->\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n#### infer.py: error: unrecognized arguments: --post-process letter\r\n\r\n\r\n\r\n### Additional context\r\nI have downloaded transformer model from wav2letter and its dictionary(renamed it as dict.txt). Also I have saved dict.ltr.txt provided in the link in wav2vec readme. \r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version : 0.9.0\r\n - PyTorch Version : 1.6.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Python version: 3.8\r\n - CUDA/cuDNN version:10.1\r\n ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2694/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2694/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2686", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2686/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2686/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2686/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2686", "id": 713951342, "node_id": "MDU6SXNzdWU3MTM5NTEzNDI=", "number": 2686, "title": "Missing hydra-core dependencies when trying to load fairseq from torch hub", "user": {"login": "yrf1", "id": 14252783, "node_id": "MDQ6VXNlcjE0MjUyNzgz", "avatar_url": "https://avatars.githubusercontent.com/u/14252783?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yrf1", "html_url": "https://github.com/yrf1", "followers_url": "https://api.github.com/users/yrf1/followers", "following_url": "https://api.github.com/users/yrf1/following{/other_user}", "gists_url": "https://api.github.com/users/yrf1/gists{/gist_id}", "starred_url": "https://api.github.com/users/yrf1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yrf1/subscriptions", "organizations_url": "https://api.github.com/users/yrf1/orgs", "repos_url": "https://api.github.com/users/yrf1/repos", "events_url": "https://api.github.com/users/yrf1/events{/privacy}", "received_events_url": "https://api.github.com/users/yrf1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-10-02T23:27:16Z", "updated_at": "2021-11-23T17:03:09Z", "closed_at": "2020-10-03T15:09:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "Got this error when trying to run a simple command: \r\n`torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')`\r\n```\r\nTraceback (most recent call last):\r\n File \"machine_translation.py\", line 10, in <module>\r\n  en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')\r\n File \"/home/yifung2/miniconda3/envs/VQA/lib/python3.7/site-packages/torch/hub.py\", line 364, in load\r\n  entry = _load_entry_from_hubconf(hub_module, model)\r\n File \"/home/yifung2/miniconda3/envs/VQA/lib/python3.7/site-packages/torch/hub.py\", line 232, in _load_entry_from_hubconf\r\n  _check_dependencies(m)\r\n File \"/home/yifung2/miniconda3/envs/VQA/lib/python3.7/site-packages/torch/hub.py\", line 221, in _check_dependencies\r\n  raise RuntimeError('Missing dependencies: {}'.format(', '.join(missing_deps)))\r\nRuntimeError: Missing dependencies: hydra-core. \r\n``` \r\n\r\nHere are all my package versions and dependencies for reproducibility:\r\n```\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main    conda-forge\r\nantlr4-python3-runtime    4.8                      pypi_0    pypi\r\nattrs                     19.3.0                     py_0    conda-forge\r\nblas                      1.0                         mkl    conda-forge\r\nbrotlipy                  0.7.0           py37h8f50634_1000    conda-forge\r\nca-certificates           2019.11.28           hecc5488_0    conda-forge\r\ncatalogue                 1.0.0                      py_0    conda-forge\r\ncertifi                   2019.11.28               py37_0    conda-forge\r\ncffi                      1.14.0                   pypi_0    pypi\r\nchardet                   3.0.4           py37hc8dfbb8_1007    conda-forge\r\nclick                     7.0                      pypi_0    pypi\r\ncryptography              2.8              py37h72c5cf5_1    conda-forge\r\ncudatoolkit               10.1.243             h6bb024c_0  \r\ncymem                     2.0.3            py37he1b5a44_0    conda-forge\r\ncython                    0.29.15                  pypi_0    pypi\r\ncython-blis               0.4.1            py37h516909a_0    conda-forge\r\nen-core-web-sm-mirror     2.2.5                    pypi_0    pypi\r\nfairseq                   0.9.0                    pypi_0    pypi\r\nfastbpe                   0.1.0                    pypi_0    pypi\r\nfreetype                  2.9.1                h8a8886c_1  \r\nhydra-core                1.0.3                    pypi_0    pypi\r\nidna                      2.8                   py37_1000    conda-forge\r\nimportlib-resources       3.0.0                    pypi_0    pypi\r\nimportlib_metadata        1.5.0                    py37_0    conda-forge\r\nintel-openmp              2020.0                      166  \r\njoblib                    0.14.1                   pypi_0    pypi\r\njpeg                      9b                   h024ee3a_2  \r\njsonschema                3.2.0                    py37_0    conda-forge\r\nlanguage-check            1.1                      pypi_0    pypi\r\nld_impl_linux-64          2.33.1               h53a641e_7    conda-forge\r\nlibedit                   3.1.20181209         hc058e9b_0  \r\nlibffi                    3.2.1                hd88cf55_4  \r\nlibgcc-ng                 9.1.0                hdf63c60_0  \r\nlibgfortran-ng            7.3.0                hdf63c60_0  \r\nlibpng                    1.6.37               hbc83047_0  \r\nlibstdcxx-ng              9.1.0                hdf63c60_0  \r\nlibtiff                   4.1.0                h2733197_0  \r\nmkl                       2020.0                      166  \r\nmkl-service               2.3.0            py37he904b0f_0  \r\nmkl_fft                   1.0.15           py37ha843d7b_0  \r\nmkl_random                1.1.0            py37hd6b4f25_0  \r\nmurmurhash                1.0.0            py37he1b5a44_0    conda-forge\r\nncurses                   6.1                  he6710b0_1  \r\nninja                     1.9.0            py37hfd86e86_0  \r\nnumpy                     1.18.1           py37h4f9e942_0  \r\nnumpy-base                1.18.1           py37hde5b4d6_1  \r\nolefile                   0.46                     py37_0  \r\nomegaconf                 2.0.2                    pypi_0    pypi\r\nopenssl                   1.1.1d               h516909a_0    conda-forge\r\npillow                    7.0.0            py37hb39fc2d_0  \r\npip                       20.0.2                   py37_1    conda-forge\r\nplac                      0.9.6                    py37_0  \r\nportalocker               1.5.2                    pypi_0    pypi\r\npreshed                   3.0.2            py37he1b5a44_1    conda-forge\r\npycparser                 2.19                     pypi_0    pypi\r\npyopenssl                 19.1.0                   py37_0    conda-forge\r\npyrsistent                0.15.7           py37h516909a_0    conda-forge\r\npysocks                   1.7.1                    py37_0    conda-forge\r\npython                    3.7.6                h0371630_2  \r\npython_abi                3.7                     1_cp37m    conda-forge\r\npytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\npyyaml                    5.3.1                    pypi_0    pypi\r\nreadline                  7.0                  h7b6447c_5  \r\nregex                     2020.1.8                 pypi_0    pypi\r\nrequests                  2.22.0                   py37_1    conda-forge\r\nsacrebleu                 1.4.3                    pypi_0    pypi\r\nsacremoses                0.0.38                   pypi_0    pypi\r\nsetuptools                45.2.0                   py37_0    conda-forge\r\nsix                       1.14.0                   py37_0    conda-forge\r\nspacy                     2.2.3            py37hc9558a2_0    conda-forge\r\nsqlite                    3.31.1               h7b6447c_0  \r\nsrsly                     1.0.0            py37he1b5a44_0    conda-forge\r\nthinc                     7.3.0            py37hc9558a2_0    conda-forge\r\ntk                        8.6.8                hbc83047_0  \r\ntorchvision               0.5.0                py37_cu101    pytorch\r\ntqdm                      4.42.1                     py_0    conda-forge\r\ntyping                    3.7.4.1                  pypi_0    pypi\r\ntyping-extensions         3.7.4.3                  pypi_0    pypi\r\nurllib3                   1.25.8                   pypi_0    pypi\r\nwasabi                    0.6.0                      py_0    conda-forge\r\nwheel                     0.34.2                   py37_0    conda-forge\r\nxz                        5.2.4                h14c3975_4  \r\nzipp                      2.2.0                      py_0    conda-forge\r\nzlib                      1.2.11               h7b6447c_3  \r\nzstd                      1.3.7                h0b5b093_0  \r\n```\r\n\r\nHelp please! Thank you!", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2686/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2686/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2681", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2681/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2681/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2681/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2681", "id": 713091080, "node_id": "MDU6SXNzdWU3MTMwOTEwODA=", "number": 2681, "title": "Performance deviation when saving/loading megatron checkpoints", "user": {"login": "joshim5", "id": 533735, "node_id": "MDQ6VXNlcjUzMzczNQ==", "avatar_url": "https://avatars.githubusercontent.com/u/533735?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joshim5", "html_url": "https://github.com/joshim5", "followers_url": "https://api.github.com/users/joshim5/followers", "following_url": "https://api.github.com/users/joshim5/following{/other_user}", "gists_url": "https://api.github.com/users/joshim5/gists{/gist_id}", "starred_url": "https://api.github.com/users/joshim5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joshim5/subscriptions", "organizations_url": "https://api.github.com/users/joshim5/orgs", "repos_url": "https://api.github.com/users/joshim5/repos", "events_url": "https://api.github.com/users/joshim5/events{/privacy}", "received_events_url": "https://api.github.com/users/joshim5/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-10-01T18:37:14Z", "updated_at": "2020-11-03T22:07:15Z", "closed_at": "2020-11-03T22:07:15Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nLoss changes when saving / loading checkpoints trained with megatron.\r\n\r\nThe problem seems like it may be amplified with `--memory-efficient-fp16`.\r\n\r\n### To Reproduce\r\n\r\nTrain a model without stopping (top) and again resuming from an intermediate checkpoint (bottom). When model-parallel is not turned on, the loss curves look the same before/after loading checkpoints.\r\n\r\n```\r\npython fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std3-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)'  --lr 0.0005 --lr-scheduler inverse_sqrt --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch roberta_large --update-freq 2 --save-interval-updates 10 --memory-efficient-fp16 \r\n\r\n2020-10-01 11:27:18 | INFO | train_inner | epoch 001:      1 / 39 loss=0.932, ppl=1.91, wps=0, ups=0, wpb=2048, bsz=16, num_updates=1, lr=1.25e-07, gnorm=0.046, loss_scale=128, train_wall=1, wall=5\r\n2020-10-01 11:27:18 | INFO | train_inner | epoch 001:      2 / 39 loss=0.941, ppl=1.92, wps=20281, ups=9.9, wpb=2048, bsz=16, num_updates=2, lr=2.5e-07, gnorm=0.056, loss_scale=128, train_wall=0, wall=5\r\n2020-10-01 11:27:18 | INFO | train_inner | epoch 001:      3 / 39 loss=0.933, ppl=1.91, wps=33879, ups=16.53, wpb=2048, bsz=16, num_updates=3, lr=3.75e-07, gnorm=0.052, loss_scale=128, train_wall=0, wall=6\r\n2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      4 / 39 loss=0.957, ppl=1.94, wps=32845, ups=16.02, wpb=2048, bsz=16, num_updates=4, lr=5e-07, gnorm=0.057, loss_scale=128, train_wall=0, wall=6\r\n2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      5 / 39 loss=0.92, ppl=1.89, wps=29635.2, ups=14.46, wpb=2048, bsz=16, num_updates=5, lr=6.25e-07, gnorm=0.057, loss_scale=128, train_wall=0, wall=6\r\n2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      6 / 39 loss=0.935, ppl=1.91, wps=30384.9, ups=14.82, wpb=2048, bsz=16, num_updates=6, lr=7.5e-07, gnorm=0.047, loss_scale=128, train_wall=0, wall=6\r\n2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      7 / 39 loss=0.945, ppl=1.92, wps=33652.5, ups=16.42, wpb=2048, bsz=16, num_updates=7, lr=8.75e-07, gnorm=0.075, loss_scale=128, train_wall=0, wall=6\r\n2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      8 / 39 loss=0.95, ppl=1.93, wps=33616.1, ups=16.4, wpb=2048, bsz=16, num_updates=8, lr=1e-06, gnorm=0.061, loss_scale=128, train_wall=0, wall=6\r\n2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      9 / 39 loss=0.947, ppl=1.93, wps=34367.3, ups=16.76, wpb=2048, bsz=16, num_updates=9, lr=1.125e-06, gnorm=0.044, loss_scale=128, train_wall=0, wall=6\r\n2020-10-01 11:27:19 | INFO | train_inner | epoch 001:     10 / 39 loss=0.931, ppl=1.91, wps=31126.4, ups=15.19, wpb=2048, bsz=16, num_updates=10, lr=1.25e-06, gnorm=0.045, loss_scale=128, train_wall=0, wall=6\r\n2020-10-01 11:27:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n2020-10-01 11:27:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.94 | ppl 1.92 | wps 66250.8 | wpb 1013.2 | bsz 7.9 | num_updates 10\r\n2020-10-01 11:27:25 | INFO | fairseq_cli.train | begin save checkpoint\r\n2020-10-01 11:27:29 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/std3-check/checkpoint_1_10.pt (epoch 1 @ 10 updates, score 0.94) (writing took 4.070859680883586 seconds)\r\n2020-10-01 11:27:29 | INFO | train_inner | epoch 001:     11 / 39 loss=0.943, ppl=1.92, wps=197.6, ups=0.1, wpb=2048, bsz=16, num_updates=11, lr=1.375e-06, gnorm=0.055, loss_scale=128, train_wall=0, wall=16\r\n2020-10-01 11:27:29 | INFO | train_inner | epoch 001:     12 / 39 loss=0.925, ppl=1.9, wps=32072.8, ups=15.65, wpb=2048, bsz=16, num_updates=12, lr=1.5e-06, gnorm=0.047, loss_scale=128, train_wall=0, wall=16\r\n2020-10-01 11:27:29 | INFO | train_inner | epoch 001:     13 / 39 loss=0.931, ppl=1.91, wps=37135.9, ups=18.11, wpb=2048, bsz=16, num_updates=13, lr=1.625e-06, gnorm=0.049, loss_scale=128, train_wall=0, wall=16\r\n2020-10-01 11:27:29 | INFO | train_inner | epoch 001:     14 / 39 loss=0.93, ppl=1.91, wps=37413.2, ups=18.25, wpb=2048, bsz=16, num_updates=14, lr=1.75e-06, gnorm=0.059, loss_scale=128, train_wall=0, wall=17\r\n2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     15 / 39 loss=0.916, ppl=1.89, wps=32179.6, ups=15.69, wpb=2048, bsz=16, num_updates=15, lr=1.875e-06, gnorm=0.049, loss_scale=128, train_wall=0, wall=17\r\n2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     16 / 39 loss=0.927, ppl=1.9, wps=38443.5, ups=18.75, wpb=2048, bsz=16, num_updates=16, lr=2e-06, gnorm=0.038, loss_scale=128, train_wall=0, wall=17\r\n2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     17 / 39 loss=0.931, ppl=1.91, wps=35584.3, ups=17.36, wpb=2048, bsz=16, num_updates=17, lr=2.125e-06, gnorm=0.046, loss_scale=128, train_wall=0, wall=17\r\n2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     18 / 39 loss=0.931, ppl=1.91, wps=32496, ups=16.88, wpb=1922, bsz=16, num_updates=18, lr=2.25e-06, gnorm=0.051, loss_scale=128, train_wall=0, wall=17\r\n2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     19 / 39 loss=0.908, ppl=1.88, wps=30532.6, ups=14.89, wpb=2048, bsz=16, num_updates=19, lr=2.375e-06, gnorm=0.052, loss_scale=128, train_wall=0, wall=17\r\n2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     20 / 39 loss=0.94, ppl=1.92, wps=37303.4, ups=18.2, wpb=2048, bsz=16, num_updates=20, lr=2.5e-06, gnorm=0.06, loss_scale=128, train_wall=0, wall=17\r\n\r\npython fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std3-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)'  --lr 0.0005 --lr-scheduler inverse_sqrt --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128  --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch roberta_large --update-freq 2 --save-interval-updates 10 --memory-efficient-fp16 \r\n\r\n2020-10-01 11:28:02 | INFO | train_inner | epoch 001:     11 / 39 loss=0.943, ppl=1.92, wps=174.9, ups=0.09, wpb=2048, bsz=16, num_updates=11, lr=1.375e-06, gnorm=0.055, loss_scale=128, train_wall=1, wall=0\r\n2020-10-01 11:28:02 | INFO | train_inner | epoch 001:     12 / 39 loss=0.925, ppl=1.9, wps=7341.3, ups=3.58, wpb=2048, bsz=16, num_updates=12, lr=1.5e-06, gnorm=0.047, loss_scale=128, train_wall=0, wall=0\r\n2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     13 / 39 loss=0.931, ppl=1.91, wps=36062.9, ups=17.59, wpb=2048, bsz=16, num_updates=13, lr=1.625e-06, gnorm=0.049, loss_scale=128, train_wall=0, wall=0\r\n2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     14 / 39 loss=0.93, ppl=1.91, wps=38712.9, ups=18.88, wpb=2048, bsz=16, num_updates=14, lr=1.75e-06, gnorm=0.059, loss_scale=128, train_wall=0, wall=0\r\n2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     15 / 39 loss=0.916, ppl=1.89, wps=33434.7, ups=16.31, wpb=2048, bsz=16, num_updates=15, lr=1.875e-06, gnorm=0.049, loss_scale=128, train_wall=0, wall=0\r\n2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     16 / 39 loss=0.927, ppl=1.9, wps=31454.2, ups=15.34, wpb=2048, bsz=16, num_updates=16, lr=2e-06, gnorm=0.038, loss_scale=128, train_wall=0, wall=0\r\n2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     17 / 39 loss=0.931, ppl=1.91, wps=33455.7, ups=16.32, wpb=2048, bsz=16, num_updates=17, lr=2.125e-06, gnorm=0.046, loss_scale=128, train_wall=0, wall=0\r\n2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     18 / 39 loss=0.931, ppl=1.91, wps=33471, ups=17.4, wpb=1922, bsz=16, num_updates=18, lr=2.25e-06, gnorm=0.051, loss_scale=128, train_wall=0, wall=0\r\n2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     19 / 39 loss=0.908, ppl=1.88, wps=34046.9, ups=16.61, wpb=2048, bsz=16, num_updates=19, lr=2.375e-06, gnorm=0.052, loss_scale=128, train_wall=0, wall=0\r\n2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     20 / 39 loss=0.94, ppl=1.92, wps=36189.7, ups=17.65, wpb=2048, bsz=16, num_updates=20, lr=2.5e-06, gnorm=0.06, loss_scale=128, train_wall=0, wall=0\r\n2020-10-01 11:28:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n2020-10-01 11:28:08 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.91 | ppl 1.88 | wps 67429.2 | wpb 1013.2 | bsz 7.9 | num_updates 20 | best_loss 0.91\r\n```\r\n\r\nAs another check, see the same thing without `-mem-efficient-fp16`:\r\n\r\n```\r\npython fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std4-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)'  --lr 0.0005 --lr-scheduler inverse_sqrt --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch roberta_large --update-freq 2 --save-interval-updates 10\r\n\r\n2020-10-01 11:29:37 | INFO | fairseq.trainer | begin training epoch 1\r\n2020-10-01 11:29:42 | INFO | train_inner | epoch 001:      1 / 39 loss=0.932, ppl=1.91, wps=0, ups=0, wpb=2048, bsz=16, num_updates=1, lr=1.25e-07, gnorm=0.046, train_wall=2, wall=6\r\n2020-10-01 11:29:43 | INFO | train_inner | epoch 001:      2 / 39 loss=0.941, ppl=1.92, wps=4584.9, ups=2.24, wpb=2048, bsz=16, num_updates=2, lr=2.5e-07, gnorm=0.056, train_wall=0, wall=6\r\n2020-10-01 11:29:43 | INFO | train_inner | epoch 001:      3 / 39 loss=0.932, ppl=1.91, wps=7799.8, ups=3.81, wpb=2048, bsz=16, num_updates=3, lr=3.75e-07, gnorm=0.052, train_wall=0, wall=7\r\n2020-10-01 11:29:43 | INFO | train_inner | epoch 001:      4 / 39 loss=0.955, ppl=1.94, wps=5649.1, ups=2.76, wpb=2048, bsz=16, num_updates=4, lr=5e-07, gnorm=0.057, train_wall=0, wall=7\r\n2020-10-01 11:29:44 | INFO | train_inner | epoch 001:      5 / 39 loss=0.915, ppl=1.89, wps=6558.9, ups=3.2, wpb=2048, bsz=16, num_updates=5, lr=6.25e-07, gnorm=0.057, train_wall=0, wall=7\r\n2020-10-01 11:29:44 | INFO | train_inner | epoch 001:      6 / 39 loss=0.928, ppl=1.9, wps=5850.9, ups=2.86, wpb=2048, bsz=16, num_updates=6, lr=7.5e-07, gnorm=0.047, train_wall=0, wall=8\r\n2020-10-01 11:29:44 | INFO | train_inner | epoch 001:      7 / 39 loss=0.933, ppl=1.91, wps=5723.3, ups=2.79, wpb=2048, bsz=16, num_updates=7, lr=8.75e-07, gnorm=0.075, train_wall=0, wall=8\r\n2020-10-01 11:29:45 | INFO | train_inner | epoch 001:      8 / 39 loss=0.935, ppl=1.91, wps=6092.8, ups=2.97, wpb=2048, bsz=16, num_updates=8, lr=1e-06, gnorm=0.061, train_wall=0, wall=8\r\n2020-10-01 11:29:45 | INFO | train_inner | epoch 001:      9 / 39 loss=0.927, ppl=1.9, wps=6992.4, ups=3.41, wpb=2048, bsz=16, num_updates=9, lr=1.125e-06, gnorm=0.042, train_wall=0, wall=9\r\n2020-10-01 11:29:45 | INFO | train_inner | epoch 001:     10 / 39 loss=0.908, ppl=1.88, wps=5788.6, ups=2.83, wpb=2048, bsz=16, num_updates=10, lr=1.25e-06, gnorm=0.045, train_wall=0, wall=9\r\n2020-10-01 11:29:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n2020-10-01 11:29:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.907 | ppl 1.87 | wps 17267.7 | wpb 1013.2 | bsz 7.9 | num_updates 10\r\n2020-10-01 11:29:55 | INFO | fairseq_cli.train | begin save checkpoint\r\n2020-10-01 11:29:59 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/std4-check/checkpoint_1_10.pt (epoch 1 @ 10 updates, score 0.907) (writing took 4.3799392599612474 seconds)\r\n2020-10-01 11:29:59 | INFO | train_inner | epoch 001:     11 / 39 loss=0.911, ppl=1.88, wps=149.6, ups=0.07, wpb=2048, bsz=16, num_updates=11, lr=1.375e-06, gnorm=0.055, train_wall=0, wall=23\r\n2020-10-01 11:30:00 | INFO | train_inner | epoch 001:     12 / 39 loss=0.887, ppl=1.85, wps=5720.2, ups=2.79, wpb=2048, bsz=16, num_updates=12, lr=1.5e-06, gnorm=0.046, train_wall=0, wall=23\r\n2020-10-01 11:30:00 | INFO | train_inner | epoch 001:     13 / 39 loss=0.893, ppl=1.86, wps=5695.3, ups=2.78, wpb=2048, bsz=16, num_updates=13, lr=1.625e-06, gnorm=0.047, train_wall=0, wall=23\r\n2020-10-01 11:30:00 | INFO | train_inner | epoch 001:     14 / 39 loss=0.881, ppl=1.84, wps=6004.7, ups=2.93, wpb=2048, bsz=16, num_updates=14, lr=1.75e-06, gnorm=0.059, train_wall=0, wall=24\r\n2020-10-01 11:30:01 | INFO | train_inner | epoch 001:     15 / 39 loss=0.861, ppl=1.82, wps=5793.5, ups=2.83, wpb=2048, bsz=16, num_updates=15, lr=1.875e-06, gnorm=0.048, train_wall=0, wall=24\r\n2020-10-01 11:30:01 | INFO | train_inner | epoch 001:     16 / 39 loss=0.868, ppl=1.83, wps=7520.7, ups=3.67, wpb=2048, bsz=16, num_updates=16, lr=2e-06, gnorm=0.036, train_wall=0, wall=24\r\n2020-10-01 11:30:01 | INFO | train_inner | epoch 001:     17 / 39 loss=0.867, ppl=1.82, wps=7838.1, ups=3.83, wpb=2048, bsz=16, num_updates=17, lr=2.125e-06, gnorm=0.043, train_wall=0, wall=25\r\n2020-10-01 11:30:01 | INFO | train_inner | epoch 001:     18 / 39 loss=0.863, ppl=1.82, wps=6999.9, ups=3.64, wpb=1922, bsz=16, num_updates=18, lr=2.25e-06, gnorm=0.046, train_wall=0, wall=25\r\n2020-10-01 11:30:02 | INFO | train_inner | epoch 001:     19 / 39 loss=0.825, ppl=1.77, wps=7828.4, ups=3.82, wpb=2048, bsz=16, num_updates=19, lr=2.375e-06, gnorm=0.047, train_wall=0, wall=25\r\n2020-10-01 11:30:02 | INFO | train_inner | epoch 001:     20 / 39 loss=0.853, ppl=1.81, wps=7703.4, ups=3.76, wpb=2048, bsz=16, num_updates=20, lr=2.5e-06, gnorm=0.056, train_wall=0, wall=25\r\n\r\npython fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std4-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)'  --lr 0.0005 --lr-scheduler inverse_sqrt --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch roberta_large --update-freq 2 --save-interval-updates 10\r\n\r\n2020-10-01 11:30:38 | INFO | fairseq.trainer | begin training epoch 1\r\n2020-10-01 11:30:44 | INFO | train_inner | epoch 001:     11 / 39 loss=0.911, ppl=1.88, wps=138.4, ups=0.07, wpb=2048, bsz=16, num_updates=11, lr=1.375e-06, gnorm=0.055, train_wall=1, wall=0\r\n2020-10-01 11:30:44 | INFO | train_inner | epoch 001:     12 / 39 loss=0.887, ppl=1.85, wps=5156, ups=2.52, wpb=2048, bsz=16, num_updates=12, lr=1.5e-06, gnorm=0.046, train_wall=0, wall=0\r\n2020-10-01 11:30:45 | INFO | train_inner | epoch 001:     13 / 39 loss=0.893, ppl=1.86, wps=5299, ups=2.59, wpb=2048, bsz=16, num_updates=13, lr=1.625e-06, gnorm=0.047, train_wall=0, wall=0\r\n2020-10-01 11:30:45 | INFO | train_inner | epoch 001:     14 / 39 loss=0.881, ppl=1.84, wps=6331.9, ups=3.09, wpb=2048, bsz=16, num_updates=14, lr=1.75e-06, gnorm=0.059, train_wall=0, wall=0\r\n2020-10-01 11:30:45 | INFO | train_inner | epoch 001:     15 / 39 loss=0.861, ppl=1.82, wps=5262.7, ups=2.57, wpb=2048, bsz=16, num_updates=15, lr=1.875e-06, gnorm=0.048, train_wall=0, wall=0\r\n2020-10-01 11:30:46 | INFO | train_inner | epoch 001:     16 / 39 loss=0.868, ppl=1.83, wps=5776.1, ups=2.82, wpb=2048, bsz=16, num_updates=16, lr=2e-06, gnorm=0.036, train_wall=0, wall=0\r\n2020-10-01 11:30:46 | INFO | train_inner | epoch 001:     17 / 39 loss=0.867, ppl=1.82, wps=8960.5, ups=4.37, wpb=2048, bsz=16, num_updates=17, lr=2.125e-06, gnorm=0.043, train_wall=0, wall=0\r\n2020-10-01 11:30:46 | INFO | train_inner | epoch 001:     18 / 39 loss=0.863, ppl=1.82, wps=11617.3, ups=6.04, wpb=1922, bsz=16, num_updates=18, lr=2.25e-06, gnorm=0.046, train_wall=0, wall=0\r\n2020-10-01 11:30:46 | INFO | train_inner | epoch 001:     19 / 39 loss=0.825, ppl=1.77, wps=5859.4, ups=2.86, wpb=2048, bsz=16, num_updates=19, lr=2.375e-06, gnorm=0.047, train_wall=0, wall=0\r\n2020-10-01 11:30:47 | INFO | train_inner | epoch 001:     20 / 39 loss=0.853, ppl=1.81, wps=5785.5, ups=2.82, wpb=2048, bsz=16, num_updates=20, lr=2.5e-06, gnorm=0.056, train_wall=0, wall=0\r\n```\r\n\r\nWith Megatron (and no memory-efficient-fp16) the curves look slightly different:\r\n```\r\npython fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std-check9    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch model_parallel_roberta_large --model-parallel-size 2 --update-freq 2 --save-interval-updates 10\r\n\r\n2020-10-01 11:52:42 | INFO | train_inner | epoch 001:      1 / 78 loss=0.962, ppl=1.95, wps=0, ups=0, wpb=1024, bsz=8, num_updates=1, lr=2.24975e-07, gnorm=0.058, train_wall=2, wall=8\r\n2020-10-01 11:52:42 | INFO | train_inner | epoch 001:      2 / 78 loss=0.981, ppl=1.97, wps=2233.4, ups=2.18, wpb=1024, bsz=8, num_updates=2, lr=3.4995e-07, gnorm=0.067, train_wall=0, wall=8\r\n2020-10-01 11:52:43 | INFO | train_inner | epoch 001:      3 / 78 loss=0.953, ppl=1.94, wps=2413.9, ups=2.36, wpb=1024, bsz=8, num_updates=3, lr=4.74925e-07, gnorm=0.065, train_wall=0, wall=9\r\n2020-10-01 11:52:43 | INFO | train_inner | epoch 001:      4 / 78 loss=0.996, ppl=2, wps=3858.7, ups=3.77, wpb=1024, bsz=8, num_updates=4, lr=5.999e-07, gnorm=0.06, train_wall=0, wall=9\r\n2020-10-01 11:52:43 | INFO | train_inner | epoch 001:      5 / 78 loss=0.953, ppl=1.94, wps=3928.6, ups=3.84, wpb=1024, bsz=8, num_updates=5, lr=7.24875e-07, gnorm=0.047, train_wall=0, wall=9\r\n2020-10-01 11:52:44 | INFO | train_inner | epoch 001:      6 / 78 loss=0.951, ppl=1.93, wps=3806, ups=3.72, wpb=1024, bsz=8, num_updates=6, lr=8.4985e-07, gnorm=0.054, train_wall=0, wall=9\r\n2020-10-01 11:52:44 | INFO | train_inner | epoch 001:      7 / 78 loss=0.936, ppl=1.91, wps=2803.3, ups=2.74, wpb=1024, bsz=8, num_updates=7, lr=9.74825e-07, gnorm=0.056, train_wall=0, wall=10\r\n2020-10-01 11:52:44 | INFO | train_inner | epoch 001:      8 / 78 loss=0.998, ppl=2, wps=2721.6, ups=2.66, wpb=1024, bsz=8, num_updates=8, lr=1.0998e-06, gnorm=0.066, train_wall=0, wall=10\r\n2020-10-01 11:52:45 | INFO | train_inner | epoch 001:      9 / 78 loss=0.974, ppl=1.96, wps=2264.3, ups=2.21, wpb=1024, bsz=8, num_updates=9, lr=1.22478e-06, gnorm=0.062, train_wall=0, wall=11\r\n2020-10-01 11:52:45 | INFO | train_inner | epoch 001:     10 / 78 loss=0.959, ppl=1.94, wps=2275, ups=2.22, wpb=1024, bsz=8, num_updates=10, lr=1.34975e-06, gnorm=0.07, train_wall=0, wall=11\r\n2020-10-01 11:52:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n2020-10-01 11:53:02 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.929 | ppl 1.9 | wps 6851.3 | wpb 509.6 | bsz 4 | num_updates 10\r\n2020-10-01 11:53:02 | INFO | fairseq_cli.train | begin save checkpoint\r\n2020-10-01 11:53:05 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/std-check9/checkpoint_1_10-model_part-0.pt (epoch 1 @ 10 updates, score 0.929) (writing took 2.5728800734505057 seconds)\r\n2020-10-01 11:53:05 | INFO | train_inner | epoch 001:     11 / 78 loss=0.939, ppl=1.92, wps=52.2, ups=0.05, wpb=1024, bsz=8, num_updates=11, lr=1.47473e-06, gnorm=0.057, train_wall=0, wall=31\r\n2020-10-01 11:53:05 | INFO | train_inner | epoch 001:     12 / 78 loss=0.938, ppl=1.92, wps=2308.6, ups=2.25, wpb=1024, bsz=8, num_updates=12, lr=1.5997e-06, gnorm=0.062, train_wall=0, wall=31\r\n2020-10-01 11:53:06 | INFO | train_inner | epoch 001:     13 / 78 loss=0.877, ppl=1.84, wps=2285.4, ups=2.23, wpb=1024, bsz=8, num_updates=13, lr=1.72468e-06, gnorm=0.089, train_wall=0, wall=31\r\n2020-10-01 11:53:06 | INFO | train_inner | epoch 001:     14 / 78 loss=0.887, ppl=1.85, wps=2493.2, ups=2.43, wpb=1024, bsz=8, num_updates=14, lr=1.84965e-06, gnorm=0.05, train_wall=0, wall=32\r\n2020-10-01 11:53:07 | INFO | train_inner | epoch 001:     15 / 78 loss=0.867, ppl=1.82, wps=2378.8, ups=2.32, wpb=1024, bsz=8, num_updates=15, lr=1.97463e-06, gnorm=0.052, train_wall=0, wall=32\r\n2020-10-01 11:53:07 | INFO | train_inner | epoch 001:     16 / 78 loss=0.891, ppl=1.85, wps=2488.1, ups=2.43, wpb=1024, bsz=8, num_updates=16, lr=2.0996e-06, gnorm=0.06, train_wall=0, wall=33\r\n2020-10-01 11:53:07 | INFO | train_inner | epoch 001:     17 / 78 loss=0.887, ppl=1.85, wps=3965, ups=3.87, wpb=1024, bsz=8, num_updates=17, lr=2.22458e-06, gnorm=0.059, train_wall=0, wall=33\r\n2020-10-01 11:53:08 | INFO | train_inner | epoch 001:     18 / 78 loss=0.862, ppl=1.82, wps=3881, ups=3.79, wpb=1024, bsz=8, num_updates=18, lr=2.34955e-06, gnorm=0.085, train_wall=0, wall=33\r\n2020-10-01 11:53:08 | INFO | train_inner | epoch 001:     19 / 78 loss=0.876, ppl=1.83, wps=4705.1, ups=4.59, wpb=1024, bsz=8, num_updates=19, lr=2.47453e-06, gnorm=0.061, train_wall=0, wall=33\r\n2020-10-01 11:53:08 | INFO | train_inner | epoch 001:     20 / 78 loss=0.818, ppl=1.76, wps=3002.5, ups=2.93, wpb=1024, bsz=8, num_updates=20, lr=2.5995e-06, gnorm=0.051, train_wall=0, wall=34\r\n\r\npython fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std-check9    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch model_parallel_roberta_large --model-parallel-size 2 --update-freq 2 --save-interval-updates 10\r\n\r\n2020-10-01 11:53:45 | INFO | train_inner | epoch 001:     11 / 78 loss=0.936, ppl=1.91, wps=44.1, ups=0.04, wpb=1024, bsz=8, num_updates=11, lr=1.47473e-06, gnorm=0.057, train_wall=2, wall=0\r\n2020-10-01 11:53:45 | INFO | train_inner | epoch 001:     12 / 78 loss=0.94, ppl=1.92, wps=3380.6, ups=3.3, wpb=1024, bsz=8, num_updates=12, lr=1.5997e-06, gnorm=0.062, train_wall=0, wall=0\r\n2020-10-01 11:53:45 | INFO | train_inner | epoch 001:     13 / 78 loss=0.877, ppl=1.84, wps=4735.6, ups=4.62, wpb=1024, bsz=8, num_updates=13, lr=1.72468e-06, gnorm=0.088, train_wall=0, wall=0\r\n2020-10-01 11:53:46 | INFO | train_inner | epoch 001:     14 / 78 loss=0.887, ppl=1.85, wps=4641.6, ups=4.53, wpb=1024, bsz=8, num_updates=14, lr=1.84965e-06, gnorm=0.05, train_wall=0, wall=0\r\n2020-10-01 11:53:46 | INFO | train_inner | epoch 001:     15 / 78 loss=0.867, ppl=1.82, wps=2367.4, ups=2.31, wpb=1024, bsz=8, num_updates=15, lr=1.97463e-06, gnorm=0.051, train_wall=0, wall=0\r\n2020-10-01 11:53:46 | INFO | train_inner | epoch 001:     16 / 78 loss=0.891, ppl=1.85, wps=3147, ups=3.07, wpb=1024, bsz=8, num_updates=16, lr=2.0996e-06, gnorm=0.06, train_wall=0, wall=0\r\n2020-10-01 11:53:47 | INFO | train_inner | epoch 001:     17 / 78 loss=0.888, ppl=1.85, wps=4429.3, ups=4.32, wpb=1024, bsz=8, num_updates=17, lr=2.22458e-06, gnorm=0.059, train_wall=0, wall=0\r\n2020-10-01 11:53:47 | INFO | train_inner | epoch 001:     18 / 78 loss=0.863, ppl=1.82, wps=2539.5, ups=2.48, wpb=1024, bsz=8, num_updates=18, lr=2.34955e-06, gnorm=0.085, train_wall=0, wall=0\r\n2020-10-01 11:53:47 | INFO | train_inner | epoch 001:     19 / 78 loss=0.877, ppl=1.84, wps=3398.7, ups=3.32, wpb=1024, bsz=8, num_updates=19, lr=2.47453e-06, gnorm=0.061, train_wall=0, wall=0\r\n2020-10-01 11:53:48 | INFO | train_inner | epoch 001:     20 / 78 loss=0.817, ppl=1.76, wps=2572.9, ups=2.51, wpb=1024, bsz=8, num_updates=20, lr=2.5995e-06, gnorm=0.051, train_wall=0, wall=0\r\n```\r\n\r\nHowever, with Megatron (+ memory-efficient-fp16) the curves look even further different:\r\n\r\n```\r\npython fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch model_parallel_roberta_large --model-parallel-size 2 --update-freq 2 --save-interval-updates 10\r\n\r\n2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      2 / 78 loss_scale=32, train_wall=0, wall=6\r\n2020-10-01 10:59:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\r\n2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      3 / 78 loss_scale=16, train_wall=0, wall=6\r\n2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      4 / 78 loss=1.006, ppl=2.01, wps=0, ups=0, wpb=1024, bsz=8, num_updates=1, lr=2.24975e-07, gnorm=0.056, loss_scale=16, train_wall=0, wall=6\r\n2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      5 / 78 loss=0.97, ppl=1.96, wps=10604.8, ups=10.35, wpb=1024, bsz=8, num_updates=2, lr=3.4995e-07, gnorm=0.053, loss_scale=16, train_wall=0, wall=6\r\n2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      6 / 78 loss=0.959, ppl=1.94, wps=15286.4, ups=14.91, wpb=1024, bsz=8, num_updates=3, lr=4.74925e-07, gnorm=0.057, loss_scale=16, train_wall=0, wall=7\r\n2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      7 / 78 loss=0.966, ppl=1.95, wps=15897.1, ups=15.51, wpb=1024, bsz=8, num_updates=4, lr=5.999e-07, gnorm=0.057, loss_scale=16, train_wall=0, wall=7\r\n2020-10-01 10:59:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\r\n2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      8 / 78 loss=None, ppl=0, wps=0, ups=0, wpb=None, bsz=None, num_updates=None, lr=None, gnorm=None, loss_scale=8, train_wall=0, wall=7\r\n2020-10-01 10:59:30 | INFO | train_inner | epoch 001:      9 / 78 loss=1.022, ppl=2.03, wps=16224.8, ups=15.83, wpb=1024, bsz=8, num_updates=5, lr=7.24875e-07, gnorm=0.066, loss_scale=8, train_wall=0, wall=7\r\n2020-10-01 10:59:30 | INFO | train_inner | epoch 001:     10 / 78 loss=1.008, ppl=2.01, wps=14974.3, ups=14.61, wpb=1024, bsz=8, num_updates=6, lr=8.4985e-07, gnorm=0.074, loss_scale=8, train_wall=0, wall=7\r\n2020-10-01 10:59:30 | INFO | train_inner | epoch 001:     11 / 78 loss=1.01, ppl=2.01, wps=14921.3, ups=14.56, wpb=1024, bsz=8, num_updates=7, lr=9.74825e-07, gnorm=0.059, loss_scale=8, train_wall=0, wall=7\r\n2020-10-01 10:59:30 | INFO | train_inner | epoch 001:     12 / 78 loss=1.008, ppl=2.01, wps=16674.8, ups=16.27, wpb=1024, bsz=8, num_updates=8, lr=1.0998e-06, gnorm=0.07, loss_scale=8, train_wall=0, wall=7\r\n2020-10-01 10:59:30 | INFO | train_inner | epoch 001:     13 / 78 loss=0.951, ppl=1.93, wps=15649, ups=15.26, wpb=1024, bsz=8, num_updates=9, lr=1.22478e-06, gnorm=0.098, loss_scale=8, train_wall=0, wall=7\r\n2020-10-01 10:59:30 | INFO | train_inner | epoch 001:     14 / 78 loss=0.965, ppl=1.95, wps=16871.8, ups=16.46, wpb=1024, bsz=8, num_updates=10, lr=1.34975e-06, gnorm=0.063, loss_scale=8, train_wall=0, wall=7\r\n2020-10-01 10:59:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n2020-10-01 10:59:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.985 | ppl 1.98 | wps 34385.5 | wpb 509.6 | bsz 4 | num_updates 10\r\n2020-10-01 10:59:36 | INFO | fairseq_cli.train | begin save checkpoint\r\n2020-10-01 10:59:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/std-check/checkpoint_1_10-model_part-0.pt (epoch 1 @ 10 updates, score 0.985) (writing took 3.1006576996296644 seconds)\r\n2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     15 / 78 loss=0.959, ppl=1.94, wps=110.3, ups=0.11, wpb=1024, bsz=8, num_updates=11, lr=1.47473e-06, gnorm=0.063, loss_scale=8, train_wall=0, wall=16\r\n2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     16 / 78 loss=0.994, ppl=1.99, wps=15599.7, ups=15.22, wpb=1024, bsz=8, num_updates=12, lr=1.5997e-06, gnorm=0.062, loss_scale=8, train_wall=0, wall=16\r\n2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     17 / 78 loss=0.991, ppl=1.99, wps=16336.3, ups=15.94, wpb=1024, bsz=8, num_updates=13, lr=1.72468e-06, gnorm=0.066, loss_scale=8, train_wall=0, wall=16\r\n2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     18 / 78 loss=0.974, ppl=1.96, wps=13780.4, ups=13.45, wpb=1024, bsz=8, num_updates=14, lr=1.84965e-06, gnorm=0.085, loss_scale=8, train_wall=0, wall=17\r\n2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     19 / 78 loss=1.013, ppl=2.02, wps=16322.8, ups=15.92, wpb=1024, bsz=8, num_updates=15, lr=1.97463e-06, gnorm=0.067, loss_scale=8, train_wall=0, wall=17\r\n2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     20 / 78 loss=0.968, ppl=1.96, wps=16886.4, ups=16.47, wpb=1024, bsz=8, num_updates=16, lr=2.0996e-06, gnorm=0.062, loss_scale=8, train_wall=0, wall=17\r\n2020-10-01 10:59:40 | INFO | train_inner | epoch 001:     21 / 78 loss=0.969, ppl=1.96, wps=15092.8, ups=14.73, wpb=1024, bsz=8, num_updates=17, lr=2.22458e-06, gnorm=0.065, loss_scale=8, train_wall=0, wall=17\r\n2020-10-01 10:59:40 | INFO | train_inner | epoch 001:     22 / 78 loss=0.932, ppl=1.91, wps=15917.8, ups=15.53, wpb=1024, bsz=8, num_updates=18, lr=2.34955e-06, gnorm=0.065, loss_scale=8, train_wall=0, wall=17\r\n2020-10-01 10:59:40 | INFO | train_inner | epoch 001:     23 / 78 loss=0.963, ppl=1.95, wps=16381.6, ups=15.98, wpb=1024, bsz=8, num_updates=19, lr=2.47453e-06, gnorm=0.069, loss_scale=8, train_wall=0, wall=17\r\n2020-10-01 10:59:40 | INFO | train_inner | epoch 001:     24 / 78 loss=0.992, ppl=1.99, wps=16278, ups=15.88, wpb=1024, bsz=8, num_updates=20, lr=2.5995e-06, gnorm=0.073, loss_scale=8, train_wall=0, wall=17\r\n2020-10-01 10:59:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n\r\n\r\npython fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch model_parallel_roberta_large --model-parallel-size 2 --update-freq 2 --save-interval-updates 10\r\n\r\n2020-10-01 11:00:10 | INFO | train_inner | epoch 001:     15 / 78 loss=0.959, ppl=1.94, wps=85.2, ups=0.08, wpb=1024, bsz=8, num_updates=11, lr=1.47473e-06, gnorm=0.063, loss_scale=8, train_wall=1, wall=0\r\n2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     16 / 78 loss=0.993, ppl=1.99, wps=8795.3, ups=8.58, wpb=1024, bsz=8, num_updates=12, lr=1.5997e-06, gnorm=0.062, loss_scale=8, train_wall=0, wall=0\r\n2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     17 / 78 loss=0.99, ppl=1.99, wps=17000.1, ups=16.59, wpb=1024, bsz=8, num_updates=13, lr=1.72468e-06, gnorm=0.066, loss_scale=8, train_wall=0, wall=0\r\n2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     18 / 78 loss=0.977, ppl=1.97, wps=16906.7, ups=16.49, wpb=1024, bsz=8, num_updates=14, lr=1.84965e-06, gnorm=0.085, loss_scale=8, train_wall=0, wall=0\r\n2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     19 / 78 loss=1.014, ppl=2.02, wps=16565.4, ups=16.16, wpb=1024, bsz=8, num_updates=15, lr=1.97463e-06, gnorm=0.068, loss_scale=8, train_wall=0, wall=0\r\n2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     20 / 78 loss=0.967, ppl=1.95, wps=17070, ups=16.65, wpb=1024, bsz=8, num_updates=16, lr=2.0996e-06, gnorm=0.062, loss_scale=8, train_wall=0, wall=0\r\n2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     21 / 78 loss=0.97, ppl=1.96, wps=17756.2, ups=17.32, wpb=1024, bsz=8, num_updates=17, lr=2.22458e-06, gnorm=0.066, loss_scale=8, train_wall=0, wall=0\r\n2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     22 / 78 loss=0.932, ppl=1.91, wps=16320.7, ups=15.92, wpb=1024, bsz=8, num_updates=18, lr=2.34955e-06, gnorm=0.065, loss_scale=8, train_wall=0, wall=0\r\n2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     23 / 78 loss=0.963, ppl=1.95, wps=17310.4, ups=16.89, wpb=1024, bsz=8, num_updates=19, lr=2.47453e-06, gnorm=0.068, loss_scale=8, train_wall=0, wall=0\r\n2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     24 / 78 loss=0.992, ppl=1.99, wps=17010.6, ups=16.58, wpb=1024, bsz=8, num_updates=20, lr=2.5995e-06, gnorm=0.073, loss_scale=8, train_wall=0, wall=0\r\n2020-10-01 11:00:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n2020-10-01 11:00:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.945 | ppl 1.92 | wps 33933.2 | wpb 509.6 | bsz 4 | num_updates 20 | best_loss 0.945\r\n```\r\n\r\n#### Code sample\r\nSee commands above\r\n\r\n### Expected behavior\r\n\r\nLoss curve should not change when saving/reloading a checkpoint.\r\n\r\n### Environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.5.0a0+4ff3872\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 418.116.00\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy==0.4.5\r\n[pip] numpy==1.18.3\r\n[pip] numpydoc==0.9.2\r\n[pip] pytorch-lightning==0.8.1\r\n[pip] pytorch-pretrained-bert==0.6.2\r\n[pip] pytorch-transformers==1.1.0\r\n[pip] torch==1.5.0a0+4ff3872\r\n[conda] blas                      1.0                         mkl\r\n[conda] libblas                   3.8.0                    15_mkl    conda-forge\r\n[conda] libcblas                  3.8.0                    15_mkl    conda-forge\r\n[conda] liblapack                 3.8.0                    15_mkl    conda-forge\r\n[conda] magma-cuda101             2.5.2                         1    pytorch\r\n[conda] mkl                       2020.1                      217\r\n[conda] mkl-include               2020.0                      166\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch-lightning         0.8.1                     <pip>\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2681/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2678", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2678/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2678/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2678/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2678", "id": 712078151, "node_id": "MDU6SXNzdWU3MTIwNzgxNTE=", "number": 2678, "title": "Omegaconf & hydra-core missing dependencies", "user": {"login": "RobertLucian", "id": 26958764, "node_id": "MDQ6VXNlcjI2OTU4NzY0", "avatar_url": "https://avatars.githubusercontent.com/u/26958764?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RobertLucian", "html_url": "https://github.com/RobertLucian", "followers_url": "https://api.github.com/users/RobertLucian/followers", "following_url": "https://api.github.com/users/RobertLucian/following{/other_user}", "gists_url": "https://api.github.com/users/RobertLucian/gists{/gist_id}", "starred_url": "https://api.github.com/users/RobertLucian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RobertLucian/subscriptions", "organizations_url": "https://api.github.com/users/RobertLucian/orgs", "repos_url": "https://api.github.com/users/RobertLucian/repos", "events_url": "https://api.github.com/users/RobertLucian/events{/privacy}", "received_events_url": "https://api.github.com/users/RobertLucian/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2020-09-30T16:04:45Z", "updated_at": "2020-11-12T15:03:32Z", "closed_at": "2020-10-03T15:09:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nAfter installing `torch`, trying to load a model will fail due to two missing dependencies: `omegaconf` and `hydra-core`.\r\n\r\n### To Reproduce\r\n\r\nRunning the following code block doesn't work:\r\n```python\r\nimport torch\r\nroberta = torch.hub.load(\"pytorch/fairseq\", \"roberta.large\")\r\nroberta.eval()\r\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nprint(f\"using device: {device}\")\r\nroberta.to(device)\r\n```\r\n\r\nThe traceback I get is:\r\n```text\r\nTraceback (most recent call last):\r\n  File \"/src/cortex/lib/type/predictor.py\", line 111, in initialize_impl\r\n    return class_impl(**args)\r\n  File \"/mnt/project/predictor.py\", line 10, in __init__\r\n    roberta = torch.hub.load(\"pytorch/fairseq\", \"roberta.large\")\r\n  File \"/opt/conda/envs/env/lib/python3.6/site-packages/torch/hub.py\", line 349, in load\r\n    hub_module = import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)\r\n  File \"/opt/conda/envs/env/lib/python3.6/site-packages/torch/hub.py\", line 71, in import_module\r\n    spec.loader.exec_module(module)\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/hubconf.py\", line 8, in <module>\r\n    from fairseq.hub_utils import BPEHubInterface as bpe  # noqa\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/__init__.py\", line 17, in <module>\r\n    import fairseq.criterions  # noqa\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/criterions/__init__.py\", line 26, in <module>\r\n    importlib.import_module('fairseq.criterions.' + module)\r\n  File \"/opt/conda/envs/env/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/criterions/adaptive_loss.py\", line 12, in <module>\r\n    from fairseq.dataclass.data_class import DDP_BACKEND_CHOICES\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/dataclass/data_class.py\", line 12, in <module>\r\n    from fairseq.tasks import TASK_DATACLASS_REGISTRY\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/tasks/__init__.py\", line 73, in <module>\r\n    importlib.import_module('fairseq.tasks.' + task_name)\r\n  File \"/opt/conda/envs/env/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/tasks/language_modeling.py\", line 36, in <module>\r\n    from omegaconf import II\r\nModuleNotFoundError: No module named 'omegaconf'\r\n```\r\n\r\nThen, if I add `omegaconf` to the `requirements.txt` file, the next error I get is:\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"/src/cortex/lib/type/predictor.py\", line 111, in initialize_impl\r\n    return class_impl(**args)\r\n  File \"/mnt/project/predictor.py\", line 10, in __init__\r\n    roberta = torch.hub.load(\"pytorch/fairseq\", \"roberta.large\")\r\n  File \"/opt/conda/envs/env/lib/python3.6/site-packages/torch/hub.py\", line 349, in load\r\n    hub_module = import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)\r\n  File \"/opt/conda/envs/env/lib/python3.6/site-packages/torch/hub.py\", line 71, in import_module\r\n    spec.loader.exec_module(module)\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/hubconf.py\", line 8, in <module>\r\n    from fairseq.hub_utils import BPEHubInterface as bpe  # noqa\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/__init__.py\", line 17, in <module>\r\n    import fairseq.criterions  # noqa\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/criterions/__init__.py\", line 26, in <module>\r\n    importlib.import_module('fairseq.criterions.' + module)\r\n  File \"/opt/conda/envs/env/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/criterions/adaptive_loss.py\", line 12, in <module>\r\n    from fairseq.dataclass.data_class import DDP_BACKEND_CHOICES\r\n  File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/dataclass/data_class.py\", line 18, in <module>\r\n    from hydra.core.config_store import ConfigStore\r\nModuleNotFoundError: No module named 'hydra'\r\n```\r\n\r\nAdding `hydra-core` to the `requirementst.txt` fixes the issue. With both of them in, `torch` works as expected - one thing I've noticed is that when installing them, they go through a complex-looking stage of compiling stuff with g++ (takes some time).\r\n\r\n### Expected behavior\r\n\r\nExpected it to work without having to install `omegaconf` and `hydra-core`. The provided example worked without these a couple of months ago - maybe something did change in `torch`?\r\n\r\n### Environment\r\n\r\nInstalled `torch` the following way:\r\n```\r\npip install --no-cache-dir --find-links https://download.pytorch.org/whl/torch_stable.html torch==1.6.0+cu101 torchvision==0.7.0+cu101\r\n```\r\n\r\n - OS (e.g., Linux): Linux.\r\n - Python version: 3.6.9.\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: A single T4 GPU.\r\n\r\n### Additional context\r\n\r\nAs fixed in https://github.com/cortexlabs/cortex/pull/1402. Suggested by @omry to post here.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2678/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2673", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2673/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2673/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2673/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2673", "id": 710860212, "node_id": "MDU6SXNzdWU3MTA4NjAyMTI=", "number": 2673, "title": "Full-context alignment is broken in transformer_align model", "user": {"login": "senarvi", "id": 2337787, "node_id": "MDQ6VXNlcjIzMzc3ODc=", "avatar_url": "https://avatars.githubusercontent.com/u/2337787?v=4", "gravatar_id": "", "url": "https://api.github.com/users/senarvi", "html_url": "https://github.com/senarvi", "followers_url": "https://api.github.com/users/senarvi/followers", "following_url": "https://api.github.com/users/senarvi/following{/other_user}", "gists_url": "https://api.github.com/users/senarvi/gists{/gist_id}", "starred_url": "https://api.github.com/users/senarvi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/senarvi/subscriptions", "organizations_url": "https://api.github.com/users/senarvi/orgs", "repos_url": "https://api.github.com/users/senarvi/repos", "events_url": "https://api.github.com/users/senarvi/events{/privacy}", "received_events_url": "https://api.github.com/users/senarvi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-09-29T07:51:05Z", "updated_at": "2020-10-01T19:37:30Z", "closed_at": "2020-10-01T19:37:30Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\ntransformer_align model, which implements the \"Jointly Learning to Align and Translate\" paper, supports full-context alignment, meaning that the auto-regressive mask is not applied to decoder self-attention. This feature is broken in the current master. When the `--full-context-alignment` flag is given to the model, it produces the error message\r\n\r\n    TypeError: forward() got an unexpected keyword argument 'full_context_alignment'\r\n\r\nin the [forward_decoder() method of TransformerAlignModel](https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer_align.py#L69). As far as I can see, this happens because the [forward() method of TransformerDecoder](https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py#L638) doesn't have the `**extra_args` argument anymore, that was passed to the extract_features() method by the [version of the code at the time the transformer_align model was merged](https://github.com/pytorch/fairseq/blob/1c6679294848f303a361cba7b306b760e299bd9c/fairseq/models/transformer.py#L514).\r\n\r\nIdeally the [test_alignment() unit test](https://github.com/pytorch/fairseq/blob/master/tests/test_binaries.py#L428) would also be updated to test this feature.\r\n\r\n### To Reproduce\r\n\r\nFollow the [instructions](https://github.com/pytorch/fairseq/blob/master/examples/joint_alignment_translation/README.md), but additionally give the `--full-context-alignment` flag to fairseq-train. It produces the following error message and stack trace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \".../bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \".../fairseq_cli/train.py\", line 351, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \".../fairseq/distributed_utils.py\", line 254, in call_main\r\n    main(args, **kwargs)\r\n  File \".../fairseq_cli/train.py\", line 125, in main\r\n    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\r\n  File \"/usr/lib64/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \".../fairseq_cli/train.py\", line 207, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/usr/lib64/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \".../fairseq/trainer.py\", line 479, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \".../fairseq/tasks/fairseq_task.py\", line 408, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \".../lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \".../fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py\", line 36, in forward\r\n    net_output = model(**sample['net_input'])\r\n  File \".../lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \".../fairseq/models/transformer_align.py\", line 51, in forward\r\n    return self.forward_decoder(prev_output_tokens, encoder_out)\r\n  File \".../fairseq/models/transformer_align.py\", line 75, in forward_decoder\r\n    **extra_args,\r\n  File \".../lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\nTypeError: forward() got an unexpected keyword argument 'full_context_alignment'\r\n```\r\n\r\n### Expected behavior\r\n\r\nI expect fairseq-train to start training the model, like it does when I don't give the `--full-context-alignment` flag, but alignment supervised conditioned on the full target context.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Python version: 3.6.8\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2673/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2673/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2654", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2654/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2654/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2654/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2654", "id": 709143604, "node_id": "MDU6SXNzdWU3MDkxNDM2MDQ=", "number": 2654, "title": "Wav2vec Decoding with KenLM", "user": {"login": "sooftware", "id": 42150335, "node_id": "MDQ6VXNlcjQyMTUwMzM1", "avatar_url": "https://avatars.githubusercontent.com/u/42150335?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sooftware", "html_url": "https://github.com/sooftware", "followers_url": "https://api.github.com/users/sooftware/followers", "following_url": "https://api.github.com/users/sooftware/following{/other_user}", "gists_url": "https://api.github.com/users/sooftware/gists{/gist_id}", "starred_url": "https://api.github.com/users/sooftware/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sooftware/subscriptions", "organizations_url": "https://api.github.com/users/sooftware/orgs", "repos_url": "https://api.github.com/users/sooftware/repos", "events_url": "https://api.github.com/users/sooftware/events{/privacy}", "received_events_url": "https://api.github.com/users/sooftware/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2020-09-25T17:33:34Z", "updated_at": "2021-03-10T17:43:05Z", "closed_at": "2020-10-15T05:09:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I try evaluating the wav2vec-2.0 model with Ken-LM, I encounter a `Segmentation fault` error.  \r\nBy debugging, I found the location that this error occurs.  \r\n  \r\nIn `examples/speech_recognition/w2l_decoder.py`  145-153 line (W2lKenLMDecoder Class)\r\n```python\r\nfor i, (word, spellings) in enumerate(self.lexicon.items()):\r\n        word_idx = self.word_dict.get_index(word)\r\n        _, score = self.lm.score(start_state, word_idx)\r\n        for spelling in spellings:\r\n            spelling_idxs = [tgt_dict.index(token) for token in spelling]\r\n            assert (\r\n                tgt_dict.unk() not in spelling_idxs\r\n            ), f\"{spelling} {spelling_idxs}\"\r\n            self.trie.insert(spelling_idxs, word_idx, score)\r\n```\r\n  \r\nI download lexicon from https://dl.fbaipublicfiles.com/fairseq/wav2vec/librispeech_lexicon.lst  \r\nand I download kenlm from https://dl.fbaipublicfiles.com/wav2letter/lexicon_free/librispeech/models/lm/lm_librispeech_kenlm_word_4g_200kvocab.bin  \r\n  \r\nI don't know what I did wrong. Please let me know.  \r\n  \r\nThank You !!\r\n\r\n* Command\r\n```\r\npython examples/speech_recognition/infer.py $MANIFEST_PATH --task audio_pretraining \\\r\n--nbest 1 --path /data/project/rw/kaki/model/wav2vec/wav2vec2_vox_960h.pt --gen-subset test-other \\\r\n--results-path /root/ --w2l-decoder kenlm \\\r\n--lm-model /data/project/rw/kaki/model/wav2vec/lm_librispeech_kenlm_word_4g_200kvocab.bin  \\\r\n--lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 \\\r\n--post-process letter --lexicon /data/project/rw/kaki/model/wav2vec/librispeech_lexicon.lst\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2654/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2654/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2582", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2582/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2582/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2582/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2582", "id": 695263135, "node_id": "MDU6SXNzdWU2OTUyNjMxMzU=", "number": 2582, "title": "Wav2vec2 sample_negatives device mismatch", "user": {"login": "arminarj", "id": 31348550, "node_id": "MDQ6VXNlcjMxMzQ4NTUw", "avatar_url": "https://avatars.githubusercontent.com/u/31348550?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arminarj", "html_url": "https://github.com/arminarj", "followers_url": "https://api.github.com/users/arminarj/followers", "following_url": "https://api.github.com/users/arminarj/following{/other_user}", "gists_url": "https://api.github.com/users/arminarj/gists{/gist_id}", "starred_url": "https://api.github.com/users/arminarj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arminarj/subscriptions", "organizations_url": "https://api.github.com/users/arminarj/orgs", "repos_url": "https://api.github.com/users/arminarj/repos", "events_url": "https://api.github.com/users/arminarj/events{/privacy}", "received_events_url": "https://api.github.com/users/arminarj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-09-07T16:30:31Z", "updated_at": "2020-09-17T09:37:16Z", "closed_at": "2020-09-17T09:37:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIt seems there is a device mismatch at these lines, [(one)](https://github.com/pytorch/fairseq/blob/0ffb94151f597ecb677551289e7046a21fb5ebaf/fairseq/models/wav2vec/wav2vec2.py#L480) and [(two)](https://github.com/pytorch/fairseq/blob/0ffb94151f597ecb677551289e7046a21fb5ebaf/fairseq/models/wav2vec/wav2vec2.py#L495). \r\n\r\n### To Reproduce\r\n\r\nDuring the training time, when calling the `sample_negatives()` function, there is a device mismatch that occurs when the `torch.randint()`[(link)](https://github.com/pytorch/fairseq/blob/0ffb94151f597ecb677551289e7046a21fb5ebaf/fairseq/models/wav2vec/wav2vec2.py#L477-L479) and the new `tszs ` [(link)](https://github.com/pytorch/fairseq/blob/0ffb94151f597ecb677551289e7046a21fb5ebaf/fairseq/models/wav2vec/wav2vec2.py#L470-L475) using the same device.\r\nAnd it seems the problem can be solved using the `device=` tags.\r\n\r\nRegards", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2582/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2582/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2574", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2574/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2574/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2574/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2574", "id": 693121444, "node_id": "MDU6SXNzdWU2OTMxMjE0NDQ=", "number": 2574, "title": "Signature of apply_sparse_mask method in MultiheadAttention", "user": {"login": "Alvant", "id": 15067981, "node_id": "MDQ6VXNlcjE1MDY3OTgx", "avatar_url": "https://avatars.githubusercontent.com/u/15067981?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Alvant", "html_url": "https://github.com/Alvant", "followers_url": "https://api.github.com/users/Alvant/followers", "following_url": "https://api.github.com/users/Alvant/following{/other_user}", "gists_url": "https://api.github.com/users/Alvant/gists{/gist_id}", "starred_url": "https://api.github.com/users/Alvant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Alvant/subscriptions", "organizations_url": "https://api.github.com/users/Alvant/orgs", "repos_url": "https://api.github.com/users/Alvant/repos", "events_url": "https://api.github.com/users/Alvant/events{/privacy}", "received_events_url": "https://api.github.com/users/Alvant/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-09-04T12:35:32Z", "updated_at": "2020-10-15T16:27:52Z", "closed_at": "2020-10-14T16:32:36Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\r\nIn MultiheadAttention module, `apply_sparse_mask` method signature\r\nhttps://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L445\r\nis invalid (but it still works). It is written like a function, not a class method.\r\n\r\nI suggest fixing the signature from\r\n```python\r\ndef apply_sparse_mask(attn_weights, ...\r\n```\r\nto\r\n```python\r\ndef apply_sparse_mask(self, attn_weights, ...\r\n```\r\nand the line https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L320 from\r\n```python\r\nattn_weights = MultiheadAttention.apply_sparse_mask(...)\r\n```\r\nto\r\n```python\r\nattn_weights = self.apply_sparse_mask(...)\r\n```\r\n\r\nWhy am I suggesting using an instance method instead of a class- or staticmethod?\r\nI noticed that SparseMultiHeadAttention redefines this `apply_sparse_mask` method https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sparse_multihead_attention.py#L101.\r\nSo it seems that `def apply_sparse_mask(self, ...)` signature would be more appropriate to use in MultiheadAttention.\r\n\r\n### P.S.\r\n\r\nIt is not quite a bug, but it is something not quite right.\r\n\r\n### P.P.S.\r\n\r\nIf the idea is OK, I could make a pullrequest.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2574/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2566", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2566/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2566/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2566/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2566", "id": 692070204, "node_id": "MDU6SXNzdWU2OTIwNzAyMDQ=", "number": 2566, "title": "sorry it fixed", "user": {"login": "mahaoyang", "id": 17137530, "node_id": "MDQ6VXNlcjE3MTM3NTMw", "avatar_url": "https://avatars.githubusercontent.com/u/17137530?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mahaoyang", "html_url": "https://github.com/mahaoyang", "followers_url": "https://api.github.com/users/mahaoyang/followers", "following_url": "https://api.github.com/users/mahaoyang/following{/other_user}", "gists_url": "https://api.github.com/users/mahaoyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/mahaoyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mahaoyang/subscriptions", "organizations_url": "https://api.github.com/users/mahaoyang/orgs", "repos_url": "https://api.github.com/users/mahaoyang/repos", "events_url": "https://api.github.com/users/mahaoyang/events{/privacy}", "received_events_url": "https://api.github.com/users/mahaoyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-09-03T15:29:18Z", "updated_at": "2020-09-03T15:44:39Z", "closed_at": "2020-09-03T15:44:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2566/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2566/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2563", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2563/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2563/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2563/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2563", "id": 691740635, "node_id": "MDU6SXNzdWU2OTE3NDA2MzU=", "number": 2563, "title": "wav2vec2 same-quantizer return None as the input_quantizer", "user": {"login": "arminarj", "id": 31348550, "node_id": "MDQ6VXNlcjMxMzQ4NTUw", "avatar_url": "https://avatars.githubusercontent.com/u/31348550?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arminarj", "html_url": "https://github.com/arminarj", "followers_url": "https://api.github.com/users/arminarj/followers", "following_url": "https://api.github.com/users/arminarj/following{/other_user}", "gists_url": "https://api.github.com/users/arminarj/gists{/gist_id}", "starred_url": "https://api.github.com/users/arminarj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arminarj/subscriptions", "organizations_url": "https://api.github.com/users/arminarj/orgs", "repos_url": "https://api.github.com/users/arminarj/repos", "events_url": "https://api.github.com/users/arminarj/events{/privacy}", "received_events_url": "https://api.github.com/users/arminarj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-09-03T08:01:24Z", "updated_at": "2020-09-04T03:37:12Z", "closed_at": "2020-09-04T03:37:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nFor reproducing models based on the Wav2Vec2 platform, there is an option for using the input quantizer and the target quantizer and use the same quantizer if it's needed.\r\nHowever, it seems there is a small issue when using the `same_quantizer` flag. Firstly, it seems the `same_quantizer` is not defined, and secondly the `input_quantizer` flag will be `None` after that.\r\n\r\n### To Reproduce\r\n\r\nYou can reproduce a model with the same quantizer module with the below flags.\r\n\r\n\r\n#### Code sample\r\n\r\nbased on the documentation we can train the base model with these flags, [Documentation](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec)\r\n\r\n```\r\n$ python train.py --distributed-world-size 64 --distributed-port $PORT /manifest/path \\\r\n--save-dir /model/path --fp16 --num-workers 6 --task audio_pretraining --criterion wav2vec --arch wav2vec2 \\\r\n--log-keys '[\"prob_perplexity\",\"code_perplexity\",\"temp\"]' --quantize-targets --extractor-mode default \\\r\n--conv-feature-layers '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2' --final-dim 256 --latent-vars 320 \\\r\n--latent-groups 2 --latent-temp '(2,0.5,0.999995)' --infonce --optimizer adam \\\r\n--adam-betas '(0.9,0.98)' --adam-eps 1e-06 --lr-scheduler polynomial_decay --total-num-update 400000 \\\r\n--lr 0.0005 --warmup-updates 32000 --mask-length 10 --mask-prob 0.65 --mask-selection static --mask-other 0 \\\r\n--encoder-layerdrop 0.05 --dropout-input 0.1 --dropout-features 0.1 --feature-grad-mult 0.1 \\\r\n--loss-weights '[0.1, 10]' --conv-pos 128 --conv-pos-groups 16 --num-negatives 100 --cross-sample-negatives 0 \\\r\n--max-sample-size 250000 --min-sample-size 32000 --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n--max-tokens 1400000 --max-update 400000 --skip-invalid-size-inputs-valid-test --ddp-backend no_c10d\r\n ```\r\n\r\nAnd if we want to use the same quantizer module as the `input_ quantizer` and `target_ quantizer`, we should use these additional flags,\r\n ```\r\n--quantize-input  --quantize-targets --same_quantizer\r\n```\r\n\r\n### Expected behavior\r\n\r\n1. We expect there should be a flag called `same_quantizer` as this line explained [link](https://github.com/pytorch/fairseq/blob/5d7ed6ab4f92d20ad10f8f792b8703e260a938ac/fairseq/models/wav2vec/wav2vec2.py#L357)\r\n\r\n2. There should exist one same Quantizer module in the `self.input_quantizer` and the `self.quantizer` when we call the `--same-quantizer` flag. However, the `self.input_quantizer` becomes the `None` as the `self.quantizer` will be defined in the next lines.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2563/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2563/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2554", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2554/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2554/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2554/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2554", "id": 690135698, "node_id": "MDU6SXNzdWU2OTAxMzU2OTg=", "number": 2554, "title": "get_normalized_probs_scriptable seems broken", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2020-09-01T13:17:58Z", "updated_at": "2021-03-29T09:22:22Z", "closed_at": "2020-10-13T15:11:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://github.com/pytorch/fairseq/blob/2887663/fairseq/models/fairseq_model.py#L59:\r\n```python\r\n  def get_normalized_probs_scriptable(\r\n        self,\r\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\r\n        log_probs: bool,\r\n        sample: Optional[Dict[str, Tensor]] = None,\r\n    ):\r\n        \"\"\"Scriptable helper function for get_normalized_probs in ~BaseFairseqModel\"\"\"\r\n        if hasattr(self, \"decoder\"):\r\n            return self.decoder.get_normalized_probs(net_output, log_probs, sample)\r\n        elif torch.is_tensor(net_output):\r\n            logits = net_output.float()\r\n            if log_probs:\r\n                return F.log_softmax(logits, dim=-1)\r\n            else:\r\n                return F.softmax(logits, dim=-1)\r\n        raise NotImplementedError\r\n```\r\n\r\nIt accepts `net_output` as tuple and later checks it to be a tensor. Is it okay?", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2554/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2543", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2543/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2543/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2543/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2543", "id": 688896376, "node_id": "MDU6SXNzdWU2ODg4OTYzNzY=", "number": 2543, "title": "./examples/speech_recognition/datasets/prepare-librispeech.sh bug", "user": {"login": "sooftware", "id": 42150335, "node_id": "MDQ6VXNlcjQyMTUwMzM1", "avatar_url": "https://avatars.githubusercontent.com/u/42150335?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sooftware", "html_url": "https://github.com/sooftware", "followers_url": "https://api.github.com/users/sooftware/followers", "following_url": "https://api.github.com/users/sooftware/following{/other_user}", "gists_url": "https://api.github.com/users/sooftware/gists{/gist_id}", "starred_url": "https://api.github.com/users/sooftware/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sooftware/subscriptions", "organizations_url": "https://api.github.com/users/sooftware/orgs", "repos_url": "https://api.github.com/users/sooftware/repos", "events_url": "https://api.github.com/users/sooftware/events{/privacy}", "received_events_url": "https://api.github.com/users/sooftware/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "okhonko", "id": 8376885, "node_id": "MDQ6VXNlcjgzNzY4ODU=", "avatar_url": "https://avatars.githubusercontent.com/u/8376885?v=4", "gravatar_id": "", "url": "https://api.github.com/users/okhonko", "html_url": "https://github.com/okhonko", "followers_url": "https://api.github.com/users/okhonko/followers", "following_url": "https://api.github.com/users/okhonko/following{/other_user}", "gists_url": "https://api.github.com/users/okhonko/gists{/gist_id}", "starred_url": "https://api.github.com/users/okhonko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/okhonko/subscriptions", "organizations_url": "https://api.github.com/users/okhonko/orgs", "repos_url": "https://api.github.com/users/okhonko/repos", "events_url": "https://api.github.com/users/okhonko/events{/privacy}", "received_events_url": "https://api.github.com/users/okhonko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "okhonko", "id": 8376885, "node_id": "MDQ6VXNlcjgzNzY4ODU=", "avatar_url": "https://avatars.githubusercontent.com/u/8376885?v=4", "gravatar_id": "", "url": "https://api.github.com/users/okhonko", "html_url": "https://github.com/okhonko", "followers_url": "https://api.github.com/users/okhonko/followers", "following_url": "https://api.github.com/users/okhonko/following{/other_user}", "gists_url": "https://api.github.com/users/okhonko/gists{/gist_id}", "starred_url": "https://api.github.com/users/okhonko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/okhonko/subscriptions", "organizations_url": "https://api.github.com/users/okhonko/orgs", "repos_url": "https://api.github.com/users/okhonko/repos", "events_url": "https://api.github.com/users/okhonko/events{/privacy}", "received_events_url": "https://api.github.com/users/okhonko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-08-31T04:56:18Z", "updated_at": "2020-09-08T03:36:00Z", "closed_at": "2020-09-08T03:36:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIn ./examples/speech_recognition/datasets/prepare-librispeech.sh, line 21 need to change fairseq-py => fairseq", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2543/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2542", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2542/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2542/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2542/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2542", "id": 688877512, "node_id": "MDU6SXNzdWU2ODg4Nzc1MTI=", "number": 2542, "title": "Cannot reduce \u201ctokens-per-sample\u201d in inference for the transformer language model", "user": {"login": "jungokasai", "id": 12591401, "node_id": "MDQ6VXNlcjEyNTkxNDAx", "avatar_url": "https://avatars.githubusercontent.com/u/12591401?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jungokasai", "html_url": "https://github.com/jungokasai", "followers_url": "https://api.github.com/users/jungokasai/followers", "following_url": "https://api.github.com/users/jungokasai/following{/other_user}", "gists_url": "https://api.github.com/users/jungokasai/gists{/gist_id}", "starred_url": "https://api.github.com/users/jungokasai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jungokasai/subscriptions", "organizations_url": "https://api.github.com/users/jungokasai/orgs", "repos_url": "https://api.github.com/users/jungokasai/repos", "events_url": "https://api.github.com/users/jungokasai/events{/privacy}", "received_events_url": "https://api.github.com/users/jungokasai/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-31T03:54:57Z", "updated_at": "2020-09-01T07:00:56Z", "closed_at": "2020-08-31T19:17:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## :bug: Bug with `tokens-per-sample` in evaluation.\r\n<!-- A clear and concise description of what the bug is. -->\r\n### To Reproduce\r\n```bash\r\nfairseq-train --task language_modeling data-bin/wikitext-103 \\\r\n--save-dir path_to_model_dir --arch transformer_lm_wiki103 --max-update 286000 --max-lr 1.0 \\\r\n--t-mult 2 --lr-period-updates 270000 --lr-scheduler cosine --lr-shrink 0.75 --warmup-updates 16000 \\\r\n--warmup-init-lr 1e-07 --min-lr 1e-09 --optimizer nag --lr 0.0001 --clip-norm 0.1 --criterion adaptive_loss \\\r\n--max-tokens 3072 --update-freq 3 --tokens-per-sample 3072 --seed 1 \\\r\n--sample-break-mode none --skip-invalid-size-inputs-valid-test --ddp-backend=no_c10d --fp16 \r\n\r\nfairseq-eval-lm data-bin/wikitext-103 \\\r\n    --path path_to_model \\\r\n    --max-sentences 1 \\\r\n    --tokens-per-sample 512 \\\r\n    --context-window 0\r\n```\r\nIn language model evaluation, I believe we can use a smaller `tokens-per-sample` compared to training. In this case, trained with `3072` but evaluated with `512`. But then I found that it\u2019s still using 3072-word segments in evaluation.\r\n```bash\r\nIn [2]: sample\r\nOut[2]:\r\n{'id': tensor([34], device='cuda:0'),\r\n 'nsentences': 1,\r\n 'ntokens': 3072,\r\n 'net_input': {'src_tokens': tensor([[   10,     4, 15793,  ...,     2,     2,    12]], device='cuda:0'),\r\n  'src_lengths': tensor([3072], device='cuda:0')},\r\n 'target': tensor([[    4, 15793,     5,  ...,     2,    12,    12]], device='cuda:0')}\r\n```\r\nIf I force --max-tokens by:\r\n```bash\r\nfairseq-eval-lm data-bin/wikitext-103 \\\r\n    --path path_to_model \\\r\n    --max-tokens 512 \\\r\n    --tokens-per-sample 512 \\\r\n    --context-window 0\r\n```\r\nI get the following error:\r\n```bash\r\n  File \"fairseq/data/data_utils_fast.pyx\", line 50, in fairseq.data.data_utils_fast.batch_by_size_fast\r\n    assert max_tokens <= 0 or sample_len <= max_tokens, (\r\nAssertionError: sentence at index 79 of size 2881 exceeds max_tokens limit of 512!\r\n```\r\nSo I suspect that `--sample-break-mode none` is not properly working during evaluation; it shouldn't impose respecting sentence boundaries.  \r\n\r\n### Environment\r\n - fairseq Version (e.g., 1.0 or master): '0.9.0'\r\n - PyTorch Version (e.g., 1.0): '1.5.1+cu101'\r\n - OS (e.g., Linux): Ubuntu 18.04.3 LTS\r\n - How you installed fairseq (`pip`, source): Source\r\n - Build command you used (if compiling from source): \r\n```\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./\r\n```\r\n - Python version: 3.7.6\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2542/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2542/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2519", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2519/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2519/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2519/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2519", "id": 684751669, "node_id": "MDU6SXNzdWU2ODQ3NTE2Njk=", "number": 2519, "title": "issue while decoding wav2vec model", "user": {"login": "MrityunjoyS", "id": 30874320, "node_id": "MDQ6VXNlcjMwODc0MzIw", "avatar_url": "https://avatars.githubusercontent.com/u/30874320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MrityunjoyS", "html_url": "https://github.com/MrityunjoyS", "followers_url": "https://api.github.com/users/MrityunjoyS/followers", "following_url": "https://api.github.com/users/MrityunjoyS/following{/other_user}", "gists_url": "https://api.github.com/users/MrityunjoyS/gists{/gist_id}", "starred_url": "https://api.github.com/users/MrityunjoyS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MrityunjoyS/subscriptions", "organizations_url": "https://api.github.com/users/MrityunjoyS/orgs", "repos_url": "https://api.github.com/users/MrityunjoyS/repos", "events_url": "https://api.github.com/users/MrityunjoyS/events{/privacy}", "received_events_url": "https://api.github.com/users/MrityunjoyS/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2020-08-24T15:12:58Z", "updated_at": "2020-12-13T22:55:15Z", "closed_at": "2020-10-29T00:21:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to do **'Evaluating a CTC model'** step :- \r\nRunning below command --\r\n\r\n```\r\nPYTHONPATH=/path/fairseq/ python3 examples/speech_recognition/infer.py /path/audio_file/wav2vec/ --task audio_pretraining \\\r\n--nbest 1 --path /path/model_exportdir1/checkpoint_best.pt --gen-subset valid --results-path /path/audio_file/wav2vec/tmp/am/ --w2l-decoder kenlm \\\r\n--lm-model /path/audio_file/wav2vec/lm_librispeech_word_transformer.pt --lexicon=/path/audio_file/wav2vec/librispeech_lexicon.lst --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 \\\r\n--post-process letter\r\n```\r\n\r\nGetting below errors --\r\n\r\n> INFO:fairseq.data.audio.raw_audio_dataset:loaded 2674, skipped 0 samples\r\n> INFO:__main__:| /path/audio_file/wav2vec/ train 2674 examples\r\n> INFO:__main__:| decoding with criterion ctc\r\n> INFO:__main__:| loading model(s) from /path/model_exportdir1/checkpoint_best.pt\r\n> Loading the LM will be faster if you build a binary file.\r\n> Reading /path/audio_file/wav2vec/lm_librispeech_word_transformer.pt\r\n> ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\r\n> ****************************************************************************************************\r\n> **Traceback (most recent call last):\r\n>   File \"examples/speech_recognition/infer.py\", line 428, in <module>\r\n>     cli_main()\r\n>   File \"examples/speech_recognition/infer.py\", line 424, in cli_main\r\n>     main(args)\r\n>   File \"examples/speech_recognition/infer.py\", line 300, in main\r\n>     generator = build_generator(args)\r\n>   File \"examples/speech_recognition/infer.py\", line 292, in build_generator\r\n>     return W2lKenLMDecoder(args, task.target_dictionary)\r\n>   File \"/path/fairseq/examples/speech_recognition/w2l_decoder.py\", line 141, in __init__\r\n>     self.lm = KenLM(args.kenlm_model, self.word_dict)\r\n> RuntimeError**\r\n\r\n\r\nCan't get what I'm missing here", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2519/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2512", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2512/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2512/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2512/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2512", "id": 683948102, "node_id": "MDU6SXNzdWU2ODM5NDgxMDI=", "number": 2512, "title": "Model can't locate bindings, even after installation", "user": {"login": "MrityunjoyS", "id": 30874320, "node_id": "MDQ6VXNlcjMwODc0MzIw", "avatar_url": "https://avatars.githubusercontent.com/u/30874320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MrityunjoyS", "html_url": "https://github.com/MrityunjoyS", "followers_url": "https://api.github.com/users/MrityunjoyS/followers", "following_url": "https://api.github.com/users/MrityunjoyS/following{/other_user}", "gists_url": "https://api.github.com/users/MrityunjoyS/gists{/gist_id}", "starred_url": "https://api.github.com/users/MrityunjoyS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MrityunjoyS/subscriptions", "organizations_url": "https://api.github.com/users/MrityunjoyS/orgs", "repos_url": "https://api.github.com/users/MrityunjoyS/repos", "events_url": "https://api.github.com/users/MrityunjoyS/events{/privacy}", "received_events_url": "https://api.github.com/users/MrityunjoyS/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-22T07:19:25Z", "updated_at": "2021-07-27T17:02:17Z", "closed_at": "2020-08-25T01:15:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've successfully installed **Python Bindings**. :- \r\n```\r\n****@**-gpu:~/wav2letter/bindings/python$ pip install -e .\r\nDefaulting to user installation because normal site-packages is not writeable\r\nObtaining file:///path/wav2letter/bindings/python\r\nInstalling collected packages: wav2letter\r\n  Running setup.py develop for wav2letter\r\nSuccessfully installed wav2letter\r\n```\r\n\r\nStill  while running below command :- \r\n\r\n`****@**-gpu:~/wav2letter/bindings/python$ PYTHONPATH=/path/fairseq python3 examples/speech_recognition/infer.py /path/audio_file/wav2vec/ --task audio_pretraining --nbest 1 --path /path/audio_file/wav2vec_small.pt --gen-subset valid --results-path /path/audio_file/wav2vec/tmp/am/ --w2l-decoder viterbi --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --post-process letter`\r\n\r\nGetting below errors :- \r\n\r\n> INFO:fairseq.data.audio.raw_audio_dataset:loaded 29, skipped 0 samples\r\n> INFO:__main__:| /path/audio_file/wav2vec/ valid 29 examples\r\n> INFO:__main__:| decoding with criterion ctc\r\n> INFO:__main__:| loading model(s) from /path/audio_file/wav2vec_small.pt\r\n> /path/fairseq/examples/speech_recognition/w2l_decoder.py:39: UserWarning: wav2letter python bindings are required to use this functionality. Please install from https://github.com/facebookresearch/wav2letter/wiki/Python-bindings\r\n>   \"wav2letter python bindings are required to use this functionality. Please install from https://github.com/facebookresearch/wav2letter/wiki/Python-bindings\"\r\n> Traceback (most recent call last):                                                                                                            \r\n>   File \"examples/speech_recognition/infer.py\", line 428, in <module>\r\n>     cli_main()\r\n>   File \"examples/speech_recognition/infer.py\", line 424, in cli_main\r\n>     main(args)\r\n>   File \"examples/speech_recognition/infer.py\", line 361, in main\r\n>     hypos = task.inference_step(generator, models, sample, prefix_tokens)\r\n>   File \"/path/fairseq/fairseq/tasks/fairseq_task.py\", line 400, in inference_step\r\n>     return generator.generate(models, sample, prefix_tokens=prefix_tokens)\r\n>   File \"/path/fairseq/examples/speech_recognition/w2l_decoder.py\", line 76, in generate\r\n>     emissions = self.get_emissions(models, encoder_input)\r\n>   File \"/path/fairseq/examples/speech_recognition/w2l_decoder.py\", line 84, in get_emissions\r\n>     emissions = models[0].get_normalized_probs(encoder_out, log_probs=True)\r\n>   File \"/path/fairseq/fairseq/models/fairseq_model.py\", line 53, in get_normalized_probs\r\n>     return self.get_normalized_probs_scriptable(net_output, log_probs, sample)\r\n>   File \"/path/fairseq/fairseq/models/fairseq_model.py\", line 74, in get_normalized_probs_scriptable\r\n>     raise NotImplementedError\r\n> NotImplementedError\r\n\r\nCan you please suggest what might be the issue @alexeib  . Thanks in advance.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2512/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2508", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2508/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2508/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2508/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2508", "id": 683326891, "node_id": "MDU6SXNzdWU2ODMzMjY4OTE=", "number": 2508, "title": "Error while trying to make Python Bindings", "user": {"login": "MrityunjoyS", "id": 30874320, "node_id": "MDQ6VXNlcjMwODc0MzIw", "avatar_url": "https://avatars.githubusercontent.com/u/30874320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MrityunjoyS", "html_url": "https://github.com/MrityunjoyS", "followers_url": "https://api.github.com/users/MrityunjoyS/followers", "following_url": "https://api.github.com/users/MrityunjoyS/following{/other_user}", "gists_url": "https://api.github.com/users/MrityunjoyS/gists{/gist_id}", "starred_url": "https://api.github.com/users/MrityunjoyS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MrityunjoyS/subscriptions", "organizations_url": "https://api.github.com/users/MrityunjoyS/orgs", "repos_url": "https://api.github.com/users/MrityunjoyS/repos", "events_url": "https://api.github.com/users/MrityunjoyS/events{/privacy}", "received_events_url": "https://api.github.com/users/MrityunjoyS/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-21T06:18:00Z", "updated_at": "2020-09-12T12:34:42Z", "closed_at": "2020-08-22T06:37:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to build python bindings for fairseq model, to use it for speech recognition modules.\r\nBut while trying to run the command \r\n`pip install -e .` , \r\ngetting below errors :-\r\n\r\n\r\n```\r\nDefaulting to user installation because normal site-packages is not writeable\r\nObtaining file:///home/mrityunjoy/wav2letter/bindings/python\r\nInstalling collected packages: wav2letter\r\nRunning setup.py develop for wav2letter\r\nERROR: Command errored out with exit status 1:\r\ncommand: /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/home/mrityunjoy/wav2letter/bindings/python/setup.py'\"'\"'; file='\"'\"'/home/mrityunjoy/wav2letter/bindings/python/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(file);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, file, '\"'\"'exec'\"'\"'))' develop --no-deps --user --prefix=\r\ncwd: /home/mrityunjoy/wav2letter/bindings/python/\r\nComplete output (436 lines):\r\nrunning develop\r\nrunning egg_info\r\nwriting wav2letter.egg-info/PKG-INFO\r\nwriting dependency_links to wav2letter.egg-info/dependency_links.txt\r\nwriting top-level names to wav2letter.egg-info/top_level.txt\r\nreading manifest file 'wav2letter.egg-info/SOURCES.txt'\r\nwriting manifest file 'wav2letter.egg-info/SOURCES.txt'\r\nrunning build_ext\r\n-- The C compiler identification is GNU 7.4.0\r\n-- The CXX compiler identification is GNU 7.4.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of void*\r\n-- Check size of void* - done\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m]\r\n-- Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m]\r\n-- Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m]\r\n-- Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m]\r\n-- Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m]\r\n-- Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m]\r\n-- Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m]\r\n-- Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m]\r\n-- Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m]\r\n-- Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m]\r\n-- Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m]\r\n-- Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m]\r\n-- Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m]\r\n-- Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m]\r\n-- Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m]\r\n-- Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m]\r\n-- Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m]\r\n-- Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m]\r\n-- Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m]\r\n-- Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m]\r\n-- Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m]\r\n-- Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m]\r\n-- Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m]\r\n-- Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m]\r\n-- Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m]\r\n-- Library mkl_intel: not found\r\n-- Checking for [mkl - guide - pthread - m]\r\n-- Library mkl: not found\r\n-- MKL library not found\r\nCMake Warning at CMakeLists.txt:32 (message):\r\nMKL not found; forcing W2L_LIBRARIES_USE_MKL=OFF.\r\n\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE\r\n-- CUDA found (library: /usr/local/cuda/lib64/libcudart_static.a;Threads::Threads;dl;/usr/lib/x86_64-linux-gnu/librt.so include: /usr/local/cuda/include)\r\n-- CUDA architecture flags: -gencodearch=compute_30,code=sm_30-gencodearch=compute_35,code=sm_35-gencodearch=compute_50,code=sm_50-gencodearch=compute_52,code=sm_52-gencodearch=compute_60,code=sm_60-gencodearch=compute_61,code=sm_61-gencodearch=compute_70,code=sm_70-gencodearch=compute_75,code=sm_75-gencodearch=compute_70,code=compute_70-gencodearch=compute_75,code=compute_75\r\n-- Found PkgConfig: /usr/bin/pkg-config (found version \"0.29.1\")\r\n-- Checking for module 'cblas'\r\n--   No package 'cblas' found\r\n-- Checking for [Accelerate]\r\n-- Checking for [vecLib]\r\n-- Checking for [cblas - atlas]\r\n-- Includes found\r\n-- Checking for [openblas]\r\n-- Includes found\r\n-- Looking for cblas_dgemm\r\n-- Looking for cblas_dgemm - found\r\n-- CBLAS Symbols FOUND\r\n-- CBLAS library found\r\n-- CBLAS found (include: /usr/include/x86_64-linux-gnu, library: /usr/lib/x86_64-linux-gnu/libopenblas.so)\r\n-- Found FFTW: /usr/include\r\n-- FFTW found\r\n-- Looking for KenLM\r\n-- Using kenlm library found in /home/mrityunjoy/kenlm/build/lib/libkenlm.a\r\n-- Using kenlm utils library found in /home/mrityunjoy/kenlm/build/lib/libkenlm.a\r\n-- kenlm lm/model.hh found in /home/mrityunjoy/kenlm/lm/model.hh\r\n-- Found kenlm: /home/mrityunjoy/kenlm\r\n-- Found kenlm (include: /home/mrityunjoy/kenlm, library: /home/mrityunjoy/kenlm/build/lib/libkenlm.a;/home/mrityunjoy/kenlm/build/lib/libkenlm_util.a)\r\n-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\r\n-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\r\n-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\r\n-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\r\n-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\r\n-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\r\n-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.2\")\r\n-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\")\r\n-- Looking for BZ2_bzCompressInit\r\n-- Looking for BZ2_bzCompressInit - found\r\n-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\")\r\n-- Found PythonInterp: /usr/bin/python3 (found version \"3.6.9\")\r\n-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.6m.so\r\n-- Performing Test HAS_FLTO\r\n-- Performing Test HAS_FLTO - Success\r\n-- LTO enabled\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/mrityunjoy/wav2letter/bindings/python/build/temp.linux-x86_64-3.6\r\nScanning dependencies of target pybind11\r\nScanning dependencies of target CUB\r\n[  1%] Creating directories for 'CUB'\r\n[  1%] Creating directories for 'pybind11'\r\n[  2%] Performing download step (git clone) for 'CUB'\r\n[  2%] Performing download step (git clone) for 'pybind11'\r\nCloning into 'pybind11'...\r\nCloning into 'CUB'...\r\nNote: checking out '9a19306fbf30642ca331d0ec88e7da54a96860f9'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b <new-branch-name>\r\n\r\nHEAD is now at 9a19306 bump version to 2.2.4\r\nSubmodule 'tools/clang' (https://github.com/wjakob/clang-cindex-python3) registered for path 'tools/clang'\r\nCloning into '/home/mrityunjoy/wav2letter/bindings/python/build/temp.linux-x86_64-3.6/bindings/python/pybind11/src/pybind11/tools/clang'...\r\nSubmodule path 'tools/clang': checked out '6a00cbc4a9b8e68b71caf7f774b3f9c753ae84d5'\r\n[  3%] Performing update step for 'pybind11'\r\n[  3%] No patch step for 'pybind11'\r\n[  4%] No configure step for 'pybind11'\r\n[  5%] No build step for 'pybind11'\r\n[  6%] No install step for 'pybind11'\r\n[  6%] Completed 'pybind11'\r\n[  6%] Built target pybind11\r\nNote: checking out 'c3cceac115c072fb63df1836ff46d8c60d9eb304'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b <new-branch-name>\r\n\r\nHEAD is now at c3cceac1 update readme to 1.8.0\r\n[  6%] Performing update step for 'CUB'\r\n[  7%] No patch step for 'CUB'\r\n[  8%] No configure step for 'CUB'\r\n[  8%] No build step for 'CUB'\r\n[  9%] No install step for 'CUB'\r\n[  9%] Completed 'CUB'\r\n[  9%] Built target CUB\r\n[ 10%] Building NVCC (Device) object src/libraries/criterion/CMakeFiles/w2l-criterion-library-cuda.dir/cuda/w2l-criterion-library-cuda_generated_ViterbiPath.cu.o\r\n[ 10%] Building NVCC (Device) object src/libraries/criterion/CMakeFiles/w2l-criterion-library-cuda.dir/cuda/w2l-criterion-library-cuda_generated_CriterionUtils.cu.o\r\n[ 10%] Building NVCC (Device) object src/libraries/criterion/CMakeFiles/w2l-criterion-library-cuda.dir/cuda/w2l-criterion-library-cuda_generated_FullConnectionCriterion.cu.o\r\n[ 11%] Building NVCC (Device) object src/libraries/criterion/CMakeFiles/w2l-criterion-library-cuda.dir/cuda/w2l-criterion-library-cuda_generated_ForceAlignmentCriterion.cu.o\r\nScanning dependencies of target w2l-criterion-library-cuda\r\n[ 11%] Linking CXX static library libw2l-criterion-library-cuda.a\r\n[ 11%] Built target w2l-criterion-library-cuda\r\nScanning dependencies of target wav2letter-libraries\r\n[ 12%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/common/Dictionary.cpp.o\r\n[ 13%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/common/Utils.cpp.o\r\n[ 13%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/common/WordUtils.cpp.o\r\n[ 14%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/criterion/cpu/CriterionUtils.cpp.o\r\n[ 14%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/criterion/cpu/ForceAlignmentCriterion.cpp.o\r\n[ 15%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/criterion/cpu/ConnectionistTemporalClassificationCriterion.cpp.o\r\n[ 16%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/criterion/cpu/FullConnectionCriterion.cpp.o\r\n[ 16%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/criterion/cpu/ViterbiPath.cpp.o\r\n[ 17%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/decoder/LexiconDecoder.cpp.o\r\n[ 17%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/decoder/LexiconFreeDecoder.cpp.o\r\n[ 18%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/decoder/LexiconSeq2SeqDecoder.cpp.o\r\n[ 19%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/decoder/LexiconFreeSeq2SeqDecoder.cpp.o\r\n[ 19%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/decoder/Trie.cpp.o\r\n[ 20%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/decoder/Utils.cpp.o\r\n[ 20%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/lm/KenLM.cpp.o\r\n[ 21%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/lm/ConvLM.cpp.o\r\n[ 21%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/lm/ZeroLM.cpp.o\r\n[ 22%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/Ceplifter.cpp.o\r\n[ 23%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/Dct.cpp.o\r\n[ 23%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/Derivatives.cpp.o\r\n[ 24%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/Dither.cpp.o\r\n[ 24%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/Mfcc.cpp.o\r\n[ 25%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/Mfsc.cpp.o\r\n[ 26%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/PowerSpectrum.cpp.o\r\n[ 26%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/PreEmphasis.cpp.o\r\n[ 27%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/SpeechUtils.cpp.o\r\n[ 27%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/TriFilterbank.cpp.o\r\n[ 28%] Building CXX object src/libraries/CMakeFiles/wav2letter-libraries.dir/feature/Windowing.cpp.o\r\n[ 29%] Linking CXX static library libwav2letter-libraries.a\r\n[ 29%] Built target wav2letter-libraries\r\nScanning dependencies of target _decoder\r\nScanning dependencies of target _common\r\nScanning dependencies of target _feature\r\nScanning dependencies of target _criterion\r\n[ 29%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/wav2letter/_decoder.cpp.o\r\n[ 30%] Building CXX object bindings/python/CMakeFiles/_common.dir/wav2letter/_common.cpp.o\r\n[ 31%] Building CXX object bindings/python/CMakeFiles/_feature.dir/wav2letter/_feature.cpp.o\r\n[ 31%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/wav2letter/_criterion.cpp.o\r\n[ 31%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/common/Dictionary.cpp.o\r\n[ 32%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/common/Dictionary.cpp.o\r\n[ 33%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/common/Utils.cpp.o\r\n[ 34%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/common/Utils.cpp.o\r\n[ 35%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/common/WordUtils.cpp.o\r\n[ 35%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/common/WordUtils.cpp.o\r\n[ 35%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/common/Dictionary.cpp.o\r\n[ 35%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/criterion/cpu/CriterionUtils.cpp.o\r\n[ 36%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/criterion/cpu/ForceAlignmentCriterion.cpp.o\r\n[ 37%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/criterion/cpu/CriterionUtils.cpp.o\r\n[ 37%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/criterion/cpu/ForceAlignmentCriterion.cpp.o\r\n[ 38%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/common/Utils.cpp.o\r\n[ 39%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/common/Dictionary.cpp.o\r\n[ 39%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/criterion/cpu/ConnectionistTemporalClassificationCriterion.cpp.o\r\n[ 40%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/criterion/cpu/FullConnectionCriterion.cpp.o\r\n[ 41%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/criterion/cpu/ConnectionistTemporalClassificationCriterion.cpp.o\r\n[ 42%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/criterion/cpu/FullConnectionCriterion.cpp.o\r\n[ 43%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/criterion/cpu/ViterbiPath.cpp.o\r\n[ 44%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/common/WordUtils.cpp.o\r\n[ 44%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/decoder/LexiconDecoder.cpp.o\r\n[ 44%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/common/Utils.cpp.o\r\n[ 44%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/criterion/cpu/ViterbiPath.cpp.o\r\n[ 45%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/decoder/LexiconDecoder.cpp.o\r\n[ 46%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/common/WordUtils.cpp.o\r\n[ 46%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/criterion/cpu/CriterionUtils.cpp.o\r\n[ 47%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/decoder/LexiconFreeDecoder.cpp.o\r\n[ 48%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/criterion/cpu/ForceAlignmentCriterion.cpp.o\r\n[ 48%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/decoder/LexiconFreeDecoder.cpp.o\r\n[ 48%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/criterion/cpu/ConnectionistTemporalClassificationCriterion.cpp.o\r\n[ 49%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/criterion/cpu/FullConnectionCriterion.cpp.o\r\n[ 49%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/criterion/cpu/CriterionUtils.cpp.o\r\n[ 49%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/decoder/LexiconSeq2SeqDecoder.cpp.o\r\n[ 50%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/criterion/cpu/ForceAlignmentCriterion.cpp.o\r\n[ 50%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/criterion/cpu/ViterbiPath.cpp.o\r\n[ 51%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/decoder/LexiconSeq2SeqDecoder.cpp.o\r\n[ 52%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/decoder/LexiconDecoder.cpp.o\r\n[ 53%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/criterion/cpu/ConnectionistTemporalClassificationCriterion.cpp.o\r\n[ 53%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/criterion/cpu/FullConnectionCriterion.cpp.o\r\n[ 54%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/criterion/cpu/ViterbiPath.cpp.o\r\n[ 54%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/decoder/LexiconDecoder.cpp.o\r\n[ 55%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/decoder/LexiconFreeDecoder.cpp.o\r\n[ 56%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/decoder/LexiconFreeSeq2SeqDecoder.cpp.o\r\n[ 57%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/decoder/LexiconFreeSeq2SeqDecoder.cpp.o\r\n[ 58%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/decoder/LexiconFreeDecoder.cpp.o\r\n[ 58%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/decoder/LexiconSeq2SeqDecoder.cpp.o\r\n[ 58%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/decoder/Trie.cpp.o\r\n[ 58%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/decoder/Trie.cpp.o\r\n[ 59%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/decoder/LexiconSeq2SeqDecoder.cpp.o\r\n[ 60%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/decoder/Utils.cpp.o\r\n[ 61%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/lm/KenLM.cpp.o\r\n[ 62%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/decoder/Utils.cpp.o\r\n[ 62%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/lm/KenLM.cpp.o\r\n[ 63%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/decoder/LexiconFreeSeq2SeqDecoder.cpp.o\r\n[ 63%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/lm/ConvLM.cpp.o\r\n[ 64%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/lm/ConvLM.cpp.o\r\n[ 64%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/decoder/LexiconFreeSeq2SeqDecoder.cpp.o\r\n[ 64%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/decoder/Trie.cpp.o\r\n[ 65%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/lm/ZeroLM.cpp.o\r\n[ 66%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/decoder/Utils.cpp.o\r\n[ 66%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/lm/ZeroLM.cpp.o\r\n[ 67%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/lm/KenLM.cpp.o\r\n[ 67%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/Ceplifter.cpp.o\r\n[ 68%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/decoder/Trie.cpp.o\r\n[ 69%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/Dct.cpp.o\r\n[ 70%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/Ceplifter.cpp.o\r\n[ 71%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/Derivatives.cpp.o\r\n[ 72%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/Dct.cpp.o\r\n[ 72%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/decoder/Utils.cpp.o\r\n[ 73%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/Dither.cpp.o\r\n[ 73%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/lm/KenLM.cpp.o\r\n[ 73%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/lm/ConvLM.cpp.o\r\n[ 73%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/Derivatives.cpp.o\r\n[ 74%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/Dither.cpp.o\r\n[ 75%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/Mfcc.cpp.o\r\n[ 75%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/Mfcc.cpp.o\r\n[ 75%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/Mfsc.cpp.o\r\n[ 76%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/lm/ConvLM.cpp.o\r\n[ 77%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/lm/ZeroLM.cpp.o\r\n[ 78%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/Mfsc.cpp.o\r\n[ 79%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/PowerSpectrum.cpp.o\r\n[ 79%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/Ceplifter.cpp.o\r\n[ 80%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/PowerSpectrum.cpp.o\r\n[ 81%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/PreEmphasis.cpp.o\r\n[ 82%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/Dct.cpp.o\r\n[ 82%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/lm/ZeroLM.cpp.o\r\n[ 83%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/Derivatives.cpp.o\r\n[ 83%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/SpeechUtils.cpp.o\r\n[ 83%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/PreEmphasis.cpp.o\r\n[ 83%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/Dither.cpp.o\r\n[ 84%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/TriFilterbank.cpp.o\r\n[ 85%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/Ceplifter.cpp.o\r\n[ 86%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/SpeechUtils.cpp.o\r\n[ 87%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/Mfcc.cpp.o\r\n[ 87%] Building CXX object bindings/python/CMakeFiles/_common.dir/__/__/src/libraries/feature/Windowing.cpp.o\r\n[ 87%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/Dct.cpp.o\r\n[ 87%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/TriFilterbank.cpp.o\r\n[ 88%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/Derivatives.cpp.o\r\n[ 89%] Linking CXX shared module ../../../../wav2letter/_common.cpython-36m-x86_64-linux-gnu.so\r\n[ 89%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/Mfsc.cpp.o\r\n[ 90%] Building CXX object bindings/python/CMakeFiles/_criterion.dir/__/__/src/libraries/feature/Windowing.cpp.o\r\n[ 90%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/Dither.cpp.o\r\n[ 91%] Linking CXX shared module ../../../../wav2letter/_criterion.cpython-36m-x86_64-linux-gnu.so\r\n[ 92%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/PowerSpectrum.cpp.o\r\n[ 93%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/Mfcc.cpp.o\r\n[ 94%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/Mfsc.cpp.o\r\n[ 94%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/PreEmphasis.cpp.o\r\n[ 94%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/PowerSpectrum.cpp.o\r\n[ 95%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/SpeechUtils.cpp.o\r\n[ 96%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/TriFilterbank.cpp.o\r\n[ 97%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/PreEmphasis.cpp.o\r\n[ 97%] Building CXX object bindings/python/CMakeFiles/_feature.dir/__/__/src/libraries/feature/Windowing.cpp.o\r\n[ 97%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/SpeechUtils.cpp.o\r\n[ 98%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/TriFilterbank.cpp.o\r\n[ 99%] Linking CXX shared module ../../../../wav2letter/_feature.cpython-36m-x86_64-linux-gnu.so\r\n[100%] Building CXX object bindings/python/CMakeFiles/_decoder.dir/__/__/src/libraries/feature/Windowing.cpp.o\r\n[100%] Linking CXX shared module ../../../../wav2letter/_decoder.cpython-36m-x86_64-linux-gnu.so\r\n/usr/bin/ld: /home/mrityunjoy/kenlm/build/lib/libkenlm.a(read_arpa.cc.o): relocation R_X86_64_PC32 against symbol `_ZTVN4util9ExceptionE' can not be used when making a shared object; recompile with -fPIC\r\n/usr/bin/ld: final link failed: Bad value\r\ncollect2: error: ld returned 1 exit status\r\nbindings/python/CMakeFiles/_common.dir/build.make:535: recipe for target '../../wav2letter/_common.cpython-36m-x86_64-linux-gnu.so' failed\r\nmake[2]: *** [../../wav2letter/_common.cpython-36m-x86_64-linux-gnu.so] Error 1\r\nCMakeFiles/Makefile2:330: recipe for target 'bindings/python/CMakeFiles/_common.dir/all' failed\r\nmake[1]: *** [bindings/python/CMakeFiles/_common.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n/usr/bin/ld: /home/mrityunjoy/kenlm/build/lib/libkenlm.a(read_arpa.cc.o): relocation R_X86_64_PC32 against symbol `_ZTVN4util9ExceptionE' can not be used when making a shared object; recompile with -fPIC\r\n/usr/bin/ld: final link failed: Bad value\r\ncollect2: error: ld returned 1 exit status\r\nbindings/python/CMakeFiles/_criterion.dir/build.make:535: recipe for target '../../wav2letter/_criterion.cpython-36m-x86_64-linux-gnu.so' failed\r\nmake[2]: *** [../../wav2letter/_criterion.cpython-36m-x86_64-linux-gnu.so] Error 1\r\nCMakeFiles/Makefile2:386: recipe for target 'bindings/python/CMakeFiles/_criterion.dir/all' failed\r\nmake[1]: *** [bindings/python/CMakeFiles/_criterion.dir/all] Error 2\r\n/usr/bin/ld: /home/mrityunjoy/kenlm/build/lib/libkenlm.a(read_arpa.cc.o): relocation R_X86_64_PC32 against symbol `_ZTVN4util9ExceptionE' can not be used when making a shared object; recompile with -fPIC\r\n/usr/bin/ld: final link failed: Bad value\r\ncollect2: error: ld returned 1 exit status\r\nbindings/python/CMakeFiles/_feature.dir/build.make:535: recipe for target '../../wav2letter/_feature.cpython-36m-x86_64-linux-gnu.so' failed\r\nmake[2]: *** [../../wav2letter/_feature.cpython-36m-x86_64-linux-gnu.so] Error 1\r\nCMakeFiles/Makefile2:444: recipe for target 'bindings/python/CMakeFiles/_feature.dir/all' failed\r\nmake[1]: *** [bindings/python/CMakeFiles/_feature.dir/all] Error 2\r\n/usr/bin/ld: /home/mrityunjoy/kenlm/build/lib/libkenlm.a(read_arpa.cc.o): relocation R_X86_64_PC32 against symbol `_ZTVN4util9ExceptionE' can not be used when making a shared object; recompile with -fPIC\r\n/usr/bin/ld: final link failed: Bad value\r\ncollect2: error: ld returned 1 exit status\r\nbindings/python/CMakeFiles/_decoder.dir/build.make:535: recipe for target '../../wav2letter/_decoder.cpython-36m-x86_64-linux-gnu.so' failed\r\nmake[2]: *** [../../wav2letter/_decoder.cpython-36m-x86_64-linux-gnu.so] Error 1\r\nCMakeFiles/Makefile2:415: recipe for target 'bindings/python/CMakeFiles/_decoder.dir/all' failed\r\nmake[1]: *** [bindings/python/CMakeFiles/_decoder.dir/all] Error 2\r\nMakefile:102: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/mrityunjoy/wav2letter/bindings/python/setup.py\", line 109, in <module>\r\n    zip_safe=False,\r\n  File \"/home/mrityunjoy/.local/lib/python3.6/site-packages/setuptools/__init__.py\", line 144, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib/python3.6/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.6/distutils/dist.py\", line 955, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"/home/mrityunjoy/.local/lib/python3.6/site-packages/setuptools/command/develop.py\", line 38, in run\r\n    self.install_for_development()\r\n  File \"/home/mrityunjoy/.local/lib/python3.6/site-packages/setuptools/command/develop.py\", line 140, in install_for_development\r\n    self.run_command('build_ext')\r\n  File \"/usr/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/usr/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"/home/mrityunjoy/wav2letter/bindings/python/setup.py\", line 48, in run\r\n    self.build_extensions()\r\n  File \"/home/mrityunjoy/wav2letter/bindings/python/setup.py\", line 91, in build_extensions\r\n    [\"cmake\", \"--build\", \".\"] + build_args, cwd=self.build_temp\r\n  File \"/usr/lib/python3.6/subprocess.py\", line 311, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '--build', '.', '--config', 'Release', '--', '-j4']' returned non-zero exit status 2.\r\n----------------------------------------\r\nERROR: Command errored out with exit status 1: /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/home/mrityunjoy/wav2letter/bindings/python/setup.py'\"'\"'; file='\"'\"'/home/mrityunjoy/wav2letter/bindings/python/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(file);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, file, '\"'\"'exec'\"'\"'))' develop --no-deps --user --prefix= Check the logs for full command output.\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2508/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2508/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2502", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2502/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2502/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2502/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2502", "id": 682571769, "node_id": "MDU6SXNzdWU2ODI1NzE3Njk=", "number": 2502, "title": "Errors when decoding with language model in wav2vec2.0", "user": {"login": "cywang97", "id": 65104236, "node_id": "MDQ6VXNlcjY1MTA0MjM2", "avatar_url": "https://avatars.githubusercontent.com/u/65104236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cywang97", "html_url": "https://github.com/cywang97", "followers_url": "https://api.github.com/users/cywang97/followers", "following_url": "https://api.github.com/users/cywang97/following{/other_user}", "gists_url": "https://api.github.com/users/cywang97/gists{/gist_id}", "starred_url": "https://api.github.com/users/cywang97/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cywang97/subscriptions", "organizations_url": "https://api.github.com/users/cywang97/orgs", "repos_url": "https://api.github.com/users/cywang97/repos", "events_url": "https://api.github.com/users/cywang97/events{/privacy}", "received_events_url": "https://api.github.com/users/cywang97/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2020-08-20T09:39:35Z", "updated_at": "2021-01-04T17:48:19Z", "closed_at": "2020-08-25T01:16:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nI am trying to reproduce the number in wav2vec2.0 paper with the released model.  The code works well when I set --w2l-decoder=viterbi to reproduce the CTC results. However, when I set --w2l-decoder='kenlm' or 'fairseqlm'. I met the following errors:\r\n\r\nFor Kenlm, I download the lm_librispeech_kenlm_word_4g_200kvocab.bin in https://github.com/facebookresearch/wav2letter/tree/master/recipes/models/sota/2019. Running command \r\n'python examples/speech_recognition/infer.py /path/to/manifest/ --task audio_pretraining --nbest 1 --path /path/to/mymodel --gen-subset dev_clean --results-path /path/to/results --w2l-decoder kenlm --lm-model /path/to/lm_librispeech_word_transformer.pt --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --post-process letter'\r\n\r\nThe code reports error in line 137 \"self.lexicon = load_words(args.lexicon)\" in fairseq/examples/speech_recognition/w2l_decoder.py as args.lexicon is None. I can't find lexicon file in model repository. Can you upload it?\r\n\r\n\r\nFor Transformer, I download the released transformer language model and dictionary in wav2letter repository.\r\nMy command is \r\n'python examples/speech_recognition/infer.py /path/to/manifest/ --task audio_pretraining --nbest 1 --path /path/to/mymodel --gen-subset dev_clean --results-path /path/to/results --w2l-decoder fairseqlm --lm-model /path/to/lm_librispeech_word_transformer.pt --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --post-process letter'\r\nThere are several issues:\r\n1. The arguments in the released model do not match the arguments defined in transformer_lm.py. For example, there is no args.decoder_layers_to_keep in lm_librispeech_word_transformer.pt. It seems the language model is not trained with the code in master branch.\r\n2. I have tried to replace fairseq/models/transformer_lm.py and fairseq/models/transformer.py with the same files in branch v0.7.0.  The command didn't report errors. However, the hypothesis is too too bad (much worse than viterbi decoding)! Can you upload the pretrained model that compatible with master branch and give more details of hyperparameters?\r\n\r\n\r\n\r\n### Environment\r\nI use the docker wav2letter/wav2letter:cuda-latest\r\n\r\n - fairseq Version : master\r\n - PyTorch Version :  1.4.0\r\n - OS : Ubuntu18.04\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 10.0\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2502/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2502/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2483", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2483/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2483/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2483/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2483", "id": 679528956, "node_id": "MDU6SXNzdWU2Nzk1Mjg5NTY=", "number": 2483, "title": " Can not train the Translator.  Buffer dtype mismatch, expected 'DTYPE_t' but got 'long'", "user": {"login": "joeqi0370", "id": 50893633, "node_id": "MDQ6VXNlcjUwODkzNjMz", "avatar_url": "https://avatars.githubusercontent.com/u/50893633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joeqi0370", "html_url": "https://github.com/joeqi0370", "followers_url": "https://api.github.com/users/joeqi0370/followers", "following_url": "https://api.github.com/users/joeqi0370/following{/other_user}", "gists_url": "https://api.github.com/users/joeqi0370/gists{/gist_id}", "starred_url": "https://api.github.com/users/joeqi0370/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joeqi0370/subscriptions", "organizations_url": "https://api.github.com/users/joeqi0370/orgs", "repos_url": "https://api.github.com/users/joeqi0370/repos", "events_url": "https://api.github.com/users/joeqi0370/events{/privacy}", "received_events_url": "https://api.github.com/users/joeqi0370/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1697206034, "node_id": "MDU6TGFiZWwxNjk3MjA2MDM0", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/windows", "name": "windows", "color": "0000ff", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2020-08-15T08:52:46Z", "updated_at": "2021-06-24T06:42:39Z", "closed_at": "2020-08-25T00:36:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "## What is your question?\r\nCan not train the translator\r\n\r\n#### Code\r\nThe commands I runned:\r\n\r\nfairseq-preprocess --source-lang en --target-lang de --trainpref D:\\fairseq\\test_translateEN-DE\\data\\train --validpref D:\\fairseq\\test_translateEN-DE\\data\\val --testpref D:\\fairseq\\test_translateEN-DE\\data\\test --destdir D:\\fairseq\\test_translateEN-DE\\model.bin\r\n#Preprocess seems ok\r\n##########################\r\n2020-08-15 16:38:32 | INFO | fairseq_cli.preprocess | [en] Dictionary: 24992 types\r\n2020-08-15 16:38:33 | INFO | fairseq_cli.preprocess | [en] D:\\fairseq\\test_translateEN-DE\\data\\train.en: 10000 sents, 234434 tokens, 0.0% replaced by <unk>\r\n2020-08-15 16:38:33 | INFO | fairseq_cli.preprocess | [en] Dictionary: 24992 types\r\n2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [en] D:\\fairseq\\test_translateEN-DE\\data\\val.en: 1419 sents, 40092 tokens, 10.5% replaced by <unk>\r\n2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [en] Dictionary: 24992 types\r\n2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [en] D:\\fairseq\\test_translateEN-DE\\data\\test.en: 1581 sents, 34996 tokens, 11.2% replaced by <unk>\r\n2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [de] Dictionary: 35808 types\r\n2020-08-15 16:38:35 | INFO | fairseq_cli.preprocess | [de] D:\\fairseq\\test_translateEN-DE\\data\\train.de: 10000 sents, 223074 tokens, 0.0% replaced by <unk>\r\n2020-08-15 16:38:35 | INFO | fairseq_cli.preprocess | [de] Dictionary: 35808 types\r\n2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | [de] D:\\fairseq\\test_translateEN-DE\\data\\val.de: 1419 sents, 40031 tokens, 17.0% replaced by <unk>\r\n2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | [de] Dictionary: 35808 types\r\n2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | [de] D:\\fairseq\\test_translateEN-DE\\data\\test.de: 1581 sents, 34635 tokens, 16.3% replaced by <unk>\r\n2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to D:\\fairseq\\test_translateEN-DE\\model.bin\r\n##########################\r\n\r\nBut the following command reports error\r\nfairseq-train D:\\fairseq\\test_translateEN-DE\\model.bin --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 --arch fconv_iwslt_de_en --save-dir D:\\fairseq\\test_translateEN-DE\\\r\n\r\nError info:\r\n##########################\r\n2020-08-15 16:39:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\r\n2020-08-15 16:39:08 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None\r\n2020-08-15 16:39:08 | INFO | fairseq.trainer | no existing checkpoint found D:\\fairseq\\test_translateEN-DE\\checkpoint_last.pt\r\n2020-08-15 16:39:08 | INFO | fairseq.trainer | loading train data for epoch 1\r\n2020-08-15 16:39:08 | INFO | fairseq.data.data_utils | loaded 10000 examples from: D:\\fairseq\\test_translateEN-DE\\model.bin\\train.en-de.en\r\n2020-08-15 16:39:08 | INFO | fairseq.data.data_utils | loaded 10000 examples from: D:\\fairseq\\test_translateEN-DE\\model.bin\\train.en-de.de\r\n2020-08-15 16:39:08 | INFO | fairseq.tasks.translation | D:\\fairseq\\test_translateEN-DE\\model.bin train en-de 10000 examples\r\nTraceback (most recent call last):\r\n  File \"D:\\Python\\Anaconda3\\envs\\NMT\\Scripts\\fairseq-train-script.py\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"d:\\fairseq\\fairseq\\fairseq_cli\\train.py\", line 333, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"d:\\fairseq\\fairseq\\fairseq\\distributed_utils.py\", line 189, in call_main\r\n    main(args, **kwargs)\r\n  File \"d:\\fairseq\\fairseq\\fairseq_cli\\train.py\", line 109, in main\r\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\r\n  File \"d:\\fairseq\\fairseq\\fairseq\\checkpoint_utils.py\", line 187, in load_checkpoint\r\n    epoch_itr = trainer.get_train_iterator(\r\n  File \"d:\\fairseq\\fairseq\\fairseq\\trainer.py\", line 335, in get_train_iterator\r\n    return self.task.get_batch_iterator(\r\n  File \"d:\\fairseq\\fairseq\\fairseq\\tasks\\fairseq_task.py\", line 214, in get_batch_iterator\r\n    batch_sampler = dataset.batch_by_size(\r\n  File \"d:\\fairseq\\fairseq\\fairseq\\data\\fairseq_dataset.py\", line 118, in batch_by_size\r\n    return data_utils.batch_by_size(\r\n  File \"d:\\fairseq\\fairseq\\fairseq\\data\\data_utils.py\", line 256, in batch_by_size\r\n    return batch_by_size_fast(\r\n  File \"fairseq\\data\\data_utils_fast.pyx\", line 27, in fairseq.data.data_utils_fast.batch_by_size_fast\r\n    cpdef list batch_by_size_fast(\r\nValueError: Buffer dtype mismatch, expected 'DTYPE_t' but got 'long'\r\n##########################\r\n\r\n#### What have you tried?\r\n1. I have tried to remove all &amps; tokens from the texts and make sure that the lines are equal in training files.\r\n2. change the --arch from fconv_iwslt_de_en to other values\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0)  : 1.2\r\n - OS (e.g., Linux):  Windows 10\r\n - How you installed fairseq (`pip`, source): install from source\r\n - Build command you used (if compiling from source):\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npython setup.py build_ext --inplace\r\npip install --editable ./\r\n\r\ngit clone https://github.com/roy-ht/editdistance\r\ncd editdistance\r\npip install --editable ./\r\n\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: cudatoolkit 10.0.130\r\n - GPU models and configuration: rank   0: capabilities =  6.1  ; total memory = 12.000 GB ; name = TITAN X (Pascal)\r\n - Any other relevant information:\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2483/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2478", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2478/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2478/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2478/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2478", "id": 678328029, "node_id": "MDU6SXNzdWU2NzgzMjgwMjk=", "number": 2478, "title": "infer.py: error: unrecognized arguments: fairseqlm", "user": {"login": "anujsaraswat741", "id": 57795947, "node_id": "MDQ6VXNlcjU3Nzk1OTQ3", "avatar_url": "https://avatars.githubusercontent.com/u/57795947?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anujsaraswat741", "html_url": "https://github.com/anujsaraswat741", "followers_url": "https://api.github.com/users/anujsaraswat741/followers", "following_url": "https://api.github.com/users/anujsaraswat741/following{/other_user}", "gists_url": "https://api.github.com/users/anujsaraswat741/gists{/gist_id}", "starred_url": "https://api.github.com/users/anujsaraswat741/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anujsaraswat741/subscriptions", "organizations_url": "https://api.github.com/users/anujsaraswat741/orgs", "repos_url": "https://api.github.com/users/anujsaraswat741/repos", "events_url": "https://api.github.com/users/anujsaraswat741/events{/privacy}", "received_events_url": "https://api.github.com/users/anujsaraswat741/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-08-13T10:24:34Z", "updated_at": "2020-10-05T12:02:36Z", "closed_at": "2020-08-13T11:25:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Trace\r\n\r\npython examples/speech_recognition/infer.py /home/anuj/temp/LibriSpeech/dev-other/ --task audio_pretraining --nbest 1 --path /home/anuj/pretrained_models/wav2vec2_vox_960h.pt --gen-subset dev_other --results-path /home/anuj/fairseq/--w2l-decoder fairseqlm --lm-model /home/anuj/language_models/lm_librispeech_word_transformer.pt --lm-weight 2 --word-score 1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 40000 --post-process letter\r\n\r\nusage: infer.py [-h] [--no-progress-bar] [--log-interval N]\r\n                [--log-format {json,none,simple,tqdm}]\r\n                [--tensorboard-logdir DIR] [--seed N] [--cpu] [--tpu] [--bf16]\r\n                [--fp16] [--memory-efficient-bf16] [--memory-efficient-fp16]\r\n                [--fp16-no-flatten-grads] [--fp16-init-scale FP16_INIT_SCALE]\r\n                [--fp16-scale-window FP16_SCALE_WINDOW]\r\n                [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\r\n                [--min-loss-scale D]\r\n                [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\r\n                [--user-dir USER_DIR] [--empty-cache-freq EMPTY_CACHE_FREQ]\r\n                [--all-gather-list-size ALL_GATHER_LIST_SIZE]\r\n                [--model-parallel-size N]\r\n                [--checkpoint-suffix CHECKPOINT_SUFFIX]\r\n                [--quantization-config-path QUANTIZATION_CONFIG_PATH]\r\n                [--profile]\r\n                [--criterion {legacy_masked_lm_loss,sentence_prediction,label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,ctc,wav2vec,nat_loss,composite_loss,masked_lm,sentence_ranking,adaptive_loss,cross_entropy,vocab_parallel_cross_entropy}]\r\n                [--scoring {sacrebleu,bleu,wer}]\r\n                [--tokenizer {nltk,moses,space}]\r\n                [--bpe {bytes,hf_byte_bpe,bert,subword_nmt,byte_bpe,characters,gpt2,sentencepiece,fastbpe}]\r\n                [--optimizer {adadelta,adafactor,adam,adagrad,sgd,nag,adamax,lamb}]\r\n                [--lr-scheduler {tri_stage,polynomial_decay,triangular,reduce_lr_on_plateau,fixed,cosine,inverse_sqrt}]\r\n                [--task TASK] [--num-workers N]\r\n                [--skip-invalid-size-inputs-valid-test] [--max-tokens N]\r\n                [--max-sentences N] [--required-batch-size-multiple N]\r\n                [--dataset-impl FORMAT] [--data-buffer-size N]\r\n                [--gen-subset SPLIT] [--num-shards N] [--shard-id ID]\r\n                [--distributed-world-size N]\r\n                [--distributed-rank DISTRIBUTED_RANK]\r\n                [--distributed-backend DISTRIBUTED_BACKEND]\r\n                [--distributed-init-method DISTRIBUTED_INIT_METHOD]\r\n                [--distributed-port DISTRIBUTED_PORT] [--device-id DEVICE_ID]\r\n                [--distributed-no-spawn] [--ddp-backend {c10d,no_c10d}]\r\n                [--bucket-cap-mb MB] [--fix-batches-to-gpus]\r\n                [--find-unused-parameters] [--fast-stat-sync]\r\n                [--broadcast-buffers] [--distributed-wrapper {DDP,SlowMo}]\r\n                [--slowmo-momentum SLOWMO_MOMENTUM]\r\n                [--slowmo-algorithm {LocalSGD,SGP}]\r\n                [--localsgd-frequency LOCALSGD_FREQUENCY]\r\n                [--nprocs-per-node N] [--path FILE]\r\n                [--remove-bpe [REMOVE_BPE]] [--quiet] [--model-overrides DICT]\r\n                [--results-path RESDIR] [--beam N] [--nbest N] [--max-len-a N]\r\n                [--max-len-b N] [--min-len N] [--match-source-len]\r\n                [--no-early-stop] [--unnormalized] [--no-beamable-mm]\r\n                [--lenpen LENPEN] [--unkpen UNKPEN]\r\n                [--replace-unk [REPLACE_UNK]] [--sacrebleu]\r\n                [--score-reference] [--prefix-size PS]\r\n                [--no-repeat-ngram-size N] [--sampling] [--sampling-topk PS]\r\n                [--sampling-topp PS] [--temperature N]\r\n                [--diverse-beam-groups N] [--diverse-beam-strength N]\r\n                [--diversity-rate N] [--print-alignment] [--print-step]\r\n                [--iter-decode-eos-penalty N] [--iter-decode-max-iter N]\r\n                [--iter-decode-force-max-iter] [--iter-decode-with-beam N]\r\n                [--iter-decode-with-external-reranker] [--retain-iter-history]\r\n                [--retain-dropout]\r\n                [--retain-dropout-modules RETAIN_DROPOUT_MODULES [RETAIN_DROPOUT_MODULES ...]]\r\n                [--decoding-format {unigram,ensemble,vote,dp,bs}]\r\n                [--kspmodel KSPMODEL] [--wfstlm WFSTLM]\r\n                [--rnnt_decoding_type RNNT_DECODING_TYPE]\r\n                [--lm-weight LM_WEIGHT] [--rnnt_len_penalty RNNT_LEN_PENALTY]\r\n                [--w2l-decoder {viterbi,kenlm,fairseqlm}] [--lexicon LEXICON]\r\n                [--unit-lm] [--kenlm-model KENLM_MODEL]\r\n                [--beam-threshold BEAM_THRESHOLD]\r\n                [--beam-size-token BEAM_SIZE_TOKEN] [--word-score WORD_SCORE]\r\n                [--unk-weight UNK_WEIGHT] [--sil-weight SIL_WEIGHT]\r\n                [--dump-emissions DUMP_EMISSIONS]\r\n                [--dump-features DUMP_FEATURES]\r\n                [--load-emissions LOAD_EMISSIONS] [--zero-infinity]\r\n                [--wer-args WER_ARGS] [--momentum M] [--weight-decay WD]\r\n                [--force-anneal N] [--lr-shrink LS] [--warmup-updates N]\r\n                [--sample-rate SAMPLE_RATE] [--normalize]\r\n                [--max-sample-size MAX_SAMPLE_SIZE]\r\n                [--min-sample-size MIN_SAMPLE_SIZE] [--enable-padding]\r\n                [--no-min-cropping] [--labels LABELS]\r\n                data\r\ninfer.py: error: unrecognized arguments: fairseqlm\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2478/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2475", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2475/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2475/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2475/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2475", "id": 678119657, "node_id": "MDU6SXNzdWU2NzgxMTk2NTc=", "number": 2475, "title": "AttributeError: 'BLEU' object has no attribute 'format'", "user": {"login": "thpun", "id": 31913095, "node_id": "MDQ6VXNlcjMxOTEzMDk1", "avatar_url": "https://avatars.githubusercontent.com/u/31913095?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thpun", "html_url": "https://github.com/thpun", "followers_url": "https://api.github.com/users/thpun/followers", "following_url": "https://api.github.com/users/thpun/following{/other_user}", "gists_url": "https://api.github.com/users/thpun/gists{/gist_id}", "starred_url": "https://api.github.com/users/thpun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thpun/subscriptions", "organizations_url": "https://api.github.com/users/thpun/orgs", "repos_url": "https://api.github.com/users/thpun/repos", "events_url": "https://api.github.com/users/thpun/events{/privacy}", "received_events_url": "https://api.github.com/users/thpun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-13T03:23:36Z", "updated_at": "2020-08-13T17:56:19Z", "closed_at": "2020-08-13T17:56:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen running `fairseq-generate`, it failed to output BLEU score.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd\r\n```\r\nCUDA_VISIBLE_DEVICES=0 fairseq-generate \\\r\n--path=$MODEL $DATA \\\r\n--task translation_multi_simple_epoch \\\r\n--encoder-langtok 'src' --decoder-langtok \\\r\n--gen-subset test \\\r\n-t $TGT -s $SRC \\\r\n--lang-dict \"$lang_list\" --lang-pairs \"$lang_pairs\" \\\r\n--bpe 'sentencepiece' --sentencepiece-model models/mbart.cc25/sentence.bpe.model \\\r\n--remove-bpe 'sentencepiece' \\\r\n--sacrebleu --fp16 --max-sentences 64\r\n```\r\n2. See error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/fairseq-generate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())\r\n  File \"/workspace/fairseq/fairseq_cli/generate.py\", line 281, in cli_main\r\n    main(args)\r\n  File \"/workspace/fairseq/fairseq_cli/generate.py\", line 38, in main\r\n    return _main(args, sys.stdout)\r\n  File \"/workspace/fairseq/fairseq_cli/generate.py\", line 272, in _main\r\n    'Generate {} with beam={}: {}'.format(args.gen_subset, args.beam, scorer.result_string()),\r\n  File \"/workspace/fairseq/fairseq/scoring/bleu.py\", line 63, in result_string\r\n    return self.sacrebleu.corpus_bleu(self.sys, [self.ref]).format()\r\nAttributeError: 'BLEU' object has no attribute 'format'\r\n```\r\n\r\n### Expected behavior\r\n\r\nIt is expected that the BLEU score is printed at the end of `fairseq-generate`, as in previous versions of fairseq.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master, commit 4c55744ec4cb26749cf2cf8dac89942f26ce4bd2\r\n - PyTorch Version (e.g., 1.0) `1.5.0a0+8f84ded`\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install --editable .`\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: V100\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2475/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2471", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2471/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2471/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2471/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2471", "id": 677809515, "node_id": "MDU6SXNzdWU2Nzc4MDk1MTU=", "number": 2471, "title": "fairseq-generate RuntimeError: Trying to pass too many CPU scalars to CUDA kernel!", "user": {"login": "ziweiji", "id": 29816163, "node_id": "MDQ6VXNlcjI5ODE2MTYz", "avatar_url": "https://avatars.githubusercontent.com/u/29816163?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ziweiji", "html_url": "https://github.com/ziweiji", "followers_url": "https://api.github.com/users/ziweiji/followers", "following_url": "https://api.github.com/users/ziweiji/following{/other_user}", "gists_url": "https://api.github.com/users/ziweiji/gists{/gist_id}", "starred_url": "https://api.github.com/users/ziweiji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ziweiji/subscriptions", "organizations_url": "https://api.github.com/users/ziweiji/orgs", "repos_url": "https://api.github.com/users/ziweiji/repos", "events_url": "https://api.github.com/users/ziweiji/events{/privacy}", "received_events_url": "https://api.github.com/users/ziweiji/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-12T16:05:00Z", "updated_at": "2020-11-04T20:29:09Z", "closed_at": "2020-08-20T15:28:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nRuntimeError: Trying to pass too many CPU scalars to CUDA kernel! \r\noccur when run fairseq-generate \r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n> CUDA_VISIBLE_DEVICES=5 fairseq-generate data-bin \\\r\n> --path models/checkpoint2.pt \\\r\n> --batch-size 32 --beam 1 --sampling --sampling-topk 4 --nbest 1 --temperature 0.8 \\\r\n> --source-lang dialog --target-lang scene \\\r\n> --max-len-b 50 --min-len 1 --lenpen 1.0 \\\r\n> --results-path 'try' \\\r\n> --no-repeat-ngram-size 4 \\\r\n> --skip-invalid-size-inputs-valid-test \\\r\n> --remove-bpe \\\r\n\r\n2. See error\r\n\r\nTraceback (most recent call last):                                                                                         \r\n  File \"/home/jiziwei/anaconda3/bin/fairseq-generate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())\r\n  File \"/home/jiziwei/fairseq/fairseq_cli/generate.py\", line 274, in cli_main\r\n    main(args)\r\n  File \"/home/jiziwei/fairseq/fairseq_cli/generate.py\", line 36, in main\r\n    return _main(args, h)\r\n  File \"/home/jiziwei/fairseq/fairseq_cli/generate.py\", line 150, in _main\r\n    hypos = task.inference_step(generator, models, sample, prefix_tokens)\r\n  File \"/home/jiziwei/fairseq/fairseq/tasks/fairseq_task.py\", line 361, in inference_step\r\n    return generator.generate(models, sample, prefix_tokens=prefix_tokens)\r\n  File \"/home/jiziwei/anaconda3/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/jiziwei/fairseq/fairseq/sequence_generator.py\", line 159, in generate\r\n    return self._generate(sample, **kwargs)\r\n  File \"/home/jiziwei/fairseq/fairseq/sequence_generator.py\", line 314, in _generate\r\n    lprobs = self._no_repeat_ngram(tokens, lprobs, bsz, beam_size, step)\r\n  File \"/home/jiziwei/fairseq/fairseq/sequence_generator.py\", line 658, in _no_repeat_ngram\r\n    ] = torch.tensor(-math.inf, dtype=torch.float)\r\nRuntimeError: Trying to pass too many CPU scalars to CUDA kernel!\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version 0.9.0\r\n - PyTorch Version 1.6\r\n - OS Linux\r\n - How you installed fairseq source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 11.0\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2471/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2469", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2469/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2469/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2469/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2469", "id": 677740305, "node_id": "MDU6SXNzdWU2Nzc3NDAzMDU=", "number": 2469, "title": "Scripted transformer fails on forward pass", "user": {"login": "erip", "id": 2348806, "node_id": "MDQ6VXNlcjIzNDg4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/2348806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erip", "html_url": "https://github.com/erip", "followers_url": "https://api.github.com/users/erip/followers", "following_url": "https://api.github.com/users/erip/following{/other_user}", "gists_url": "https://api.github.com/users/erip/gists{/gist_id}", "starred_url": "https://api.github.com/users/erip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erip/subscriptions", "organizations_url": "https://api.github.com/users/erip/orgs", "repos_url": "https://api.github.com/users/erip/repos", "events_url": "https://api.github.com/users/erip/events{/privacy}", "received_events_url": "https://api.github.com/users/erip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-12T14:30:55Z", "updated_at": "2020-08-12T16:49:45Z", "closed_at": "2020-08-12T16:46:08Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen using a transformer that has been scripted for translation, the model fails to decode.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Install fairseq from source\r\n2. Preprocess:\r\n```sh\r\nfairseq-preprocess --source-lang $SRC --target-lang $TGT \\\r\n    --trainpref data/bpe_pretrain --validpref data/bpe_valid --testpref data/bpe_test \\\r\n    --destdir data-bin/ \\\r\n    --workers 20\r\n```\r\n3. Train: \r\n```sh\r\nfairseq-train data-bin \\\r\n    --source-lang $SRC --target-lang $TGT \\\r\n    --clip-norm 0.1 --dropout 0.2 --max-tokens 10240 \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr  5e-4 --lr-scheduler inverse_sqrt \\\r\n    --label-smoothing 0.1 \\\r\n    --warmup-init-lr 1e-7 \\\r\n    --warmup-updates 8000 \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --save-dir \"checkpoints\" \\\r\n    --arch transformer \\\r\n    --keep-last-epochs 10 \\\r\n    --max-epoch 100 \\\r\n    --num-workers 8 \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --ddp-backend no_c10d \\\r\n    --tensorboard-logdir \"logdir-$SLURM_JOB_ID\"\r\n```\r\n4. Export: \r\n```python\r\n#!/usr/bin/env python\r\n\r\nimport torch\r\n\r\nfrom fairseq.sequence_generator import SequenceGenerator\r\nfrom fairseq.models.transformer import TransformerModel\r\nfrom fairseq.data import Dictionary\r\n\r\nif __name__ == \"__main__\":\r\n    tgt_dict = Dictionary.load(open('dict.fr.txt'))\r\n    model = TransformerModel.from_pretrained('.', 'checkpoints/checkpoint_best.pt', '.', bpe='sentencepiece', sentencepiece_model='spm.model')\r\n\r\n    generator = SequenceGenerator(model.models, tgt_dict)\r\n\r\n    scripted_gen = torch.jit.script(generator)\r\n    scripted_gen.save('checkpoint_best.scripted.pt')\r\n```\r\n5. Use `checkpoint_best.scripted.pt` to generate sequence.\r\n\r\n\r\n2. See error\r\n\r\n```\r\n26548: Error: Traceback of TorchScript, original code (most recent call last):\r\n26548: Error:   File \"/Users/erippeth/miniconda3/envs/fairseq-temp/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 116, in forward\r\n26548: Error:                 (default: self.eos)\r\n26548: Error:         \"\"\"\r\n26548: Error:         return self._generate(sample, prefix_tokens, bos_token)\r\n26548: Error:                ~~~~~~~~~~~~~~ <--- HERE\r\n26548: Error:   File \"/Users/erippeth/miniconda3/envs/fairseq-temp/lib/python3.6/site-packages/fairseq/models/fairseq_encoder.py\", line 48, in forward_torchscript\r\n26548: Error:         \"\"\"\r\n26548: Error:         if torch.jit.is_scripting():\r\n26548: Error:             return self.forward(\r\n26548: Error:                    ~~~~~~~~~~~~ <--- HERE\r\n26548: Error:                 src_tokens=net_input[\"src_tokens\"],\r\n26548: Error:                 src_lengths=net_input[\"src_lengths\"],\r\n26548: Error:   File \"/Users/erippeth/miniconda3/envs/fairseq-temp/lib/python3.6/site-packages/fairseq/models/transformer.py\", line 399, in forward\r\n26548: Error:                   Only populated if *return_all_hiddens* is True.\r\n26548: Error:         \"\"\"\r\n26548: Error:         x, encoder_embedding = self.forward_embedding(src_tokens)\r\n26548: Error:                                ~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n26548: Error:         # B x T x C -> T x B x C\r\n26548: Error:   File \"/Users/erippeth/miniconda3/envs/fairseq-temp/lib/python3.6/site-packages/torch/nn/modules/sparse.py\", line 124, in forward\r\n26548: Error:     def forward(self, input: Tensor) -> Tensor:\r\n26548: Error:         return F.embedding(\r\n26548: Error:                ~~~~~~~~~~~ <--- HERE\r\n26548: Error:             input, self.weight, self.padding_idx, self.max_norm,\r\n26548: Error:             self.norm_type, self.scale_grad_by_freq, self.sparse)\r\n26548: Error:   File \"/Users/erippeth/miniconda3/envs/fairseq-temp/lib/python3.6/site-packages/torch/nn/functional.py\", line 1814, in embedding\r\n26548: Error:         # remove once script supports set_grad_enabled\r\n26548: Error:         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\r\n26548: Error:     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n26548: Error:            ~~~~~~~~~~~~~~~ <--- HERE\r\n26548: Error: RuntimeError: index out of range in self\r\n```\r\n\r\n#### Code sample\r\n\r\nLittle hard to untangle. :-)\r\n\r\n### Expected behavior\r\n\r\nShapes should match and index should not be out of range when running a forward pass.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.6.0\r\n - OS (e.g., Linux): OS X\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `CC=clang CFLAGS='-stdlib=libc++' pip install .`\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: \r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2469/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2446", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2446/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2446/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2446/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2446", "id": 674841250, "node_id": "MDU6SXNzdWU2NzQ4NDEyNTA=", "number": 2446, "title": "Unsafe round in ctc criterion logging", "user": {"login": "Squire-tomsk", "id": 5622473, "node_id": "MDQ6VXNlcjU2MjI0NzM=", "avatar_url": "https://avatars.githubusercontent.com/u/5622473?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Squire-tomsk", "html_url": "https://github.com/Squire-tomsk", "followers_url": "https://api.github.com/users/Squire-tomsk/followers", "following_url": "https://api.github.com/users/Squire-tomsk/following{/other_user}", "gists_url": "https://api.github.com/users/Squire-tomsk/gists{/gist_id}", "starred_url": "https://api.github.com/users/Squire-tomsk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Squire-tomsk/subscriptions", "organizations_url": "https://api.github.com/users/Squire-tomsk/orgs", "repos_url": "https://api.github.com/users/Squire-tomsk/repos", "events_url": "https://api.github.com/users/Squire-tomsk/events{/privacy}", "received_events_url": "https://api.github.com/users/Squire-tomsk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-07T08:05:29Z", "updated_at": "2020-08-17T13:56:35Z", "closed_at": "2020-08-17T13:56:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThis lambda https://github.com/pytorch/fairseq/blob/0bb7bc3777b880c282df794fb7edb56d7280449b/fairseq/criterions/ctc.py#L221 throws `TypeError: type Tensor doesn't define __round__ method` if AverageMeter.sum returns torch.Tensor. \r\n\r\nReplacing `round` to `safe_round` from https://github.com/pytorch/fairseq/blob/0bb7bc3777b880c282df794fb7edb56d7280449b/fairseq/logging/meters.py#L53 solve it.\r\n\r\n### To Reproduce\r\n\r\nRun\r\n\r\n```\r\npython fairseq/train.py \r\n/data/librispeech/train-clean-100-manifest\r\n--distributed-world-size 2 \r\n--save-dir <path to save dir> --fp16\r\n--post-process letter --valid-subset valid \r\n--no-epoch-checkpoints \r\n--best-checkpoint-metric wer --num-workers 12\r\n--max-update 80000 --sentence-avg\r\n--task audio_pretraining --arch wav2vec_ctc \r\n--w2v-path wav2vec/wav2vec_2_small/wav2vec_small.pt\r\n--labels ltr --apply-mask --mask-selection static\r\n--mask-other 0 --mask-length 10 --mask-prob 0.5 --layerdrop 0.1\r\n--mask-channel-selection static --mask-channel-other 0 \r\n--mask-channel-length 64 --mask-channel-prob 0.5 --zero-infinity\r\n--feature-grad-mult 0.0 --freeze-finetune-updates 10000 \r\n--validate-after-updates 10000 --optimizer adam\r\n--adam-betas '(0.9, 0.98)' --adam-eps 1e-08 --lr 2e-05\r\n--lr-scheduler tri_stage --warmup-steps 8000 --hold-steps 32000\r\n--decay-steps 40000 --final-lr-scale 0.05 --final-dropout 0.0 \r\n--dropout 0.0 --activation-dropout 0.1 --criterion ctc\r\n--attention-dropout 0.0 --max-tokens 1280000 --seed 2337\r\n--log-format json --log-interval 1 --ddp-backend no_c10d\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo `TypeError` in CtcCriterion.reduce_metrics if AverageMeter.sum returns torch.Tensor. \r\n\r\n### Environment\r\n\r\n - fairseq Version: Master\r\n - PyTorch Version: 1.6.0\r\n - OS: Linux\r\n - How you installed fairseq: Source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration:", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2446/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2446/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2434", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2434/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2434/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2434/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2434", "id": 673640254, "node_id": "MDU6SXNzdWU2NzM2NDAyNTQ=", "number": 2434, "title": "Command in wav2vec documentation no longer works: --criterion binary_cross_entropy is unrecognized.", "user": {"login": "dmjef", "id": 15038975, "node_id": "MDQ6VXNlcjE1MDM4OTc1", "avatar_url": "https://avatars.githubusercontent.com/u/15038975?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dmjef", "html_url": "https://github.com/dmjef", "followers_url": "https://api.github.com/users/dmjef/followers", "following_url": "https://api.github.com/users/dmjef/following{/other_user}", "gists_url": "https://api.github.com/users/dmjef/gists{/gist_id}", "starred_url": "https://api.github.com/users/dmjef/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dmjef/subscriptions", "organizations_url": "https://api.github.com/users/dmjef/orgs", "repos_url": "https://api.github.com/users/dmjef/repos", "events_url": "https://api.github.com/users/dmjef/events{/privacy}", "received_events_url": "https://api.github.com/users/dmjef/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-05T15:41:10Z", "updated_at": "2020-08-17T13:56:43Z", "closed_at": "2020-08-17T13:56:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nOn [this page](https://github.com/pytorch/fairseq/blob/master/examples/wav2vec/README.md), the command to pre-train wav2vec (version 1) fails because `--criterion binary_cross_entropy` is no longer recognized. Changing that argument to `cross_entropy` fails due to a NotImplementedError - it's not obvious what to use.\r\n\r\n### To Reproduce\r\n\r\nRun the following command, taken verbatim from the wav2vec readme, with the proper code and data setup:\r\n`python train.py /manifest/path --save-dir /model/path --num-workers 6 --fp16 --max-update 400000 --save-interval 1 --no-epoch-checkpoints \\\r\n--arch wav2vec --task audio_pretraining --lr 1e-06 --min-lr 1e-09 --optimizer adam --max-lr 0.005 --lr-scheduler cosine \\\r\n--conv-feature-layers [(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)] \\\r\n--conv-aggregator-layers [(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)] \\\r\n--skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion binary_cross_entropy --num-negatives 10 \\\r\n--max-sample-size 150000 --max-tokens 1500000 --skip-invalid-size-inputs-valid-test`\r\n\r\nThe command fails with the following error message:\r\n`train.py: error: argument --criterion: invalid choice: 'binary_cross_entropy' (choose from 'legacy_masked_lm_loss', 'masked_lm', 'sentence_prediction', 'label_smoothed_cross_entropy', 'nat_loss', 'composite_loss', 'ctc', 'cross_entropy', 'adaptive_loss', 'sentence_ranking', 'wav2vec', 'label_smoothed_cross_entropy_with_alignment', 'vocab_parallel_cross_entropy')`\r\n\r\n### Additional context\r\n\r\nThis is happening with the current version on master but not version cf87f759b9f09769dc416761738cee72382252df. So this issue was introduced with word2vec 2.0.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2434/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2427", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2427/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2427/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2427/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2427", "id": 673215079, "node_id": "MDU6SXNzdWU2NzMyMTUwNzk=", "number": 2427, "title": "cannot import name 'libbleu', encountered when running torch hub translation example", "user": {"login": "blmoistawinde", "id": 32953014, "node_id": "MDQ6VXNlcjMyOTUzMDE0", "avatar_url": "https://avatars.githubusercontent.com/u/32953014?v=4", "gravatar_id": "", "url": "https://api.github.com/users/blmoistawinde", "html_url": "https://github.com/blmoistawinde", "followers_url": "https://api.github.com/users/blmoistawinde/followers", "following_url": "https://api.github.com/users/blmoistawinde/following{/other_user}", "gists_url": "https://api.github.com/users/blmoistawinde/gists{/gist_id}", "starred_url": "https://api.github.com/users/blmoistawinde/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/blmoistawinde/subscriptions", "organizations_url": "https://api.github.com/users/blmoistawinde/orgs", "repos_url": "https://api.github.com/users/blmoistawinde/repos", "events_url": "https://api.github.com/users/blmoistawinde/events{/privacy}", "received_events_url": "https://api.github.com/users/blmoistawinde/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-08-05T03:00:12Z", "updated_at": "2020-08-17T18:21:43Z", "closed_at": "2020-08-17T18:21:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI am witnessing `ImportError: cannot import name 'libbleu'`, when running this [torch hub translation example on Google Colab](https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/pytorch_fairseq_translation.ipynb) \r\n\r\n### To Reproduce\r\n\r\nrun the second code snippet of the [notebook](https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/pytorch_fairseq_translation.ipynb). And the error occurs at the `torch.hub.load` line.\r\n\r\nError message\r\n![image](https://user-images.githubusercontent.com/32953014/89366784-20504e00-d70a-11ea-96de-a59b077628c6.png)\r\n\r\n\r\n### Environment\r\n - fairseq Version: 0.9.0 in torch.hub\r\n - PyTorch Version (e.g., 1.0): torch==1.6.0+cu101\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): in torch.hub\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2427/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2427/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2425", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2425/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2425/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2425/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2425", "id": 673172365, "node_id": "MDU6SXNzdWU2NzMxNzIzNjU=", "number": 2425, "title": "Missing libbleu for hub tests ", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-08-05T00:49:05Z", "updated_at": "2020-08-17T18:21:21Z", "closed_at": "2020-08-17T18:21:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "We're seeing missing libbleu.so in hub nightly tests. Could someone help take a look? \r\nhttps://app.circleci.com/pipelines/github/pytorch/hub/513/workflows/e1425a52-73f0-475a-a74e-1d5f27019e12/jobs/741\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2425/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2425/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2413", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2413/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2413/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2413/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2413", "id": 671935582, "node_id": "MDU6SXNzdWU2NzE5MzU1ODI=", "number": 2413, "title": "RuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half'", "user": {"login": "Skylixia", "id": 12053610, "node_id": "MDQ6VXNlcjEyMDUzNjEw", "avatar_url": "https://avatars.githubusercontent.com/u/12053610?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Skylixia", "html_url": "https://github.com/Skylixia", "followers_url": "https://api.github.com/users/Skylixia/followers", "following_url": "https://api.github.com/users/Skylixia/following{/other_user}", "gists_url": "https://api.github.com/users/Skylixia/gists{/gist_id}", "starred_url": "https://api.github.com/users/Skylixia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Skylixia/subscriptions", "organizations_url": "https://api.github.com/users/Skylixia/orgs", "repos_url": "https://api.github.com/users/Skylixia/repos", "events_url": "https://api.github.com/users/Skylixia/events{/privacy}", "received_events_url": "https://api.github.com/users/Skylixia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 26, "created_at": "2020-08-03T09:27:01Z", "updated_at": "2023-04-10T08:04:59Z", "closed_at": "2020-08-03T12:52:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nAfter following the Roberta [pretraining documentation](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md) with the exact same steps and data.\r\nWhen running fairseq-train I get an error\r\n\r\n### To Reproduce\r\n\r\n```\r\nTOTAL_UPDATES=125000    # Total number of training steps\r\nWARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\r\nPEAK_LR=0.0005          # Peak learning rate, adjust as needed\r\nTOKENS_PER_SAMPLE=512   # Max sequence length\r\nMAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\r\nMAX_SENTENCES=16        # Number of sequences per batch (batch size)\r\nUPDATE_FREQ=16          # Increase the batch size 16x\r\n\r\nDATA_DIR=data-bin/wikitext-103\r\n\r\nfairseq-train --fp16 $DATA_DIR \\\r\n    --task masked_lm --criterion masked_lm \\\r\n    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\r\n    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\r\n    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1\r\n```\r\n\r\nThen I get the following stack trace:\r\n```\r\n\r\n2020-08-03 09:13:25 | INFO | fairseq_cli.train | begin training epoch 1\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/fairseq/fairseq_cli/train.py\", line 350, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/fairseq/fairseq/distributed_utils.py\", line 189, in call_main\r\n    main(args, **kwargs)\r\n  File \"/fairseq/fairseq_cli/train.py\", line 121, in main\r\n    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/fairseq/fairseq_cli/train.py\", line 217, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/fairseq/fairseq/trainer.py\", line 457, in train_step\r\n    raise e\r\n  File \"/fairseq/fairseq/trainer.py\", line 431, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/fairseq/fairseq/tasks/fairseq_task.py\", line 347, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/fairseq/fairseq/criterions/masked_lm.py\", line 52, in forward\r\n    logits = model(**sample['net_input'], masked_tokens=masked_tokens)[0]\r\n  File \"/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/fairseq/fairseq/models/roberta/model.py\", line 119, in forward\r\n    x, extra = self.encoder(src_tokens, features_only, return_all_hiddens, **kwargs)\r\n  File \"/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/fairseq/fairseq/models/roberta/model.py\", line 337, in forward\r\n    x, extra = self.extract_features(src_tokens, return_all_hiddens=return_all_hiddens)\r\n  File \"/fairseq/fairseq/models/roberta/model.py\", line 345, in extract_features\r\n    last_state_only=not return_all_hiddens,\r\n  File \"/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/fairseq/fairseq/modules/transformer_sentence_encoder.py\", line 250, in forward\r\n    x = self.emb_layer_norm(x)\r\n  File \"/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/.local/lib/python3.6/site-packages/torch/nn/modules/normalization.py\", line 170, in forward\r\n    input, self.normalized_shape, self.weight, self.bias, self.eps)\r\n  File \"/.local/lib/python3.6/site-packages/torch/nn/functional.py\", line 2049, in layer_norm\r\n    torch.backends.cudnn.enabled)\r\nRuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half'\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version : master\r\n - PyTorch Version : 1.6.0\r\n - OS : Linux (Ubuntu) \r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): \r\n```\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable .\r\n```\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 11.0\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2413/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2387", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2387/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2387/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2387/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2387", "id": 666854144, "node_id": "MDU6SXNzdWU2NjY4NTQxNDQ=", "number": 2387, "title": "Distributed training crashes without `--distributed-no-spawn`", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-07-28T08:01:32Z", "updated_at": "2020-08-07T22:08:12Z", "closed_at": "2020-08-07T22:08:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI run distributed training on a single node with 4 GPUs.\r\nMy training script works normally with ` --distributed-no-spawn`. But when I delete `--distributed-no-spawn` and run the script, it crashes.    \r\nI find the argument in the documentation, where it shows that \"do not spawn multiple processes even if multiple GPUs are visible, Default: False\". Actually, I think I need the program to spawn multiple processes in my situation. But it crashes without that flag.   What does the flag really mean? When should I use it? And when shouldn't I use it? Why does my program crash without it?\r\n\r\nThe stdout and stderr are as the following: \r\n```\r\n*****************************************\r\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal per\r\nformance in your application as needed.\r\n*****************************************\r\n2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://\r\n2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://\r\n2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | distributed init (rank 5): env://\r\n2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | distributed init (rank 0): env://\r\n2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | distributed init (rank 1): env://\r\n2020-07-28 07:52:07 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 1\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 5): env://\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 5\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 2\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 3\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 0\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 4\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 1): env://\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 1\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 2\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 2\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 4\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 3\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 3\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | distributed init (rank 6): env://\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 6\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 3\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 4\r\n2020-07-28 07:52:08 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000006 as rank 5\r\n2020-07-28 07:52:12 | INFO | fairseq_cli.train | Namespace(activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_input_cutoff=No\r\nne, adaptive_input_factor=4, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, add_bos_token=False, all_gather_list_size=16384, arch='\r\ntransformer_lm', attention_dropout=0.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, char_embedder_highway_layers=2, c\r\nharacter_embedding_dim=4, character_embeddings=False, character_filters='[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', checkpoint_suffix='', cl\r\nip_norm=0.0, cpu=False, criterion='cross_entropy', curriculum=0, data='/mnt/distributed/bin', data_buffer_size=10, dataset_impl=None, ddp\r\n_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=3, decoder_l\r\nayers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl',\r\n distributed_init_method='env://', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dropout=0.0,\r\n empty_cache_freq=0, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_\r\nflatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, future_target=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, laye\r\nrnorm_embedding=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0007], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentence\r\ns_valid=None, max_target_positions=None, max_tokens=8000, max_tokens_valid=8000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory\r\n_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_decoder_final_norm=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no\r\n_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node\r\n=4, num_workers=20, optimizer='adam', optimizer_overrides='{}', output_dictionary_size=-1, past_target=False, patience=-1, profile=False, quant_noise_pq=0, quant_noise_p\r\nq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False\r\n, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='eos', save_dir='/mnt/distributed/checkpoints/transformer_l\r\nm/1blm-3L-1024ED-4096FD-16H-shared-lr0.0007-SEED1-DROPOUT0.0-CRITERIONcross_entropy', save_interval=1, save_interval_updates=0, seed=1, self_target=Fa\r\nlse, sentence_avg=False, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algo\r\nrithm='LocalSGD', slowmo_momentum=None, task='language_modeling', tensorboard_logdir='/mnt/distributed/checkpoints/transformer_lm/1blm-3L\r\n-1024ED-4096FD-16H-shared-lr0.0007-SEED1-DROPOUT0.0-CRITERIONcross_entropy', threshold_loss_scale=None, tie_adaptive_proj=False, tie_adaptive_weights=\r\nFalse, tokenizer=None, tokens_per_sample=1024, tpu=False, train_subset='train', update_freq=[8], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid',\r\n validate_interval=1, warmup_init_lr=1e-07, warmup_updates=8000, weight_decay=0.0)\r\n2020-07-28 07:52:13 | INFO | fairseq.tasks.language_modeling | dictionary: 50261 types\r\n2020-07-28 07:52:13 | INFO | fairseq.data.data_utils | loaded 300613 examples from: /mnt/distributed/bin/valid\r\nTraceback (most recent call last):\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 372, in cli_main\r\n    cli_main_helper(args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 387, in cli_main_helper\r\n    nprocs=torch.cuda.device_count(),\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 360, in distributed_main\r\n    args, init_distributed=True, after_distributed_init_fn=after_distributed_init_fn\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 63, in main\r\n    args.distributed_rank = distributed_utils.distributed_init(args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq/distributed_utils.py\", line 99, in distributed_init\r\n    dist.all_reduce(torch.zeros(1).cuda())\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 902, in all_reduce\r\n    work = _default_pg.allreduce([tensor], opts)\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1579022034529/work/torch/lib/c10d/../c10d/NCCLUtils.hpp:78, invalid argument, NCCL version 2.4.8\r\nTraceback (most recent call last):\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 372, in cli_main\r\n    cli_main_helper(args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 387, in cli_main_helper\r\n    nprocs=torch.cuda.device_count(),\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 2 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 360, in distributed_main\r\n    args, init_distributed=True, after_distributed_init_fn=after_distributed_init_fn\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 63, in main\r\n    args.distributed_rank = distributed_utils.distributed_init(args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq/distributed_utils.py\", line 99, in distributed_init\r\n    dist.all_reduce(torch.zeros(1).cuda())\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 902, in all_reduce\r\n    work = _default_pg.allreduce([tensor], opts)\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1579022034529/work/torch/lib/c10d/../c10d/NCCLUtils.hpp:78, invalid argument, NCCL version 2.4.8\r\nTraceback (most recent call last):\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 372, in cli_main\r\n    cli_main_helper(args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 387, in cli_main_helper\r\n    nprocs=torch.cuda.device_count(),\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 3 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 360, in distributed_main\r\n    args, init_distributed=True, after_distributed_init_fn=after_distributed_init_fn\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 63, in main\r\n    args.distributed_rank = distributed_utils.distributed_init(args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq/distributed_utils.py\", line 99, in distributed_init\r\n    dist.all_reduce(torch.zeros(1).cuda())\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 902, in all_reduce\r\n    work = _default_pg.allreduce([tensor], opts)\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1579022034529/work/torch/lib/c10d/../c10d/NCCLUtils.hpp:78, invalid argument, NCCL version 2.4.8\r\nTraceback (most recent call last):\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 372, in cli_main\r\n    cli_main_helper(args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 387, in cli_main_helper\r\n    nprocs=torch.cuda.device_count(),\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 360, in distributed_main\r\n    args, init_distributed=True, after_distributed_init_fn=after_distributed_init_fn\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq_cli/train.py\", line 63, in main\r\n    args.distributed_rank = distributed_utils.distributed_init(args)\r\n  File \"/home/msrauser/xiangxin/project/fairseq/fairseq/distributed_utils.py\", line 99, in distributed_init\r\n    dist.all_reduce(torch.zeros(1).cuda())\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 902, in all_reduce\r\n    work = _default_pg.allreduce([tensor], opts)\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1579022034529/work/torch/lib/c10d/ProcessGroupNCCL.cpp:410, unhandled system error, NCCL version 2.4.8\r\nTraceback (most recent call last):\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/launch.py\", line 263, in <module>\r\n    main()\r\n  File \"/home/msrauser/anaconda3/envs/fairseq-py36-torch14/lib/python3.6/site-packages/torch/distributed/launch.py\", line 259, in main\r\n    cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/python', '-u', '/home/msrauser/anaconda3/envs/fairseq-py36-torch14/bin/f\r\nairseq-train', '--local_rank=3', '/mnt/distributed-xiangxin-50006/sampling-distill/bin', '--task', 'language_modeling', '--share-decoder-input-output-embed', '--arch', '\r\ntransformer_lm', '--clip-norm', '0.0', '--lr-scheduler', 'inverse_sqrt', '--warmup-init-lr', '1e-07', '--warmup-updates', '8000', '--optimizer', 'adam', '--adam-betas',\r\n'(0.9, 0.98)', '--decoder-layers', '3', '--decoder-embed-dim', '1024', '--decoder-ffn-embed-dim', '4096', '--decoder-attention-heads', '16', '--lr', '0.0007', '--min-lr'\r\n, '1e-09', '--weight-decay', '0.0', '--criterion', 'cross_entropy', '--dropout', '0.0', '--save-dir', '/mnt/distributed/checkpoints/trans\r\nformer_lm/1blm-3L-1024ED-4096FD-16H-shared-lr0.0007-SEED1-DATAsampling-beam1-DROPOUT0.0-CRITERIONcross_entropy', '--max-epoch', '200', '--max-tokens', '8000', '--update-\r\nfreq', '8', '--no-progress-bar', '--seed', '1', '--sample-break-mode', 'eos', '--num-workers', '20', '--tensorboard-logdir', '/mnt/distributed/checkpoints/transformer_lm/1blm-3L-1024ED-4096FD-16H-shared-lr0.0007-SEED1-DROPOUT0.0-CRITERIONcross_entropy', '--distributed-backend', 'nccl']'\r\n returned non-zero exit status 1.\r\n\r\n```\r\n\r\n### To Reproduce\r\n\r\nMy training script is as the following: \r\n```\r\nMASTER_IP=\"127.0.0.1\"\r\nMASTER_PORT=1234\r\nTOTAL_NODES_NUM=1\r\nNODE_RANK=$1\r\n\r\nDATA_DIR=/mnt/distributed/bin\r\n\r\narch=transformer_lm\r\nseed=1\r\nlr=0.0007\r\ndecoder_layers=3\r\ndecoder_embed_dim=1024\r\ndecoder_ffn_embed_dim=4096\r\ndecoder_attention_heads=16\r\ndropout=0.0\r\ncriterion=cross_entropy\r\nlabel_smoothing=0.0\r\n\r\nSAVE_DIR=/mnt/distributed/checkpoints/${arch}/1blm-${decoder_layers}L-${decoder_embed_dim}ED-${decoder_ffn_embed_dim}FD-${decoder_attention_heads}H-shared-lr${lr}-SEED${seed}-DROPOUT${dropout}-CRITERION${criterion}\r\n\r\nRESTORE_FILE=/mnt/distributed/restore-checkpoint/checkpoint23.pt\r\n\r\nRESTORE_FILE=/mnt/distributed-xiangxin-${SERVER_PORT}/sampling-distill/restore-checkpoint/checkpoint23.pt\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 \\\r\n    --nnodes=${TOTAL_NODES_NUM} --node_rank=${NODE_RANK} --master_addr=${MASTER_IP} \\\r\n    --master_port=${MASTER_PORT} \\\r\n    $(which fairseq-train) ${DATA_DIR} \\\r\n    --task language_modeling \\\r\n    --share-decoder-input-output-embed \\\r\n    --arch ${arch} \\\r\n    --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 8000 \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n    --decoder-layers ${decoder_layers} \\\r\n    --decoder-embed-dim ${decoder_embed_dim} \\\r\n    --decoder-ffn-embed-dim ${decoder_ffn_embed_dim} \\\r\n    --decoder-attention-heads ${decoder_attention_heads} \\\r\n    --lr ${lr} --min-lr 1e-09 --weight-decay 0.0 \\\r\n    --criterion ${criterion} --dropout ${dropout} \\\r\n    --save-dir ${SAVE_DIR} \\\r\n    --max-epoch 200 --max-tokens 8000 --update-freq 8 \\\r\n    --no-progress-bar --seed ${seed} \\\r\n    --sample-break-mode eos \\\r\n    --num-workers 20 \\\r\n    --tensorboard-logdir ${SAVE_DIR} \\\r\n    --distributed-backend nccl \\\r\n    --distributed-no-spawn \\\r\n```\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) : 1.4.0\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: python3.6\r\n - CUDA/cuDNN version: CUDA10.1\r\n - GPU models and configuration: 4 V100\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2387/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2387/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2379", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2379/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2379/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2379/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2379", "id": 666271366, "node_id": "MDU6SXNzdWU2NjYyNzEzNjY=", "number": 2379, "title": "`keep_last_checkpoints` doesn't work with `checkpoint_suffix`", "user": {"login": "erip", "id": 2348806, "node_id": "MDQ6VXNlcjIzNDg4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/2348806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erip", "html_url": "https://github.com/erip", "followers_url": "https://api.github.com/users/erip/followers", "following_url": "https://api.github.com/users/erip/following{/other_user}", "gists_url": "https://api.github.com/users/erip/gists{/gist_id}", "starred_url": "https://api.github.com/users/erip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erip/subscriptions", "organizations_url": "https://api.github.com/users/erip/orgs", "repos_url": "https://api.github.com/users/erip/repos", "events_url": "https://api.github.com/users/erip/events{/privacy}", "received_events_url": "https://api.github.com/users/erip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-27T12:53:53Z", "updated_at": "2020-07-30T02:22:18Z", "closed_at": "2020-07-30T02:22:18Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen a suffix is added to checkpoints, the last n checkpoints aren't retained when keeping last N checkpoints.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. \r\n```sh\r\nfairseq-train pretrain-data-bin \\\r\n    --source-lang $SRC --target-lang $TGT \\\r\n    --clip-norm 0.1 --dropout 0.2 --max-tokens 4096 \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr  0.0005 --lr-scheduler inverse_sqrt \\\r\n    --label-smoothing 0.1 \\\r\n    --warmup-init-lr 1e-7 \\\r\n    --checkpoint-suffix \"$SLURM_JOB_ID\" \\\r\n    --warmup-updates 4000 \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --save-dir \"checkpoints-$SLURM_JOB_ID\" \\\r\n    --arch transformer \\\r\n    --no-epoch-checkpoints \\\r\n    --keep-last-epochs 10 \\\r\n    --max-epoch 100 \\\r\n    --num-workers 8 \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --ddp-backend no_c10d \\\r\n    --tensorboard-logdir \"logdir-$SLURM_JOB_ID\"\r\n```\r\n\r\n### Expected behavior\r\n\r\nI expect the suffix to be resuffixed with the epoch number and to be trained according to [checkpoint_paths](https://github.com/pytorch/fairseq/blob/145bc9de1278414812b2aef837be9ca0e9c1aebc/fairseq/checkpoint_utils.py#L216).\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 5d88d379cab13c8351b39341306d7a30a2a8acb8\r\n - PyTorch Version (e.g., 1.0) 1.5.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): n/a\r\n - Python version: 3.6.8\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2379/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2379/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2376", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2376/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2376/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2376/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2376", "id": 666062522, "node_id": "MDU6SXNzdWU2NjYwNjI1MjI=", "number": 2376, "title": "Sentence prediction task does not add BOS token", "user": {"login": "prihoda", "id": 2894124, "node_id": "MDQ6VXNlcjI4OTQxMjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/2894124?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prihoda", "html_url": "https://github.com/prihoda", "followers_url": "https://api.github.com/users/prihoda/followers", "following_url": "https://api.github.com/users/prihoda/following{/other_user}", "gists_url": "https://api.github.com/users/prihoda/gists{/gist_id}", "starred_url": "https://api.github.com/users/prihoda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prihoda/subscriptions", "organizations_url": "https://api.github.com/users/prihoda/orgs", "repos_url": "https://api.github.com/users/prihoda/repos", "events_url": "https://api.github.com/users/prihoda/events{/privacy}", "received_events_url": "https://api.github.com/users/prihoda/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-27T07:36:11Z", "updated_at": "2020-07-27T13:07:33Z", "closed_at": "2020-07-27T13:07:33Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI noticed that even though the code expects the BOS token to be present in sentence prediction (\r\nhttps://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/model.py#L273), it is actually not added by the `sentence_prediction` task. Is this expected behavior? I guess the task still works, but might produce unexpected results when different tokens appear at the first position.  \r\n\r\n### To Reproduce\r\n\r\nThis is clear from the code, but can be validated by running `test_roberta_sentence_prediction` test in `test_binaries.py` in debug mode and inspecting the input dataset.\r\n\r\n### Expected behavior\r\n\r\nBOS token should be added by `sentence_prediciton` task, same as in the `masked_lm` task: https://github.com/pytorch/fairseq/blob/master/fairseq/tasks/masked_lm.py#L126\r\n\r\n### Environment\r\n\r\nN/A", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2376/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2362", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2362/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2362/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2362/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2362", "id": 664047016, "node_id": "MDU6SXNzdWU2NjQwNDcwMTY=", "number": 2362, "title": "Masked LM BERT training broken due to \"has_marked_unused_parameters_\"", "user": {"login": "oshaikh13", "id": 11527433, "node_id": "MDQ6VXNlcjExNTI3NDMz", "avatar_url": "https://avatars.githubusercontent.com/u/11527433?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oshaikh13", "html_url": "https://github.com/oshaikh13", "followers_url": "https://api.github.com/users/oshaikh13/followers", "following_url": "https://api.github.com/users/oshaikh13/following{/other_user}", "gists_url": "https://api.github.com/users/oshaikh13/gists{/gist_id}", "starred_url": "https://api.github.com/users/oshaikh13/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oshaikh13/subscriptions", "organizations_url": "https://api.github.com/users/oshaikh13/orgs", "repos_url": "https://api.github.com/users/oshaikh13/repos", "events_url": "https://api.github.com/users/oshaikh13/events{/privacy}", "received_events_url": "https://api.github.com/users/oshaikh13/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-22T21:00:11Z", "updated_at": "2020-07-22T21:41:20Z", "closed_at": "2020-07-22T21:28:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nRan the scripts verbatim [here](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md), except I used bert_base as the architecture.\r\n\r\nSpecifically, this script:\r\n\r\n```\r\nTOTAL_UPDATES=125000    # Total number of training steps\r\nWARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\r\nPEAK_LR=0.0005          # Peak learning rate, adjust as needed\r\nTOKENS_PER_SAMPLE=512   # Max sequence length\r\nMAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\r\nMAX_SENTENCES=16        # Number of sequences per batch (batch size)\r\nUPDATE_FREQ=16          # Increase the batch size 16x\r\n\r\nDATA_DIR=data-bin/wikitext-103\r\n\r\nfairseq-train --fp16 $DATA_DIR \\\r\n    --task masked_lm --criterion masked_lm \\\r\n    --arch bert_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\r\n    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\r\n    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\nHere's the stack-trace error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/nethome/oshaikh3/miniconda3/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/nethome/oshaikh3/og_fairseq/fairseq_cli/train.py\", line 349, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/nethome/oshaikh3/og_fairseq/fairseq/distributed_utils.py\", line 174, in call_main\r\n    args.distributed_world_size,\r\n  File \"/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/nethome/oshaikh3/og_fairseq/fairseq/distributed_utils.py\", line 156, in distributed_main\r\n    main(args, **kwargs)\r\n  File \"/nethome/oshaikh3/og_fairseq/fairseq_cli/train.py\", line 121, in main\r\n    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\r\n  File \"/nethome/oshaikh3/miniconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/nethome/oshaikh3/og_fairseq/fairseq_cli/train.py\", line 216, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/nethome/oshaikh3/miniconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/nethome/oshaikh3/og_fairseq/fairseq/trainer.py\", line 457, in train_step\r\n    raise e\r\n  File \"/nethome/oshaikh3/og_fairseq/fairseq/trainer.py\", line 431, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/nethome/oshaikh3/og_fairseq/fairseq/tasks/fairseq_task.py\", line 350, in train_step\r\n    optimizer.backward(loss)\r\n  File \"/nethome/oshaikh3/og_fairseq/fairseq/optim/fp16_optimizer.py\", line 117, in backward\r\n    loss.backward()\r\n  File \"/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1591914880026/work/torch/csrc/distributed/c10d/reducer.cpp:327, please report a bug to PyTorch.  (mark_variable_ready at /opt/conda/conda-bld/pytorch_1591914880026/work/torch/csrc/distributed/c10d/reducer.cpp:327)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x4e (0x7fdd2d48db5e in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: c10d::Reducer::mark_variable_ready(c10d::Reducer::VariableIndex) + 0x9ba (0x7fdd5ae6f3aa in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #2: c10d::Reducer::autograd_hook(c10d::Reducer::VariableIndex) + 0x2d0 (0x7fdd5ae6f910 in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #3: <unknown function> + 0x8a395c (0x7fdd5ae6495c in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #4: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x60d (0x7fdd5758100d in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\r\nframe #5: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fdd57582ed2 in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\r\nframe #6: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fdd5757b549 in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\r\nframe #7: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fdd5aacb638 in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #8: <unknown function> + 0xc819d (0x7fdd5d33f19d in /nethome/oshaikh3/miniconda3/lib/python3.7/site-packages/torch/lib/../../../.././libstdc++.so.6)\r\nframe #9: <unknown function> + 0x76db (0x7fdd766776db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #10: clone + 0x3f (0x7fdd763a088f in /lib/x86_64-linux-gnu/libc.so.6)\r\n```\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\nModel training starts.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.5\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: 4xV100\r\n - Any other relevant information:\r\n\r\n### Additional Context\r\n\r\nThis error was referenced in #1709 towards the end of the PR. Also similar to these issues:\r\npytorch/pytorch#32490 and pytorch/pytorch#31035. ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2362/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2362/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2355", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2355/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2355/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2355/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2355", "id": 662080954, "node_id": "MDU6SXNzdWU2NjIwODA5NTQ=", "number": 2355, "title": "fairseq-generate not printing score", "user": {"login": "FadyEssam", "id": 29877798, "node_id": "MDQ6VXNlcjI5ODc3Nzk4", "avatar_url": "https://avatars.githubusercontent.com/u/29877798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FadyEssam", "html_url": "https://github.com/FadyEssam", "followers_url": "https://api.github.com/users/FadyEssam/followers", "following_url": "https://api.github.com/users/FadyEssam/following{/other_user}", "gists_url": "https://api.github.com/users/FadyEssam/gists{/gist_id}", "starred_url": "https://api.github.com/users/FadyEssam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FadyEssam/subscriptions", "organizations_url": "https://api.github.com/users/FadyEssam/orgs", "repos_url": "https://api.github.com/users/FadyEssam/repos", "events_url": "https://api.github.com/users/FadyEssam/events{/privacy}", "received_events_url": "https://api.github.com/users/FadyEssam/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-07-20T17:39:16Z", "updated_at": "2020-08-20T15:27:49Z", "closed_at": "2020-08-20T15:27:49Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://github.com/pytorch/fairseq/blob/93f5128509278f425afb6bcf0da574c0af0e0c16/fairseq/bleu.py#L58\r\n\r\nI think **.format()** should be added to the return line as the latest sacrebleu.corpus_bleu() now returns an object not a string", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2355/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2355/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2338", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2338/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2338/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2338/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2338", "id": 658952262, "node_id": "MDU6SXNzdWU2NTg5NTIyNjI=", "number": 2338, "title": "Benchmarking Transformer on V3-8 and GPU", "user": {"login": "kkissmart", "id": 13355967, "node_id": "MDQ6VXNlcjEzMzU1OTY3", "avatar_url": "https://avatars.githubusercontent.com/u/13355967?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kkissmart", "html_url": "https://github.com/kkissmart", "followers_url": "https://api.github.com/users/kkissmart/followers", "following_url": "https://api.github.com/users/kkissmart/following{/other_user}", "gists_url": "https://api.github.com/users/kkissmart/gists{/gist_id}", "starred_url": "https://api.github.com/users/kkissmart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kkissmart/subscriptions", "organizations_url": "https://api.github.com/users/kkissmart/orgs", "repos_url": "https://api.github.com/users/kkissmart/repos", "events_url": "https://api.github.com/users/kkissmart/events{/privacy}", "received_events_url": "https://api.github.com/users/kkissmart/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-07-17T07:16:13Z", "updated_at": "2020-07-21T16:11:40Z", "closed_at": "2020-07-21T16:09:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Myle,\r\n\r\nMy command to run on TPU:\r\npython train.py /home/xwu/pytorch-tutorial-data/wmt18_en_de_bpej32k  --arch=transformer_vaswani_wmt_en_de_big  -s en -t de --criterion cross_entropy --encoder-normalize-before --decoder-normalize-before --task translation --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' --lr-scheduler polynomial_decay --lr 1e-04 --min-lr -1 --warmup-updates 10000 --total-num-update 500000 --dropout 0.0 --attention-dropout 0.0 --weight-decay 0.0 --max-tokens 2052 --seed 2 --log-format simple --log-interval 100  --max-source-positions 1026 --max-target-positions 1026 --save-interval-updates 5000 --skip-invalid-size-inputs-valid-test  --num-batch-buckets 8 --tpu\r\n\r\nMy command to run on GPU:\r\nython train.py /data/pytorch-tutorial-data/wmt18_en_de_bpej32k  --arch=transformer_vaswani_wmt_en_de_big  -s en -t de --criterion cross_entropy --encoder-normalize-before --decoder-normalize-before --task translation --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' --lr-scheduler polynomial_decay --lr 1e-04 --min-lr -1 --warmup-updates 10000 --total-num-update 500000 --dropout 0.0 --attention-dropout 0.0 --weight-decay 0.0 --max-tokens 2048 --seed 2 --log-format simple --log-interval 100  --max-source-positions 1026 --max-target-positions 1026 --save-interval-updates 5000 --skip-invalid-size-inputs-valid-test --fp16\r\n\r\nTPU logs:\r\n2020-07-17 06:57:45 | INFO | train_inner | epoch 001:    100 / 160368 loss=14.065, ppl=17143.1, wps=0, ups=0, wpb=1669, bsz=40, num_updates=100, lr=1e-06, gnorm=4.168, train_wall=321, wall=439\r\n2020-07-17 06:57:45 | INFO | root | NOTE: XLA compilation detected; too many of these can lead to slow training, but we expect a few in the beginning\r\n2020-07-17 06:58:14 | INFO | root | NOTE: XLA compilation detected; too many of these can lead to slow training, but we expect a few in the beginning\r\n2020-07-17 06:58:33 | INFO | train_inner | epoch 001:    200 / 160368 loss=13.504, ppl=11620.5, wps=9.6, ups=0.02, wpb=460, bsz=8, num_updates=200, lr=2e-06, gnorm=4.103, train_wall=34, wall=487\r\n2020-07-17 06:58:33 | INFO | root | NOTE: XLA compilation detected; too many of these can lead to slow training, but we expect a few in the beginning\r\n2020-07-17 06:59:03 | INFO | train_inner | epoch 001:    300 / 160368 loss=12.501, ppl=5798.48, wps=52.5, ups=0.03, wpb=1585, bsz=56, num_updates=300, lr=3e-06, gnorm=2.206, train_wall=16, wall=517\r\n2020-07-17 06:59:33 | INFO | train_inner | epoch 001:    400 / 160368 loss=12.314, ppl=5092.42, wps=15.3, ups=0.03, wpb=463, bsz=8, num_updates=400, lr=4e-06, gnorm=2.927, train_wall=16, wall=547\r\n2020-07-17 07:00:04 | INFO | train_inner | epoch 001:    500 / 160368 loss=12.06, ppl=4268.62, wps=55.5, ups=0.03, wpb=1686, bsz=40, num_updates=500, lr=5e-06, gnorm=1.576, train_wall=16, wall=578\r\n2020-07-17 07:00:33 | INFO | train_inner | epoch 001:    600 / 160368 loss=11.612, ppl=3129.2, wps=57.5, ups=0.03, wpb=1668, bsz=48, num_updates=600, lr=6e-06, gnorm=1.75, train_wall=15, wall=607\r\n2020-07-17 07:01:02 | INFO | train_inner | epoch 001:    700 / 160368 loss=11.286, ppl=2497.29, wps=62.6, ups=0.03, wpb=1822, bsz=64, num_updates=700, lr=7e-06, gnorm=1.592, train_wall=15, wall=636\r\n2020-07-17 07:01:30 | INFO | train_inner | epoch 001:    800 / 160368 loss=10.75, ppl=1721.78, wps=15, ups=0.03, wpb=428, bsz=8, num_updates=800, lr=8e-06, gnorm=2.819, train_wall=14, wall=664\r\n2020-07-17 07:01:59 | INFO | train_inner | epoch 001:    900 / 160368 loss=10.877, ppl=1880.37, wps=57.5, ups=0.03, wpb=1652, bsz=48, num_updates=900, lr=9e-06, gnorm=1.48, train_wall=14, wall=693\r\n2020-07-17 07:02:28 | INFO | train_inner | epoch 001:   1000 / 160368 loss=10.626, ppl=1580.54, wps=17, ups=0.03, wpb=502, bsz=8, num_updates=1000, lr=1e-05, gnorm=2.802, train_wall=15, wall=723\r\n2020-07-17 07:02:58 | INFO | train_inner | epoch 001:   1100 / 160368 loss=11.066, ppl=2144.5, wps=19, ups=0.03, wpb=550, bsz=8, num_updates=1100, lr=1.1e-05, gnorm=3.895, train_wall=14, wall=752\r\n2020-07-17 07:03:27 | INFO | train_inner | epoch 001:   1200 / 160368 loss=10.876, ppl=1878.94, wps=16.4, ups=0.03, wpb=480, bsz=8, num_updates=1200, lr=1.2e-05, gnorm=3.95, train_wall=15, wall=781\r\n2020-07-17 07:03:56 | INFO | train_inner | epoch 001:   1300 / 160368 loss=9.903, ppl=957.11, wps=16.6, ups=0.03, wpb=478, bsz=8, num_updates=1300, lr=1.3e-05, gnorm=3.182, train_wall=14, wall=810\r\n2020-07-17 07:04:24 | INFO | train_inner | epoch 001:   1400 / 160368 loss=10.77, ppl=1745.88, wps=16.7, ups=0.03, wpb=482, bsz=8, num_updates=1400, lr=1.4e-05, gnorm=3.08, train_wall=14, wall=838\r\n2020-07-17 07:04:53 | INFO | train_inner | epoch 001:   1500 / 160368 loss=10.418, ppl=1368.59, wps=15.4, ups=0.04, wpb=434, bsz=8, num_updates=1500, lr=1.5e-05, gnorm=3.361, train_wall=13, wall=867\r\n2020-07-17 07:05:22 | INFO | train_inner | epoch 001:   1600 / 160368 loss=9.917, ppl=966.55, wps=15.5, ups=0.03, wpb=455, bsz=8, num_updates=1600, lr=1.6e-05, gnorm=3.666, train_wall=15, wall=896\r\n2020-07-17 07:05:51 | INFO | train_inner | epoch 001:   1700 / 160368 loss=10.035, ppl=1049.38, wps=59.2, ups=0.03, wpb=1744, bsz=72, num_updates=1700, lr=1.7e-05, gnorm=2.067, train_wall=15, wall=925\r\n2020-07-17 07:06:19 | INFO | train_inner | epoch 001:   1800 / 160368 loss=9.286, ppl=624.27, wps=16.6, ups=0.04, wpb=465, bsz=8, num_updates=1800, lr=1.8e-05, gnorm=4.038, train_wall=13, wall=953\r\n2020-07-17 07:06:48 | INFO | train_inner | epoch 001:   1900 / 160368 loss=9.64, ppl=797.76, wps=62.2, ups=0.04, wpb=1757, bsz=88, num_updates=1900, lr=1.9e-05, gnorm=2.211, train_wall=13, wall=982\r\n2020-07-17 07:07:16 | INFO | train_inner | epoch 001:   2000 / 160368 loss=10.043, ppl=1055.05, wps=18.3, ups=0.03, wpb=525, bsz=8, num_updates=2000, lr=2e-05, gnorm=3.888, train_wall=14, wall=1010\r\n2020-07-17 07:07:45 | INFO | train_inner | epoch 001:   2100 / 160368 loss=9.814, ppl=899.94, wps=61.5, ups=0.04, wpb=1748, bsz=72, num_updates=2100, lr=2.1e-05, gnorm=3.225, train_wall=13, wall=1039\r\n2020-07-17 07:08:13 | INFO | train_inner | epoch 001:   2200 / 160368 loss=9.796, ppl=888.87, wps=61, ups=0.04, wpb=1721, bsz=40, num_updates=2200, lr=2.2e-05, gnorm=2.058, train_wall=13, wall=1067\r\n2020-07-17 07:08:41 | INFO | train_inner | epoch 001:   2300 / 160368 loss=9.557, ppl=753.38, wps=60.9, ups=0.04, wpb=1723, bsz=40, num_updates=2300, lr=2.3e-05, gnorm=1.864, train_wall=13, wall=1095\r\n2020-07-17 07:09:10 | INFO | train_inner | epoch 001:   2400 / 160368 loss=9.778, ppl=877.69, wps=58.2, ups=0.03, wpb=1683, bsz=48, num_updates=2400, lr=2.4e-05, gnorm=2.108, train_wall=14, wall=1124\r\n2020-07-17 07:09:38 | INFO | train_inner | epoch 001:   2500 / 160368 loss=9.563, ppl=756.6, wps=60.1, ups=0.04, wpb=1690, bsz=40, num_updates=2500, lr=2.5e-05, gnorm=2.048, train_wall=13, wall=1152\r\n2020-07-17 07:10:06 | INFO | train_inner | epoch 001:   2600 / 160368 loss=10.827, ppl=1816.58, wps=21.5, ups=0.04, wpb=600, bsz=8, num_updates=2600, lr=2.6e-05, gnorm=2.829, train_wall=13, wall=1180\r\n2020-07-17 07:10:34 | INFO | train_inner | epoch 001:   2700 / 160368 loss=9.462, ppl=705.37, wps=61.3, ups=0.04, wpb=1713, bsz=40, num_updates=2700, lr=2.7e-05, gnorm=1.84, train_wall=13, wall=1208\r\n2020-07-17 07:11:02 | INFO | train_inner | epoch 001:   2800 / 160368 loss=9.255, ppl=611.12, wps=57.3, ups=0.04, wpb=1613, bsz=80, num_updates=2800, lr=2.8e-05, gnorm=3.23, train_wall=13, wall=1236\r\n2020-07-17 07:11:31 | INFO | train_inner | epoch 001:   2900 / 160368 loss=8.979, ppl=504.73, wps=53.5, ups=0.04, wpb=1504, bsz=64, num_updates=2900, lr=2.9e-05, gnorm=2.39, train_wall=13, wall=1265\r\n2020-07-17 07:12:00 | INFO | train_inner | epoch 001:   3000 / 160368 loss=9.376, ppl=664.29, wps=16.2, ups=0.03, wpb=469, bsz=8, num_updates=3000, lr=3e-05, gnorm=3.214, train_wall=14, wall=1294\r\n2020-07-17 07:12:27 | INFO | train_inner | epoch 001:   3100 / 160368 loss=9.251, ppl=609.46, wps=60.4, ups=0.04, wpb=1661, bsz=48, num_updates=3100, lr=3.1e-05, gnorm=1.859, train_wall=12, wall=1321\r\n2020-07-17 07:12:55 | INFO | train_inner | epoch 001:   3200 / 160368 loss=10.278, ppl=1241.54, wps=16.9, ups=0.04, wpb=469, bsz=8, num_updates=3200, lr=3.2e-05, gnorm=3.02, train_wall=13, wall=1349\r\n\r\nGPU logs:\r\n2020-07-17 06:48:46 | INFO | train_inner | epoch 001:    100 / 167138 loss=15.073, ppl=34470.4, wps=4004.3, ups=4.08, wpb=980.3, bsz=35.4, num_updates=100, lr=1e-06, gnorm=7.216, loss_scale=128, train_wall=25, wall=83\r\n2020-07-17 06:49:10 | INFO | train_inner | epoch 001:    200 / 167138 loss=13.403, ppl=10832.5, wps=3854.3, ups=4.09, wpb=943.4, bsz=30, num_updates=200, lr=2e-06, gnorm=3.941, loss_scale=128, train_wall=24, wall=108\r\n2020-07-17 06:49:35 | INFO | train_inner | epoch 001:    300 / 167138 loss=12.732, ppl=6803.28, wps=3758.6, ups=4.08, wpb=921.2, bsz=33.3, num_updates=300, lr=3e-06, gnorm=3.067, loss_scale=128, train_wall=24, wall=132\r\n2020-07-17 06:49:59 | INFO | train_inner | epoch 001:    400 / 167138 loss=12.379, ppl=5326.36, wps=3601.1, ups=4.08, wpb=883.3, bsz=28.8, num_updates=400, lr=4e-06, gnorm=2.736, loss_scale=128, train_wall=24, wall=157\r\n2020-07-17 06:50:24 | INFO | train_inner | epoch 001:    500 / 167138 loss=12.012, ppl=4130.8, wps=3976.9, ups=4.07, wpb=976.2, bsz=35.4, num_updates=500, lr=5e-06, gnorm=2.507, loss_scale=128, train_wall=24, wall=181\r\n2020-07-17 06:50:48 | INFO | train_inner | epoch 001:    600 / 167138 loss=11.653, ppl=3220.91, wps=3852.7, ups=4.07, wpb=946.4, bsz=32.1, num_updates=600, lr=6e-06, gnorm=2.44, loss_scale=128, train_wall=24, wall=206\r\n2020-07-17 06:51:13 | INFO | train_inner | epoch 001:    700 / 167138 loss=11.342, ppl=2596.52, wps=3730.3, ups=4.06, wpb=918.1, bsz=29.9, num_updates=700, lr=7e-06, gnorm=2.396, loss_scale=128, train_wall=25, wall=230\r\n2020-07-17 06:51:38 | INFO | train_inner | epoch 001:    800 / 167138 loss=11.027, ppl=2086.98, wps=3781.3, ups=4.07, wpb=929, bsz=31.7, num_updates=800, lr=8e-06, gnorm=2.348, loss_scale=128, train_wall=24, wall=255\r\n2020-07-17 06:52:02 | INFO | train_inner | epoch 001:    900 / 167138 loss=10.9, ppl=1910.22, wps=3707.8, ups=4.07, wpb=911.3, bsz=30.9, num_updates=900, lr=9e-06, gnorm=2.447, loss_scale=128, train_wall=24, wall=279\r\n2020-07-17 06:52:27 | INFO | train_inner | epoch 001:   1000 / 167138 loss=10.692, ppl=1654.68, wps=3481, ups=4.06, wpb=856.8, bsz=28, num_updates=1000, lr=1e-05, gnorm=2.537, loss_scale=128, train_wall=25, wall=304\r\n2020-07-17 06:52:51 | INFO | train_inner | epoch 001:   1100 / 167138 loss=10.556, ppl=1505.76, wps=3732.9, ups=4.07, wpb=917.5, bsz=32.2, num_updates=1100, lr=1.1e-05, gnorm=2.572, loss_scale=128, train_wall=24, wall=329\r\n2020-07-17 06:53:16 | INFO | train_inner | epoch 001:   1200 / 167138 loss=10.477, ppl=1425.54, wps=3522.9, ups=4.07, wpb=866.6, bsz=28.7, num_updates=1200, lr=1.2e-05, gnorm=2.642, loss_scale=128, train_wall=25, wall=353\r\n2020-07-17 06:53:41 | INFO | train_inner | epoch 001:   1300 / 167138 loss=10.335, ppl=1291.53, wps=3713.6, ups=4.07, wpb=913.1, bsz=29.7, num_updates=1300, lr=1.3e-05, gnorm=2.615, loss_scale=128, train_wall=24, wall=378\r\n2020-07-17 06:54:05 | INFO | train_inner | epoch 001:   1400 / 167138 loss=10.329, ppl=1286.27, wps=3648.2, ups=4.06, wpb=899.5, bsz=28.8, num_updates=1400, lr=1.4e-05, gnorm=2.669, loss_scale=128, train_wall=25, wall=402\r\n2020-07-17 06:54:30 | INFO | train_inner | epoch 001:   1500 / 167138 loss=10.197, ppl=1174.06, wps=3499.8, ups=4.05, wpb=863.4, bsz=28, num_updates=1500, lr=1.5e-05, gnorm=2.778, loss_scale=128, train_wall=25, wall=427\r\n2020-07-17 06:54:55 | INFO | train_inner | epoch 001:   1600 / 167138 loss=10.183, ppl=1162.2, wps=3441.2, ups=4.05, wpb=850.2, bsz=27, num_updates=1600, lr=1.6e-05, gnorm=2.936, loss_scale=128, train_wall=25, wall=452\r\n2020-07-17 06:55:19 | INFO | train_inner | epoch 001:   1700 / 167138 loss=10.076, ppl=1079.72, wps=3432.7, ups=4.04, wpb=848.9, bsz=29, num_updates=1700, lr=1.7e-05, gnorm=2.85, loss_scale=128, train_wall=25, wall=477\r\n2020-07-17 06:55:44 | INFO | train_inner | epoch 001:   1800 / 167138 loss=10.019, ppl=1037.53, wps=3773, ups=4.06, wpb=929, bsz=30.4, num_updates=1800, lr=1.8e-05, gnorm=2.732, loss_scale=128, train_wall=25, wall=501\r\n2020-07-17 06:56:09 | INFO | train_inner | epoch 001:   1900 / 167138 loss=9.935, ppl=979.13, wps=3662.8, ups=4.05, wpb=904.6, bsz=29.6, num_updates=1900, lr=1.9e-05, gnorm=2.863, loss_scale=128, train_wall=25, wall=526\r\n2020-07-17 06:56:33 | INFO | train_inner | epoch 001:   2000 / 167138 loss=9.865, ppl=932.27, wps=3459.6, ups=4.05, wpb=854.2, bsz=26.3, num_updates=2000, lr=2e-05, gnorm=2.875, loss_scale=128, train_wall=25, wall=551\r\n2020-07-17 06:56:58 | INFO | train_inner | epoch 001:   2100 / 167138 loss=9.754, ppl=863.65, wps=3779.8, ups=4.07, wpb=929.5, bsz=32.5, num_updates=2100, lr=2.1e-05, gnorm=2.861, loss_scale=128, train_wall=24, wall=575\r\n2020-07-17 06:57:23 | INFO | train_inner | epoch 001:   2200 / 167138 loss=9.741, ppl=855.5, wps=3619.4, ups=4.07, wpb=889, bsz=29.3, num_updates=2200, lr=2.2e-05, gnorm=2.932, loss_scale=128, train_wall=24, wall=600\r\n2020-07-17 06:57:47 | INFO | train_inner | epoch 001:   2300 / 167138 loss=9.618, ppl=785.55, wps=3834.2, ups=4.08, wpb=940.3, bsz=33.9, num_updates=2300, lr=2.3e-05, gnorm=2.784, loss_scale=128, train_wall=24, wall=624\r\n2020-07-17 06:58:12 | INFO | train_inner | epoch 001:   2400 / 167138 loss=9.509, ppl=728.74, wps=3961.5, ups=4.07, wpb=973.1, bsz=33.9, num_updates=2400, lr=2.4e-05, gnorm=2.783, loss_scale=128, train_wall=24, wall=649\r\n2020-07-17 06:58:36 | INFO | train_inner | epoch 001:   2500 / 167138 loss=9.486, ppl=717.05, wps=3512.7, ups=4.05, wpb=866.6, bsz=26.6, num_updates=2500, lr=2.5e-05, gnorm=2.851, loss_scale=128, train_wall=25, wall=673\r\n2020-07-17 06:59:01 | INFO | train_inner | epoch 001:   2600 / 167138 loss=9.371, ppl=661.99, wps=3738.4, ups=4.07, wpb=918.2, bsz=31.4, num_updates=2600, lr=2.6e-05, gnorm=2.809, loss_scale=128, train_wall=24, wall=698\r\n2020-07-17 06:59:26 | INFO | train_inner | epoch 001:   2700 / 167138 loss=9.308, ppl=633.77, wps=3681.4, ups=4.06, wpb=907, bsz=28.7, num_updates=2700, lr=2.7e-05, gnorm=2.841, loss_scale=128, train_wall=25, wall=723\r\n2020-07-17 06:59:50 | INFO | train_inner | epoch 001:   2800 / 167138 loss=9.185, ppl=582.03, wps=3664, ups=4.06, wpb=903.1, bsz=30.4, num_updates=2800, lr=2.8e-05, gnorm=2.726, loss_scale=128, train_wall=25, wall=747\r\n2020-07-17 07:00:15 | INFO | train_inner | epoch 001:   2900 / 167138 loss=9.128, ppl=559.68, wps=3830.1, ups=4.07, wpb=940.1, bsz=32.2, num_updates=2900, lr=2.9e-05, gnorm=2.75, loss_scale=128, train_wall=24, wall=772\r\n2020-07-17 07:00:39 | INFO | train_inner | epoch 001:   3000 / 167138 loss=9.132, ppl=561.23, wps=3668.7, ups=4.05, wpb=905.1, bsz=28.8, num_updates=3000, lr=3e-05, gnorm=2.85, loss_scale=128, train_wall=25, wall=797\r\n2020-07-17 07:01:04 | INFO | train_inner | epoch 001:   3100 / 167138 loss=9.076, ppl=539.76, wps=3558.3, ups=4.06, wpb=877.1, bsz=27.8, num_updates=3100, lr=3.1e-05, gnorm=2.736, loss_scale=128, train_wall=25, wall=821\r\n2020-07-17 07:01:29 | INFO | train_inner | epoch 001:   3200 / 167138 loss=8.952, ppl=495.35, wps=3694.6, ups=4.07, wpb=908.7, bsz=29.9, num_updates=3200, lr=3.2e-05, gnorm=2.724, loss_scale=128, train_wall=24, wall=846\r\n2020-07-17 07:01:53 | INFO | train_inner | epoch 001:   3300 / 167138 loss=8.91, ppl=480.92, wps=3925.6, ups=4.07, wpb=965, bsz=32.3, num_updates=3300, lr=3.3e-05, gnorm=2.608, loss_scale=128, train_wall=24, wall=870\r\n2020-07-17 07:02:18 | INFO | train_inner | epoch 001:   3400 / 167138 loss=8.839, ppl=457.86, wps=3751.3, ups=4.06, wpb=923.5, bsz=30.6, num_updates=3400, lr=3.4e-05, gnorm=2.602, loss_scale=128, train_wall=25, wall=895\r\n2020-07-17 07:02:43 | INFO | train_inner | epoch 001:   3500 / 167138 loss=8.793, ppl=443.65, wps=3807.2, ups=4.05, wpb=939.4, bsz=31.6, num_updates=3500, lr=3.5e-05, gnorm=2.686, loss_scale=128, train_wall=25, wall=920\r\n2020-07-17 07:03:07 | INFO | train_inner | epoch 001:   3600 / 167138 loss=8.645, ppl=400.34, wps=3720.7, ups=4.05, wpb=918.2, bsz=33.7, num_updates=3600, lr=3.6e-05, gnorm=2.661, loss_scale=128, train_wall=25, wall=944\r\n2020-07-17 07:03:32 | INFO | train_inner | epoch 001:   3700 / 167138 loss=8.624, ppl=394.59, wps=3671.7, ups=4.06, wpb=903.7, bsz=30.7, num_updates=3700, lr=3.7e-05, gnorm=2.671, loss_scale=128, train_wall=25, wall=969\r\n2020-07-17 07:03:56 | INFO | train_inner | epoch 001:   3800 / 167138 loss=8.612, ppl=391.22, wps=3601.4, ups=4.07, wpb=884.9, bsz=28.3, num_updates=3800, lr=3.8e-05, gnorm=2.699, loss_scale=128, train_wall=24, wall=994\r\n2020-07-17 07:04:21 | INFO | train_inner | epoch 001:   3900 / 167138 loss=8.575, ppl=381.31, wps=3618.8, ups=4.06, wpb=891.2, bsz=28.5, num_updates=3900, lr=3.9e-05, gnorm=2.666, loss_scale=128, train_wall=25, wall=1018\r\n2020-07-17 07:04:46 | INFO | train_inner | epoch 001:   4000 / 167138 loss=8.53, ppl=369.76, wps=3240.5, ups=4.04, wpb=801.4, bsz=24.1, num_updates=4000, lr=4e-05, gnorm=2.708, loss_scale=128, train_wall=25, wall=1043\r\n2020-07-17 07:05:10 | INFO | train_inner | epoch 001:   4100 / 167138 loss=8.514, ppl=365.54, wps=3703, ups=4.07, wpb=910.9, bsz=30.3, num_updates=4100, lr=4.1e-05, gnorm=2.657, loss_scale=128, train_wall=24, wall=1068\r\n2020-07-17 07:05:35 | INFO | train_inner | epoch 001:   4200 / 167138 loss=8.422, ppl=342.92, wps=3606.6, ups=4.06, wpb=887.8, bsz=29.6, num_updates=4200, lr=4.2e-05, gnorm=2.588, loss_scale=128, train_wall=25, wall=1092\r\n2020-07-17 07:05:59 | INFO | train_inner | epoch 001:   4300 / 167138 loss=8.45, ppl=349.72, wps=3786.9, ups=4.08, wpb=928.8, bsz=28.4, num_updates=4300, lr=4.3e-05, gnorm=2.572, loss_scale=128, train_wall=24, wall=1117\r\n2020-07-17 07:06:24 | INFO | train_inner | epoch 001:   4400 / 167138 loss=8.32, ppl=319.68, wps=3741.7, ups=4.07, wpb=919.6, bsz=30.7, num_updates=4400, lr=4.4e-05, gnorm=2.57, loss_scale=128, train_wall=24, wall=1141\r\n2020-07-17 07:06:49 | INFO | train_inner | epoch 001:   4500 / 167138 loss=8.321, ppl=319.77, wps=3761.8, ups=4.07, wpb=923.7, bsz=29.7, num_updates=4500, lr=4.5e-05, gnorm=2.577, loss_scale=128, train_wall=24, wall=1166\r\n2020-07-17 07:07:13 | INFO | train_inner | epoch 001:   4600 / 167138 loss=8.263, ppl=307.26, wps=3806.7, ups=4.06, wpb=937.6, bsz=31.2, num_updates=4600, lr=4.6e-05, gnorm=2.533, loss_scale=128, train_wall=25, wall=1190\r\n2020-07-17 07:07:38 | INFO | train_inner | epoch 001:   4700 / 167138 loss=8.237, ppl=301.71, wps=3571.1, ups=4.06, wpb=880.2, bsz=27.3, num_updates=4700, lr=4.7e-05, gnorm=2.579, loss_scale=128, train_wall=25, wall=1215\r\n2020-07-17 07:08:02 | INFO | train_inner | epoch 001:   4800 / 167138 loss=8.033, ppl=261.86, wps=3924.6, ups=4.08, wpb=962, bsz=36.1, num_updates=4800, lr=4.8e-05, gnorm=2.512, loss_scale=128, train_wall=24, wall=1240\r\n\r\n\r\nthey seem to behave very differently. \r\nthis aligns with the other arch \r\nhttps://github.com/pytorch/fairseq/issues/2332\r\n\r\n@myleott @taylanbil \r\n\r\nlet me know your thought. or any hint on how to debug TPU. \r\n\r\nThanks", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2338/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2335", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2335/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2335/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2335/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2335", "id": 658669059, "node_id": "MDU6SXNzdWU2NTg2NjkwNTk=", "number": 2335, "title": "Incorrect build of dataset during inference of translation_lev task.", "user": {"login": "qavion", "id": 1917998, "node_id": "MDQ6VXNlcjE5MTc5OTg=", "avatar_url": "https://avatars.githubusercontent.com/u/1917998?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qavion", "html_url": "https://github.com/qavion", "followers_url": "https://api.github.com/users/qavion/followers", "following_url": "https://api.github.com/users/qavion/following{/other_user}", "gists_url": "https://api.github.com/users/qavion/gists{/gist_id}", "starred_url": "https://api.github.com/users/qavion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qavion/subscriptions", "organizations_url": "https://api.github.com/users/qavion/orgs", "repos_url": "https://api.github.com/users/qavion/repos", "events_url": "https://api.github.com/users/qavion/events{/privacy}", "received_events_url": "https://api.github.com/users/qavion/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-16T23:41:50Z", "updated_at": "2020-08-27T17:12:04Z", "closed_at": "2020-08-27T17:12:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIn translation_lev task, `bos` should add when building a dataset, but it doesn't for inference.\r\nAs a result, the output in `fairseq-interactive` becomes strange and differs from the output in `fairseq-generate`.\r\n\r\n### To Reproduce\r\n\r\n0. Prepare data, pre-process and learn a Levenshtein Transformer model.\r\n1. Run cmd `fairseq-interactive data-bin/lang8_mojimoji --buffer-size 1000 --input data/lang8/test.tok.mjmj.bpe.src --task translation_lev --path checkpoints/levt_lang8_mojimoji/checkpoint47.pt --iter-decode-max-iter 9 --iter-decode-eos-penalty 0 --max-tokens 2000 > interactive.out`\r\n2. Run cmd `fairseq-generate data-bin/lang8_mojimoji --task translation_lev --path checkpoints/levt_lang8_mojimoji/checkpoint47.pt --iter-decode-max-iter 9 --iter-decode-eos-penalty 0 --max-tokens 2000 > generate.out`\r\n\r\n- pre-processed data directory: `data-bin/lang8_mojimoji`\r\n- source file: `data/lang8/test.tok.mjmj.bpe.src`\r\n- learned model: `checkpoints/levt_lang8_mojimoji/checkpoint47.pt`\r\n\r\n### Expected behavior\r\n\r\nI expect that the generated results of the `interactive.out` and the `generate.out` will be equal.\r\nHowever, the output of the `interactive.out` is strange.\r\n\r\n`interactive.out`\r\n\r\n```\r\n\ufe19\r\n2020-07-17 08:32:18 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/levt_lang8_mojimoji/checkpoint39.pt\r\n2020-07-17 08:32:25 | INFO | fairseq_cli.interactive | Sentence buffer size: 1000\r\n2020-07-17 08:32:25 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\r\n2020-07-17 08:32:25 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\r\nS-0\t\u2581\u7537\u6027 \u2581\u3068 \u2581\u5973\u6027 \u2581\u306e \u2581\u8a00\u8449 \u2581\u306e \u2581\u9055\u3044 \u2581?\r\nH-0\t-0.6313951015472412\t\u2581\u3068 \u2581\u5973\u6027 \u2581\u306e \u2581\u8a00\u8449 \u2581\u306e \u2581\u7537\u6027 \u2581\u306e \u2581\u9055\u3044 \u2581?\r\nD-0\t-0.6313951015472412\t\u2581\u3068 \u2581\u5973\u6027 \u2581\u306e \u2581\u8a00\u8449 \u2581\u306e \u2581\u7537\u6027 \u2581\u306e \u2581\u9055\u3044 \u2581?\r\nP-0\t0.0000 -0.8670 -0.6678 -0.6739 -0.4745 -0.3614 -2.1308 -0.9909 -0.6863 -0.0927 0.0000\r\n\ufe19\r\nS-6\t\u2581\u52c9\u5f37 \u2581\u3064\u3082\u308a \u2581\u3060\u3063 \u2581\u305f \u2581\u3002 \u2581\u3002 \u2581\u3002 \u2581( \u25812 \u2581\u56de \u2581)\r\nH-6\t-0.1993567794561386\t\u2581\u3064\u3082\u308a \u2581\u3060\u3063 \u2581\u305f \u2581\u3002 \u2581\u3002 \u2581\u3002 \u2581( \u25812 \u2581\u56de \u2581)\r\nD-6\t-0.1993567794561386\t\u2581\u3064\u3082\u308a \u2581\u3060\u3063 \u2581\u305f \u2581\u3002 \u2581\u3002 \u2581\u3002 \u2581( \u25812 \u2581\u56de \u2581)\r\nP-6\t0.0000 -0.5071 -0.1198 -0.2476 -0.4717 -0.1938 -0.1455 -0.2593 -0.1719 -0.2082 -0.0674 0.0000\r\n\ufe19\r\n```\r\nIn interactive, the first token expected to be output may not be output.\r\n\r\n`generate.out`\r\n\r\n```\r\n\ufe19\r\n2020-07-17 08:33:42 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/levt_lang8_mojimoji/checkpoint39.pt\r\n\ufe19\r\nS-0\t\u2581\u7537\u6027 \u2581\u3068 \u2581\u5973\u6027 \u2581\u306e \u2581\u8a00\u8449 \u2581\u306e \u2581\u9055\u3044 \u2581?\r\nT-0\t\u2581\u7537\u6027 \u2581\u3068 \u2581\u5973\u6027 \u2581\u306e \u2581\u8a00\u8449 \u2581\u306e \u2581\u9055\u3044 \u2581?\r\nH-0\t-0.527859091758728\t\u2581\u7537\u6027 \u2581\u3068 \u2581\u5973\u6027 \u2581\u306e \u2581\u8a00\u8449 \u2581\u306e \u2581\u9055\u3044 \u2581\u306f \u2581\u3042\u308b \u2581?\r\nD-0\t-0.527859091758728\t\u2581\u7537\u6027 \u2581\u3068 \u2581\u5973\u6027 \u2581\u306e \u2581\u8a00\u8449 \u2581\u306e \u2581\u9055\u3044 \u2581\u306f \u2581\u3042\u308b \u2581?\r\nP-0\t0.0000 -0.0860 -0.3473 -0.1767 -0.7589 -0.4253 -1.4291 -0.7916 -1.0230 -1.2009 -0.0955 0.0000\r\n\ufe19\r\nS-6\t\u2581\u52c9\u5f37 \u2581\u3064\u3082\u308a \u2581\u3060\u3063 \u2581\u305f \u2581\u3002 \u2581\u3002 \u2581\u3002 \u2581( \u25812 \u2581\u56de \u2581)\r\nT-6\t\u2581\u52c9\u5f37 \u2581\u3059\u308b \u2581\u3064\u3082\u308a \u2581\u3067\u3057 \u2581\u305f \u2581\u3002 \u2581\u3002 \u2581\u3002 \u2581( \u25812 \u2581\u56de \u2581)\r\nH-6\t-0.22894743084907532\t\u2581\u52c9\u5f37 \u2581\u3059\u308b \u2581\u3064\u3082\u308a \u2581\u3060\u3063 \u2581\u305f \u2581\u3002 \u2581\u3002 \u2581\u3002 \u2581( \u25812 \u2581\u56de \u2581)\r\nD-6\t-0.22894743084907532\t\u2581\u52c9\u5f37 \u2581\u3059\u308b \u2581\u3064\u3082\u308a \u2581\u3060\u3063 \u2581\u305f \u2581\u3002 \u2581\u3002 \u2581\u3002 \u2581( \u25812 \u2581\u56de \u2581)\r\nP-6\t0.0000 -0.0959 -0.2483 -0.2130 -0.6924 -1.2037 -0.0973 -0.0615 -0.1622 -0.0879 -0.1282 -0.1485 -0.0664 0.0000\r\n\ufe19\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install --editable ./` `python setup.py build_ext --inplace`\r\n\r\n### Additional context\r\n\r\nI've already modified the code, so I'll send a pull request.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2335/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2325", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2325/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2325/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2325/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2325", "id": 656531791, "node_id": "MDU6SXNzdWU2NTY1MzE3OTE=", "number": 2325, "title": "BPE fairseq-interactive: error", "user": {"login": "rupjyotiBaruah", "id": 42119861, "node_id": "MDQ6VXNlcjQyMTE5ODYx", "avatar_url": "https://avatars.githubusercontent.com/u/42119861?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rupjyotiBaruah", "html_url": "https://github.com/rupjyotiBaruah", "followers_url": "https://api.github.com/users/rupjyotiBaruah/followers", "following_url": "https://api.github.com/users/rupjyotiBaruah/following{/other_user}", "gists_url": "https://api.github.com/users/rupjyotiBaruah/gists{/gist_id}", "starred_url": "https://api.github.com/users/rupjyotiBaruah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rupjyotiBaruah/subscriptions", "organizations_url": "https://api.github.com/users/rupjyotiBaruah/orgs", "repos_url": "https://api.github.com/users/rupjyotiBaruah/repos", "events_url": "https://api.github.com/users/rupjyotiBaruah/events{/privacy}", "received_events_url": "https://api.github.com/users/rupjyotiBaruah/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-14T11:13:06Z", "updated_at": "2020-07-14T15:47:17Z", "closed_at": "2020-07-14T15:47:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nRun the Inference switch from this link:\r\nhttps://github.com/pytorch/fairseq/blob/master/examples/byte_level_bpe/README.md\r\n\r\n### Run the Command:\r\n\"fairseq-interactive \"data/bin_bbpe2048\" --task translation --user-dir gru_transformer --path \"checkpoints/bbpe2048/checkpoint_last.pt\" --input data/test.hi --tokenizer moses --moses-source-lang hi --moses-target-lang mr --bpe byte_bpe --sentencepiece-model-path data/spm_bbpe2048.model --buffer-size 1000 --max-tokens 10000\"\r\n\r\n\r\n### To Reproduce\r\n\r\n### Error\r\nusage: fairseq-interactive [-h] [--no-progress-bar] [--log-interval N]\r\n                           [--log-format {json,none,simple,tqdm}]\r\n                           [--tensorboard-logdir DIR] [--seed N] [--cpu]\r\n                           [--fp16] [--memory-efficient-fp16]\r\n                           [--fp16-init-scale FP16_INIT_SCALE]\r\n                           [--fp16-scale-window FP16_SCALE_WINDOW]\r\n                           [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\r\n                           [--min-loss-scale D]\r\n                           [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\r\n                           [--user-dir USER_DIR]\r\n                           [--empty-cache-freq EMPTY_CACHE_FREQ]\r\n                           [--criterion {sentence_ranking,sentence_prediction,binary_cross_entropy,legacy_masked_lm_loss,adapt                               ive_loss,composite_loss,label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,nat_loss,masked_lm,cross_entr                               opy}]\r\n                           [--tokenizer {space,moses,nltk}]\r\n                           [--bpe {subword_nmt,bert,gpt2,fastbpe,sentencepiece}]\r\n                           [--optimizer {adam,nag,sgd,adamax,adadelta,adafactor,adagrad}]\r\n                           [--lr-scheduler {polynomial_decay,cosine,triangular,tri_stage,reduce_lr_on_plateau,inverse_sqrt,fix                               ed}]\r\n                           [--task TASK] [--num-workers N]\r\n                           [--skip-invalid-size-inputs-valid-test]\r\n                           [--max-tokens N] [--max-sentences N]\r\n                           [--required-batch-size-multiple N]\r\n                           [--dataset-impl FORMAT] [--gen-subset SPLIT]\r\n                           [--num-shards N] [--shard-id ID] [--path FILE]\r\n                           [--remove-bpe [REMOVE_BPE]] [--quiet]\r\n                           [--model-overrides DICT] [--results-path RESDIR]\r\n                           [--beam N] [--nbest N] [--max-len-a N]\r\n                           [--max-len-b N] [--min-len N] [--match-source-len]\r\n                           [--no-early-stop] [--unnormalized]\r\n                           [--no-beamable-mm] [--lenpen LENPEN]\r\n                           [--unkpen UNKPEN] [--replace-unk [REPLACE_UNK]]\r\n                           [--sacrebleu] [--score-reference]\r\n                           [--prefix-size PS] [--no-repeat-ngram-size N]\r\n                           [--sampling] [--sampling-topk PS]\r\n                           [--sampling-topp PS] [--temperature N]\r\n                           [--diverse-beam-groups N]\r\n                           [--diverse-beam-strength N] [--print-alignment]\r\n                           [--print-step] [--iter-decode-eos-penalty N]\r\n                           [--iter-decode-max-iter N]\r\n                           [--iter-decode-force-max-iter]\r\n                           [--retain-iter-history]\r\n                           [--decoding-format {unigram,ensemble,vote,dp,bs}]\r\n                           [--buffer-size N] [--input FILE]\r\nfairseq-interactive: error: argument --bpe: invalid choice: 'byte_bpe' (choose from 'subword_nmt', 'bert', 'gpt2', 'fastbpe',                                'sentencepiece')", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2325/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2325/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2296", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2296/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2296/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2296/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2296", "id": 650085616, "node_id": "MDU6SXNzdWU2NTAwODU2MTY=", "number": 2296, "title": "Transformer model for Librispeech is not working", "user": {"login": "PhenixCFLi", "id": 36611483, "node_id": "MDQ6VXNlcjM2NjExNDgz", "avatar_url": "https://avatars.githubusercontent.com/u/36611483?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhenixCFLi", "html_url": "https://github.com/PhenixCFLi", "followers_url": "https://api.github.com/users/PhenixCFLi/followers", "following_url": "https://api.github.com/users/PhenixCFLi/following{/other_user}", "gists_url": "https://api.github.com/users/PhenixCFLi/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhenixCFLi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhenixCFLi/subscriptions", "organizations_url": "https://api.github.com/users/PhenixCFLi/orgs", "repos_url": "https://api.github.com/users/PhenixCFLi/repos", "events_url": "https://api.github.com/users/PhenixCFLi/events{/privacy}", "received_events_url": "https://api.github.com/users/PhenixCFLi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-02T17:10:16Z", "updated_at": "2020-07-06T13:15:12Z", "closed_at": "2020-07-06T13:15:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nFailed running the librispeech training with  speech_transformer_librispeech architecture.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nWhen I change the script \"run.sh\" under folder \"<espresso_root>/examples/asr_librispeech\" to use arch \"speech_transformer_librispeech\",.\r\n\r\n1. I changed the \"run.sh\" on below lines:\r\n```\r\n# Just start training and skip other preparation process\r\n# stage=1 \r\nstage=8\r\n```\r\nand \r\n```\r\n# Change arch from speech_conv_lstm_librispeech to speech_transformer_librispeech\r\n  CUDA_VISIBLE_DEVICES=$free_gpu speech_train.py data --task speech_recognition_espresso --seed 1 --user-dir espresso \\\r\n    --num-workers 0 --data-buffer-size 0 --max-tokens 26000 --max-sentences 24 --curriculum 1 \\\r\n    --valid-subset $valid_subset --max-sentences-valid 48 --ddp-backend no_c10d \\\r\n    --distributed-world-size $ngpus --distributed-port $(if [ $ngpus -gt 1 ]; then echo 100; else echo -1; fi) \\\r\n    --optimizer adam --lr 0.001 --weight-decay 0.0 --clip-norm 2.0 \\\r\n    --save-dir $dir --restore-file checkpoint_last.pt --save-interval-updates $((6000/ngpus)) \\\r\n    --keep-interval-updates 3 --keep-last-epochs 5 --validate-interval 1 --best-checkpoint-metric wer \\\r\n    --dict $dict --bpe sentencepiece --sentencepiece-vocab ${sentencepiece_model}.model \\\r\n    --max-source-positions 9999 --max-target-positions 999 \\\r\n    --log-interval $((8000/ngpus)) --log-format simple \\\r\n    --arch **speech_transformer_librispeech** --criterion cross_entropy_v2 \\\r\n    --print-training-sample-interval $((4000/ngpus)) \\\r\n    $opts --specaugment-config \"$specaug_config\" 2>&1 | tee $log_file\r\n```\r\n1. Run cmd './run.sh'\r\n2. Got error\r\n```-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/nfs/mercury-13/u20/cli/miniconda3/envs/espresso/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/espresso/speech_train.py\", line 341, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/espresso/speech_train.py\", line 72, in main\r\n    model = task.build_model(args)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/espresso/tasks/speech_recognition.py\", line 339, in build_model\r\n    model = super().build_model(args)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/fairseq/tasks/fairseq_task.py\", line 211, in build_model\r\n    model = models.build_model(args, self)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/fairseq/models/__init__.py\", line 48, in build_model\r\n    return ARCH_MODEL_REGISTRY[args.arch].build_model(args, task)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/espresso/models/speech_transformer.py\", line 132, in build_model\r\n    return cls(encoder, decoder)\r\nTypeError: __init__() missing 1 required positional argument: 'decoder'\r\n```\r\n3. This is due to the constructor not insert with args, and can be fixed by add back the args param in `speech_transformer.py` as below\r\n```\r\n        # return cls(encoder, decoder)\r\n        return cls(args, encoder, decoder)\r\n```\r\n\r\n4. But after that, it got more complicated error following as below\r\n```\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/nfs/mercury-13/u20/cli/miniconda3/envs/espresso/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/espresso/speech_train.py\", line 341, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/espresso/speech_train.py\", line 121, in main\r\n    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\r\n  File \"/nfs/mercury-13/u20/cli/miniconda3/envs/espresso/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/espresso/speech_train.py\", line 210, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/nfs/mercury-13/u20/cli/miniconda3/envs/espresso/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/fairseq/trainer.py\", line 408, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/fairseq/tasks/fairseq_task.py\", line 342, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/nfs/mercury-13/u20/cli/miniconda3/envs/espresso/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/espresso/criterions/cross_entropy_v2.py\", line 49, in forward\r\n    net_output = model(**sample[\"net_input\"], epoch=self.epoch)\r\n  File \"/nfs/mercury-13/u20/cli/miniconda3/envs/espresso/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/nfs/mercury-13/u20/cli/src/espresso/fairseq/legacy_distributed_data_parallel.py\", line 86, in forward\r\n    return self.module(*inputs, **kwargs)\r\n  File \"/nfs/mercury-13/u20/cli/miniconda3/envs/espresso/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\nTypeError: forward() got an unexpected keyword argument 'epoch'\r\n``` \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2296/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2292", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2292/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2292/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2292/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2292", "id": 649084837, "node_id": "MDU6SXNzdWU2NDkwODQ4Mzc=", "number": 2292, "title": "TransformerModel with share-all-embeddings flag cannot `from_tretrained` after training", "user": {"login": "hscspring", "id": 7485553, "node_id": "MDQ6VXNlcjc0ODU1NTM=", "avatar_url": "https://avatars.githubusercontent.com/u/7485553?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hscspring", "html_url": "https://github.com/hscspring", "followers_url": "https://api.github.com/users/hscspring/followers", "following_url": "https://api.github.com/users/hscspring/following{/other_user}", "gists_url": "https://api.github.com/users/hscspring/gists{/gist_id}", "starred_url": "https://api.github.com/users/hscspring/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hscspring/subscriptions", "organizations_url": "https://api.github.com/users/hscspring/orgs", "repos_url": "https://api.github.com/users/hscspring/repos", "events_url": "https://api.github.com/users/hscspring/events{/privacy}", "received_events_url": "https://api.github.com/users/hscspring/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1756261282, "node_id": "MDU6TGFiZWwxNzU2MjYxMjgy", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/unable%20to%20repro", "name": "unable to repro", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-01T15:44:31Z", "updated_at": "2020-07-04T06:11:05Z", "closed_at": "2020-07-04T06:11:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n\r\n- Train a transformer model with `share-all-embeddings` flag\r\n- run `model = TransformerModel.from_pretrained(...)`\r\n\r\n2. See error\r\n\r\n```bash\r\nRuntimeError: Error(s) in loading state_dict for TransformerModel:\r\n\tUnexpected key(s) in state_dict: \"decoder.output_projection.weight\". \r\n```\r\nThe reason might be `features_only=False`, so the `output_layer` is used.\r\n\r\n```python\r\n# fairseq/models/transformer.py   line 608-633\r\n        self.adaptive_softmax = None\r\n        self.output_projection = None\r\n        if args.adaptive_softmax_cutoff is not None:\r\n            self.adaptive_softmax = AdaptiveSoftmax(\r\n                len(dictionary),\r\n                self.output_embed_dim,\r\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int),\r\n                dropout=args.adaptive_softmax_dropout,\r\n                adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None,\r\n                factor=args.adaptive_softmax_factor,\r\n                tie_proj=args.tie_adaptive_proj,\r\n            )\r\n        elif self.share_input_output_embed:\r\n            # Here is the output_projection\r\n            # self.share_input_output_embed = args.share_decoder_input_output_embed\r\n            # if args.share_all_embeddings: args.share_decoder_input_output_embed = True\r\n           \r\n            self.output_projection = nn.Linear(\r\n                self.embed_tokens.weight.shape[1],\r\n                self.embed_tokens.weight.shape[0],\r\n                bias=False,\r\n            )\r\n            self.output_projection.weight = self.embed_tokens.weight\r\n        else:\r\n            self.output_projection = nn.Linear(\r\n                self.output_embed_dim, len(dictionary), bias=False\r\n            )\r\n            nn.init.normal_(\r\n                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\r\n            )\r\n\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.5.0\r\n - OS (e.g., Linux): MacOS Mojave version 10.14.6\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: python3.7.7\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: see `training sh file`\r\n - Any other relevant information:\r\n\r\n```bash\r\n# training sh file\r\nCUDA_VISIBLE_DEVICES=0 fairseq-train \\\r\n    ${DATA_BIN_DIR} \\\r\n    --save-dir ${OUT_DIR} \\\r\n    --task translation \\\r\n    --arch transformer \\\r\n    --share-all-embeddings \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --dropout 0.3 --weight-decay 0.0001 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens 4096 \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\r\n    --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\r\n\r\n\r\n```\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2292/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2292/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2287", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2287/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2287/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2287/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2287", "id": 648141138, "node_id": "MDU6SXNzdWU2NDgxNDExMzg=", "number": 2287, "title": "UnicodeEncodeError with fairseq-generate on Devanagari characters", "user": {"login": "TovlyDeutsch", "id": 12242351, "node_id": "MDQ6VXNlcjEyMjQyMzUx", "avatar_url": "https://avatars.githubusercontent.com/u/12242351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TovlyDeutsch", "html_url": "https://github.com/TovlyDeutsch", "followers_url": "https://api.github.com/users/TovlyDeutsch/followers", "following_url": "https://api.github.com/users/TovlyDeutsch/following{/other_user}", "gists_url": "https://api.github.com/users/TovlyDeutsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/TovlyDeutsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TovlyDeutsch/subscriptions", "organizations_url": "https://api.github.com/users/TovlyDeutsch/orgs", "repos_url": "https://api.github.com/users/TovlyDeutsch/repos", "events_url": "https://api.github.com/users/TovlyDeutsch/events{/privacy}", "received_events_url": "https://api.github.com/users/TovlyDeutsch/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-30T12:42:20Z", "updated_at": "2020-07-15T19:20:30Z", "closed_at": "2020-07-15T19:20:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nA `UnicodeEncodeError` is raised when trying to translate source text with Devanagari (presumably any Unicode) text with `fairseq-generate`. The error is raised [this line](https://github.com/pytorch/fairseq/blob/894ae64858b62927d849c0fbc05e8f55d680a4f1/fairseq_cli/generate.py#L33)  of fairseq_cli/generate.py: `with open(output_path, 'w', buffering=1) as h:`. It can be fixed by just adding a `encoding = 'utf-8'` to the `open` call. The PR #460 addressed this issue for a bunch of other call sites of `open`, but not this one (perhaps it didn't exist at the time). That PR also mentions inconsistencies between platforms, therefore I think this is just an issue on Windows. I'm happy to make a PR if this is the right fix.\r\n\r\n### To Reproduce\r\n\r\n1. Run \"fairseq-generate ...\" with a source language that contains Devanagari characters\r\n\r\n### Expected behavior\r\n\r\nNo error is raised.\r\n\r\n### Environment\r\n\r\n - fairseq Version: master\r\n - PyTorch Version: 1.4.0\r\n - OS: Windows\r\n - How you installed fairseq : `pip install git+https://github.com/pytorch/fairseq`\r\n - Python version: 3.6.8\r\n - CUDA/cuDNN version: 10.2", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2287/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2275", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2275/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2275/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2275/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2275", "id": 646497334, "node_id": "MDU6SXNzdWU2NDY0OTczMzQ=", "number": 2275, "title": "task translation_moe is in the wrong place", "user": {"login": "sk-g", "id": 22529552, "node_id": "MDQ6VXNlcjIyNTI5NTUy", "avatar_url": "https://avatars.githubusercontent.com/u/22529552?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sk-g", "html_url": "https://github.com/sk-g", "followers_url": "https://api.github.com/users/sk-g/followers", "following_url": "https://api.github.com/users/sk-g/following{/other_user}", "gists_url": "https://api.github.com/users/sk-g/gists{/gist_id}", "starred_url": "https://api.github.com/users/sk-g/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sk-g/subscriptions", "organizations_url": "https://api.github.com/users/sk-g/orgs", "repos_url": "https://api.github.com/users/sk-g/repos", "events_url": "https://api.github.com/users/sk-g/events{/privacy}", "received_events_url": "https://api.github.com/users/sk-g/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-26T20:33:50Z", "updated_at": "2020-06-30T13:05:23Z", "closed_at": "2020-06-30T13:05:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nTried following the instructions to run Mixture of learners from: https://github.com/pytorch/fairseq/tree/master/examples/translation_moe\r\n\r\nPart of the instruction is to invoke a `translation_moe` task as `--task translation_moe` but the task registry module is in the `src` folder. Should this be moved to https://github.com/pytorch/fairseq/tree/master/fairseq/tasks and `translation_moe` needs to be included in the task registry as well?\r\n\r\n### To Reproduce\r\nRun the example configuration from the readme.\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd\r\n ```\r\n+ /d4m/material/software/python/singularity/bin/singularity-python.sh -i python3.6-cuda10.0 -v pytorch1.4-gpu --gpu -l\r\n     /d4m/material/releases/fairseq/R2020_06_25:/d4m/material/releases/hycube/R2020_05_18/pycube:/d4m/material/releases/tensorflow-models/R2018_12_05:/d4m/material\\\r\n     /releases/tensor2tensor/R2019_03_15 /d4m/material/releases/fairseq/R2020_06_25/train.py --adam-betas '(0.9, 0.98)' --arch transformer --attention-dropout 0.1\r\n     --clip-norm 0 --criterion label_smoothed_cross_entropy --dropout 0.3 --keep-last-epochs 10 --label-smoothing 0.1 --log-format simple --lr 0.0007\r\n     --lr-scheduler inverse_sqrt --max-tokens 4000 --max-update 150000 --mean-pool-gating-network --method hMoElp --min-lr 1e-9 --optimizer adam --save-dir\r\n     /d4m/ears/expts/46895_pashto_nmt_tr_1.forward/expts/fairseq-train --save-interval 1 --seed 7 --share-decoder-input-output-embed --task translation_moe\r\n     --update-freq 8 --validate-interval 1 --warmup-init-lr 1e-07 --warmup-updates 4000 --weight-decay 0.0001\r\n     /d4m/ears/expts/46882.trans.word.eval2020.60k/expts/fairseq-data\r\n```\r\n2. Error\r\n\r\n```\r\nusage: train.py [-h] [--no-progress-bar] [--log-interval N]\r\n  48                 [--log-format {json,none,simple,tqdm}]\r\n  49                 [--tensorboard-logdir DIR] [--seed N] [--cpu] [--tpu] [--bf16]\r\n  50                 [--fp16] [--memory-efficient-bf16] [--memory-efficient-fp16]\r\n  51                 [--fp16-no-flatten-grads] [--fp16-init-scale FP16_INIT_SCALE]\r\n  52                 [--fp16-scale-window FP16_SCALE_WINDOW]\r\n  53                 [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\r\n  54                 [--min-loss-scale D]\r\n  55                 [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\r\n  56                 [--user-dir USER_DIR] [--empty-cache-freq EMPTY_CACHE_FREQ]\r\n  57                 [--all-gather-list-size ALL_GATHER_LIST_SIZE]\r\n  58                 [--model-parallel-size N]\r\n  59                 [--checkpoint-suffix CHECKPOINT_SUFFIX]\r\n  60                 [--quantization-config-path QUANTIZATION_CONFIG_PATH]\r\n  61                 [--generate-lattice] [--lattice-outdir LATTICE_OUTDIR]\r\n  62                 [--token-boosting-file TOKEN_BOOSTING_FILE]\r\n  63                 [--criterion                                                                                                                                        {cross_entropy,binary_cross_entropy,label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,sentence_ranking,legacy_masked_lm_loss,adaptive_l\\     oss,masked_lm,nat_loss,composite_loss,sentence_prediction,vocab_parallel_cross_entropy}]\r\n  64                 [--tokenizer {space,nltk,moses}]\r\n  65                 [--bpe {bytes,characters,gpt2,fastbpe,byte_bpe,hf_byte_bpe,subword_nmt,sentencepiece,bert}]\r\n  66                 [--optimizer {adadelta,adafactor,nag,adamax,lamb,adagrad,adam,sgd}]\r\n  67                 [--lr-scheduler {triangular,inverse_sqrt,cosine,reduce_lr_on_plateau,polynomial_decay,tri_stage,fixed}]\r\n  68                 [--task TASK] [--num-workers N]\r\n  69                 [--skip-invalid-size-inputs-valid-test] [--max-tokens N]\r\n  70                 [--max-sentences N] [--required-batch-size-multiple N]\r\n  71                 [--dataset-impl FORMAT] [--data-buffer-size N]\r\n  72                 [--train-subset SPLIT] [--valid-subset SPLIT]\r\n  73                 [--validate-interval N] [--fixed-validation-seed N]\r\n  74                 [--disable-validation] [--max-tokens-valid N]\r\n  75                 [--max-sentences-valid N] [--curriculum N]\r\n  76                 [--distributed-world-size N]\r\n  77                 [--distributed-rank DISTRIBUTED_RANK]\r\n  78                 [--distributed-backend DISTRIBUTED_BACKEND]\r\n  79                 [--distributed-init-method DISTRIBUTED_INIT_METHOD]                                                                                              80                 [--distributed-port DISTRIBUTED_PORT] [--device-id DEVICE_ID]\r\n  81                 [--distributed-no-spawn] [--ddp-backend {c10d,no_c10d}]\r\n  82                 [--bucket-cap-mb MB] [--fix-batches-to-gpus]\r\n  83                 [--find-unused-parameters] [--fast-stat-sync]\r\n  84                 [--broadcast-buffers] [--distributed-wrapper {DDP,SlowMo}]                                                                                       85                 [--slowmo-momentum SLOWMO_MOMENTUM]                                                                                                              86                 [--slowmo-algorithm {LocalSGD,SGP}]                                                                                                              87                 [--localsgd-frequency LOCALSGD_FREQUENCY]\r\n  88                 [--nprocs-per-node N] [--arch ARCH] [--max-epoch N]\r\n  89                 [--max-update N] [--clip-norm NORM] [--sentence-avg]\r\n  90                 [--update-freq N1,N2,...,N_K] [--lr LR_1,LR_2,...,LR_N]\r\n  91                 [--min-lr LR] [--use-bmuf] [--save-dir DIR]\r\n  92                 [--restore-file RESTORE_FILE] [--reset-dataloader]\r\n  93                 [--reset-lr-scheduler] [--reset-meters] [--reset-optimizer]\r\n  94                 [--optimizer-overrides DICT] [--save-interval N]\r\n  95                 [--save-interval-updates N] [--keep-interval-updates N]\r\n  96                 [--keep-last-epochs N] [--keep-best-checkpoints N] [--no-save]\r\n  97                 [--no-epoch-checkpoints] [--no-last-checkpoints]\r\n  98                 [--no-save-optimizer-state]\r\n  99                 [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\r\n 100                 [--maximize-best-checkpoint-metric] [--patience N]\r\n 101 train.py: error: argument --task: invalid choice: 'translation_moe' (choose from 'translation', 'translation_from_pretrained_xlm', 'denoising',\r\n     'cross_lingual_lm', 'multilingual_masked_lm', 'sentence_ranking', 'legacy_masked_lm', 'translation_lev', 'multilingual_denoising',\r\n     'multilingual_translation', 'semisupervised_translation', 'language_modeling', 'masked_lm', 'audio_pretraining', 'translation_from_pretrained_bart',\r\n     'sentence_prediction', 'dummy_lm', 'dummy_masked_lm')\r\n 102 7.15user 7.59system 0:07.60elapsed 193%CPU (0avgtext+0avgdata 1234268maxresident)k\r\n 103 63026inputs+64outputs (337major+372614minor)pagefaults 0swaps\r\n 104 0.00user 0.00system 0:00.00elapsed 66%CPU (0avgtext+0avgdata 552maxresident)k\r\n 105 0inputs+0outputs (0major+249minor)pagefaults 0swaps\r\n 106 GENERIC JOB FOR   CRASHED AT  Fri Jun 26 16:20:36 2020\r\n 107 STATUS CODE  2\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): `master`\r\n - PyTorch Version : `1.4`\r\n - OS (e.g., Linux): centOS7 \r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version: `3.6`\r\n - CUDA/cuDNN version: `10.0`\r\n - GPU models and configuration: `v100`\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2275/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2275/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2255", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2255/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2255/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2255/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2255", "id": 642297223, "node_id": "MDU6SXNzdWU2NDIyOTcyMjM=", "number": 2255, "title": "memory leakage caused by logging_output in criterion \"masked_lm\"", "user": {"login": "hury07", "id": 34572090, "node_id": "MDQ6VXNlcjM0NTcyMDkw", "avatar_url": "https://avatars.githubusercontent.com/u/34572090?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hury07", "html_url": "https://github.com/hury07", "followers_url": "https://api.github.com/users/hury07/followers", "following_url": "https://api.github.com/users/hury07/following{/other_user}", "gists_url": "https://api.github.com/users/hury07/gists{/gist_id}", "starred_url": "https://api.github.com/users/hury07/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hury07/subscriptions", "organizations_url": "https://api.github.com/users/hury07/orgs", "repos_url": "https://api.github.com/users/hury07/repos", "events_url": "https://api.github.com/users/hury07/events{/privacy}", "received_events_url": "https://api.github.com/users/hury07/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-06-20T03:17:36Z", "updated_at": "2020-06-22T18:08:03Z", "closed_at": "2020-06-22T18:08:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nI found memory (not GPU memory) leakage event when trying masked language modeling task.\r\nThe memory usage was increasing when the trainning proceeded.\r\n\r\nThe bug was caused by 'loss' item in logging_output in the file criterions/masked_lm.py.\r\nThe original code is,\r\n 64         logging_output = {\r\n 65             'loss': loss,\r\n 66             'ntokens': sample['ntokens'],\r\n 67             'nsentences': sample['nsentences'],\r\n 68             'sample_size': sample_size,\r\n 69         }\r\n\r\nThe bug could be fixed with the following code.\r\n 64         logging_output = {\r\n 65             'loss': loss.data,\r\n 66             'ntokens': sample['ntokens'],\r\n 67             'nsentences': sample['nsentences'],\r\n 68             'sample_size': sample_size,\r\n 69         }", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2255/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2241", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2241/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2241/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2241/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2241", "id": 638872460, "node_id": "MDU6SXNzdWU2Mzg4NzI0NjA=", "number": 2241, "title": "RU to EN translation eat part of sentence", "user": {"login": "hadaev8", "id": 20247085, "node_id": "MDQ6VXNlcjIwMjQ3MDg1", "avatar_url": "https://avatars.githubusercontent.com/u/20247085?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hadaev8", "html_url": "https://github.com/hadaev8", "followers_url": "https://api.github.com/users/hadaev8/followers", "following_url": "https://api.github.com/users/hadaev8/following{/other_user}", "gists_url": "https://api.github.com/users/hadaev8/gists{/gist_id}", "starred_url": "https://api.github.com/users/hadaev8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hadaev8/subscriptions", "organizations_url": "https://api.github.com/users/hadaev8/orgs", "repos_url": "https://api.github.com/users/hadaev8/repos", "events_url": "https://api.github.com/users/hadaev8/events{/privacy}", "received_events_url": "https://api.github.com/users/hadaev8/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-15T13:52:38Z", "updated_at": "2020-06-15T19:45:48Z", "closed_at": "2020-06-15T19:45:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n#### Code sample\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nRun this notebook\r\nhttps://colab.research.google.com/drive/1gqTNv8GK5tx_SUvpn37biwGdkqlIna3J?usp=sharing\r\n\r\n### Expected behavior\r\n\r\nIt translates \r\n\r\n'When coexistence in conditions of limited resources exists, it is necessary to develop a special form of social organization. This has led to a rise of a stateless, classless utopia. This society is based entirely on a stateless, classless, wandering fleet of starships.'\r\n\r\nto\r\n\r\n'\u041a\u043e\u0433\u0434\u0430 \u0441\u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u0435 \u0432 \u0443\u0441\u043b\u043e\u0432\u0438\u044f\u0445 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u043d\u044b\u0445 \u0440\u0435\u0441\u0443\u0440\u0441\u043e\u0432 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0437\u0432\u0438\u0432\u0430\u0442\u044c \u043e\u0441\u043e\u0431\u0443\u044e \u0444\u043e\u0440\u043c\u0443 \u0441\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438. \u042d\u0442\u043e \u043f\u0440\u0438\u0432\u0435\u043b\u043e \u043a \u0432\u043e\u0437\u043d\u0438\u043a\u043d\u043e\u0432\u0435\u043d\u0438\u044e \u0431\u0435\u0437\u0433\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439, \u0431\u0435\u0441\u043a\u043b\u0430\u0441\u0441\u043e\u0432\u043e\u0439 \u0443\u0442\u043e\u043f\u0438\u0438.' \r\n\r\nBut ignore the second part of the text 'This society is based entirely on a stateless, classless, wandering fleet of starships.'\r\n\r\n### Environment\r\n\r\nGoogle colab\r\n\r\n### Additional context\r\n\r\nI have no additional context.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2241/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2241/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2213", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2213/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2213/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2213/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2213", "id": 631354269, "node_id": "MDU6SXNzdWU2MzEzNTQyNjk=", "number": 2213, "title": "ImportError: cannot import name 'rerank'", "user": {"login": "Bachstelze", "id": 19904888, "node_id": "MDQ6VXNlcjE5OTA0ODg4", "avatar_url": "https://avatars.githubusercontent.com/u/19904888?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Bachstelze", "html_url": "https://github.com/Bachstelze", "followers_url": "https://api.github.com/users/Bachstelze/followers", "following_url": "https://api.github.com/users/Bachstelze/following{/other_user}", "gists_url": "https://api.github.com/users/Bachstelze/gists{/gist_id}", "starred_url": "https://api.github.com/users/Bachstelze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Bachstelze/subscriptions", "organizations_url": "https://api.github.com/users/Bachstelze/orgs", "repos_url": "https://api.github.com/users/Bachstelze/repos", "events_url": "https://api.github.com/users/Bachstelze/events{/privacy}", "received_events_url": "https://api.github.com/users/Bachstelze/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-06-05T06:46:49Z", "updated_at": "2020-10-27T18:26:22Z", "closed_at": "2020-10-27T18:26:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "I executed `examples/noisychannel/rerank_tune.py` following the [noisy channel example](https://github.com/pytorch/fairseq/tree/master/examples/noisychannel) and get `  File \"examples/noisychannel/rerank_tune.py\", line 12, in <module>\r\n    from . import rerank, rerank_options\r\nImportError: cannot import name 'rerank'` or\r\n```\r\n  File \"examples/noisychannel/rerank_tune.py\", line 93, in <module>\r\n    cli_main()\r\n  File \"examples/noisychannel/rerank_tune.py\", line 89, in cli_main\r\n    random_search(args)\r\n  File \"examples/noisychannel/rerank_tune.py\", line 60, in random_search\r\n    best_lenpen, best_weight1, best_weight2, best_weight3, best_score = rerank.rerank(rerank_args)\r\n  File \"/media/kalle/Sprachdaten/noisy/fairseq/examples/noisychannel/rerank.py\", line 262, in rerank\r\n    rerank_score_lm.score_lm(args)\r\n  File \"/media/kalle/Sprachdaten/noisy/fairseq/examples/noisychannel/rerank_score_lm.py\", line 44, in score_lm\r\n    args.source_lang, prefix_len=args.prefix_len)\r\n  File \"/media/kalle/Sprachdaten/noisy/fairseq/examples/noisychannel/rerank_utils.py\", line 619, in lm_scoring\r\n    preprocess.main(input_args)\r\n  File \"/media/kalle/Sprachdaten/noisy/fairseq/fairseq_cli/preprocess.py\", line 259, in main\r\n    make_all(args.source_lang, src_dict)\r\n  File \"/media/kalle/Sprachdaten/noisy/fairseq/fairseq_cli/preprocess.py\", line 241, in make_all\r\n    make_dataset(vocab, args.trainpref, \"train\", lang, num_workers=args.workers)\r\n  File \"/media/kalle/Sprachdaten/noisy/fairseq/fairseq_cli/preprocess.py\", line 237, in make_dataset\r\n    make_binary_dataset(vocab, input_prefix, output_prefix, lang, num_workers)\r\n  File \"/media/kalle/Sprachdaten/noisy/fairseq/fairseq_cli/preprocess.py\", line 169, in make_binary_dataset\r\n    100 * sum(replaced.values()) / n_seq_tok[1],\r\nZeroDivisionError: division by zero\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2213/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2213/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2183", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2183/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2183/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2183/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2183", "id": 624774097, "node_id": "MDU6SXNzdWU2MjQ3NzQwOTc=", "number": 2183, "title": "-", "user": {"login": "dhecloud", "id": 25906470, "node_id": "MDQ6VXNlcjI1OTA2NDcw", "avatar_url": "https://avatars.githubusercontent.com/u/25906470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dhecloud", "html_url": "https://github.com/dhecloud", "followers_url": "https://api.github.com/users/dhecloud/followers", "following_url": "https://api.github.com/users/dhecloud/following{/other_user}", "gists_url": "https://api.github.com/users/dhecloud/gists{/gist_id}", "starred_url": "https://api.github.com/users/dhecloud/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dhecloud/subscriptions", "organizations_url": "https://api.github.com/users/dhecloud/orgs", "repos_url": "https://api.github.com/users/dhecloud/repos", "events_url": "https://api.github.com/users/dhecloud/events{/privacy}", "received_events_url": "https://api.github.com/users/dhecloud/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-26T10:23:53Z", "updated_at": "2020-05-26T10:47:55Z", "closed_at": "2020-05-26T10:47:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2183/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2182", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2182/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2182/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2182/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2182", "id": 624736160, "node_id": "MDU6SXNzdWU2MjQ3MzYxNjA=", "number": 2182, "title": "Validation happens after every epoch, validate_interval is ignored", "user": {"login": "villmow", "id": 2743060, "node_id": "MDQ6VXNlcjI3NDMwNjA=", "avatar_url": "https://avatars.githubusercontent.com/u/2743060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/villmow", "html_url": "https://github.com/villmow", "followers_url": "https://api.github.com/users/villmow/followers", "following_url": "https://api.github.com/users/villmow/following{/other_user}", "gists_url": "https://api.github.com/users/villmow/gists{/gist_id}", "starred_url": "https://api.github.com/users/villmow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/villmow/subscriptions", "organizations_url": "https://api.github.com/users/villmow/orgs", "repos_url": "https://api.github.com/users/villmow/repos", "events_url": "https://api.github.com/users/villmow/events{/privacy}", "received_events_url": "https://api.github.com/users/villmow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-05-26T09:24:44Z", "updated_at": "2020-05-27T14:58:24Z", "closed_at": "2020-05-27T14:58:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI used to set the validation (epoch) frequency with `--validate_interval`. Currently on master validation is triggered after every epoch and i can't find a way to trigger validation only every x epochs. \r\n\r\nHas there been a change that caused this behaviour? Is this wanted?\r\n\r\nLooking at the code in master, I assume that train.py has been refactored from this:\r\n\r\nhttps://github.com/pytorch/fairseq/blob/9ac98bbd6de414f2acee76a9bc43bf157574b7ec/train.py#L103-L106\r\n\r\nTo this:\r\n\r\nhttps://github.com/pytorch/fairseq/blob/9f25ffb02ab54a443ba181f732d5dcc00db7bea8/fairseq_cli/train.py#L238-L252\r\n\r\nAnd as we're always at the end of an epoch (L244), the argument `validate_interval` is basically ignored.\r\n\r\n### Expected behavior\r\n\r\nI should be able to trigger validation only every x epochs, but save epoch_checkpoints after every epoch.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2182/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2181", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2181/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2181/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2181/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2181", "id": 624430495, "node_id": "MDU6SXNzdWU2MjQ0MzA0OTU=", "number": 2181, "title": "RuntimeError: \"mul_cpu\" not implemented for 'Half'", "user": {"login": "AlexandderGorodetski", "id": 20683945, "node_id": "MDQ6VXNlcjIwNjgzOTQ1", "avatar_url": "https://avatars.githubusercontent.com/u/20683945?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AlexandderGorodetski", "html_url": "https://github.com/AlexandderGorodetski", "followers_url": "https://api.github.com/users/AlexandderGorodetski/followers", "following_url": "https://api.github.com/users/AlexandderGorodetski/following{/other_user}", "gists_url": "https://api.github.com/users/AlexandderGorodetski/gists{/gist_id}", "starred_url": "https://api.github.com/users/AlexandderGorodetski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AlexandderGorodetski/subscriptions", "organizations_url": "https://api.github.com/users/AlexandderGorodetski/orgs", "repos_url": "https://api.github.com/users/AlexandderGorodetski/repos", "events_url": "https://api.github.com/users/AlexandderGorodetski/events{/privacy}", "received_events_url": "https://api.github.com/users/AlexandderGorodetski/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-25T18:15:58Z", "updated_at": "2020-06-19T04:40:32Z", "closed_at": "2020-05-25T19:23:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI am trying to train a LM transformer according to the following example.\r\n\r\nhttps://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md\r\n\r\nI got following error when running fairseq-train\r\n\r\nRuntimeError: \"mul_cpu\" not implemented for 'Half'\r\n\r\nCould you help please.\r\n\r\nThanks,\r\nAlexG.\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2181/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2175", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2175/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2175/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2175/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2175", "id": 623511524, "node_id": "MDU6SXNzdWU2MjM1MTE1MjQ=", "number": 2175, "title": "mBart cc25 doesn't work", "user": {"login": "shonenkov", "id": 32220681, "node_id": "MDQ6VXNlcjMyMjIwNjgx", "avatar_url": "https://avatars.githubusercontent.com/u/32220681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shonenkov", "html_url": "https://github.com/shonenkov", "followers_url": "https://api.github.com/users/shonenkov/followers", "following_url": "https://api.github.com/users/shonenkov/following{/other_user}", "gists_url": "https://api.github.com/users/shonenkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/shonenkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shonenkov/subscriptions", "organizations_url": "https://api.github.com/users/shonenkov/orgs", "repos_url": "https://api.github.com/users/shonenkov/repos", "events_url": "https://api.github.com/users/shonenkov/events{/privacy}", "received_events_url": "https://api.github.com/users/shonenkov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-22T22:55:46Z", "updated_at": "2020-05-24T09:59:30Z", "closed_at": "2020-05-24T09:59:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. load https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.CC25.tar.gz\r\n2. extract tar.gz \r\n2. use python:\r\n```python\r\nfrom fairseq.models.bart import BARTModel\r\nmodel = BARTModel.from_pretrained(model_name_or_path='./mbart.CC25')\r\n\r\n>>> /usr/local/lib/python3.6/site-packages/fairseq/models/bart/model.py in from_pretrained(cls, model_name_or_path, checkpoint_file, data_name_or_path, bpe, **kwargs)\r\n    109             bpe=bpe,\r\n    110             load_checkpoint_heads=True,\r\n--> 111             **kwargs,\r\n    112         )\r\n    113         return BARTHubInterface(x['args'], x['task'], x['models'][0])\r\n\r\n/usr/local/lib/python3.6/site-packages/fairseq/hub_utils.py in from_pretrained(model_name_or_path, checkpoint_file, data_name_or_path, archive_map, **kwargs)\r\n     71     models, args, task = checkpoint_utils.load_model_ensemble_and_task(\r\n     72         [os.path.join(model_path, cpt) for cpt in checkpoint_file.split(os.pathsep)],\r\n---> 73         arg_overrides=kwargs,\r\n     74     )\r\n     75 \r\n\r\n/usr/local/lib/python3.6/site-packages/fairseq/checkpoint_utils.py in load_model_ensemble_and_task(filenames, arg_overrides, task, strict, suffix)\r\n    209         # build model for ensemble\r\n    210         model = task.build_model(args)\r\n--> 211         model.load_state_dict(state[\"model\"], strict=strict, args=args)\r\n    212         ensemble.append(model)\r\n    213     return ensemble, args, task\r\n\r\n/usr/local/lib/python3.6/site-packages/fairseq/models/fairseq_model.py in load_state_dict(self, state_dict, strict, args)\r\n     91         self.upgrade_state_dict(state_dict)\r\n     92         new_state_dict = prune_state_dict(state_dict, args)\r\n---> 93         return super().load_state_dict(new_state_dict, strict)\r\n     94 \r\n     95     def upgrade_state_dict(self, state_dict):\r\n\r\n/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict)\r\n    828         if len(error_msgs) > 0:\r\n    829             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n--> 830                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\n    831         return _IncompatibleKeys(missing_keys, unexpected_keys)\r\n    832 \r\n\r\nRuntimeError: Error(s) in loading state_dict for BARTModel:\r\n\tUnexpected key(s) in state_dict: \"encoder.layernorm_embedding.weight\", \"encoder.layernorm_embedding.bias\", \"decoder.layernorm_embedding.weight\", \"decoder.layernorm_embedding.bias\". \r\n\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\nmatched all layers success\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Ubuntu\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n```python\r\n!git clone https://github.com/pytorch/fairseq\r\n!pip install --no-deps './fairseq'\r\n```\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: using cpu\r\n - Any other relevant information: -\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2175/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2175/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2173", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2173/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2173/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2173/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2173", "id": 623205086, "node_id": "MDU6SXNzdWU2MjMyMDUwODY=", "number": 2173, "title": "Generating Alignments take longer time than prediction on a GPU", "user": {"login": "kalyangvs", "id": 42022935, "node_id": "MDQ6VXNlcjQyMDIyOTM1", "avatar_url": "https://avatars.githubusercontent.com/u/42022935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kalyangvs", "html_url": "https://github.com/kalyangvs", "followers_url": "https://api.github.com/users/kalyangvs/followers", "following_url": "https://api.github.com/users/kalyangvs/following{/other_user}", "gists_url": "https://api.github.com/users/kalyangvs/gists{/gist_id}", "starred_url": "https://api.github.com/users/kalyangvs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kalyangvs/subscriptions", "organizations_url": "https://api.github.com/users/kalyangvs/orgs", "repos_url": "https://api.github.com/users/kalyangvs/repos", "events_url": "https://api.github.com/users/kalyangvs/events{/privacy}", "received_events_url": "https://api.github.com/users/kalyangvs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-22T13:27:41Z", "updated_at": "2020-05-26T22:46:43Z", "closed_at": "2020-05-26T22:46:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n--print-alignment takes too much time on GPU probably due to extract_hard_alignment in utils\r\n\r\n### To Reproduce\r\n\r\n1. Run command in [here](https://github.com/pytorch/fairseq/tree/master/examples/translation#example-usage-cli-tools).\r\n2. Same command with --print-alignment takes around 3X the time.\r\n\r\n### Expected behavior\r\n1. On CPU it takes only 70 ms to generate alignments for a batch size 32, beam size 5 and an average sequence size of 5.\r\n2. On GPU it is supposed to take around the same time, but is around 250 ms for the same sequences.\r\n\r\n### Environment\r\n\r\n - fairseq Version - 0.9.0 / master(e.g., 1.0 or master):\r\n - PyTorch Version - 1.5.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source): pip install --editable\r\n - Python version: 3.7.5\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: fconv pre-trained\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2173/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2173/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2153", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2153/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2153/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2153/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2153", "id": 620982372, "node_id": "MDU6SXNzdWU2MjA5ODIzNzI=", "number": 2153, "title": "pip install fairseq failed.", "user": {"login": "sohailhaider", "id": 17184054, "node_id": "MDQ6VXNlcjE3MTg0MDU0", "avatar_url": "https://avatars.githubusercontent.com/u/17184054?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sohailhaider", "html_url": "https://github.com/sohailhaider", "followers_url": "https://api.github.com/users/sohailhaider/followers", "following_url": "https://api.github.com/users/sohailhaider/following{/other_user}", "gists_url": "https://api.github.com/users/sohailhaider/gists{/gist_id}", "starred_url": "https://api.github.com/users/sohailhaider/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sohailhaider/subscriptions", "organizations_url": "https://api.github.com/users/sohailhaider/orgs", "repos_url": "https://api.github.com/users/sohailhaider/repos", "events_url": "https://api.github.com/users/sohailhaider/events{/privacy}", "received_events_url": "https://api.github.com/users/sohailhaider/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-19T13:34:41Z", "updated_at": "2020-08-20T19:35:59Z", "closed_at": "2020-05-19T13:56:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nCan not install fairseq, though everything else is installed.\r\n\r\n### To Reproduce\r\n\r\nCan not install fariseq from pip:\r\n\r\n1. pip install --no-cache-dir fairseq\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n`SohailHaider:~/environment $ pip install --no-cache-dir fairseq\r\nDefaulting to user installation because normal site-packages is not writeable\r\nCollecting fairseq\r\n  Downloading fairseq-0.9.0.tar.gz (306 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306 kB 48.9 MB/s \r\nRequirement already satisfied: cffi in /home/ec2-user/.local/lib/python3.6/site-packages (from fairseq) (1.14.0)\r\nRequirement already satisfied: cython in /home/ec2-user/.local/lib/python3.6/site-packages (from fairseq) (0.29.18)\r\nRequirement already satisfied: numpy in /home/ec2-user/.local/lib/python3.6/site-packages (from fairseq) (1.18.4)\r\nRequirement already satisfied: regex in /usr/local/lib64/python3.6/site-packages (from fairseq) (2019.12.19)\r\nRequirement already satisfied: sacrebleu in /home/ec2-user/.local/lib/python3.6/site-packages (from fairseq) (1.4.9)\r\nRequirement already satisfied: torch in /home/ec2-user/.local/lib/python3.6/site-packages (from fairseq) (1.5.0)\r\nRequirement already satisfied: tqdm in /home/ec2-user/.local/lib/python3.6/site-packages (from fairseq) (4.46.0)\r\nRequirement already satisfied: pycparser in /home/ec2-user/.local/lib/python3.6/site-packages (from cffi->fairseq) (2.20)\r\nRequirement already satisfied: portalocker in /home/ec2-user/.local/lib/python3.6/site-packages (from sacrebleu->fairseq) (1.7.0)\r\nRequirement already satisfied: typing in /home/ec2-user/.local/lib/python3.6/site-packages (from sacrebleu->fairseq) (3.7.4.1)\r\nRequirement already satisfied: future in /usr/local/lib/python3.6/site-packages (from torch->fairseq) (0.18.2)\r\nBuilding wheels for collected packages: fairseq\r\n  Building wheel for fairseq (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-bkh87xw9/fairseq/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-bkh87xw9/fairseq/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-nqhvboec\r\n       cwd: /tmp/pip-install-bkh87xw9/fairseq/\r\n  Complete output (326 lines):\r\n  which: no hipcc in (/home/ec2-user/.nvm/versions/node/v10.18.0/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3@global/bin:/home/ec2-user/.rvm/rubies/ruby-2.6.3/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3@global/bin:/home/ec2-user/.rvm/rubies/ruby-2.6.3/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3@global/bin:/home/ec2-user/.rvm/rubies/ruby-2.6.3/bin:/usr/local/bin:/bin:/usr/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/home/ec2-user/.rvm/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/home/ec2-user/.rvm/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/home/ec2-user/.rvm/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin)\r\n  running bdist_wheel\r\n  /home/ec2-user/.local/lib/python3.6/site-packages/torch/utils/cpp_extension.py:304: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\r\n    warnings.warn(msg.format('we could not find ninja.'))\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.linux-x86_64-3.6\r\n  creating build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/search.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/pdb.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/binarizer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/legacy_distributed_data_parallel.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/distributed_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/bleu.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/__init__.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/checkpoint_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/sequence_scorer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/progress_bar.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/iterative_refinement_generator.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/hub_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/registry.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/trainer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/options.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/file_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/sequence_generator.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/meters.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  creating build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/interactive.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/setup.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/train.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/__init__.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/preprocess.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/generate.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/eval_lm.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/score.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  creating build/lib.linux-x86_64-3.6/examples\r\n  copying examples/__init__.py -> build/lib.linux-x86_64-3.6/examples\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/vggblock.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/lightweight_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sparse_transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/layer_norm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/dynamic_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/beamable_mm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/gelu.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/unfold.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/scalar_bias.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sparse_transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/mean_pool_gating_network.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/adaptive_input.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/learned_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/highway.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sparse_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/grad_multiply.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sinusoidal_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/linearized_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/character_token_embedder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/conv_tbc.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/adaptive_softmax.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/logsumexp_moe.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/downsampled_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transformer_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/fairseq_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/nat_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/legacy_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/composite_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/binary_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/label_smoothed_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/sentence_prediction.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/sentence_ranking.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/adaptive_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/fairseq_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/append_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/round_robin_zip_datasets.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/transform_eos_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/transform_eos_lang_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/token_block_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/subsample_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/language_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/list_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/plasma_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/sharded_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/multi_corpus_sampled_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/id_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/replace_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/nested_dictionary_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/roll_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/lru_cache_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/concat_sentences_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/numel_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/denoising_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/offset_tokens_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/concat_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/strip_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/indexed_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/raw_label_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/prepend_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/lm_context_window_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/monolingual_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/noising.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/pad_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/resampling_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/mask_tokens_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/truncate_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/sort_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/backtranslation_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/colorize_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/iterators.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/num_samples_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/prepend_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/base_wrapper_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  creating build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/nag.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/fairseq_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adamax.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/sgd.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adadelta.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adafactor.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adagrad.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/bmuf.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adam.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  creating build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/fairseq_task.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_from_pretrained_xlm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/legacy_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/multilingual_translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_lev.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_moe.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/multilingual_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/audio_pretraining.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/language_modeling.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/semisupervised_translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/cross_lingual_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/denoising.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/sentence_prediction.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/sentence_ranking.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer_from_pretrained_xlm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/insertion_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lightconv_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/composite_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/multilingual_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fconv_self_att.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lstm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_incremental_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/distributed_fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/wav2vec.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lightconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/nonautoregressive_ensembles.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/nonautoregressive_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/cmlm_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/iterative_nonautoregressive_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/levenshtein_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/model_utils.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fconv_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/setup.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/lightconv_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/setup.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/dynamicconv_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  copying fairseq/data/audio/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  copying fairseq/data/audio/raw_audio_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/masked_lm_dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/block_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/masked_lm_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/hf_bert_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/gpt2_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/moses_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/gpt2_bpe_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/subword_nmt_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/fastbpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/sentencepiece_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/space_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/nltk_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  creating build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/polynomial_decay_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/inverse_square_root_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/cosine_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/fixed_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/triangular_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  copying fairseq/models/bart/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  copying fairseq/models/bart/model.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  copying fairseq/models/bart/hub_interface.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/alignment_utils.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/model.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/hub_interface.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  creating build/lib.linux-x86_64-3.6/examples/noisychannel\r\n  copying examples/noisychannel/rerank_score_bw.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n  copying examples/noisychannel/rerank.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n  copying examples/noisychannel/rerank_score_lm.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n  copying examples/noisychannel/rerank_generate.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n  copying examples/noisychannel/__init__.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n  copying examples/noisychannel/rerank_options.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n  copying examples/noisychannel/rerank_tune.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n  copying examples/noisychannel/rerank_utils.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n  creating build/lib.linux-x86_64-3.6/examples/speech_recognition\r\n  copying examples/speech_recognition/__init__.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition\r\n  copying examples/speech_recognition/infer.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition\r\n  copying examples/speech_recognition/w2l_decoder.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition\r\n  creating build/lib.linux-x86_64-3.6/examples/speech_recognition/criterions\r\n  copying examples/speech_recognition/criterions/ASG_loss.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/criterions\r\n  copying examples/speech_recognition/criterions/__init__.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/criterions\r\n  copying examples/speech_recognition/criterions/cross_entropy_acc.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/criterions\r\n  copying examples/speech_recognition/criterions/CTC_loss.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/criterions\r\n  creating build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n  copying examples/speech_recognition/data/replabels.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n  copying examples/speech_recognition/data/__init__.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n  copying examples/speech_recognition/data/data_utils.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n  copying examples/speech_recognition/data/collaters.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n  copying examples/speech_recognition/data/asr_dataset.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n  creating build/lib.linux-x86_64-3.6/examples/speech_recognition/tasks\r\n  copying examples/speech_recognition/tasks/__init__.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/tasks\r\n  copying examples/speech_recognition/tasks/speech_recognition.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/tasks\r\n  creating build/lib.linux-x86_64-3.6/examples/speech_recognition/models\r\n  copying examples/speech_recognition/models/__init__.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/models\r\n  copying examples/speech_recognition/models/vggtransformer.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/models\r\n  copying examples/speech_recognition/models/w2l_conv_glu_enc.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/models\r\n  running build_ext\r\n  /home/ec2-user/.local/lib/python3.6/site-packages/torch/utils/cpp_extension.py:244: UserWarning:\r\n  \r\n                                 !! WARNING !!\r\n  \r\n  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n  Your compiler (g++ 4.8.5) may be ABI-incompatible with PyTorch!\r\n  Please use a compiler that is ABI-compatible with GCC 5.0 and above.\r\n  See https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.\r\n  \r\n  See https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\r\n  for instructions on how to install GCC 5 or higher.\r\n  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n  \r\n                                !! WARNING !!\r\n  \r\n    warnings.warn(ABI_INCOMPATIBILITY_WARNING.format(compiler))\r\n  cythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\r\n  cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\r\n  warning: fairseq/data/token_block_utils_fast.pyx:99:40: the result of using negative indices inside of code sections marked as 'wraparound=False' is undefined\r\n  building 'fairseq.libbleu' extension\r\n  creating build/temp.linux-x86_64-3.6\r\n  creating build/temp.linux-x86_64-3.6/fairseq\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib/libbleu\r\n  gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\r\n  gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\r\n  g++ -pthread -shared -g build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -L/usr/lib64 -lpython3.6m -o build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.data.data_utils_fast' extension\r\n  creating build/temp.linux-x86_64-3.6/fairseq/data\r\n  gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include -I/home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,\r\n                   from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\r\n                   from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                   from fairseq/data/data_utils_fast.cpp:613:\r\n  /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n   #warning \"Using deprecated NumPy API, disable it with \" \\\r\n    ^\r\n  g++ -pthread -shared -g build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -L/usr/lib64 -lpython3.6m -o build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.data.token_block_utils_fast' extension\r\n  gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include -I/home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,\r\n                   from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\r\n                   from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                   from fairseq/data/token_block_utils_fast.cpp:614:\r\n  /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n   #warning \"Using deprecated NumPy API, disable it with \" \\\r\n    ^\r\n  fairseq/data/token_block_utils_fast.cpp: In function \u2018PyArrayObject* __pyx_f_7fairseq_4data_22token_block_utils_fast__get_slice_indices_fast(PyArrayObject*, PyObject*, int, int, int)\u2019:\r\n  fairseq/data/token_block_utils_fast.cpp:3303:38: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         __pyx_t_4 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\r\n                                        ^\r\n  fairseq/data/token_block_utils_fast.cpp:3498:38: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         __pyx_t_3 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\r\n                                        ^\r\n  g++ -pthread -shared -g build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -L/usr/lib64 -lpython3.6m -o build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.libnat' extension\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib/libnat\r\n  gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/home/ec2-user/.local/lib/python3.6/site-packages/torch/include -I/home/ec2-user/.local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/home/ec2-user/.local/lib/python3.6/site-packages/torch/include/TH -I/home/ec2-user/.local/lib/python3.6/site-packages/torch/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\r\n  gcc: error: unrecognized command line option \u2018-std=c++14\u2019\r\n  error: command 'gcc' failed with exit status 1\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for fairseq\r\n  Running setup.py clean for fairseq\r\nFailed to build fairseq\r\nInstalling collected packages: fairseq\r\n    Running setup.py install for fairseq ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-bkh87xw9/fairseq/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-bkh87xw9/fairseq/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-i3z9ci8t/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/ec2-user/.local/include/python3.6m/fairseq\r\n         cwd: /tmp/pip-install-bkh87xw9/fairseq/\r\n    Complete output (325 lines):\r\n    which: no hipcc in (/home/ec2-user/.nvm/versions/node/v10.18.0/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3@global/bin:/home/ec2-user/.rvm/rubies/ruby-2.6.3/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3@global/bin:/home/ec2-user/.rvm/rubies/ruby-2.6.3/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3/bin:/home/ec2-user/.rvm/gems/ruby-2.6.3@global/bin:/home/ec2-user/.rvm/rubies/ruby-2.6.3/bin:/usr/local/bin:/bin:/usr/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/home/ec2-user/.rvm/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/home/ec2-user/.rvm/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/home/ec2-user/.rvm/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin)\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build/lib.linux-x86_64-3.6\r\n    creating build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/search.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/pdb.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/binarizer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/legacy_distributed_data_parallel.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/distributed_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/bleu.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/__init__.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/checkpoint_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/sequence_scorer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/progress_bar.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/iterative_refinement_generator.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/hub_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/registry.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/trainer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/options.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/file_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/sequence_generator.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    copying fairseq/meters.py -> build/lib.linux-x86_64-3.6/fairseq\r\n    creating build/lib.linux-x86_64-3.6/fairseq_cli\r\n    copying fairseq_cli/interactive.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n    copying fairseq_cli/setup.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n    copying fairseq_cli/train.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n    copying fairseq_cli/__init__.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n    copying fairseq_cli/preprocess.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n    copying fairseq_cli/generate.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n    copying fairseq_cli/eval_lm.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n    copying fairseq_cli/score.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n    creating build/lib.linux-x86_64-3.6/examples\r\n    copying examples/__init__.py -> build/lib.linux-x86_64-3.6/examples\r\n    creating build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/vggblock.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/lightweight_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/sparse_transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/layer_norm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/dynamic_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/beamable_mm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/gelu.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/unfold.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/scalar_bias.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/sparse_transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/mean_pool_gating_network.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/adaptive_input.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/learned_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/highway.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/sparse_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/grad_multiply.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/sinusoidal_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/linearized_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/character_token_embedder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/conv_tbc.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/adaptive_softmax.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/logsumexp_moe.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/downsampled_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    copying fairseq/modules/transformer_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n    creating build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/fairseq_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/nat_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/legacy_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/composite_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/binary_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/label_smoothed_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/sentence_prediction.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/sentence_ranking.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    copying fairseq/criterions/adaptive_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n    creating build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/fairseq_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/append_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/round_robin_zip_datasets.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/transform_eos_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/transform_eos_lang_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/token_block_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/subsample_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/language_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/list_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/plasma_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/sharded_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/multi_corpus_sampled_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/id_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/replace_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/nested_dictionary_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/roll_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/lru_cache_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/concat_sentences_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/numel_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/denoising_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/offset_tokens_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/concat_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/strip_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/indexed_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/raw_label_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/prepend_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/lm_context_window_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/monolingual_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/noising.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/pad_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/resampling_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/mask_tokens_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/truncate_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/sort_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/backtranslation_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/colorize_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/iterators.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/num_samples_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/prepend_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    copying fairseq/data/base_wrapper_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n    creating build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/nag.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/fairseq_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/adamax.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/sgd.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/adadelta.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/adafactor.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/adagrad.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/bmuf.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    copying fairseq/optim/adam.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n    creating build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/fairseq_task.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/translation_from_pretrained_xlm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/legacy_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/multilingual_translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/translation_lev.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/translation_moe.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/multilingual_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/audio_pretraining.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/language_modeling.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/semisupervised_translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/cross_lingual_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/denoising.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/sentence_prediction.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    copying fairseq/tasks/sentence_ranking.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n    creating build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/transformer_from_pretrained_xlm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/insertion_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/lightconv_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/composite_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/multilingual_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/fconv_self_att.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/transformer_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/fairseq_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/lstm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/fairseq_incremental_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/distributed_fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/fconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/wav2vec.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/lightconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/nonautoregressive_ensembles.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/nonautoregressive_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/cmlm_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/iterative_nonautoregressive_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/levenshtein_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/model_utils.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/fairseq_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    copying fairseq/models/fconv_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n    creating build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n    copying fairseq/modules/lightconv_layer/setup.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n    copying fairseq/modules/lightconv_layer/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n    copying fairseq/modules/lightconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n    copying fairseq/modules/lightconv_layer/lightconv_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n    creating build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n    copying fairseq/modules/dynamicconv_layer/setup.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n    copying fairseq/modules/dynamicconv_layer/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n    copying fairseq/modules/dynamicconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n    copying fairseq/modules/dynamicconv_layer/dynamicconv_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n    creating build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n    copying fairseq/data/audio/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n    copying fairseq/data/audio/raw_audio_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n    creating build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n    copying fairseq/data/legacy/masked_lm_dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n    copying fairseq/data/legacy/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n    copying fairseq/data/legacy/block_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n    copying fairseq/data/legacy/masked_lm_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n    creating build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/hf_bert_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/gpt2_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/moses_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/gpt2_bpe_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/subword_nmt_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/fastbpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/sentencepiece_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/space_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/nltk_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n    creating build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/polynomial_decay_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/inverse_square_root_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/cosine_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/fixed_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/triangular_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n    creating build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n    copying fairseq/models/bart/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n    copying fairseq/models/bart/model.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n    copying fairseq/models/bart/hub_interface.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n    creating build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n    copying fairseq/models/roberta/alignment_utils.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n    copying fairseq/models/roberta/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n    copying fairseq/models/roberta/model.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n    copying fairseq/models/roberta/hub_interface.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n    creating build/lib.linux-x86_64-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_score_bw.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_score_lm.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_generate.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n    copying examples/noisychannel/__init__.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_options.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_tune.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_utils.py -> build/lib.linux-x86_64-3.6/examples/noisychannel\r\n    creating build/lib.linux-x86_64-3.6/examples/speech_recognition\r\n    copying examples/speech_recognition/__init__.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition\r\n    copying examples/speech_recognition/infer.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition\r\n    copying examples/speech_recognition/w2l_decoder.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition\r\n    creating build/lib.linux-x86_64-3.6/examples/speech_recognition/criterions\r\n    copying examples/speech_recognition/criterions/ASG_loss.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/criterions\r\n    copying examples/speech_recognition/criterions/__init__.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/criterions\r\n    copying examples/speech_recognition/criterions/cross_entropy_acc.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/criterions\r\n    copying examples/speech_recognition/criterions/CTC_loss.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/criterions\r\n    creating build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n    copying examples/speech_recognition/data/replabels.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n    copying examples/speech_recognition/data/__init__.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n    copying examples/speech_recognition/data/data_utils.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n    copying examples/speech_recognition/data/collaters.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n    copying examples/speech_recognition/data/asr_dataset.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/data\r\n    creating build/lib.linux-x86_64-3.6/examples/speech_recognition/tasks\r\n    copying examples/speech_recognition/tasks/__init__.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/tasks\r\n    copying examples/speech_recognition/tasks/speech_recognition.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/tasks\r\n    creating build/lib.linux-x86_64-3.6/examples/speech_recognition/models\r\n    copying examples/speech_recognition/models/__init__.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/models\r\n    copying examples/speech_recognition/models/vggtransformer.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/models\r\n    copying examples/speech_recognition/models/w2l_conv_glu_enc.py -> build/lib.linux-x86_64-3.6/examples/speech_recognition/models\r\n    running build_ext\r\n    /home/ec2-user/.local/lib/python3.6/site-packages/torch/utils/cpp_extension.py:304: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\r\n      warnings.warn(msg.format('we could not find ninja.'))\r\n    /home/ec2-user/.local/lib/python3.6/site-packages/torch/utils/cpp_extension.py:244: UserWarning:\r\n    \r\n                                   !! WARNING !!\r\n    \r\n    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    Your compiler (g++ 4.8.5) may be ABI-incompatible with PyTorch!\r\n    Please use a compiler that is ABI-compatible with GCC 5.0 and above.\r\n    See https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.\r\n    \r\n    See https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\r\n    for instructions on how to install GCC 5 or higher.\r\n    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    \r\n                                  !! WARNING !!\r\n    \r\n      warnings.warn(ABI_INCOMPATIBILITY_WARNING.format(compiler))\r\n    skipping 'fairseq/data/data_utils_fast.cpp' Cython extension (up-to-date)\r\n    skipping 'fairseq/data/token_block_utils_fast.cpp' Cython extension (up-to-date)\r\n    building 'fairseq.libbleu' extension\r\n    creating build/temp.linux-x86_64-3.6\r\n    creating build/temp.linux-x86_64-3.6/fairseq\r\n    creating build/temp.linux-x86_64-3.6/fairseq/clib\r\n    creating build/temp.linux-x86_64-3.6/fairseq/clib/libbleu\r\n    gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\r\n    gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\r\n    g++ -pthread -shared -g build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -L/usr/lib64 -lpython3.6m -o build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so\r\n    building 'fairseq.data.data_utils_fast' extension\r\n    creating build/temp.linux-x86_64-3.6/fairseq/data\r\n    gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include -I/home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\r\n    In file included from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,\r\n                     from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\r\n                     from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                     from fairseq/data/data_utils_fast.cpp:613:\r\n    /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n     #warning \"Using deprecated NumPy API, disable it with \" \\\r\n      ^\r\n    g++ -pthread -shared -g build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -L/usr/lib64 -lpython3.6m -o build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so\r\n    building 'fairseq.data.token_block_utils_fast' extension\r\n    gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include -I/home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\r\n    In file included from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,\r\n                     from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\r\n                     from /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                     from fairseq/data/token_block_utils_fast.cpp:614:\r\n    /home/ec2-user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n     #warning \"Using deprecated NumPy API, disable it with \" \\\r\n      ^\r\n    fairseq/data/token_block_utils_fast.cpp: In function \u2018PyArrayObject* __pyx_f_7fairseq_4data_22token_block_utils_fast__get_slice_indices_fast(PyArrayObject*, PyObject*, int, int, int)\u2019:\r\n    fairseq/data/token_block_utils_fast.cpp:3303:38: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n           __pyx_t_4 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\r\n                                          ^\r\n    fairseq/data/token_block_utils_fast.cpp:3498:38: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n           __pyx_t_3 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\r\n                                          ^\r\n    g++ -pthread -shared -g build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -L/usr/lib64 -lpython3.6m -o build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so\r\n    building 'fairseq.libnat' extension\r\n    creating build/temp.linux-x86_64-3.6/fairseq/clib/libnat\r\n    gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/home/ec2-user/.local/lib/python3.6/site-packages/torch/include -I/home/ec2-user/.local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/home/ec2-user/.local/lib/python3.6/site-packages/torch/include/TH -I/home/ec2-user/.local/lib/python3.6/site-packages/torch/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\r\n    gcc: error: unrecognized command line option \u2018-std=c++14\u2019\r\n    error: command 'gcc' failed with exit status 1\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-bkh87xw9/fairseq/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-bkh87xw9/fairseq/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-i3z9ci8t/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/ec2-user/.local/include/python3.6m/fairseq Check the logs for full command output.`\r\n\r\n### Expected behavior\r\n\r\nShould install fairseq\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): pip latest\r\n - PyTorch Version (e.g., 1.0): latest for pip3\r\n - OS (e.g., Linux): linux amazon\r\n - How you installed fairseq (`pip`, source): pip\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2153/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2148", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2148/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2148/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2148/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2148", "id": 620735898, "node_id": "MDU6SXNzdWU2MjA3MzU4OTg=", "number": 2148, "title": "buid_ext failed using torch1.5 ", "user": {"login": "xiongjun19", "id": 53548403, "node_id": "MDQ6VXNlcjUzNTQ4NDAz", "avatar_url": "https://avatars.githubusercontent.com/u/53548403?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiongjun19", "html_url": "https://github.com/xiongjun19", "followers_url": "https://api.github.com/users/xiongjun19/followers", "following_url": "https://api.github.com/users/xiongjun19/following{/other_user}", "gists_url": "https://api.github.com/users/xiongjun19/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiongjun19/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiongjun19/subscriptions", "organizations_url": "https://api.github.com/users/xiongjun19/orgs", "repos_url": "https://api.github.com/users/xiongjun19/repos", "events_url": "https://api.github.com/users/xiongjun19/events{/privacy}", "received_events_url": "https://api.github.com/users/xiongjun19/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-19T07:12:13Z", "updated_at": "2020-05-19T07:41:16Z", "closed_at": "2020-05-19T07:41:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI'm trying to use fairseq under torch1.5 version, but failed to build with setup.py script.\r\n\r\n\r\n### To Reproduce\r\n1. run command: \r\n`python setup.py build_ext -i`\r\n2. and the output is: \r\nthe error info is:\r\ntorch/include/torch/csrc/api/include/torch/nn/cloneable.h:44:65: error: invalid static_cast from type \u2018const torch::OrderedDict<std::basic_string<char>,\r\nat::Tensor>\u2019 to type \u2018torch::OrderedDict<std::basic_string<char>, at::Tensor>&\u2019\r\n/home/dev/anaconda3/envs/tor15/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:56:59: error: invalid static_cast from type \u2018const torch::OrderedDict<std::basic_string<char>,\r\nat::Tensor>\u2019 to type \u2018torch::OrderedDict<std::basic_string<char>, at::Tensor>&\u2019\r\n/home/dev/anaconda3/envs/tor15/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:68:61: error: invalid static_cast from type \u2018const torch::OrderedDict<std::basic_string<char>,\r\nstd::shared_ptr<torch::nn::Module> >\u2019 to type \u2018torch::OrderedDict<std::basic_string<char>, std::shared_ptr<torch::nn::Module> >&\u2019\r\n/home/dev/anaconda3/envs/tor15/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h: In instantiation of \u2018std::shared_ptr<torch::nn::Module> torch::nn::Cloneable<Derived>::clone(c\r\nonst c10::optional<c10::Device>&) const [with Derived = torch::nn::EmbeddingBagImpl]\u2019:\r\n/tmp/tmpxft_00008f72_00000000-5_edit_dist.cudafe1.stub.c:225:27:   required from here\r\n/home/dev/anaconda3/envs/tor15/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:44:65: error: invalid static_cast from type \u2018const torch::OrderedDict<std::basic_string<char>,\r\nat::Tensor>\u2019 to type \u2018torch::OrderedDict<std::basic_string<char>, at::Tensor>&\u2019\r\n/home/dev/anaconda3/envs/tor15/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:56:59: error: invalid static_cast from type \u2018const torch::OrderedDict<std::basic_string<char>,\r\nat::Tensor>\u2019 to type \u2018torch::OrderedDict<std::basic_string<char>, at::Tensor>&\u2019\r\n/home/dev/anaconda3/envs/tor15/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:68:61: error: invalid static_cast from type \u2018const torch::OrderedDict<std::basic_string<char>,\r\nstd::shared_ptr<torch::nn::Module> >\u2019 to type \u2018torch::OrderedDict<std::basic_string<char>, std::shared_ptr<torch::nn::Module> >&\u2019\r\n/home/dev/anaconda3/envs/tor15/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h: In instantiation of \u2018std::shared_ptr<torch::nn::Module> torch::nn::Cloneable<Derived>::clone(c\r\nonst c10::optional<c10::Device>&) const [with Derived = torch::nn::EmbeddingImpl]\u2019:\r\n/tmp/tmpxft_00008f72_00000000-5_edit_dist.cudafe1.stub.c:225:27:   required from here\r\n/home/dev/anaconda3/envs/tor15/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:44:65: error: invalid static_cast from type \u2018const torch::OrderedDict<std::basic_string<char>,\r\nat::Tensor>\u2019 to type \u2018torch::OrderedDict<std::basic_string<char>, at::Tensor>&\u2019\r\n\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version ( master):\r\n - PyTorch Version (1.5)\r\n - OS ( Linux):\r\n\r\n - Python version: 3.7 \r\n - CUDA/cuDNN version: 10.1/7.6\r\n - GPU models and configuration: \r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2148/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2148/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2108", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2108/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2108/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2108/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2108", "id": 614632797, "node_id": "MDU6SXNzdWU2MTQ2MzI3OTc=", "number": 2108, "title": "About Noisy channel reranking, empty file error", "user": {"login": "ElliottYan", "id": 10862038, "node_id": "MDQ6VXNlcjEwODYyMDM4", "avatar_url": "https://avatars.githubusercontent.com/u/10862038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ElliottYan", "html_url": "https://github.com/ElliottYan", "followers_url": "https://api.github.com/users/ElliottYan/followers", "following_url": "https://api.github.com/users/ElliottYan/following{/other_user}", "gists_url": "https://api.github.com/users/ElliottYan/gists{/gist_id}", "starred_url": "https://api.github.com/users/ElliottYan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ElliottYan/subscriptions", "organizations_url": "https://api.github.com/users/ElliottYan/orgs", "repos_url": "https://api.github.com/users/ElliottYan/repos", "events_url": "https://api.github.com/users/ElliottYan/events{/privacy}", "received_events_url": "https://api.github.com/users/ElliottYan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-08T09:43:40Z", "updated_at": "2020-06-05T16:38:58Z", "closed_at": "2020-05-13T05:46:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nHi, I'm trying to reproduce the result of noisy channel reranking using my mac (without cuda environment). But some empty file error happens.\r\n\r\n\r\nI follow the exact instructions stated here. https://github.com/pytorch/fairseq/tree/be86e7ebefb21f694b190487b789f4e61132fc13/examples/noisychannel\r\n\r\nThe following error happens.\r\n![image](https://user-images.githubusercontent.com/10862038/81393556-25c1ea80-9153-11ea-8f3b-64a2d99b2c1c.png)\r\n\r\nI have no idea what's going on. Could somebody help me here? Thx\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0) 1.5.0\r\n - OS (e.g., Linux): MacOSX\r\n - How you installed fairseq (`pip`, source): sourcec\r\n - Build command you used (if compiling from source): pip install --editable .\r\n - Python version: 3.7\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2108/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2108/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2107", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2107/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2107/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2107/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2107", "id": 614630950, "node_id": "MDU6SXNzdWU2MTQ2MzA5NTA=", "number": 2107, "title": "speech_recognition with ctc_loss failed", "user": {"login": "luweishuang", "id": 12368977, "node_id": "MDQ6VXNlcjEyMzY4OTc3", "avatar_url": "https://avatars.githubusercontent.com/u/12368977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luweishuang", "html_url": "https://github.com/luweishuang", "followers_url": "https://api.github.com/users/luweishuang/followers", "following_url": "https://api.github.com/users/luweishuang/following{/other_user}", "gists_url": "https://api.github.com/users/luweishuang/gists{/gist_id}", "starred_url": "https://api.github.com/users/luweishuang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luweishuang/subscriptions", "organizations_url": "https://api.github.com/users/luweishuang/orgs", "repos_url": "https://api.github.com/users/luweishuang/repos", "events_url": "https://api.github.com/users/luweishuang/events{/privacy}", "received_events_url": "https://api.github.com/users/luweishuang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-05-08T09:40:00Z", "updated_at": "2020-05-28T16:10:50Z", "closed_at": "2020-05-28T14:55:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\nI wanted used \"--criterion ctc_loss\" instead of \"--criterion cross_entropy_acc\" and other parameters keep the same with examples/speech_recognition/README.md, I got a \"TypeError: build_criterion() missing 1 required positional argument: 'task'\" and I don't know how to fix this error", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2107/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2106", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2106/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2106/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2106/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2106", "id": 614546322, "node_id": "MDU6SXNzdWU2MTQ1NDYzMjI=", "number": 2106, "title": "Wav2Vec ImportError: Please build Cython components with: `pip install --editable .` or `python setup.py build_ext --inplace`", "user": {"login": "zengchang233", "id": 35031709, "node_id": "MDQ6VXNlcjM1MDMxNzA5", "avatar_url": "https://avatars.githubusercontent.com/u/35031709?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zengchang233", "html_url": "https://github.com/zengchang233", "followers_url": "https://api.github.com/users/zengchang233/followers", "following_url": "https://api.github.com/users/zengchang233/following{/other_user}", "gists_url": "https://api.github.com/users/zengchang233/gists{/gist_id}", "starred_url": "https://api.github.com/users/zengchang233/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zengchang233/subscriptions", "organizations_url": "https://api.github.com/users/zengchang233/orgs", "repos_url": "https://api.github.com/users/zengchang233/repos", "events_url": "https://api.github.com/users/zengchang233/events{/privacy}", "received_events_url": "https://api.github.com/users/zengchang233/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-05-08T06:55:09Z", "updated_at": "2022-12-01T13:13:51Z", "closed_at": "2020-05-08T11:51:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nHi, thanks for your nice work! When I pretrain wav2vec model by using aishell1 dataset, an import error occurred and the error information is the following\r\n\r\n```\r\n2020-05-08 14:44:42 | INFO | fairseq_cli.train | model wav2vec, criterion BinaryCrossEntropyCriterion\r\n2020-05-08 14:44:42 | INFO | fairseq_cli.train | num. model params: 511808 (num. trained: 511808)\r\n2020-05-08 14:44:42 | INFO | fairseq_cli.train | training on 4 GPUs\r\n2020-05-08 14:44:42 | INFO | fairseq_cli.train | max tokens per GPU = 1500000 and max sentences per GPU = None\r\n2020-05-08 14:44:42 | INFO | fairseq.trainer | no existing checkpoint found /root/zengchang/models/checkpoint_last.pt\r\n2020-05-08 14:44:42 | INFO | fairseq.trainer | loading train data for epoch 1\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/root/zengchang/fairseq/fairseq_cli/train.py\", line 355, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/root/zengchang/fairseq/fairseq/data/data_utils.py\", line 220, in batch_by_size\r\n    from fairseq.data.data_utils_fast import batch_by_size_fast\r\nModuleNotFoundError: No module named 'fairseq.data.data_utils_fast'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/root/zengchang/fairseq/fairseq_cli/train.py\", line 324, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/root/zengchang/fairseq/fairseq_cli/train.py\", line 104, in main\r\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\r\n  File \"/root/zengchang/fairseq/fairseq/checkpoint_utils.py\", line 157, in load_checkpoint\r\n    epoch=1, load_dataset=True, **passthrough_args\r\n  File \"/root/zengchang/fairseq/fairseq/trainer.py\", line 296, in get_train_iterator\r\n    epoch=epoch\r\n  File \"/root/zengchang/fairseq/fairseq/tasks/fairseq_task.py\", line 181, in get_batch_iterator\r\n    required_batch_size_multiple=required_batch_size_multiple,\r\n  File \"/root/zengchang/fairseq/fairseq/data/data_utils.py\", line 223, in batch_by_size\r\n    'Please build Cython components with: `pip install --editable .` '\r\nImportError: Please build Cython components with: `pip install --editable .` or `python setup.py build_ext --inplace`\r\n```\r\n\r\n### To Reproduce\r\n\r\nThe command I ran is the following\r\n\r\n```\r\npython train.py /disk1/speech/aishell1 --save-dir /root/zengchang/models/ --num-workers 6 --fp16 --max-update 400000 --save-interval 10 --no-epoch-checkpoints --arch wav2vec --task audio_pretraining --lr 1e-06 --min-lr 1e-09 --optimizer adam --max-lr 0.005 --lr-scheduler cosine --conv-feature-layers '[(64, 10, 5), (64, 8, 4), (64, 4, 2), (64, 4, 2), (64, 4, 2), (64, 1, 1), (64, 1, 1)]' --conv-aggregator-layers '[(64, 2, 1), (64, 3, 1), (64, 4, 1), (64, 5, 1), (64, 6, 1), (64, 7, 1), (64, 8, 1), (64, 9, 1), (64, 10, 1), (64, 11, 1), (64, 12, 1), (64, 13, 1)]' --skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion binary_cross_entropy --num-negatives 10 --max-sample-size 150000 --max-tokens 1500000 --skip-invalid-size-inputs-valid-test\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version 0.9.0 from Pypi\r\n - PyTorch Version 1.4.0\r\n - OS (e.g., Linux): CentOS 7.6\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 10.0\r\n\r\n### Additional context\r\n\r\nDo you have any idea about this error.\r\n\r\nI also tried to install fairseq from source, but there is error like the following\r\n\r\n```\r\n(zeng) [root@iZbp12wrkn96xlow3hmb2wZ fairseq]# pip install --editable .\r\nLooking in indexes: http://mirrors.cloud.aliyuncs.com/pypi/simple/\r\nObtaining file:///root/zengchang/tools/fairseq\r\n  Installing build dependencies ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /root/anaconda3/envs/zeng/bin/python3.7 /root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-gw98n38u/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i http://mirrors.cloud.aliyuncs.com/pypi/simple/ --trusted-host mirrors.cloud.aliyuncs.com -- setuptools wheel cython\r\n       cwd: None\r\n  Complete output (44 lines):\r\n  Traceback (most recent call last):\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n      \"__main__\", mod_spec)\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/runpy.py\", line 85, in _run_code\r\n      exec(code, run_globals)\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/__main__.py\", line 26, in <module>\r\n      sys.exit(_main())\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_internal/cli/main.py\", line 73, in main\r\n      command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_internal/commands/__init__.py\", line 104, in create_command\r\n      module = importlib.import_module(module_path)\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n      return _bootstrap._gcd_import(name[level:], package, level)\r\n    File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n    File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n    File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n    File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n    File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n    File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_internal/commands/install.py\", line 24, in <module>\r\n      from pip._internal.cli.req_command import RequirementCommand, with_cleanup\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_internal/cli/req_command.py\", line 16, in <module>\r\n      from pip._internal.index.package_finder import PackageFinder\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_internal/index/package_finder.py\", line 21, in <module>\r\n      from pip._internal.index.collector import parse_links\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_internal/index/collector.py\", line 14, in <module>\r\n      from pip._vendor import html5lib, requests\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_vendor/requests/__init__.py\", line 114, in <module>\r\n      from . import utils\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_vendor/requests/utils.py\", line 25, in <module>\r\n      from . import certs\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_vendor/requests/certs.py\", line 15, in <module>\r\n      from pip._vendor.certifi import where\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_vendor/certifi/__init__.py\", line 1, in <module>\r\n      from .core import contents, where\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip/_vendor/certifi/core.py\", line 12, in <module>\r\n      from importlib.resources import read_text\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/importlib/resources.py\", line 11, in <module>\r\n      from typing import Iterable, Iterator, Optional, Set, Union   # noqa: F401\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/typing.py\", line 1357, in <module>\r\n      class Callable(extra=collections_abc.Callable, metaclass=CallableMeta):\r\n    File \"/root/anaconda3/envs/zeng/lib/python3.7/site-packages/typing.py\", line 1005, in __new__\r\n      self._abc_registry = extra._abc_registry\r\n  AttributeError: type object 'Callable' has no attribute '_abc_registry'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: /root/anaconda3/envs/zeng/bin/python3.7 /root/anaconda3/envs/zeng/lib/python3.7/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-gw98n38u/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i http://mirrors.cloud.aliyuncs.com/pypi/simple/ --trusted-host mirrors.cloud.aliyuncs.com -- setuptools wheel cython Check the logs for full command output.\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2106/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2097", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2097/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2097/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2097/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2097", "id": 614112524, "node_id": "MDU6SXNzdWU2MTQxMTI1MjQ=", "number": 2097, "title": "TypeError: get_batch_iterator() got an unexpected keyword argument 'buffer_size'", "user": {"login": "bcmi220", "id": 39052744, "node_id": "MDQ6VXNlcjM5MDUyNzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/39052744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bcmi220", "html_url": "https://github.com/bcmi220", "followers_url": "https://api.github.com/users/bcmi220/followers", "following_url": "https://api.github.com/users/bcmi220/following{/other_user}", "gists_url": "https://api.github.com/users/bcmi220/gists{/gist_id}", "starred_url": "https://api.github.com/users/bcmi220/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bcmi220/subscriptions", "organizations_url": "https://api.github.com/users/bcmi220/orgs", "repos_url": "https://api.github.com/users/bcmi220/repos", "events_url": "https://api.github.com/users/bcmi220/events{/privacy}", "received_events_url": "https://api.github.com/users/bcmi220/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-07T14:41:58Z", "updated_at": "2020-05-07T14:43:33Z", "closed_at": "2020-05-07T14:43:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThe latest code removed the buffer_size parameter in the get_batch_iterator, but still keep passed it in the call. Should this parameter also be removed in the call?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2097/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2087", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2087/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2087/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2087/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2087", "id": 611472537, "node_id": "MDU6SXNzdWU2MTE0NzI1Mzc=", "number": 2087, "title": "Using match_source_len=True in model.sample and get assert step < max_len AssertionError", "user": {"login": "zixiliuUSC", "id": 49173327, "node_id": "MDQ6VXNlcjQ5MTczMzI3", "avatar_url": "https://avatars.githubusercontent.com/u/49173327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zixiliuUSC", "html_url": "https://github.com/zixiliuUSC", "followers_url": "https://api.github.com/users/zixiliuUSC/followers", "following_url": "https://api.github.com/users/zixiliuUSC/following{/other_user}", "gists_url": "https://api.github.com/users/zixiliuUSC/gists{/gist_id}", "starred_url": "https://api.github.com/users/zixiliuUSC/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zixiliuUSC/subscriptions", "organizations_url": "https://api.github.com/users/zixiliuUSC/orgs", "repos_url": "https://api.github.com/users/zixiliuUSC/repos", "events_url": "https://api.github.com/users/zixiliuUSC/events{/privacy}", "received_events_url": "https://api.github.com/users/zixiliuUSC/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-05-03T18:11:30Z", "updated_at": "2020-05-19T18:17:47Z", "closed_at": "2020-05-19T18:17:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues: #1655 \r\n2. search the docs: official command line tool  \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI build a custom generator py file myself and use it to generate the sentence, and I can generate successfully without `match_source_len=True` in sampling function. But since my task is GEC task, I want to use this flag so that the corrected sentence can match on the input. \r\n#### Code\r\n\r\n```\r\ngenerate_custom.py\r\nimport torch \r\nimport sys\r\nfrom fairseq.models.transformer import TransformerModel\r\n#import argparse\r\n#parser = argparse.ArgumentParser()\r\n\r\n\r\ndatapath = sys.argv[1]\r\nmodeldir = sys.argv[2]\r\nmodelname = sys.argv[3]\r\nbpe_type = sys.argv[4]\r\nbpe_codes = sys.argv[5]\r\nsource = sys.argv[6]\r\ntarget = sys.argv[7]\r\ntokenizer = sys.argv[8]\r\n\r\n#parser.add_argument('--tokenizer',default='nltk')\r\n#args = parser.parse_args(['--tokenizer',tokenizer])\r\n\r\nmodel = TransformerModel.from_pretrained(\r\n    modeldir,\r\n    checkpoint_file=modelname,\r\n    data_name_or_path=datapath,\r\n    bpe=bpe_type,\r\n    bpe_codes=bpe_codes,\r\n    tokenizer=tokenizer,max_target_positions=1500\r\n    )\r\nmodel.cuda()\r\nmodel.eval()\r\nmodel.half()\r\ncount = 1\r\nbsz = 500\r\nwith open(source,'r') as source, open(target, 'w') as fout:\r\n    sline = source.readline().strip()\r\n    slines = [sline]\r\n    for sline in source:\r\n        if count % bsz == 0:\r\n            with torch.no_grad():\r\n                hypotheses_batch = model.sample(slines, beam=4, lenpen=2.0, max_len_b=600, min_len=1, no_repeat_ngram_size=3,match_source_len=True)\r\n\r\n            for hypothesis in hypotheses_batch:\r\n                fout.write(hypothesis + '\\n')\r\n                fout.flush()\r\n            slines = []\r\n\r\n        slines.append(sline.strip())\r\n        count += 1\r\n    if slines != []:\r\n        hypotheses_batch = model.sample(slines, beam=4, lenpen=2.0, max_len_b=600, min_len=1, no_repeat_ngram_size=3,match_source_len=True)\r\n        for hypothesis in hypotheses_batch:\r\n            fout.write(hypothesis + '\\n')\r\n            fout.flush()\r\n\r\ngenerate_custom.sh\r\ndatapath=temp/bpe/bin\r\nmodeldir=checkpoints/transformer_fp16\r\nmodelname=checkpoint.best_loss_0.90.pt\r\nbpe_type=subword_nmt\r\nbpe_codes=temp/bpe/code\r\nsource=temp/test.src\r\ntarget=temp/test_inference.txt\r\ntokenizer=nltk\r\n\r\nCUDA_VISIBLE_DEVICES=0 python generate_custom.py $datapath $modeldir $modelname $bpe_type $bpe_codes $source $target $tokenizer\r\n```  \r\nthe error trace back is \r\n```\r\nbash generate_custom.sh \r\n/opt/conda/conda-bld/pytorch_1587428398394/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\r\nTraceback (most recent call last):\r\n  File \"generate_custom.py\", line 39, in <module>\r\n    hypotheses_batch = model.sample(slines, beam=4, lenpen=2.0, max_len_b=600, min_len=1, no_repeat_ngram_size=3,match_source_len=True)\r\n  File \"/home/zixi/EE-599/new/fairseq/fairseq/hub_utils.py\", line 135, in sample\r\n    batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)\r\n  File \"/home/zixi/EE-599/new/fairseq/fairseq/hub_utils.py\", line 172, in generate\r\n    generator, self.models, batch, **inference_step_args\r\n  File \"/home/zixi/EE-599/new/fairseq/fairseq/tasks/fairseq_task.py\", line 354, in inference_step\r\n    return generator.generate(models, sample, prefix_tokens=prefix_tokens)\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/zixi/EE-599/new/fairseq/fairseq/sequence_generator.py\", line 160, in generate\r\n    return self._generate(sample, **kwargs)\r\n  File \"/home/zixi/EE-599/new/fairseq/fairseq/sequence_generator.py\", line 357, in _generate\r\n    assert step < max_len\r\nAssertionError\r\n```\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):0.9\r\n - PyTorch Version (e.g., 1.0):1.5\r\n - OS (e.g., Linux):ubuntu 18.04LTS\r\n - How you installed fairseq (`pip`, source):source\r\n - Build command you used (if compiling from source):pip install --editable\r\n - Python version:3.7\r\n - CUDA/cuDNN version:10.1\r\n - GPU models and configuration:RTX 2070\r\n - Any other relevant information:\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2087/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2087/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2085", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2085/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2085/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2085/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2085", "id": 611429384, "node_id": "MDU6SXNzdWU2MTE0MjkzODQ=", "number": 2085, "title": "A small issue by using the multiprocessing_bpe_encoder.py", "user": {"login": "14H034160212", "id": 23516191, "node_id": "MDQ6VXNlcjIzNTE2MTkx", "avatar_url": "https://avatars.githubusercontent.com/u/23516191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/14H034160212", "html_url": "https://github.com/14H034160212", "followers_url": "https://api.github.com/users/14H034160212/followers", "following_url": "https://api.github.com/users/14H034160212/following{/other_user}", "gists_url": "https://api.github.com/users/14H034160212/gists{/gist_id}", "starred_url": "https://api.github.com/users/14H034160212/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/14H034160212/subscriptions", "organizations_url": "https://api.github.com/users/14H034160212/orgs", "repos_url": "https://api.github.com/users/14H034160212/repos", "events_url": "https://api.github.com/users/14H034160212/events{/privacy}", "received_events_url": "https://api.github.com/users/14H034160212/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-03T14:44:43Z", "updated_at": "2020-05-03T14:48:15Z", "closed_at": "2020-05-03T14:47:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nHi, there is a small issue by using the multiprocessing_bpe_encoder.py. It shows the error of unrecognized arguments: --keep-empty. Does anyone know the error? Many thanks.\r\n```\r\nusage: multiprocessing_bpe_encoder.py [-h] [--encoder-json ENCODER_JSON]\r\n                                      [--vocab-bpe VOCAB_BPE]\r\n                                      [--inputs INPUTS [INPUTS ...]]\r\n                                      [--outputs OUTPUTS [OUTPUTS ...]]\r\n                                      [--keep-empty] [--workers WORKERS]\r\nmultiprocessing_bpe_encoder.py: error: unrecognized arguments: --keep-empty\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2085/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2083", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2083/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2083/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2083/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2083", "id": 610367351, "node_id": "MDU6SXNzdWU2MTAzNjczNTE=", "number": 2083, "title": "\"argument --distributed-world-size: conflicting option string: --distributed-world-size\" Error", "user": {"login": "eshaan-pathak", "id": 64287074, "node_id": "MDQ6VXNlcjY0Mjg3MDc0", "avatar_url": "https://avatars.githubusercontent.com/u/64287074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eshaan-pathak", "html_url": "https://github.com/eshaan-pathak", "followers_url": "https://api.github.com/users/eshaan-pathak/followers", "following_url": "https://api.github.com/users/eshaan-pathak/following{/other_user}", "gists_url": "https://api.github.com/users/eshaan-pathak/gists{/gist_id}", "starred_url": "https://api.github.com/users/eshaan-pathak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eshaan-pathak/subscriptions", "organizations_url": "https://api.github.com/users/eshaan-pathak/orgs", "repos_url": "https://api.github.com/users/eshaan-pathak/repos", "events_url": "https://api.github.com/users/eshaan-pathak/events{/privacy}", "received_events_url": "https://api.github.com/users/eshaan-pathak/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-04-30T20:33:44Z", "updated_at": "2020-05-12T14:19:54Z", "closed_at": "2020-05-11T19:43:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nAfter training my model, I would like to evaluate it; however, I run into an argument parse error, as seen below. I am using the command lines from [here](https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md) and have slightly modified them where I am using a patience of 3, no-epoch-checkpoints, removed fp16, and distributed-world-size of 1 when training. I also changed the paths to reflect my own directory structure. These are the only changes I have made from the link, and I am sure that they are properly formatted. Any help is appreciated. :)\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\nTraceback (most recent call last):\r\nFile \"/home/e/miniconda3/envs/eshaan/bin/fairseq-eval-lm\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-eval-lm')()\r\n  File \"/srv/home/e/eshaan/fairseq/fairseq_cli/eval_lm.py\", line 251, in cli_main\r\n    add_distributed_training_args(parser)\r\n  File \"/srv/home/e/eshaan/fairseq/fairseq/options.py\", line 356, in add_distributed_training_args\r\n    help='total number of GPUs across all nodes (default: all visible GPUs)')\r\n  File \"/home/e/miniconda3/envs/eshaan/lib/python3.6/argparse.py\", line 1352, in add_argument\r\n    return self._add_action(action)\r\n  File \"/home/e/miniconda3/envs/eshaan/lib/python3.6/argparse.py\", line 1556, in _add_action\r\n    action = super(_ArgumentGroup, self)._add_action(action)\r\n  File \"/home/e/miniconda3/envs/eshaan/lib/python3.6/argparse.py\", line 1366, in _add_action\r\n    self._check_conflict(action)\r\n  File \"/home/e/miniconda3/envs/eshaan/lib/python3.6/argparse.py\", line 1505, in _check_conflict\r\n    conflict_handler(action, confl_optionals)\r\n  File \"/home/e/miniconda3/envs/eshaan/lib/python3.6/argparse.py\", line 1514, in _handle_conflict_error\r\n    raise ArgumentError(action, message % conflict_string)\r\nargparse.ArgumentError: argument --distributed-world-size: conflicting option string: --distributed-world-size\r\n\r\n#### What have you tried?\r\nI have tried retraining my model in case it was an issue with how my checkpoints were stored, despite how the output always said my distributed world size is 1. I have also looked at [this similar error](https://github.com/tensorflow/tensorflow/issues/8389) to make sure that no other python processes are running.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Ubuntu 16.04.6 LTS (Xenial Xerus)\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install -e fairseq/\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: CUDA release 10.1, V10.1.243\r\n - GPU models and configuration: NVIDIA GeForce GTX 1080 Ti\r\n - Any other relevant information: Using a miniconda3 environment. There are 8 GPUs on the server that I am SSH'd into, but I am only connected to 1.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2083/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2079", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2079/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2079/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2079/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2079", "id": 609630878, "node_id": "MDU6SXNzdWU2MDk2MzA4Nzg=", "number": 2079, "title": "training transformer model with normalize-before generates extremely high ppl and a \"list assignment index out of range\" error", "user": {"login": "zixiliuUSC", "id": 49173327, "node_id": "MDQ6VXNlcjQ5MTczMzI3", "avatar_url": "https://avatars.githubusercontent.com/u/49173327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zixiliuUSC", "html_url": "https://github.com/zixiliuUSC", "followers_url": "https://api.github.com/users/zixiliuUSC/followers", "following_url": "https://api.github.com/users/zixiliuUSC/following{/other_user}", "gists_url": "https://api.github.com/users/zixiliuUSC/gists{/gist_id}", "starred_url": "https://api.github.com/users/zixiliuUSC/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zixiliuUSC/subscriptions", "organizations_url": "https://api.github.com/users/zixiliuUSC/orgs", "repos_url": "https://api.github.com/users/zixiliuUSC/repos", "events_url": "https://api.github.com/users/zixiliuUSC/events{/privacy}", "received_events_url": "https://api.github.com/users/zixiliuUSC/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-04-30T06:33:08Z", "updated_at": "2020-05-04T14:16:41Z", "closed_at": "2020-05-04T14:16:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI use fairseq transformer model to build a grammar error correction model. The dataset I use is Lang8 and Conll2014. The two datasets are prepared as translation task in which source file is sentences with gramma error and target file is corrected sentences. Different sentences is seperated by '\\n'. The two dataset is concatonated and preprocessed by subword-nmt and then binarized by fairseq-preprocess. \r\n\r\nThe problem I meet is that I train the model with or without flags, --decoder-normalize-before  --encoder-normalize-before, and get very different result and a strange error. \r\n\r\n#### Code\r\nTrain with `--decoder-normalize-before  --encoder-normalize-before`\r\n```\r\npython train.py temp/bpe/bin \\\r\n    --save-dir checkpoints/transformer --arch transformer \\\r\n    --activation-fn relu \\\r\n    --dropout 0.1 --attention-dropout 0.1 --activation-dropout 0.1 \\\r\n    --encoder-embed-dim 256 \\\r\n    --encoder-ffn-embed-dim 256 --encoder-layers 4 \\\r\n    --encoder-attention-heads 4 \\\r\n    --decoder-embed-dim 256 --decoder-ffn-embed-dim 256 \\\r\n    --decoder-layers 4 \\\r\n    --decoder-attention-heads 4 \\\r\n    --encoder-layerdrop 0.1 --decoder-layerdrop 0.1 \\\r\n    --optimizer adam --lr 0.005 --lr-shrink 0.5 \\\r\n    --max-tokens 5000 --task translation        \\\r\n    --keep-best-checkpoints 10 \\\r\n    --bpe subword_nmt \\\r\n    --update-freq 8 \\\r\n    --patience 8 \\\r\n    --no-save-optimizer-state \\\r\n    --best-checkpoint-metric ppl \\\r\n    --decoder-normalize-before --encoder-normalize-before \\\r\n    --ddp-backend no_c10d\r\n```\r\nTraining log: Since the error apeared in epoch4, I directly reran the script. And got the same error after three epoch. \r\n```\r\nbash train1.sh \r\n2020-04-29 21:50:54 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:14321\r\n2020-04-29 21:50:54 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:14321\r\n2020-04-29 21:50:55 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 1\r\n2020-04-29 21:50:55 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 0\r\n2020-04-29 21:50:57 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.1, best_checkpoint_metric='ppl', bpe='subword_nmt', bpe_codes=None, bpe_separator='@@', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='temp/bpe/bin', data_buffer_size=0, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=4, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=256, decoder_input_dim=256, decoder_layerdrop=0.1, decoder_layers=4, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:14321', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=256, encoder_layerdrop=0.1, encoder_layers=4, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=10, keep_interval_updates=-1, keep_last_epochs=-1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.005], lr_scheduler='fixed', lr_shrink=0.5, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5000, max_tokens_valid=5000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=True, no_scale_embedding=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=8, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/transformer', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[8], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\r\n2020-04-29 21:50:57 | INFO | fairseq.tasks.translation | [src] dictionary: 48947 types\r\n2020-04-29 21:50:57 | INFO | fairseq.tasks.translation | [tgt] dictionary: 48613 types\r\n2020-04-29 21:50:57 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.src\r\n2020-04-29 21:50:57 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.tgt\r\n2020-04-29 21:50:57 | INFO | fairseq.tasks.translation | temp/bpe/bin valid src-tgt 8204 examples\r\n2020-04-29 21:50:58 | INFO | fairseq_cli.train | TransformerModel(\r\n  (encoder): TransformerEncoder(\r\n    (embed_tokens): Embedding(48947, 256, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (decoder): TransformerDecoder(\r\n    (embed_tokens): Embedding(48613, 256, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n    (output_projection): Linear(in_features=256, out_features=48613, bias=False)\r\n  )\r\n)\r\n2020-04-29 21:50:58 | INFO | fairseq_cli.train | model transformer, criterion CrossEntropyCriterion\r\n2020-04-29 21:50:58 | INFO | fairseq_cli.train | num. model params: 41642240 (num. trained: 41642240)\r\n2020-04-29 21:50:58 | INFO | fairseq_cli.train | training on 2 GPUs\r\n2020-04-29 21:50:58 | INFO | fairseq_cli.train | max tokens per GPU = 5000 and max sentences per GPU = None\r\n2020-04-29 21:50:58 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/transformer/checkpoint_last.pt\r\n2020-04-29 21:50:58 | INFO | fairseq.trainer | loading train data for epoch 1\r\n2020-04-29 21:50:58 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.src\r\n2020-04-29 21:50:58 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.tgt\r\n2020-04-29 21:50:58 | INFO | fairseq.tasks.translation | temp/bpe/bin train src-tgt 1130841 examples\r\n2020-04-29 21:51:05 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\r\nepoch 001:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\r\n\tadd_(Number alpha, Tensor other)\r\nConsider using one of the following signatures instead:\r\n\tadd_(Tensor other, *, Number alpha)\r\n/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\r\n\tadd_(Number alpha, Tensor other)\r\nConsider using one of the following signatures instead:\r\n\tadd_(Tensor other, *, Number alpha)\r\nepoch 001 | valid on 'valid' subset | loss 4.067 | ppl 16.77 | wps 119770 | wpb 6793.9 | bsz 341.8 | num_updates 210                                                                        \r\nepoch 001 | valid on 'valid' subset | loss 4.067 | ppl 16.77 | wps 119685 | wpb 6793.9 | bsz 341.8 | num_updates 210                                                                        \r\nepoch 001 | loss 6.743 | ppl 107.11 | wps 59759.2 | ups 0.79 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.564 | clip 0 | train_wall 236 | wall 276                       \r\nepoch 002:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 21:55:35 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint1.pt (epoch 1 @ 210 updates, score 16.77) (writing took 0.49035206900043704 seconds)\r\nepoch 001 | loss 6.743 | ppl 107.11 | wps 59649.1 | ups 0.78 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.564 | clip 0 | train_wall 236 | wall 276                       \r\nepoch 002 | valid on 'valid' subset | loss 1.464 | ppl 2.76 | wps 121245 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 2.76                                                         \r\nepoch 002 | valid on 'valid' subset | loss 1.464 | ppl 2.76 | wps 118177 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 2.76                                                         \r\nepoch 002 | loss 1.986 | ppl 3.96 | wps 57927.6 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.262 | clip 0 | train_wall 243 | wall 551                         \r\nepoch 003:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:00:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint2.pt (epoch 2 @ 420 updates, score 2.76) (writing took 3.3108816809999553 seconds)\r\nepoch 002 | loss 1.986 | ppl 3.96 | wps 57341.3 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.262 | clip 0 | train_wall 243 | wall 555                         \r\nepoch 003 | valid on 'valid' subset | loss 1.184 | ppl 2.27 | wps 120108 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 2.27                                                         \r\nepoch 003 | valid on 'valid' subset | loss 1.184 | ppl 2.27 | wps 117797 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 2.27                                                         \r\nepoch 003 | loss 1.224 | ppl 2.34 | wps 57350.6 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.137 | clip 0 | train_wall 246 | wall 830                         \r\nepoch 004:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:04:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint3.pt (epoch 3 @ 630 updates, score 2.27) (writing took 3.109906989000592 seconds)\r\nepoch 003 | loss 1.224 | ppl 2.34 | wps 57392.5 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.137 | clip 0 | train_wall 243 | wall 833                         \r\nTraceback (most recent call last):                                                                                                                                                          \r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 355, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 324, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 117, in main\r\n    valid_losses = train(args, trainer, task, epoch_itr, max_update)\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 187, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/trainer.py\", line 379, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/tasks/fairseq_task.py\", line 341, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/criterions/cross_entropy.py\", line 29, in forward\r\n    net_output = model(**sample['net_input'])\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/legacy_distributed_data_parallel.py\", line 86, in forward\r\n    return self.module(*inputs, **kwargs)\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/models/transformer.py\", line 272, in forward\r\n    return_all_hiddens=return_all_hiddens,\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/models/transformer.py\", line 498, in forward\r\n    encoder_states[-1] = x\r\nIndexError: list assignment index out of range\r\n```\r\n```\r\n$ bash train1.sh \r\n2020-04-29 22:29:25 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:19129\r\n2020-04-29 22:29:25 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:19129\r\n2020-04-29 22:29:26 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 1\r\n2020-04-29 22:29:26 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 0\r\n2020-04-29 22:29:29 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.1, best_checkpoint_metric='ppl', bpe='subword_nmt', bpe_codes=None, bpe_separator='@@', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='temp/bpe/bin', data_buffer_size=0, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=4, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=256, decoder_input_dim=256, decoder_layerdrop=0.1, decoder_layers=4, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:19129', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=256, encoder_layerdrop=0.1, encoder_layers=4, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=10, keep_interval_updates=-1, keep_last_epochs=-1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.005], lr_scheduler='fixed', lr_shrink=0.5, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5000, max_tokens_valid=5000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=True, no_scale_embedding=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=8, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/transformer', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[8], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\r\n2020-04-29 22:29:29 | INFO | fairseq.tasks.translation | [src] dictionary: 48947 types\r\n2020-04-29 22:29:29 | INFO | fairseq.tasks.translation | [tgt] dictionary: 48613 types\r\n2020-04-29 22:29:29 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.src\r\n2020-04-29 22:29:29 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.tgt\r\n2020-04-29 22:29:29 | INFO | fairseq.tasks.translation | temp/bpe/bin valid src-tgt 8204 examples\r\n2020-04-29 22:29:29 | INFO | fairseq_cli.train | TransformerModel(\r\n  (encoder): TransformerEncoder(\r\n    (embed_tokens): Embedding(48947, 256, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (decoder): TransformerDecoder(\r\n    (embed_tokens): Embedding(48613, 256, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n    (output_projection): Linear(in_features=256, out_features=48613, bias=False)\r\n  )\r\n)\r\n2020-04-29 22:29:29 | INFO | fairseq_cli.train | model transformer, criterion CrossEntropyCriterion\r\n2020-04-29 22:29:29 | INFO | fairseq_cli.train | num. model params: 41642240 (num. trained: 41642240)\r\n2020-04-29 22:29:29 | INFO | fairseq_cli.train | training on 2 GPUs\r\n2020-04-29 22:29:29 | INFO | fairseq_cli.train | max tokens per GPU = 5000 and max sentences per GPU = None\r\n2020-04-29 22:29:29 | INFO | fairseq.trainer | loaded checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 3 @ 0 updates)\r\n2020-04-29 22:29:29 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\r\n2020-04-29 22:29:29 | INFO | fairseq.trainer | loading train data for epoch 3\r\n2020-04-29 22:29:29 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.src\r\n2020-04-29 22:29:30 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.tgt\r\n2020-04-29 22:29:30 | INFO | fairseq.tasks.translation | temp/bpe/bin train src-tgt 1130841 examples\r\nepoch 004:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\r\n\tadd_(Number alpha, Tensor other)\r\nConsider using one of the following signatures instead:\r\n\tadd_(Tensor other, *, Number alpha)\r\n/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\r\n\tadd_(Number alpha, Tensor other)\r\nConsider using one of the following signatures instead:\r\n\tadd_(Tensor other, *, Number alpha)\r\nepoch 004 | valid on 'valid' subset | loss 1.075 | ppl 2.11 | wps 121131 | wpb 6793.9 | bsz 341.8 | num_updates 210 | best_ppl 2.11                                                         \r\nepoch 004 | valid on 'valid' subset | loss 1.075 | ppl 2.11 | wps 121005 | wpb 6793.9 | bsz 341.8 | num_updates 210 | best_ppl 2.11                                                         \r\nepoch 004 | loss 1.182 | ppl 2.27 | wps 57897.8 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.138 | clip 0 | train_wall 479 | wall 0                           \r\nepoch 005:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:34:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint4.pt (epoch 4 @ 210 updates, score 2.11) (writing took 3.373271124999519 seconds)\r\nepoch 004 | loss 1.182 | ppl 2.27 | wps 57536.8 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.138 | clip 0 | train_wall 479 | wall 0                           \r\nepoch 005 | valid on 'valid' subset | loss 0.997 | ppl 2 | wps 121011 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 2                                                               \r\nepoch 005 | loss 1.005 | ppl 2.01 | wps 57402.3 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.1 | clip 0 | train_wall 245 | wall 0                             \r\nepoch 005 | valid on 'valid' subset | loss 0.997 | ppl 2 | wps 117614 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 2                                                               \r\nepoch 005: 100%|\u2589| 209/210 [04:34<00:01,  1.33s/it, loss=0.99, ppl=1.99, wps=58585.5, ups=0.77, wpb=76273.2, bsz=5474.4, num_updates=400, lr=0.005, gnorm=0.098, clip=0, train_wall=116, wal2020-04-29 22:38:47 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint5.pt (epoch 5 @ 420 updates, score 2.0) (writing took 3.413826895999591 seconds)\r\nepoch 005 | loss 1.005 | ppl 2.01 | wps 57391.1 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.1 | clip 0 | train_wall 242 | wall 0                             \r\nepoch 006 | valid on 'valid' subset | loss 0.963 | ppl 1.95 | wps 120335 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 1.95                                                         \r\n                                                                                                                                                                                           epoch 006 | loss 0.954 | ppl 1.94 | wps 56921.8 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.089 | clip 0 | train_wall 248 | wall 0                            \r\nepoch 006 | valid on 'valid' subset | loss 0.963 | ppl 1.95 | wps 117267 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 1.95                                                         \r\nepoch 007:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:43:28 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint6.pt (epoch 6 @ 630 updates, score 1.95) (writing took 3.180665442998361 seconds)\r\nepoch 006 | loss 0.954 | ppl 1.94 | wps 56971.1 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.089 | clip 0 | train_wall 244 | wall 0                           \r\nTraceback (most recent call last):                                                                                                                                                          \r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 355, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 324, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 117, in main\r\n    valid_losses = train(args, trainer, task, epoch_itr, max_update)\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 187, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/trainer.py\", line 379, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/tasks/fairseq_task.py\", line 341, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/criterions/cross_entropy.py\", line 29, in forward\r\n    net_output = model(**sample['net_input'])\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/legacy_distributed_data_parallel.py\", line 86, in forward\r\n    return self.module(*inputs, **kwargs)\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/models/transformer.py\", line 272, in forward\r\n    return_all_hiddens=return_all_hiddens,\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/zixi/EE-599/fairseq/fairseq/models/transformer.py\", line 498, in forward\r\n    encoder_states[-1] = x\r\nIndexError: list assignment index out of range\r\n\r\n```\r\nTrain without these two flags. The error doesn't raise anymore, but the ppl in this training is extremely bad. It seems the model learns nothing. \r\nTraining script:\r\n```\r\npython train.py temp/bpe/bin \\\r\n    --save-dir checkpoints/transformer1 --arch transformer \\\r\n    --activation-fn relu \\\r\n    --dropout 0.1 --attention-dropout 0.1 --activation-dropout 0.1 \\\r\n    --encoder-embed-dim 256 \\\r\n    --encoder-ffn-embed-dim 256 --encoder-layers 4 \\\r\n    --encoder-attention-heads 4 \\\r\n    --decoder-embed-dim 256 --decoder-ffn-embed-dim 256 \\\r\n    --decoder-layers 4 \\\r\n    --decoder-attention-heads 4 \\\r\n    --encoder-layerdrop 0.1 --decoder-layerdrop 0.1 \\\r\n    --optimizer adam --lr 0.005 --lr-shrink 0.5 \\\r\n    --max-tokens 5000 --task translation        \\\r\n    --keep-best-checkpoints 10 \\\r\n    --bpe subword_nmt \\\r\n    --update-freq 8 \\\r\n    --patience 8 \\\r\n    --no-save-optimizer-state \\\r\n    --best-checkpoint-metric ppl \\\r\n    --ddp-backend no_c10d\r\n\r\n```\r\n```\r\nbash train.sh\r\n2020-04-29 22:53:37 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:10163\r\n2020-04-29 22:53:37 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:10163\r\n2020-04-29 22:53:37 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 1\r\n2020-04-29 22:53:37 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 0\r\n2020-04-29 22:53:40 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.1, best_checkpoint_metric='ppl', bpe='subword_nmt', bpe_codes=None, bpe_separator='@@', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='temp/bpe/bin', data_buffer_size=0, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=4, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=256, decoder_input_dim=256, decoder_layerdrop=0.1, decoder_layers=4, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:10163', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=256, encoder_layerdrop=0.1, encoder_layers=4, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=10, keep_interval_updates=-1, keep_last_epochs=-1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.005], lr_scheduler='fixed', lr_shrink=0.5, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5000, max_tokens_valid=5000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=True, no_scale_embedding=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=8, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/transformer1', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[8], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\r\n2020-04-29 22:53:40 | INFO | fairseq.tasks.translation | [src] dictionary: 48947 types\r\n2020-04-29 22:53:40 | INFO | fairseq.tasks.translation | [tgt] dictionary: 48613 types\r\n2020-04-29 22:53:40 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.src\r\n2020-04-29 22:53:40 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.tgt\r\n2020-04-29 22:53:40 | INFO | fairseq.tasks.translation | temp/bpe/bin valid src-tgt 8204 examples\r\n2020-04-29 22:53:40 | INFO | fairseq_cli.train | TransformerModel(\r\n  (encoder): TransformerEncoder(\r\n    (embed_tokens): Embedding(48947, 256, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n  )\r\n  (decoder): TransformerDecoder(\r\n    (embed_tokens): Embedding(48613, 256, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\r\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=256, out_features=256, bias=True)\r\n        (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n    (output_projection): Linear(in_features=256, out_features=48613, bias=False)\r\n  )\r\n)\r\n2020-04-29 22:53:40 | INFO | fairseq_cli.train | model transformer, criterion CrossEntropyCriterion\r\n2020-04-29 22:53:40 | INFO | fairseq_cli.train | num. model params: 41641216 (num. trained: 41641216)\r\n2020-04-29 22:53:41 | INFO | fairseq_cli.train | training on 2 GPUs\r\n2020-04-29 22:53:41 | INFO | fairseq_cli.train | max tokens per GPU = 5000 and max sentences per GPU = None\r\n2020-04-29 22:53:41 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/transformer1/checkpoint_last.pt\r\n2020-04-29 22:53:41 | INFO | fairseq.trainer | loading train data for epoch 1\r\n2020-04-29 22:53:41 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.src\r\n2020-04-29 22:53:41 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.tgt\r\n2020-04-29 22:53:41 | INFO | fairseq.tasks.translation | temp/bpe/bin train src-tgt 1130841 examples\r\nepoch 001:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:53:48 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\r\nepoch 001:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\r\n\tadd_(Number alpha, Tensor other)\r\nConsider using one of the following signatures instead:\r\n\tadd_(Tensor other, *, Number alpha)\r\n/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\r\n\tadd_(Number alpha, Tensor other)\r\nConsider using one of the following signatures instead:\r\n\tadd_(Tensor other, *, Number alpha)\r\nepoch 001 | valid on 'valid' subset | loss 9.883 | ppl 944.34 | wps 123133 | wpb 6793.9 | bsz 341.8 | num_updates 210                                                                       \r\nepoch 001 | valid on 'valid' subset | loss 9.883 | ppl 944.34 | wps 120444 | wpb 6793.9 | bsz 341.8 | num_updates 210                                                                       \r\nepoch 001 | loss 9.305 | ppl 632.7 | wps 60234.8 | ups 0.79 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.414 | clip 0 | train_wall 234 | wall 274                        \r\nepoch 002:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:58:15 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer1/checkpoint1.pt (epoch 1 @ 210 updates, score 944.34) (writing took 0.5157724929995311 seconds)\r\nepoch 001 | loss 9.305 | ppl 632.7 | wps 60118.6 | ups 0.79 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.414 | clip 0 | train_wall 234 | wall 275                        \r\nepoch 002:  44%|\u258d| 92/210 [01:57<02:29,  1.27s/it, loss=9.176, ppl=578.29, wps=57866.5, ups=0.76, wpb=75663.3, bsz=5346.4, num_updates=300, lr=0.005, gnorm=0.23, clip=0, train_wall=113, was=0.76, wpb=75663.3, bsz=5346.4, num_updates=300, lr=0.005, gnorm=0.23, clip=0, train_wallepoch 002:  44%|\u258d| 92/210 [01:58<02:29,  1.27s/it, loss=9.176, ppl=578.29, wps=57866.3, ups=0.76, wpb=75663.3, bsz=5346.4, num_updates=300, lr=0.005, gnorm=0.23, clip=0, train_wallepoch 002 | valid on 'valid' subset | loss 11.463 | ppl 2822.93 | wps 121199 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 944.34                                                   \r\nepoch 002 | valid on 'valid' subset | loss 11.463 | ppl 2822.93 | wps 120872 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 944.34                                                   \r\nepoch 002 | loss 9.112 | ppl 553.22 | wps 58548.5 | ups 0.77 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.256 | clip 0 | train_wall 241 | wall 547                       \r\nepoch 003:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 23:02:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer1/checkpoint2.pt (epoch 2 @ 420 updates, score 2822.93) (writing took 1.710711199000798 seconds)\r\nepoch 002 | loss 9.112 | ppl 553.22 | wps 58292.6 | ups 0.77 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.256 | clip 0 | train_wall 239 | wall 548                       \r\nepoch 003 | valid on 'valid' subset | loss 11.745 | ppl 3433.39 | wps 118206 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 944.34                                                   \r\nepoch 003 | valid on 'valid' subset | loss 11.745 | ppl 3433.39 | wps 119624 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 944.34                                                   \r\nepoch 003 | loss 8.952 | ppl 495.12 | wps 57402.9 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.279 | clip 0 | train_wall 245 | wall 825                       \r\nepoch 004:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 23:07:28 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer1/checkpoint3.pt (epoch 3 @ 630 updates, score 3433.39) (writing took 2.4265237050021824 seconds)\r\nepoch 003 | loss 8.952 | ppl 495.12 | wps 57254.1 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.279 | clip 0 | train_wall 244 | wall 827                       \r\nepoch 004 | valid on 'valid' subset | loss 11.445 | ppl 2787.56 | wps 117655 | wpb 6793.9 | bsz 341.8 | num_updates 840 | best_ppl 944.34                                                   \r\nepoch 004 | valid on 'valid' subset | loss 11.445 | ppl 2787.56 | wps 121146 | wpb 6793.9 | bsz 341.8 | num_updates 840 | best_ppl 944.34\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:02<00:00, 11.55it/s]\r\nepoch 004 | loss 8.884 | ppl 472.4 | wps 57052 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 840 | lr 0.005 | gnorm 0.256 | clip 0 | train_wall 248 | wall 1104                         \r\nepoch 005:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 23:12:07 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer1/checkpoint4.pt (epoch 4 @ 840 updates, score 2787.56) (writing took 1.7434893480021856 seconds)\r\nepoch 004 | loss 8.884 | ppl 472.4 | wps 57191.2 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 840 | lr 0.005 | gnorm 0.256 | clip 0 | train_wall 245 | wall 1106                       \r\nepoch 005:  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                           | 23/210 [00:32<04:04,  1.31s/it]^CTraceback (most recent call last):\r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 355, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 78, in join\r\n    timeout=timeout,\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\r\n    ready = selector.select(timeout)\r\n  File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/selectors.py\", line 415, in select\r\n    fd_event_list = self._selector.poll(timeout)\r\nKeyboardInterrupt\r\n\r\n```\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nI check my data again and I think there is nothing wrong with the data. As for the bugs, I have no idea. \r\n#### What's your environment?\r\n\r\n - fairseq Version : master, I install fairseq using git clone and pip install --editable .\r\n - PyTorch Version: 1.5\r\n - OS (e.g., Linux): Ubuntu 18.04 LTS\r\n - How you installed fairseq: pip install --editable .\r\n - Build command you used (if compiling from source): pip install --editable .\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: RTX-2070 and RTX-2060s, I use two cards to train the model. \r\n - Any other relevant information: none\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2079/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2075", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2075/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2075/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2075/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2075", "id": 609462387, "node_id": "MDU6SXNzdWU2MDk0NjIzODc=", "number": 2075, "title": "fairseq.modules.quantization is not a module", "user": {"login": "erip", "id": 2348806, "node_id": "MDQ6VXNlcjIzNDg4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/2348806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erip", "html_url": "https://github.com/erip", "followers_url": "https://api.github.com/users/erip/followers", "following_url": "https://api.github.com/users/erip/following{/other_user}", "gists_url": "https://api.github.com/users/erip/gists{/gist_id}", "starred_url": "https://api.github.com/users/erip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erip/subscriptions", "organizations_url": "https://api.github.com/users/erip/orgs", "repos_url": "https://api.github.com/users/erip/repos", "events_url": "https://api.github.com/users/erip/events{/privacy}", "received_events_url": "https://api.github.com/users/erip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-04-30T00:27:00Z", "updated_at": "2020-04-30T20:17:45Z", "closed_at": "2020-04-30T20:17:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nLoading a pretrained model currently fails because `quantization_utils` is imported which triggers the import of a non-module.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Install fairseq \r\n2. Try to load a pretrained model.\r\n\r\n#### Code sample\r\n\r\nA simpler reproducer is below:\r\n\r\n```\r\n$ python -c \"from fairseq.modules.quantization import pq, quantization_options, scalar\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'fairseq.modules.quantization'\r\n```\r\n\r\n### Expected behavior\r\n\r\nThis should ... work. \ud83d\ude04 \r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.5.0\r\n - OS (e.g., Linux): OS X\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `CFLAGS='-stdlib=libc++' pip install git+https://github.com/pytorch/fairseq.git`\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2075/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2075/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2068", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2068/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2068/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2068/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2068", "id": 607247192, "node_id": "MDU6SXNzdWU2MDcyNDcxOTI=", "number": 2068, "title": "Issue loading BART-large model finetuned on CNN-DM dataset", "user": {"login": "rmovva", "id": 17256372, "node_id": "MDQ6VXNlcjE3MjU2Mzcy", "avatar_url": "https://avatars.githubusercontent.com/u/17256372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmovva", "html_url": "https://github.com/rmovva", "followers_url": "https://api.github.com/users/rmovva/followers", "following_url": "https://api.github.com/users/rmovva/following{/other_user}", "gists_url": "https://api.github.com/users/rmovva/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmovva/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmovva/subscriptions", "organizations_url": "https://api.github.com/users/rmovva/orgs", "repos_url": "https://api.github.com/users/rmovva/repos", "events_url": "https://api.github.com/users/rmovva/events{/privacy}", "received_events_url": "https://api.github.com/users/rmovva/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-04-27T05:31:19Z", "updated_at": "2020-05-04T16:23:49Z", "closed_at": "2020-05-04T16:23:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to load bart.large.cnn finetuned weights according to the README:\r\n\r\n`bart = torch.hub.load('pytorch/fairseq', 'bart.large.cnn')`\r\n\r\nBut I'm getting the following error that indicates an off-by-one shape mismatch between the trained weights and the weights that the model architecture is expecting:\r\n\r\n`RuntimeError: Error(s) in loading state_dict for BARTModel:\r\n\tsize mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([50265, 1024]) from checkpoint, the shape in current model is torch.Size([50264, 1024]).`\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version 1.5\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 10.2\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2068/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2068/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2062", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2062/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2062/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2062/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2062", "id": 606788464, "node_id": "MDU6SXNzdWU2MDY3ODg0NjQ=", "number": 2062, "title": "Wired issue when I test a binary classification task by using RoBERTa-large", "user": {"login": "14H034160212", "id": 23516191, "node_id": "MDQ6VXNlcjIzNTE2MTkx", "avatar_url": "https://avatars.githubusercontent.com/u/23516191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/14H034160212", "html_url": "https://github.com/14H034160212", "followers_url": "https://api.github.com/users/14H034160212/followers", "following_url": "https://api.github.com/users/14H034160212/following{/other_user}", "gists_url": "https://api.github.com/users/14H034160212/gists{/gist_id}", "starred_url": "https://api.github.com/users/14H034160212/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/14H034160212/subscriptions", "organizations_url": "https://api.github.com/users/14H034160212/orgs", "repos_url": "https://api.github.com/users/14H034160212/repos", "events_url": "https://api.github.com/users/14H034160212/events{/privacy}", "received_events_url": "https://api.github.com/users/14H034160212/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-04-25T15:06:37Z", "updated_at": "2020-05-01T11:13:36Z", "closed_at": "2020-05-01T11:13:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nHi, I use RoBERTa-large as pre-trained model and use RACE as intermediate training dataset, then I use a binary classification task dataset as the fine-tuning downstream task. The accuracy for the binary classification task on the train and dev dataset is quite high which is nearly 0.99. But in the test dataset which is nearly 0.01 which is quite wired. I have double check the test script which is fine. I use 1 to represent the True label and use 0 to represent False label. So, It will be great if any of you can make any comment. Thanks a lot.\r\n\r\nHere is the data format script\r\n```\r\nimport argparse\r\nimport os\r\nimport random\r\nfrom glob import glob\r\nimport json_lines\r\nimport re\r\n\r\nrandom.seed(0)\r\n\r\ndef main(args):\r\n    for split in ['train', 'dev', 'test']:\r\n        samples = []\r\n        cur_dir = os.path.join(args.datadir, split)\r\n        for filename in os.listdir(cur_dir):\r\n            cur_path = os.path.join(cur_dir, filename)\r\n            with open(cur_path, 'r') as f:\r\n                for item in json_lines.reader(f):\r\n                    questions = item[\"questions\"]\r\n                    context = item[\"context\"].replace(\"\\n\", \" \")\r\n                    context = re.sub(r'\\s+', ' ', context)\r\n                    for i in range(len(questions)):\r\n                        text = questions[i][\"text\"]\r\n                        label = questions[i][\"label\"]\r\n                        if label == True:\r\n                            label_num = 1\r\n                        else:\r\n                            label_num = 0\r\n                        text = re.sub(r'\\s+', ' ', text)\r\n                        context_text = context + ' ' + text\r\n                        samples.append((context_text, label_num))\r\n\r\n        random.shuffle(samples)\r\n        out_fname = 'train' if split == 'train' else 'dev'\r\n        f1 = open(os.path.join(args.datadir, out_fname + '.input0'), 'w')\r\n        f2 = open(os.path.join(args.datadir, out_fname + '.label'), 'w')\r\n        for sample in samples:\r\n            f1.write(sample[0] + '\\n')\r\n            f2.write(str(sample[1]) + '\\n')\r\n        f1.close()\r\n        f2.close()\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--datadir', default='fairseq-master/depth-5')\r\n    args = parser.parse_args()\r\n    main(args)\r\n```\r\n\r\nHere is the test script\r\n```\r\nfrom fairseq.models.roberta import RobertaModel\r\nimport json_lines\r\nimport re\r\n\r\nroberta = RobertaModel.from_pretrained(\r\n    'checkpoints/',\r\n    checkpoint_file='checkpoint_best.pt',\r\n    data_name_or_path='PARARULE-bin'\r\n)\r\n\r\nncorrect, nsamples = 0, 0\r\nroberta.cuda(2)\r\nroberta.eval()\r\nwith open('test/test.jsonl', 'r') as f:\r\n    for item in json_lines.reader(f):\r\n        questions = item[\"questions\"]\r\n        context = item[\"context\"].replace(\"\\n\", \" \")\r\n        context = re.sub(r'\\s+', ' ', context)\r\n        for i in range(len(questions)):\r\n            text = questions[i][\"text\"]\r\n            label = questions[i][\"label\"]\r\n            if label == True:\r\n                label_num = 1\r\n            else:\r\n                label_num = 0\r\n            text = re.sub(r'\\s+', ' ', text)\r\n            context_text = context + ' ' + text\r\n            tokens = roberta.encode(context_text)\r\n            prediction = roberta.predict('PARARULE_head', tokens).argmax().item()\r\n            ncorrect += int(prediction == label_num)\r\n            nsamples += 1\r\nprint('| Accuracy: ', float(ncorrect) / float(nsamples))\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2062/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2062/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2045", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2045/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2045/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2045/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2045", "id": 604841133, "node_id": "MDU6SXNzdWU2MDQ4NDExMzM=", "number": 2045, "title": "'multilingual_denoising' task not supported", "user": {"login": "mcggood", "id": 32761481, "node_id": "MDQ6VXNlcjMyNzYxNDgx", "avatar_url": "https://avatars.githubusercontent.com/u/32761481?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcggood", "html_url": "https://github.com/mcggood", "followers_url": "https://api.github.com/users/mcggood/followers", "following_url": "https://api.github.com/users/mcggood/following{/other_user}", "gists_url": "https://api.github.com/users/mcggood/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcggood/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcggood/subscriptions", "organizations_url": "https://api.github.com/users/mcggood/orgs", "repos_url": "https://api.github.com/users/mcggood/repos", "events_url": "https://api.github.com/users/mcggood/events{/privacy}", "received_events_url": "https://api.github.com/users/mcggood/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-22T15:17:12Z", "updated_at": "2020-04-23T06:33:48Z", "closed_at": "2020-04-23T06:33:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen I load bart large model. (multi-lang-bart in fact)\r\nbart = BARTModel.from_pretrained('PATH', checkpoint_file='model.pt')\r\nI got this error\r\nKeyError: 'multilingual_denoising'\r\n\r\nWhile in the model file. in args, task='multilingual_denoising'. this task seems not supported by fairseq.\r\n### To Reproduce\r\n\r\nbart = BARTModel.from_pretrained('PATH', checkpoint_file='model.pt')\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2045/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2045/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2044", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2044/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2044/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2044/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2044", "id": 604651330, "node_id": "MDU6SXNzdWU2MDQ2NTEzMzA=", "number": 2044, "title": "assert torch.all(all_layers[-1] == last_layer_features)", "user": {"login": "HAWLYQ", "id": 12978686, "node_id": "MDQ6VXNlcjEyOTc4Njg2", "avatar_url": "https://avatars.githubusercontent.com/u/12978686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HAWLYQ", "html_url": "https://github.com/HAWLYQ", "followers_url": "https://api.github.com/users/HAWLYQ/followers", "following_url": "https://api.github.com/users/HAWLYQ/following{/other_user}", "gists_url": "https://api.github.com/users/HAWLYQ/gists{/gist_id}", "starred_url": "https://api.github.com/users/HAWLYQ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HAWLYQ/subscriptions", "organizations_url": "https://api.github.com/users/HAWLYQ/orgs", "repos_url": "https://api.github.com/users/HAWLYQ/repos", "events_url": "https://api.github.com/users/HAWLYQ/events{/privacy}", "received_events_url": "https://api.github.com/users/HAWLYQ/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-04-22T10:40:46Z", "updated_at": "2020-05-13T14:53:25Z", "closed_at": "2020-05-13T14:53:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHi, @myleott , I meet an AssertionError when I run the demo code to extract features from RoBERTa. It shows the last layer features is not the same as 'all_layers[-1]'. \r\n#### Code\r\nimport torch\r\nroberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\r\ntokens = roberta.encode('Hello world!')\r\nlast_layer_features = roberta.extract_features(tokens)\r\nall_layers = roberta.extract_features(tokens, return_all_hiddens=True)\r\nassert torch.all(all_layers[-1] == last_layer_features)\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nThere is nothing wrong with the size of last_layer_features and the length of all_layers.\r\nThe outputs of last_layers_features and all_layers[-1] are shown below:\r\n\r\n![image](https://user-images.githubusercontent.com/12978686/79972567-bbf5df80-84c8-11ea-8706-0622532fa8fb.png)\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):0.9.0\r\n - PyTorch Version (e.g., 1.0) 1.4.0\r\n - OS (e.g., Linux):linux\r\n - How you installed fairseq (`pip`, source):pip\r\n - Build command you used (if compiling from source):\r\n - Python version:3.6\r\n - CUDA/cuDNN version:10.0\r\n - GPU models and configuration:-\r\n - Any other relevant information:\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2044/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2044/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2042", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2042/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2042/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2042/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2042", "id": 604549069, "node_id": "MDU6SXNzdWU2MDQ1NDkwNjk=", "number": 2042, "title": "Masked LM training with BERT fails ", "user": {"login": "mukhal", "id": 5109053, "node_id": "MDQ6VXNlcjUxMDkwNTM=", "avatar_url": "https://avatars.githubusercontent.com/u/5109053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mukhal", "html_url": "https://github.com/mukhal", "followers_url": "https://api.github.com/users/mukhal/followers", "following_url": "https://api.github.com/users/mukhal/following{/other_user}", "gists_url": "https://api.github.com/users/mukhal/gists{/gist_id}", "starred_url": "https://api.github.com/users/mukhal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mukhal/subscriptions", "organizations_url": "https://api.github.com/users/mukhal/orgs", "repos_url": "https://api.github.com/users/mukhal/repos", "events_url": "https://api.github.com/users/mukhal/events{/privacy}", "received_events_url": "https://api.github.com/users/mukhal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-22T08:11:49Z", "updated_at": "2020-05-01T11:05:58Z", "closed_at": "2020-05-01T11:05:58Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "\r\nTraining with a masked lm objective as follows:\r\n```bash\r\nfairseq-train $DATA_DIR \\\r\n\t\t--task masked_lm --criterion masked_lm\\\r\n\t\t--arch bert_base \\\r\n\t\t--optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 1.0 \\\r\n\t\t--lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\r\n\t\t--dropout 0.1 --weight-decay 0.01 \\\r\n\t\t--max-tokens $TOKENS_PER_SAMPLE --update-freq $UPDATE_FREQ \\\r\n\t\t--max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --save-interval-updates 1000\\\r\n\t\t--valid-subset valid \\\r\n\t\t--mask-prob 0.25\\\r\n\t\t--random-token-prob 0.2\\\r\n\t\t--skip-invalid-size-inputs-valid-test \\\r\n```\r\n\r\nGives me the following exception:\r\n```\r\nFile \"/home/mkhalifa/py36/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/home/mkhalifa/fairseq/fairseq_cli/train.py\", line 321, in cli_main\r\n    main(args)\r\n  File \"/home/mkhalifa/fairseq/fairseq_cli/train.py\", line 96, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/mkhalifa/fairseq/fairseq_cli/train.py\", line 176, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/mkhalifa/fairseq/fairseq/trainer.py\", line 319, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/home/mkhalifa/fairseq/fairseq/tasks/fairseq_task.py\", line 337, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/home/mkhalifa/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/mkhalifa/fairseq/fairseq/criterions/masked_lm.py\", line 52, in forward\r\n    ignore_index=self.padding_idx,\r\n  File \"/home/mkhalifa/py36/lib/python3.6/site-packages/torch/nn/functional.py\", line 1836, in nll_loss\r\n    .format(input.size(0), target.size(0)))\r\nValueError: Expected input batch_size (892) to match target batch_size (223).\r\n\r\n```\r\n\r\nThe same error occurs when using other `masked_lm` architectures such as xlm_base. Roberta architectures work fine.\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2042/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2031", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2031/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2031/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2031/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2031", "id": 603137271, "node_id": "MDU6SXNzdWU2MDMxMzcyNzE=", "number": 2031, "title": "Runtime error Missing key(s) in state_dict: \"decoder.output_projection.weight\".  when loading NMT models from torch.hub", "user": {"login": "akhileshgotmare", "id": 22371805, "node_id": "MDQ6VXNlcjIyMzcxODA1", "avatar_url": "https://avatars.githubusercontent.com/u/22371805?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akhileshgotmare", "html_url": "https://github.com/akhileshgotmare", "followers_url": "https://api.github.com/users/akhileshgotmare/followers", "following_url": "https://api.github.com/users/akhileshgotmare/following{/other_user}", "gists_url": "https://api.github.com/users/akhileshgotmare/gists{/gist_id}", "starred_url": "https://api.github.com/users/akhileshgotmare/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akhileshgotmare/subscriptions", "organizations_url": "https://api.github.com/users/akhileshgotmare/orgs", "repos_url": "https://api.github.com/users/akhileshgotmare/repos", "events_url": "https://api.github.com/users/akhileshgotmare/events{/privacy}", "received_events_url": "https://api.github.com/users/akhileshgotmare/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2020-04-20T10:44:30Z", "updated_at": "2020-07-08T18:05:39Z", "closed_at": "2020-04-22T02:54:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Getting this error when trying to load Transformer NMT models from torch.hub\r\nHave tried running it on Google Colab shared at https://pytorch.org/hub/pytorch_fairseq_translation/ - (https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/pytorch_fairseq_translation.ipynb) and also with pytorch latest docker image. \r\n\r\n```\r\npip install fastBPE regex requests sacremoses subword_nmt\r\nimport torch\r\n\r\n# Load an En-Fr Transformer model trained on WMT'14 data :\r\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\r\n```\r\n\r\n\r\n```\r\nDownloading: \"https://github.com/pytorch/fairseq/archive/master.zip\" to /root/.cache/torch/hub/master.zip\r\n\r\nNo CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\r\nrunning build_ext\r\ncythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\r\ncythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\r\nbuilding 'fairseq.libbleu' extension\r\ncreating build\r\ncreating build/temp.linux-x86_64-3.6\r\ncreating build/temp.linux-x86_64-3.6/fairseq\r\ncreating build/temp.linux-x86_64-3.6/fairseq/clib\r\ncreating build/temp.linux-x86_64-3.6/fairseq/clib/libbleu\r\nx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\r\nx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\r\ncreating build/lib.linux-x86_64-3.6\r\ncreating build/lib.linux-x86_64-3.6/fairseq\r\nx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so\r\nbuilding 'fairseq.data.data_utils_fast' extension\r\ncreating build/temp.linux-x86_64-3.6/fairseq/data\r\nx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\r\ncreating build/lib.linux-x86_64-3.6/fairseq/data\r\nx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -o build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so\r\nbuilding 'fairseq.data.token_block_utils_fast' extension\r\nx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\r\nx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -o build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so\r\nbuilding 'fairseq.libnat' extension\r\ncreating build/temp.linux-x86_64-3.6/fairseq/clib/libnat\r\nx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\nx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -o build/lib.linux-x86_64-3.6/fairseq/libnat.cpython-36m-x86_64-linux-gnu.so\r\ncopying build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so -> fairseq\r\ncopying build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so -> fairseq/data\r\ncopying build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so -> fairseq/data\r\ncopying build/lib.linux-x86_64-3.6/fairseq/libnat.cpython-36m-x86_64-linux-gnu.so -> fairseq\r\n\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2316140317/2316140317 [01:19<00:00, 29277581.40B/s]\r\n\r\n---------------------------------------------------------------------------\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n\r\n<ipython-input-2-b7fac14eceab> in <module>()\r\n      2 \r\n      3 # Load an En-Fr Transformer model trained on WMT'14 data :\r\n----> 4 en2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\r\n\r\n5 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict)\r\n    828         if len(error_msgs) > 0:\r\n    829             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\n--> 830                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\n    831         return _IncompatibleKeys(missing_keys, unexpected_keys)\r\n    832 \r\n\r\nRuntimeError: Error(s) in loading state_dict for TransformerModel:\r\n\tMissing key(s) in state_dict: \"decoder.output_projection.weight\". \r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2031/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2031/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2027", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2027/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2027/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2027/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2027", "id": 602732379, "node_id": "MDU6SXNzdWU2MDI3MzIzNzk=", "number": 2027, "title": "max_positions with tuple of size > 2 returns a tuple of size 2", "user": {"login": "mgaido91", "id": 8821783, "node_id": "MDQ6VXNlcjg4MjE3ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/8821783?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mgaido91", "html_url": "https://github.com/mgaido91", "followers_url": "https://api.github.com/users/mgaido91/followers", "following_url": "https://api.github.com/users/mgaido91/following{/other_user}", "gists_url": "https://api.github.com/users/mgaido91/gists{/gist_id}", "starred_url": "https://api.github.com/users/mgaido91/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mgaido91/subscriptions", "organizations_url": "https://api.github.com/users/mgaido91/orgs", "repos_url": "https://api.github.com/users/mgaido91/repos", "events_url": "https://api.github.com/users/mgaido91/events{/privacy}", "received_events_url": "https://api.github.com/users/mgaido91/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-19T14:42:49Z", "updated_at": "2020-04-21T13:01:23Z", "closed_at": "2020-04-21T13:01:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThis problem appears adding a custom task in which the `max_positions` is a tuple of size 3 (or more).  In this case, currently fairseq ignores all elements after the 2nd in the tuple, when resolving the actual max_positions.\r\n\r\nThe cause is the implementation of `_match_types`, which assumes tuples to have size 2.\r\n\r\n### To Reproduce\r\n\r\nAdd a custom task with `max_postions` returning a tuple of 3 and check that the actual max positions used: it is a tuple of 2.\r\n\r\n#### Code sample\r\n\r\n```\r\nutils.resolve_max_positions(None, (2000, 100, 2000), 12000)  # returns (2000, 100)\r\n```\r\n\r\n### Expected behavior\r\n\r\n```\r\nutils.resolve_max_positions(None, (2000, 100, 2000), 12000)  # should return (2000, 100, 2000)\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.4\r\n - OS (e.g., Linux): ubuntu\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `python setup.py install -e .`\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: K80\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2027/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2023", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2023/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2023/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2023/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2023", "id": 602366558, "node_id": "MDU6SXNzdWU2MDIzNjY1NTg=", "number": 2023, "title": "get different size of dictionary in preprocessing and subword-nmt get-vocab", "user": {"login": "zixiliuUSC", "id": 49173327, "node_id": "MDQ6VXNlcjQ5MTczMzI3", "avatar_url": "https://avatars.githubusercontent.com/u/49173327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zixiliuUSC", "html_url": "https://github.com/zixiliuUSC", "followers_url": "https://api.github.com/users/zixiliuUSC/followers", "following_url": "https://api.github.com/users/zixiliuUSC/following{/other_user}", "gists_url": "https://api.github.com/users/zixiliuUSC/gists{/gist_id}", "starred_url": "https://api.github.com/users/zixiliuUSC/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zixiliuUSC/subscriptions", "organizations_url": "https://api.github.com/users/zixiliuUSC/orgs", "repos_url": "https://api.github.com/users/zixiliuUSC/repos", "events_url": "https://api.github.com/users/zixiliuUSC/events{/privacy}", "received_events_url": "https://api.github.com/users/zixiliuUSC/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-18T03:22:29Z", "updated_at": "2020-04-20T13:51:53Z", "closed_at": "2020-04-20T13:51:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the following command and get results as showing blow the code\r\n```\r\n> cd examples/translation/\r\n> bash prepare-iwslt14.sh\r\n> cd ../..\r\n> TEXT=examples/translation/iwslt14.tokenized.de-en\r\n> fairseq-preprocess --source-lang de --target-lang en \\\r\n    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\r\n    --destdir data-bin/iwslt14.tokenized.de-en\r\n```\r\nresult:\r\n```\r\n2020-04-18 03:03:25 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bpe=None, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='./temp/iwslt14.tokenized.de-en', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='de', srcdict=None, target_lang='en', task='translation', tensorboard_logdir='', testpref='examples/translation/iwslt14.tokenized.de-en/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='examples/translation/iwslt14.tokenized.de-en/train', user_dir=None, validpref='examples/translation/iwslt14.tokenized.de-en/valid', workers=1)\r\n2020-04-18 03:03:48 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8847 types\r\n2020-04-18 03:04:20 | INFO | fairseq_cli.preprocess | [de] examples/translation/iwslt14.tokenized.de-en/train.de: 160239 sents, 4035591 tokens, 0.0% replaced by <unk>\r\n2020-04-18 03:04:20 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8847 types\r\n2020-04-18 03:04:21 | INFO | fairseq_cli.preprocess | [de] examples/translation/iwslt14.tokenized.de-en/valid.de: 7283 sents, 182592 tokens, 0.0192% replaced by <unk>\r\n2020-04-18 03:04:21 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8847 types\r\n2020-04-18 03:04:22 | INFO | fairseq_cli.preprocess | [de] examples/translation/iwslt14.tokenized.de-en/test.de: 6750 sents, 161838 tokens, 0.0636% replaced by <unk>\r\n2020-04-18 03:04:22 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6631 types\r\n2020-04-18 03:04:53 | INFO | fairseq_cli.preprocess | [en] examples/translation/iwslt14.tokenized.de-en/train.en: 160239 sents, 3949114 tokens, 0.0% replaced by <unk>\r\n2020-04-18 03:04:53 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6631 types\r\n2020-04-18 03:04:54 | INFO | fairseq_cli.preprocess | [en] examples/translation/iwslt14.tokenized.de-en/valid.en: 7283 sents, 178622 tokens, 0.00448% replaced by <unk>\r\n2020-04-18 03:04:54 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6631 types\r\n2020-04-18 03:04:55 | INFO | fairseq_cli.preprocess | [en] examples/translation/iwslt14.tokenized.de-en/test.en: 6750 sents, 156928 tokens, 0.00892% replaced by <unk>\r\n2020-04-18 03:04:55 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ./temp/iwslt14.tokenized.de-en\r\n```\r\n2. get dictionary using subword-nmt\r\n```\r\ncd examples/translation/iwslt14.tokenized.de-en\r\nsubword-nmt get-vocab < train.en > train.en.vocab\r\nsubword-nmt get-vocab < train.de > train.de.vocab\r\n```\r\nmanually check the size of english vocab, which is 6628 and for german, it is 8842. \r\nSo, there is a small difference in the dictionary size, and I want to know why the difference happen. \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n### Expected behavior\r\n\r\nI think maybe fairseq adds several tokens which are necessary for model training, but why german vocab is adding 5 tokens but english vocab adding 4 tokens? \r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version: 0.9.0\r\n - PyTorch Version: 1.4.0\r\n - OS (e.g., Linux): colab\r\n - How you installed fairseq: I git clone fairseq and run the python file directly\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: None\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2023/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2023/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2022", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2022/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2022/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2022/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2022", "id": 602242514, "node_id": "MDU6SXNzdWU2MDIyNDI1MTQ=", "number": 2022, "title": "Recent SequenceGenerator update makes it unusable with sub-classes that have a different `net_input`", "user": {"login": "villmow", "id": 2743060, "node_id": "MDQ6VXNlcjI3NDMwNjA=", "avatar_url": "https://avatars.githubusercontent.com/u/2743060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/villmow", "html_url": "https://github.com/villmow", "followers_url": "https://api.github.com/users/villmow/followers", "following_url": "https://api.github.com/users/villmow/following{/other_user}", "gists_url": "https://api.github.com/users/villmow/gists{/gist_id}", "starred_url": "https://api.github.com/users/villmow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/villmow/subscriptions", "organizations_url": "https://api.github.com/users/villmow/orgs", "repos_url": "https://api.github.com/users/villmow/repos", "events_url": "https://api.github.com/users/villmow/events{/privacy}", "received_events_url": "https://api.github.com/users/villmow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-17T21:26:00Z", "updated_at": "2020-05-10T13:13:17Z", "closed_at": "2020-05-10T13:13:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThis is not a real bug, I rather want to discuss a recent change in master. \r\n\r\n**Situation**: My (transformer) encoder sub-class needs additional input during the forward path. Currently I'm feeding this to my encoder by adding it to the `net_input` dictionary. During validation/generation I need the additional input as well. This worked out of the box for generation, because the generator only used to forward the `net_input` to the encoder:\r\n\r\nhttps://github.com/pytorch/fairseq/blob/630701eaa750efda4f7aeb1a6d693eb5e690cab1/fairseq/sequence_generator.py#L133\r\n\r\nWith a recent change of the `SequenceGenerator` this is not possible anymore. The generator now only feeds `src_tokens` and `src_lengths` to the encoder (probably for torchscript):\r\n\r\nhttps://github.com/pytorch/fairseq/blob/57526c63433c0b1c997fc91c0881867532567266/fairseq/sequence_generator.py#L197-L200\r\n\r\nThis change requires me to sub-class the generator and add the input there as well. \r\n\r\n### Expected behavior\r\n\r\nI should be able to add additional input to my encoder, without having to sub-class the generator. \r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2022/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2022/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2018", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2018/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2018/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2018/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2018", "id": 600186838, "node_id": "MDU6SXNzdWU2MDAxODY4Mzg=", "number": 2018, "title": "Learning rate changes when restarting training from checkpoint", "user": {"login": "davkovacs", "id": 25356984, "node_id": "MDQ6VXNlcjI1MzU2OTg0", "avatar_url": "https://avatars.githubusercontent.com/u/25356984?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davkovacs", "html_url": "https://github.com/davkovacs", "followers_url": "https://api.github.com/users/davkovacs/followers", "following_url": "https://api.github.com/users/davkovacs/following{/other_user}", "gists_url": "https://api.github.com/users/davkovacs/gists{/gist_id}", "starred_url": "https://api.github.com/users/davkovacs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davkovacs/subscriptions", "organizations_url": "https://api.github.com/users/davkovacs/orgs", "repos_url": "https://api.github.com/users/davkovacs/repos", "events_url": "https://api.github.com/users/davkovacs/events{/privacy}", "received_events_url": "https://api.github.com/users/davkovacs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-04-15T10:19:27Z", "updated_at": "2020-05-04T16:25:34Z", "closed_at": "2020-05-04T16:25:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen training a Levensthein Transformer model and trying to restart training from a checkpoint the learning rate, that should be fixed changes to a small value. \r\nThis is the log for writing the checkpoint: \r\n```\r\n2020-04-08 05:30:58 | INFO | train_inner | epoch 774:     87 / 181 loss=0.524, nll_loss=0.118, mask_ins=0.392, word_ins=0.118, word_del=0.014, ppl=1.44, wps=10375.3, ups=4.6, wpb=2256.7, bsz=51, num_updates=140000, lr=0.0005, gnorm=1.151, clip=0, train_wall=444, wall=29522\r\n\r\n2020-04-08 05:31:00 | INFO | valid | epoch 774 | valid on 'valid' subset | loss 1.979 | nll_loss 1.104 | mask_ins 0.634 | word_ins 1.104 | word_del 0.242 | ppl 3.94 | wps 38870.5 | wpb 2070.1 | bsz 47.4 | num_updates 140000 | best_loss 1.513\r\n2020-04-08 05:31:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /home/dpk25/rds/hpc-work/lev_transformer/USPTO-15k_5/checkpoints/checkpoint_774_140000.pt (epoch 774 @ 140000 updates, score 1.979) (writing took 1.704027434810996 seconds)\r\n``` \r\nThe learning rate is 0.005 as is set in the input file. but when restarting training using the `--restore-file checkpoint_774_140000.pt` The learning rate is not fixed any more and goes to very small values even though the checkpoint is successfully loaded. \r\n```\r\n2020-04-08 14:02:38 | INFO | fairseq.trainer | loaded checkpoint /home/dpk25/rds/hpc-work/lev_transformer/USPTO-15k_5/checkpoints/checkpoint_774_140000.pt (epoch 774 @ 140000 updates)\r\n\r\n2020-04-08 14:02:58 | INFO | train | epoch 774 | loss 0.526 | nll_loss 0.12 | mask_ins 0.392 | word_ins 0.12 | word_del 0.014 | ppl 1.44 | wps 9996.3 | ups 4.43 | wpb 2254.3 | bsz 51 | num_updates 140094 | lr 6.25e-08 | gnorm 1.135 | clip 0 | train_wall 33 | wall 0\r\n```\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\nTrain a Levensthein Transformer model with fixed learning rate:\r\n1. Run\r\n```\r\nCUDA_VISIBLE_DEVICES=0 fairseq-train $TEXT/data-bin \\\r\n    --save-dir  $SAVE_DIR \\\r\n    --tensorboard-logdir $LOG_DIR \\\r\n    --ddp-backend=no_c10d \\\r\n    --task translation_lev \\\r\n    --criterion nat_loss \\\r\n    --arch levenshtein_transformer \\\r\n    --noise random_delete \\\r\n    --share-all-embeddings \\\r\n    --optimizer adam --adam-betas '(0.9,0.98)' \\\r\n    --clip-norm 25 \\\r\n    --lr 0.0005 --lr-scheduler fixed \\\r\n    --warmup-updates 8000 \\\r\n    --label-smoothing 0 \\\r\n    --dropout 0.15  \\\r\n    --encoder-layers 4 --encoder-attention-heads 8 \\\r\n    --encoder-embed-dim 256 --encoder-ffn-embed-dim 2048 \\\r\n    --encoder-normalize-before \\\r\n    --decoder-layers 4 --decoder-attention-heads 8 \\\r\n    --decoder-normalize-before --early-exit \"4,4,4\" \\\r\n    --attention-dropout 0.15 --activation-dropout 0.15 \\\r\n    --log-format 'simple' --log-interval 2500 \\\r\n    --fixed-validation-seed 7 \\\r\n    --max-tokens 4096 \\\r\n    --save-interval-updates 10000 \\\r\n    --keep-interval-updates 0 --keep-last-epochs 0 \\\r\n    --max-update 400000 --seed 42\r\n ```\r\nAnd then run the same command just adding to the end \r\n```\r\n--restore-file $SAVE_DIR/checkpoint_774_140000.pt \r\n````\r\n2. See error\r\nThe learning rate in the restarted training will change to some very small value. (and remain fixed for the rest of the training at that small value)\r\n\r\n### Expected behavior\r\nI am hoping to be able to continue the training with the same learning rate\r\n\r\n### Environment\r\n\r\n - fairseq Version git cloned from here\r\n - PyTorch Version 1.4.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): git clone and then setup following instructions inlcuding the apex.\r\n - Build command you used (if compiling from source):  python setup.py build_ext --inplace\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: CUDA 10.1\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2018/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2018/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2008", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2008/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2008/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2008/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2008", "id": 599295120, "node_id": "MDU6SXNzdWU1OTkyOTUxMjA=", "number": 2008, "title": "Unknown penalty (--unkpen) has no effect on the generated text", "user": {"login": "ajd12342", "id": 39792269, "node_id": "MDQ6VXNlcjM5NzkyMjY5", "avatar_url": "https://avatars.githubusercontent.com/u/39792269?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ajd12342", "html_url": "https://github.com/ajd12342", "followers_url": "https://api.github.com/users/ajd12342/followers", "following_url": "https://api.github.com/users/ajd12342/following{/other_user}", "gists_url": "https://api.github.com/users/ajd12342/gists{/gist_id}", "starred_url": "https://api.github.com/users/ajd12342/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ajd12342/subscriptions", "organizations_url": "https://api.github.com/users/ajd12342/orgs", "repos_url": "https://api.github.com/users/ajd12342/repos", "events_url": "https://api.github.com/users/ajd12342/events{/privacy}", "received_events_url": "https://api.github.com/users/ajd12342/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-14T04:50:47Z", "updated_at": "2020-04-16T03:51:18Z", "closed_at": "2020-04-16T03:51:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nWhile using `fairseq-generate` to generate text in the target language for a translation task, setting the `--unkpen` value to even large numbers like 1e5, 1e6, etc. does not change the hypotheses. As a result I am unable to remove the `<<unk>>` tokens using this, as was suggested in #481 .\r\n\r\n### To Reproduce\r\n1. Train a model using `fairseq-train`\r\n2. Generate text using `fairseq-generate` with the `--unkpen` flag\r\n(See the code below)\r\n\r\n#### Code sample\r\n```\r\nfairseq-train $preprocess \\\r\n    --seed $seed --tensorboard-logdir $expdir/tensorboard \\\r\n    --lr-scheduler fixed \\\r\n    --task=translation \\\r\n    --num-workers $nw --batch-size $batch \\\r\n    --arch=transformer \\\r\n    --dropout $dropout \\\r\n    --decoder-embed-path ${decoder_embed_path} --decoder-embed-dim ${dec_dim} \\\r\n    --max-epoch ${max_epoch} \\\r\n    --clip-norm $cn --lr $lr --update-freq 1 \\\r\n    --save-dir $expdir/checkpoints \\\r\n    --keep-last-epochs 5\r\n```\r\n```\r\nfairseq-generate $preprocess \\\r\n    --seed 1 \\\r\n     --num-workers $nw --batch-size $batch \\\r\n    --gen-subset valid \\\r\n    --path $expdir/checkpoints/checkpoint_best.pt \\\r\n    --results-path $resultsfile \\\r\n    --unkpen $unkpen\r\n```\r\nHere `$unkpen` can be set to any value.\r\n\r\n### Expected behavior\r\nThe number of `<<unk>>` tokens decreases as the unknown penalty increases.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0): 1.2.0\r\n - OS (e.g., Linux): Linux (Ubuntu 18.04.2)\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.8\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: GeForce GTX 1080 Ti\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2008/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2008/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2002", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2002/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2002/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2002/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/2002", "id": 598565793, "node_id": "MDU6SXNzdWU1OTg1NjU3OTM=", "number": 2002, "title": "lstm-lm arguments appear to be circular regarding attention", "user": {"login": "mortonjt", "id": 4184797, "node_id": "MDQ6VXNlcjQxODQ3OTc=", "avatar_url": "https://avatars.githubusercontent.com/u/4184797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mortonjt", "html_url": "https://github.com/mortonjt", "followers_url": "https://api.github.com/users/mortonjt/followers", "following_url": "https://api.github.com/users/mortonjt/following{/other_user}", "gists_url": "https://api.github.com/users/mortonjt/gists{/gist_id}", "starred_url": "https://api.github.com/users/mortonjt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mortonjt/subscriptions", "organizations_url": "https://api.github.com/users/mortonjt/orgs", "repos_url": "https://api.github.com/users/mortonjt/repos", "events_url": "https://api.github.com/users/mortonjt/events{/privacy}", "received_events_url": "https://api.github.com/users/mortonjt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2020-04-12T20:33:06Z", "updated_at": "2020-05-04T16:22:53Z", "closed_at": "2020-05-04T16:22:52Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIt doesn't seem to be possible to enable attention when running lstm-lm\r\n\r\nHere is an example running the lstm-lm archicture\r\n```\r\n$(which fairseq-train) $DATA_DIR \\\r\n    --task language_modeling  \\\r\n    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\r\n    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\r\n    --max-update $TOTAL_UPDATES --log-format simple --log-interval 100 \\\r\n    --ddp-backend=no_c10d \\\r\n    --arch lstm_lm \\\r\n    --decoder-attention \\\r\n    --tensorboard-logdir $TB_DIR \\\r\n    --bpe gpt2 --memory-efficient-fp16 \\\r\n    --num-workers $NPROC_PER_NODE \\\r\n    --save-interval-updates 3600 \\\r\n    --save-dir $SAVE_DIR\r\n```\r\nAnd here is the reported error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/mnt/home/jmorton/venvs/roberta/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/mnt/home/jmorton/software/fairseq/fairseq_cli/train.py\", line 307, in cli_main\r\n    distributed_main(args.device_id, args)\r\n  File \"/mnt/home/jmorton/software/fairseq/fairseq_cli/train.py\", line 286, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/mnt/home/jmorton/software/fairseq/fairseq_cli/train.py\", line 96, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/cm/shared/sw/pkg/devel/python3/3.7.3/lib64/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/mnt/home/jmorton/software/fairseq/fairseq_cli/train.py\", line 176, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/cm/shared/sw/pkg/devel/python3/3.7.3/lib64/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/mnt/home/jmorton/software/fairseq/fairseq/trainer.py\", line 319, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/mnt/home/jmorton/software/fairseq/fairseq/tasks/fairseq_task.py\", line 337, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/mnt/home/jmorton/venvs/roberta/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/mnt/home/jmorton/software/fairseq/fairseq/criterions/cross_entropy.py\", line 29, in forward\r\n    net_output = model(**sample['net_input'])\r\n  File \"/mnt/home/jmorton/venvs/roberta/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/mnt/home/jmorton/venvs/roberta/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 447, in forward\r\n    output = self.module(*inputs[0], **kwargs[0])\r\n  File \"/mnt/home/jmorton/venvs/roberta/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/mnt/home/jmorton/software/fairseq/fairseq/models/fairseq_model.py\", line 436, in forward\r\n    return self.decoder(src_tokens, **kwargs)\r\n  File \"/mnt/home/jmorton/venvs/roberta/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/mnt/home/jmorton/software/fairseq/fairseq/models/lstm.py\", line 374, in forward\r\n    prev_output_tokens, encoder_out, incremental_state\r\n  File \"/mnt/home/jmorton/software/fairseq/fairseq/models/lstm.py\", line 431, in extract_features\r\n    \"attention is not supported if there are no encoder outputs\"\r\nAssertionError: attention is not supported if there are no encoder outputs\r\n```\r\nI bet that this line is the culprit: https://github.com/pytorch/fairseq/blob/master/fairseq/models/lstm_lm.py#L100\r\n\r\n### Environment\r\n\r\n - fairseq Version : 0.9\r\n - PyTorch Version 1.4\r\n - OS (e.g., Linux): Redhat\r\n - How you installed fairseq : pip\r\n - Build command you used : pip install -e .\r\n - Python version: Python 3.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: V100\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2002/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1987", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1987/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1987/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1987/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1987", "id": 597300326, "node_id": "MDU6SXNzdWU1OTczMDAzMjY=", "number": 1987, "title": "Fine-tuning another dataset based on the intermediate training (RoBERTa-large + RACE)", "user": {"login": "14H034160212", "id": 23516191, "node_id": "MDQ6VXNlcjIzNTE2MTkx", "avatar_url": "https://avatars.githubusercontent.com/u/23516191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/14H034160212", "html_url": "https://github.com/14H034160212", "followers_url": "https://api.github.com/users/14H034160212/followers", "following_url": "https://api.github.com/users/14H034160212/following{/other_user}", "gists_url": "https://api.github.com/users/14H034160212/gists{/gist_id}", "starred_url": "https://api.github.com/users/14H034160212/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/14H034160212/subscriptions", "organizations_url": "https://api.github.com/users/14H034160212/orgs", "repos_url": "https://api.github.com/users/14H034160212/repos", "events_url": "https://api.github.com/users/14H034160212/events{/privacy}", "received_events_url": "https://api.github.com/users/14H034160212/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-09T13:55:25Z", "updated_at": "2020-04-14T08:43:14Z", "closed_at": "2020-04-14T07:01:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nHi, I am doing fine-tuning another dataset based on the intermediate training model (RoBERTa-large + RACE). The issue that I met is the train and valid accuracy floating around 50% cause the dataset is a binary classification task. Here is a simple example of the dataset \"ParaRule\". The dataset is from the paper from Allen AI \"Transformers as Soft Reasoners over Language\" (https://rule-reasoning.apps.allenai.org/about).\r\n![transformer_as_soft_reasoner_example](https://user-images.githubusercontent.com/23516191/78901510-87e3ed00-7acc-11ea-9616-50ac731caacb.png)\r\n \r\nYou can see the task is a binary classification task. But in this paper, it mentioned the accuracy can reach nearly 100%. You can see the difference between ParaRule and RACE is the former is a binary classification task and the second one is a classical machine reading comprehension task. To be specifically, the later one has the options for each question, while the former does not have.\r\n\r\nBy the way, I have got the intermediate training model (RoBERTa-large + RACE) successfully and the test accuracy is about 86.8% and then I choose the best checkpoint model as the fine-tuning model on the ParaRule dataset.\r\n\r\nHere is the part of the result when I am doing the fine-tuning on ParaRule dataset. You can see the loss is floating around 1.03-1.04 and the accuracy is floating around 50.0. (P.S. One possible reason can be also the initial starting points. But do you know why in the RACE case, it is working by using UPDATE_FREQ=16 rather than UPDATE_FREQ=8? I guess that can be a possible way to solve my issue in this case.)\r\n```\r\nepoch 002:   3%| | 126/3721 [08:34<3:48:09,  3.81s/it, loss=1.039, nll_loss=0.009, accuracy=49.2\r\nepoch 002:  13%|\\u258f| 476/3721 [30:54<3:48:22,  4.22s/it, loss=1.039, nll_loss=0.009, accuracy=49.9\r\nepoch 002:  13%|\\u258f| 477/3721 [31:19<3:47:49,  4.21s/it, loss=1.039, nll_loss=0.009, accuracy=49.9\r\nepoch 002:  13%|\\u258f| 478/3721 [31:24<3:48:39,  4.23s/it, loss=1.039, nll_loss=0.009, accuracy=49.9\r\nepoch 002:  24%|\\u258f| 898/3721 [59:14<3:06:09,  3.96s/it, loss=1.028, nll_loss=0.008, accuracy=52.4\r\nepoch 002:  33%|\\u258e| 1245/3721 [1:23:07<2:45:00,  4.00s/it, loss=1.028, nll_loss=0.008, accuracy=50.4\r\n```\r\n\r\nHere is the fine-tuning command that I am using on the ParaRule dataset.\r\n```\r\nMAX_EPOCH=5           # Number of training epochs.\r\nLR=1e-05              # Peak LR for fixed LR scheduler.\r\nNUM_CLASSES=4\r\nMAX_SENTENCES=1       # Batch size per GPU.\r\nUPDATE_FREQ=16         # Accumulate gradients to simulate training on 16 GPUs. The original is 8 which is not work in my case.\r\nDATA_DIR=/path/to/race-output-dir\r\nROBERTA_PATH=/path/to/roberta/model.pt\r\n\r\nCUDA_VISIBLE_DEVICES=0,1 fairseq-train $DATA_DIR --ddp-backend=no_c10d \\\r\n    --restore-file $ROBERTA_PATH \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\r\n    --task sentence_ranking \\\r\n    --num-classes $NUM_CLASSES \\\r\n    --init-token 0 --separator-token 2 \\\r\n    --max-option-length 128 \\\r\n    --max-positions 512 \\\r\n    --truncate-sequence \\\r\n    --arch roberta_large \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n    --criterion sentence_ranking \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 \\\r\n    --clip-norm 0.0 \\\r\n    --lr-scheduler fixed --lr $LR \\\r\n    --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\r\n    --max-sentences $MAX_SENTENCES \\\r\n    --required-batch-size-multiple 1 \\\r\n    --update-freq $UPDATE_FREQ \\\r\n    --max-epoch $MAX_EPOCH\r\n```\r\n\r\nIt will be great if you have any suggestion! Many thanks in advance.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1987/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1983", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1983/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1983/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1983/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1983", "id": 596799442, "node_id": "MDU6SXNzdWU1OTY3OTk0NDI=", "number": 1983, "title": "Sampling from GeneratorHubInterface does not generate anything", "user": {"login": "noe", "id": 4972, "node_id": "MDQ6VXNlcjQ5NzI=", "avatar_url": "https://avatars.githubusercontent.com/u/4972?v=4", "gravatar_id": "", "url": "https://api.github.com/users/noe", "html_url": "https://github.com/noe", "followers_url": "https://api.github.com/users/noe/followers", "following_url": "https://api.github.com/users/noe/following{/other_user}", "gists_url": "https://api.github.com/users/noe/gists{/gist_id}", "starred_url": "https://api.github.com/users/noe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/noe/subscriptions", "organizations_url": "https://api.github.com/users/noe/orgs", "repos_url": "https://api.github.com/users/noe/repos", "events_url": "https://api.github.com/users/noe/events{/privacy}", "received_events_url": "https://api.github.com/users/noe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-04-08T19:15:44Z", "updated_at": "2020-04-14T17:28:32Z", "closed_at": "2020-04-14T17:28:32Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen calling function `sample` from `hub_utils.GeneratorHubInterface`, as specified in the [language modeling readme](https://github.com/pytorch/fairseq/tree/master/examples/language_model), the result is always the same prefix passed as argument to the function, with no extra tokens sampled from the LM.\r\n\r\nInternally, the problem is that, when the sentence is converted to token IDs, the EOS token ID is added. When the model is presented with a prefix that \"has already ended\", it just generates another EOS and finishes.\r\n\r\n### To Reproduce\r\n\r\nJust follow the code to sample from an LM from the [language modeling readme](https://github.com/pytorch/fairseq/tree/master/examples/language_model).\r\n\r\n#### Code sample\r\n\r\n```\r\nfrom fairseq.models.transformer_lm import TransformerLanguageModel\r\ncustom_lm = TransformerLanguageModel.from_pretrained('/path/to/model/dir', 'checkpoint100.pt')\r\ncustom_lm.sample('Barack Obama', beam=1)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe returned sampled sentences have more tokens apart from the ones already supplied as argument to `sample`.\r\n\r\n### Environment\r\n\r\n - fairseq Version: master:\r\n - PyTorch Version: 1.4.0\r\n - OS: Ubuntu 16.04.2 LTS\r\n - How you installed fairseq: from source\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: none\r\n - GPU models and configuration: none\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1983/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1978", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1978/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1978/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1978/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1978", "id": 596137190, "node_id": "MDU6SXNzdWU1OTYxMzcxOTA=", "number": 1978, "title": "Loading adaptive language models from Torch Hub fails: FileNotFoundError", "user": {"login": "aryamccarthy", "id": 5395205, "node_id": "MDQ6VXNlcjUzOTUyMDU=", "avatar_url": "https://avatars.githubusercontent.com/u/5395205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aryamccarthy", "html_url": "https://github.com/aryamccarthy", "followers_url": "https://api.github.com/users/aryamccarthy/followers", "following_url": "https://api.github.com/users/aryamccarthy/following{/other_user}", "gists_url": "https://api.github.com/users/aryamccarthy/gists{/gist_id}", "starred_url": "https://api.github.com/users/aryamccarthy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aryamccarthy/subscriptions", "organizations_url": "https://api.github.com/users/aryamccarthy/orgs", "repos_url": "https://api.github.com/users/aryamccarthy/repos", "events_url": "https://api.github.com/users/aryamccarthy/events{/privacy}", "received_events_url": "https://api.github.com/users/aryamccarthy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-04-07T20:37:46Z", "updated_at": "2020-04-13T22:39:27Z", "closed_at": "2020-04-13T22:39:26Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nLoading adaptive language models from Torch Hub fails; there's a `FileNotFoundError`. The \"missing\" file is actually one level down in the directory structure. I haven't tested this with other models besides LMs. This is not broken, though, for the WMT language models.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Attempt to load a language model.\r\n```\r\n# Using the command from https://github.com/pytorch/fairseq/tree/master/examples/language_model\r\nTORCH_HOME=/tmp python -c \"import torch; en_lm = torch.hub.load('pytorch/fairseq', 'transformer_lm.wiki103.adaptive', tokenizer='moses', bpe='fastbpe')\"\r\n```\r\n2. See error\r\n```\r\ncopying build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so -> fairseq\r\ncopying build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\r\ncopying build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\r\ncopying build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so -> fairseq\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1383631246/1383631246 [07:03<00:00, 3263955.74B/s]               \u2502\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/arya/anaconda3/envs/transformer_lm/lib/python3.7/site-packages/torch/hub.py\", line 366, in load\r\n    model = entry(*args, **kwargs)\r\n  File \"/tmp/hub/pytorch_fairseq_master/fairseq/models/fairseq_model.py\", line 218, in from_pretrained\r\n    **kwargs,\r\n  File \"/tmp/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 73, in from_pretrained\r\n    arg_overrides=kwargs,\r\n  File \"/tmp/hub/pytorch_fairseq_master/fairseq/checkpoint_utils.py\", line 206, in load_model_ensemble_and_task\r\n    task = tasks.setup_task(args)\r\n  File \"/tmp/hub/pytorch_fairseq_master/fairseq/tasks/__init__.py\", line 17, in setup_task\r\n    return TASK_REGISTRY[args.task].setup_task(args, **kwargs)\r\n  File \"/tmp/hub/pytorch_fairseq_master/fairseq/tasks/language_modeling.py\", line 115, in setup_task\r\n    dictionary = Dictionary.load(os.path.join(paths[0], \"dict.txt\"))\r\n  File \"/tmp/hub/pytorch_fairseq_master/fairseq/data/dictionary.py\", line 195, in load\r\n    d.add_from_file(f)\r\n  File \"/tmp/hub/pytorch_fairseq_master/fairseq/data/dictionary.py\", line 208, in add_from_file\r\n    raise fnfe\r\n  File \"/tmp/hub/pytorch_fairseq_master/fairseq/data/dictionary.py\", line 205, in add_from_file\r\n    with PathManager.open(f, \"r\", encoding=\"utf-8\") as fd:\r\n  File \"/tmp/hub/pytorch_fairseq_master/fairseq/file_io.py\", line 51, in open\r\n    newline=newline,\r\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/pytorch_fairseq/8f5513900ccbfa5196131abf5c673ce3b924f1b01df911ee9b70c3aedf3eaf09.313c555a1dd483cb77c344f7e3361a50ebcac3792ab221e999bc1c9f735f2db2/dict.txt'\r\n\r\n```\r\n\r\n3. Go into the directory.\r\n\r\n```\r\nls /tmp/pytorch_fairseq/8f5513900ccbfa5196131abf5c673ce3b924f1b01df911ee9b70c3aedf3eaf09.313c555a1dd483cb77c344f7e3361a50ebcac3792ab221e999bc1c9f735f2db2\r\n# data-bin/  eval.sh  model.pt\r\nls !$/data-bin/\r\n# dict.txt  test.bin  test.idx  valid.bin  valid.idx\r\n```\r\n\r\n4. Scratch head\u2014there's the missing file, just one level down!\r\n\r\n### Expected behavior\r\n\r\nWell, sure hope the file would be found! \r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): `pip`\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.7\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1978/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1978/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1960", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1960/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1960/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1960/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1960", "id": 593297695, "node_id": "MDU6SXNzdWU1OTMyOTc2OTU=", "number": 1960, "title": "fairseq-generate on train dataset generate non english ", "user": {"login": "gaopengcuhk", "id": 51525797, "node_id": "MDQ6VXNlcjUxNTI1Nzk3", "avatar_url": "https://avatars.githubusercontent.com/u/51525797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gaopengcuhk", "html_url": "https://github.com/gaopengcuhk", "followers_url": "https://api.github.com/users/gaopengcuhk/followers", "following_url": "https://api.github.com/users/gaopengcuhk/following{/other_user}", "gists_url": "https://api.github.com/users/gaopengcuhk/gists{/gist_id}", "starred_url": "https://api.github.com/users/gaopengcuhk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gaopengcuhk/subscriptions", "organizations_url": "https://api.github.com/users/gaopengcuhk/orgs", "repos_url": "https://api.github.com/users/gaopengcuhk/repos", "events_url": "https://api.github.com/users/gaopengcuhk/events{/privacy}", "received_events_url": "https://api.github.com/users/gaopengcuhk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-03T10:56:53Z", "updated_at": "2020-04-03T10:57:20Z", "closed_at": "2020-04-03T10:57:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1960/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1960/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1959", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1959/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1959/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1959/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1959", "id": 593288672, "node_id": "MDU6SXNzdWU1OTMyODg2NzI=", "number": 1959, "title": "add_cpu with fp16 training", "user": {"login": "moussaKam", "id": 28675016, "node_id": "MDQ6VXNlcjI4Njc1MDE2", "avatar_url": "https://avatars.githubusercontent.com/u/28675016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/moussaKam", "html_url": "https://github.com/moussaKam", "followers_url": "https://api.github.com/users/moussaKam/followers", "following_url": "https://api.github.com/users/moussaKam/following{/other_user}", "gists_url": "https://api.github.com/users/moussaKam/gists{/gist_id}", "starred_url": "https://api.github.com/users/moussaKam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/moussaKam/subscriptions", "organizations_url": "https://api.github.com/users/moussaKam/orgs", "repos_url": "https://api.github.com/users/moussaKam/repos", "events_url": "https://api.github.com/users/moussaKam/events{/privacy}", "received_events_url": "https://api.github.com/users/moussaKam/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-04-03T10:40:26Z", "updated_at": "2020-04-06T12:53:43Z", "closed_at": "2020-04-06T12:53:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I trained a model for 3 epochs using fp16. When I restored the checkpoint to continue the training I got this error.\r\n\r\nTraceback (most recent call last):\r\n  File \"/data/home/moussa/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/data/home/moussa/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/data/home/moussa/fairseq/train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/data/home/moussa/fairseq/fairseq_cli/train.py\", line 349, in cli_main\r\n    main(args)\r\n  File \"/data/home/moussa/fairseq/fairseq_cli/train.py\", line 100, in main\r\n    should_end_training = train(args, trainer, task, epoch_itr)\r\n  File \"/data/home/moussa/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/data/home/moussa/fairseq/fairseq_cli/train.py\", line 176, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/data/home/moussa/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/data/home/moussa/fairseq/fairseq/trainer.py\", line 452, in train_step\r\n    raise e\r\n  File \"/data/home/moussa/fairseq/fairseq/trainer.py\", line 422, in train_step\r\n    logging_outputs, sample_size, grad_norm,\r\n  File \"/data/home/moussa/fairseq/fairseq/trainer.py\", line 741, in _reduce_and_log_stats\r\n    metrics.log_scalar(\"gnorm\", grad_norm, priority=400, round=3)\r\n  File \"/data/home/moussa/fairseq/fairseq/logging/metrics.py\", line 132, in log_scalar\r\n    agg[key].update(value, weight)\r\n  File \"/data/home/moussa/fairseq/fairseq/logging/meters.py\", line 70, in update\r\n    self.sum = self.sum + (val * n)\r\nRuntimeError: \"add_cpu/sub_cpu\" not implemented for 'Half'\r\n\r\nHere's the command:\r\n\r\npython -m train data-bin --arch bart_large --task translation --source-lang source --target-lang target --criterion label_smoothed_cross_entropy --label-smoothing 0.2 --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' --lr 3e-05 --max-tokens 1024 --update-freq 3 --valid-subset valid --fp16 --memory-efficient-fp16 --skip-invalid-size-inputs-valid-test --restore-file checkpoints/checkpoint3.pt\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1959/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1952", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1952/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1952/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1952/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1952", "id": 592065932, "node_id": "MDU6SXNzdWU1OTIwNjU5MzI=", "number": 1952, "title": "`@register_model_architecture` does not register model as expected.", "user": {"login": "rawkintrevo", "id": 5852441, "node_id": "MDQ6VXNlcjU4NTI0NDE=", "avatar_url": "https://avatars.githubusercontent.com/u/5852441?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rawkintrevo", "html_url": "https://github.com/rawkintrevo", "followers_url": "https://api.github.com/users/rawkintrevo/followers", "following_url": "https://api.github.com/users/rawkintrevo/following{/other_user}", "gists_url": "https://api.github.com/users/rawkintrevo/gists{/gist_id}", "starred_url": "https://api.github.com/users/rawkintrevo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rawkintrevo/subscriptions", "organizations_url": "https://api.github.com/users/rawkintrevo/orgs", "repos_url": "https://api.github.com/users/rawkintrevo/repos", "events_url": "https://api.github.com/users/rawkintrevo/events{/privacy}", "received_events_url": "https://api.github.com/users/rawkintrevo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-01T17:01:17Z", "updated_at": "2021-02-05T10:21:34Z", "closed_at": "2020-04-01T17:56:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nWhen following https://fairseq.readthedocs.io/en/latest/tutorial_simple_lstm.html we register a model with `@register_model_architecture(...` however when we run \r\n\r\n```\r\nfairseq-train data-bin/test01 \\\r\n  --arch my_model \\\r\n```\r\n\r\nyields: \r\n\r\n`fairseq-train: error: argument --arch/-a: invalid choice: 'my_model' `\r\n\r\n\r\n#### Code sample\r\n\r\nfollowing the tutorial is a sufficient code sample.\r\n\r\n### Expected behavior\r\n\r\nWe expected `my_model` to be an available architecture\r\n\r\n### Environment\r\n\r\n - fairseq Version (0.8.0):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): pip (I think?)\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: not applicable\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1952/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1949", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1949/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1949/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1949/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1949", "id": 591201077, "node_id": "MDU6SXNzdWU1OTEyMDEwNzc=", "number": 1949, "title": "Bug while decoding a trained es-en translation model", "user": {"login": "darsh10", "id": 11456255, "node_id": "MDQ6VXNlcjExNDU2MjU1", "avatar_url": "https://avatars.githubusercontent.com/u/11456255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/darsh10", "html_url": "https://github.com/darsh10", "followers_url": "https://api.github.com/users/darsh10/followers", "following_url": "https://api.github.com/users/darsh10/following{/other_user}", "gists_url": "https://api.github.com/users/darsh10/gists{/gist_id}", "starred_url": "https://api.github.com/users/darsh10/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/darsh10/subscriptions", "organizations_url": "https://api.github.com/users/darsh10/orgs", "repos_url": "https://api.github.com/users/darsh10/repos", "events_url": "https://api.github.com/users/darsh10/events{/privacy}", "received_events_url": "https://api.github.com/users/darsh10/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-03-31T15:25:30Z", "updated_at": "2020-05-04T16:23:04Z", "closed_at": "2020-04-02T11:28:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "This is the error that I get on using the fairseq-generate command on a trained model\r\n\r\nload_state_dict\r\n    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\nRuntimeError: Error(s) in loading state_dict for FConvModel:\r\n        Unexpected key(s) in state_dict: \"decoder.convolutions.0._linearized_weight\", \"decoder.convolutions.1._linearized_weight\", \"decoder.convolutions.2._linearized_weight\".\r\n\r\nSurprisingly, the translation and generate work for en-es pair and not reverse.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1949/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1949/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1947", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1947/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1947/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1947/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1947", "id": 590967529, "node_id": "MDU6SXNzdWU1OTA5Njc1Mjk=", "number": 1947, "title": "BART generation results for top k differ in each run", "user": {"login": "tuhinjubcse", "id": 3104771, "node_id": "MDQ6VXNlcjMxMDQ3NzE=", "avatar_url": "https://avatars.githubusercontent.com/u/3104771?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tuhinjubcse", "html_url": "https://github.com/tuhinjubcse", "followers_url": "https://api.github.com/users/tuhinjubcse/followers", "following_url": "https://api.github.com/users/tuhinjubcse/following{/other_user}", "gists_url": "https://api.github.com/users/tuhinjubcse/gists{/gist_id}", "starred_url": "https://api.github.com/users/tuhinjubcse/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tuhinjubcse/subscriptions", "organizations_url": "https://api.github.com/users/tuhinjubcse/orgs", "repos_url": "https://api.github.com/users/tuhinjubcse/repos", "events_url": "https://api.github.com/users/tuhinjubcse/events{/privacy}", "received_events_url": "https://api.github.com/users/tuhinjubcse/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2020-03-31T10:02:29Z", "updated_at": "2020-03-31T21:04:42Z", "closed_at": "2020-03-31T21:04:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nEvery time I do Bart.sample for topk the results are different. For reproducibility is there a way to fix this?\r\n\r\n`                hypotheses_batch = bart.sample(slines, sampling=True, sampling_topk=5, temperature=0.7, lenpen=2.0, max_len_b=30, min_len=7, no_repeat_ngram_size=3)\r\n`\r\n### Environment\r\n\r\nEnvironment\r\n- fairseq Version (e.g., 1.0 or master): 0.9.0\r\n- PyTorch Version (e.g., 1.0) 1.3.1\r\n- OS (e.g., Linux): Linux\r\n- How you installed fairseq (pip, source):\r\n- Build command you used (if compiling from source):\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.1.243\r\n- GPU models and configuration: RTX 2080 ( 4 gpus)\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1947/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1947/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1946", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1946/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1946/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1946/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1946", "id": 590885538, "node_id": "MDU6SXNzdWU1OTA4ODU1Mzg=", "number": 1946, "title": "The accuracy that I have done for RoBERTa-large fine-tuning on RACE has a big gap with the paper proposed", "user": {"login": "14H034160212", "id": 23516191, "node_id": "MDQ6VXNlcjIzNTE2MTkx", "avatar_url": "https://avatars.githubusercontent.com/u/23516191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/14H034160212", "html_url": "https://github.com/14H034160212", "followers_url": "https://api.github.com/users/14H034160212/followers", "following_url": "https://api.github.com/users/14H034160212/following{/other_user}", "gists_url": "https://api.github.com/users/14H034160212/gists{/gist_id}", "starred_url": "https://api.github.com/users/14H034160212/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/14H034160212/subscriptions", "organizations_url": "https://api.github.com/users/14H034160212/orgs", "repos_url": "https://api.github.com/users/14H034160212/repos", "events_url": "https://api.github.com/users/14H034160212/events{/privacy}", "received_events_url": "https://api.github.com/users/14H034160212/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-31T08:01:24Z", "updated_at": "2020-04-06T11:58:50Z", "closed_at": "2020-04-02T11:09:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nHi, \r\n\r\nI have tried to use 2 Tesla V100 GPU with 32 GB memory to fine-tuning the RoBERTa-large model with RACE dataset. Here is the part of the result. Now I have finished the running with 5 epochs, but the loss and accuracy look like nearly no change. Does anyone have met the problem before? I just use the exact same parameters from that website for the fine-tuning on RACE dataset(https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.race.md). Will be great to have any suggestions from you. \r\n\r\nMany thanks.\r\n\r\nHere is the command and parameters that I used.\r\n```\r\nMAX_EPOCH=5           # Number of training epochs.\r\nLR=1e-05              # Peak LR for fixed LR scheduler.\r\nNUM_CLASSES=4\r\nMAX_SENTENCES=1       # Batch size per GPU.\r\nUPDATE_FREQ=8         # Accumulate gradients to simulate training on 8 GPUs.\r\nDATA_DIR=/path/to/race-output-dir\r\nROBERTA_PATH=/path/to/roberta/model.pt\r\n\r\nCUDA_VISIBLE_DEVICES=0,1 fairseq-train $DATA_DIR --ddp-backend=no_c10d \\\r\n    --restore-file $ROBERTA_PATH \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\r\n    --task sentence_ranking \\\r\n    --num-classes $NUM_CLASSES \\\r\n    --init-token 0 --separator-token 2 \\\r\n    --max-option-length 128 \\\r\n    --max-positions 512 \\\r\n    --truncate-sequence \\\r\n    --arch roberta_large \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n    --criterion sentence_ranking \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 \\\r\n    --clip-norm 0.0 \\\r\n    --lr-scheduler fixed --lr $LR \\\r\n    --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\r\n    --max-sentences $MAX_SENTENCES \\\r\n    --required-batch-size-multiple 1 \\\r\n    --update-freq $UPDATE_FREQ \\\r\n    --max-epoch $MAX_EPOCH\r\n```\r\n\r\nHere is the final training result after 5 epoch. The best accuracy is only 0.261661.\r\n\r\n![roberta_large_race_result_epoch_5_final](https://user-images.githubusercontent.com/23516191/78081272-bc77ea80-73e2-11ea-8e79-d555967bf8ae.png)\r\n\r\nHere is the evaluation result and the accuracy is only 0.245.\r\n![roberta_large_evaluation_result](https://user-images.githubusercontent.com/23516191/78084957-1fba4a80-73ec-11ea-9018-226355769621.png)\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1946/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1946/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1933", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1933/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1933/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1933/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1933", "id": 589703754, "node_id": "MDU6SXNzdWU1ODk3MDM3NTQ=", "number": 1933, "title": "RoBERTa-large model fine-tuning RACE dataset with RuntimeError: CUDA out of memory", "user": {"login": "14H034160212", "id": 23516191, "node_id": "MDQ6VXNlcjIzNTE2MTkx", "avatar_url": "https://avatars.githubusercontent.com/u/23516191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/14H034160212", "html_url": "https://github.com/14H034160212", "followers_url": "https://api.github.com/users/14H034160212/followers", "following_url": "https://api.github.com/users/14H034160212/following{/other_user}", "gists_url": "https://api.github.com/users/14H034160212/gists{/gist_id}", "starred_url": "https://api.github.com/users/14H034160212/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/14H034160212/subscriptions", "organizations_url": "https://api.github.com/users/14H034160212/orgs", "repos_url": "https://api.github.com/users/14H034160212/repos", "events_url": "https://api.github.com/users/14H034160212/events{/privacy}", "received_events_url": "https://api.github.com/users/14H034160212/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-03-29T02:53:43Z", "updated_at": "2020-04-14T07:03:28Z", "closed_at": "2020-04-14T07:03:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nHi, Does anyone have tried fine-tuning the RoBERTa-large model with RACE dataset on the Colab GPU K80 with 7.43 GiB total capacity? I have run it, but with the RuntimeError: CUDA out of memory issue. \r\nI have tries to change the MAX_SENTENCES to 1 and UPDATE_FREQ to 16, but still have the same issue. I have also tried run that on my own GTX-1660Ti GPU with 6 GiB total capacity. The result is the same. So, Does anyone have any suggestion? Thanks a lot. \r\n\r\n```\r\nRuntimeError: CUDA out of memory. Tried to allocate 1.33 GiB (GPU 0; 7.43 GiB total capacity; 5.98 GiB already allocated; 952.94 MiB free; 5.98 GiB reserved in total by PyTorch)\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1933/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1933/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1932", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1932/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1932/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1932/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1932", "id": 589543935, "node_id": "MDU6SXNzdWU1ODk1NDM5MzU=", "number": 1932, "title": "A rookie question of RoBERTa finetuning on RACE dataset", "user": {"login": "14H034160212", "id": 23516191, "node_id": "MDQ6VXNlcjIzNTE2MTkx", "avatar_url": "https://avatars.githubusercontent.com/u/23516191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/14H034160212", "html_url": "https://github.com/14H034160212", "followers_url": "https://api.github.com/users/14H034160212/followers", "following_url": "https://api.github.com/users/14H034160212/following{/other_user}", "gists_url": "https://api.github.com/users/14H034160212/gists{/gist_id}", "starred_url": "https://api.github.com/users/14H034160212/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/14H034160212/subscriptions", "organizations_url": "https://api.github.com/users/14H034160212/orgs", "repos_url": "https://api.github.com/users/14H034160212/repos", "events_url": "https://api.github.com/users/14H034160212/events{/privacy}", "received_events_url": "https://api.github.com/users/14H034160212/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-28T09:48:48Z", "updated_at": "2020-03-28T23:03:27Z", "closed_at": "2020-03-28T23:03:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nHi, I am trying to use the fine-tuning code of roberta on race dataset on Colab with GPU. But I cannot run the following code on the Colab. Does any know how to solve it? Thanks a lot.\r\nhttps://github.com/pytorch/fairseq/blob/master/examples/roberta/README.race.md#3-fine-tuning-on-race\r\n\r\nHere is the error information.\r\n```\r\n  File \"<ipython-input-8-17151a1d11e0>\", line 9\r\n    CUDA_VISIBLE_DEVICES=0 fairseq-train $DATA_DIR --ddp-backend=no_c10d   --restore-file $ROBERTA_PATH   --reset-optimizer --reset-dataloader --reset-meters   --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric   --task sentence_ranking   --num-classes $NUM_CLASSES   --init-token 0 --separator-token 2   --max-option-length 128   --max-positions 512   --truncate-sequence   --arch roberta_large   --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01   --criterion sentence_ranking   --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06   --clip-norm 0.0   --lr-scheduler fixed --lr $LR   --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128   --max-sentences $MAX_SENTENCES   --required-batch-size-multiple 1   --update-freq $UPDATE_FREQ   --max-epoch $MAX_EPOCH\r\n                                 ^\r\nSyntaxError: invalid syntax\r\n```\r\n\r\nAnd there is a red wavy line of that line\r\n```\r\n\"CUDA_VISIBLE_DEVICES=0 fairseq-train $DATA_DIR --ddp-backend=no_c10d \\\"\r\n```\r\n\r\nI guess probably the problem is from the \"fairseq-train\". Does anyone knows that?\r\n\r\nMany thanks!", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1932/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1932/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1918", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1918/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1918/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1918/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1918", "id": 588468271, "node_id": "MDU6SXNzdWU1ODg0NjgyNzE=", "number": 1918, "title": "unused variable in TranslationFromPretrainedBart.build_dataset_for_inference", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-03-26T14:34:42Z", "updated_at": "2020-05-19T18:19:00Z", "closed_at": "2020-05-19T18:18:59Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "The `source_tokens` variable in\r\nhttps://github.com/pytorch/fairseq/blob/bb1750d0442a617b08d7d727c049ab8c8e5cf460/fairseq/tasks/translation_from_pretrained_bart.py#L111\r\n\r\nis created and then not used.\r\n\r\nIs line 112 supposed to be \r\n\r\n```python\r\ndataset = LanguagePairDataset(source_tokens, src_lengths, self.source_dictionary)\r\n``` \r\n@yinhanliu", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1918/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1918/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1915", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1915/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1915/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1915/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1915", "id": 588214505, "node_id": "MDU6SXNzdWU1ODgyMTQ1MDU=", "number": 1915, "title": "Test fails starting from a recent commit", "user": {"login": "freewym", "id": 3506322, "node_id": "MDQ6VXNlcjM1MDYzMjI=", "avatar_url": "https://avatars.githubusercontent.com/u/3506322?v=4", "gravatar_id": "", "url": "https://api.github.com/users/freewym", "html_url": "https://github.com/freewym", "followers_url": "https://api.github.com/users/freewym/followers", "following_url": "https://api.github.com/users/freewym/following{/other_user}", "gists_url": "https://api.github.com/users/freewym/gists{/gist_id}", "starred_url": "https://api.github.com/users/freewym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/freewym/subscriptions", "organizations_url": "https://api.github.com/users/freewym/orgs", "repos_url": "https://api.github.com/users/freewym/repos", "events_url": "https://api.github.com/users/freewym/events{/privacy}", "received_events_url": "https://api.github.com/users/freewym/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-26T07:35:09Z", "updated_at": "2020-03-26T20:23:17Z", "closed_at": "2020-03-26T20:23:17Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nTest fails starting from [this commit](https://github.com/pytorch/fairseq/commit/42f65d65776327598a2d3ded2e92e5818c70a125):\r\n\r\n File \"/home/runner/work/fairseq/fairseq/fairseq/criterions/cross_entropy.py\", line 59, in reduce_metrics\r\n    metrics.log_scalar('loss', loss_sum / sample_size / math.log(2), sample_size, round=3)\r\n  File \"/home/runner/work/fairseq/fairseq/fairseq/logging/metrics.py\", line 123, in log_scalar\r\n    agg[key].update(value, weight)\r\n  File \"/home/runner/work/fairseq/fairseq/fairseq/logging/meters.py\", line 71, in update\r\n    self.count += n\r\nRuntimeError: result type Float can't be cast to the desired output type Long\r\n\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nJust check the test report at that commit\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1915/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1910", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1910/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1910/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1910/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1910", "id": 587974523, "node_id": "MDU6SXNzdWU1ODc5NzQ1MjM=", "number": 1910, "title": "valid_best_loss not actually the best", "user": {"login": "SamuelLarkin", "id": 7314973, "node_id": "MDQ6VXNlcjczMTQ5NzM=", "avatar_url": "https://avatars.githubusercontent.com/u/7314973?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SamuelLarkin", "html_url": "https://github.com/SamuelLarkin", "followers_url": "https://api.github.com/users/SamuelLarkin/followers", "following_url": "https://api.github.com/users/SamuelLarkin/following{/other_user}", "gists_url": "https://api.github.com/users/SamuelLarkin/gists{/gist_id}", "starred_url": "https://api.github.com/users/SamuelLarkin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SamuelLarkin/subscriptions", "organizations_url": "https://api.github.com/users/SamuelLarkin/orgs", "repos_url": "https://api.github.com/users/SamuelLarkin/repos", "events_url": "https://api.github.com/users/SamuelLarkin/events{/privacy}", "received_events_url": "https://api.github.com/users/SamuelLarkin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-25T20:35:32Z", "updated_at": "2020-03-25T22:13:12Z", "closed_at": "2020-03-25T22:13:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n\"valid_best_loss\" reported in the logs is not the best but rather the value of the first epoch.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n```\r\n      fairseq-train \\\r\n         $data_dir \\\r\n         --seed 582 \\\r\n         --arch transformer \\\r\n         --optimizer adam \\\r\n         --adam-betas '(0.9, 0.98)' \\\r\n         --clip-norm 0.0\\\r\n         --lr 5e-4 \\\r\n         --lr-scheduler inverse_sqrt \\\r\n         --min-lr '1e-09' \\\r\n         --warmup-updates 4000\\\r\n         --warmup-init-lr '1e-07' \\\r\n         --dropout 0.3 \\\r\n         --weight-decay 0.0001\\\r\n         --criterion label_smoothed_cross_entropy \\\r\n         --label-smoothing 0.1 \\\r\n         --max-tokens 4096 \\\r\n         --max-update 300000 \\\r\n         --update-freq 4 \\\r\n         --tensorboard-logdir model/logs/tensorboard \\\r\n         --save-dir model \\\r\n         --tensorboard-logdir model \\\r\n         --log-format json \\\r\n         --keep-last-epochs 10 \\\r\n         --maximize-best-checkpoint-metric\r\n```\r\n\r\n```\r\nNamespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:19541', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=10, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format='json', log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='model', save_interval=1, save_interval_updates=0, seed=582, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='model', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[4], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001)\r\n```\r\n\r\n```\r\n| model transformer, criterion LabelSmoothedCrossEntropyCriterion\r\n| num. model params: 51720192 (num. trained: 51720192)\r\n| training on 4 GPUs\r\n| max tokens per GPU = 4096 and max sentences per GPU = None\r\n| no existing checkpoint found model/checkpoint_last.pt\r\n| loading train data for epoch 0\r\n| loaded 1293348 examples from: data/train.iu-en.iu\r\n| loaded 1293348 examples from: data/train.iu-en.en\r\n| data train iu-en 1293348 examples\r\n| NOTICE: your device may support faster training with --fp16\r\n{\"epoch\": 1, \"train_loss\": \"9.433\", \"train_nll_loss\": \"8.973\", \"train_ppl\": \"502.42\", \"train_wps\": \"92565\", \"train_ups\": \"2\", \"train_wpb\": \"58247.305\", \"train_bsz\": \"3086.749\", \"train_num_updates\": \"419\", \"train_lr\": \"5.24645e-05\", \"train_gnorm\": \"1.646\", \"train_clip\": \"0.000\", \"train_oom\": \"0.000\", \"train_wall\": \"279\", \"train_train_wall\": \"249\"}\r\n{\"epoch\": 1, \"valid_loss\": \"8.451\", \"valid_nll_loss\": \"7.769\", \"valid_ppl\": \"218.09\", \"valid_num_updates\": \"419\"}\r\n| saved checkpoint model/checkpoint1.pt (epoch 1 @ 419 updates) (writing took 2.014841318130493 seconds)\r\n...\r\n{\"epoch\": 401, \"train_loss\": \"2.898\", \"train_nll_loss\": \"1.428\", \"train_ppl\": \"2.69\", \"train_wps\": \"93509\", \"train_ups\": \"2\", \"train_wpb\": \"58247.305\", \"train_bsz\": \"3086.749\", \"train_num_updates\": \"168019\", \"train_lr\": \"7.71473e-05\", \"train_gnorm\": \"0.359\", \"train_clip\": \"0.000\", \"train_oom\": \"0.000\", \"train_wall\": \"107362\", \"train_train_wall\": \"98269\"}\r\n{\"epoch\": 401, \"valid_loss\": \"3.580\", \"valid_nll_loss\": \"2.113\", \"valid_ppl\": \"4.33\", \"valid_num_updates\": \"168019\", \"valid_best_loss\": \"8.45141\"}\r\n| saved checkpoint model/checkpoint401.pt (epoch 401 @ 168019 updates) (writing took 1.8081376552581787 seconds)\r\n```\r\n\r\nNote that the `valid_best_lost` at epoch 401 is the same as `valid_loss` at epoch 1 and stays like that through out training.  Looks like it should be better when getting smaller.\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n`valid_best_lost` should go down when the training `criterion` is `label_smoothed_cross_entropy` as such, it should not be stuck reflecting the value at epoch 1.  In the example above, it should at least be `3.580`\r\n\r\n### Environment\r\n\r\nInstallation procedure\r\n\r\n```\r\nconda \\\r\n   create \\\r\n      --yes \\\r\n      --name fairseq-0.9.0 \\\r\n      --channel conda-forge \\\r\n      --channel pytorch \\\r\n      python=3 \\\r\n      cudatoolkit=10 \\\r\n      cudnn \\\r\n      nvidia-apex \\\r\n      pytorch=1.2 \\\r\n      tensorboard\r\n\r\nconda activate fairseq-0.9.0\r\n\r\npip \\\r\n    install \\\r\n        fairseq==0.9.0 \\\r\n        cython \\\r\n        regex \\\r\n        typing \\\r\n        portalocker \\\r\n        sacrebleu \\\r\n        tensorboardX\r\n```\r\n\r\nResulting in\r\n\r\n```\r\nname: fairseq-0.9.0\r\nchannels:\r\n  - pytorch\r\n  - conda-forge\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=conda_forge\r\n  - _openmp_mutex=4.5=1_llvm\r\n  - absl-py=0.9.0=py37_0\r\n  - asn1crypto=1.3.0=py37_0\r\n  - attrs=19.3.0=py_0\r\n  - blinker=1.4=py37_0\r\n  - c-ares=1.15.0=h7b6447c_1001\r\n  - ca-certificates=2020.1.1=0\r\n  - cachetools=3.1.1=py_0\r\n  - certifi=2019.11.28=py37_0\r\n  - cffi=1.13.2=py37h8022711_0\r\n  - chardet=3.0.4=py37_1003\r\n  - click=7.0=py37_0\r\n  - cryptography=2.8=py37h1ba5d50_0\r\n  - cudatoolkit=10.0.130=0\r\n  - cudnn=7.6.5=cuda10.0_0\r\n  - cxxfilt=0.2.0=py37he1b5a44_0\r\n  - google-auth=1.11.2=py_0\r\n  - google-auth-oauthlib=0.4.1=py_2\r\n  - grpcio=1.27.2=py37hf8bcb03_0\r\n  - idna=2.8=py37_0\r\n  - importlib_metadata=1.5.0=py37_0\r\n  - ld_impl_linux-64=2.33.1=h53a641e_8\r\n  - libblas=3.8.0=15_openblas\r\n  - libcblas=3.8.0=15_openblas\r\n  - libffi=3.2.1=he1b5a44_1006\r\n  - libgcc-ng=9.2.0=h24d8f2e_2\r\n  - libgfortran-ng=7.3.0=hdf63c60_5\r\n  - liblapack=3.8.0=15_openblas\r\n  - libopenblas=0.3.8=h5ec1e0e_0\r\n  - libprotobuf=3.11.4=hd408876_0\r\n  - libstdcxx-ng=9.2.0=hdf63c60_2\r\n  - llvm-openmp=9.0.1=hc9558a2_2\r\n  - markdown=3.1.1=py37_0\r\n  - mkl=2020.0=166\r\n  - more-itertools=8.2.0=py_0\r\n  - ncurses=6.1=hf484d3e_1002\r\n  - ninja=1.10.0=hc9558a2_0\r\n  - numpy=1.18.1=py37h95a1406_0\r\n  - nvidia-apex=0.1=py37h8d9616a_1\r\n  - oauthlib=3.1.0=py_0\r\n  - openssl=1.1.1d=h7b6447c_4\r\n  - packaging=20.1=py_0\r\n  - pip=20.0.2=py_2\r\n  - pluggy=0.12.0=py_0\r\n  - protobuf=3.11.4=py37he6710b0_0\r\n  - py=1.8.1=py_0\r\n  - pyasn1=0.4.8=py_0\r\n  - pyasn1-modules=0.2.7=py_0\r\n  - pycparser=2.19=py_2\r\n  - pyjwt=1.7.1=py37_0\r\n  - pyopenssl=19.1.0=py37_0\r\n  - pyparsing=2.4.6=py_0\r\n  - pysocks=1.7.1=py37_0\r\n  - pytest=5.3.5=py37_1\r\n  - python=3.7.6=h357f687_2\r\n  - pytorch=1.2.0=py3.7_cuda10.0.130_cudnn7.6.2_0\r\n  - pyyaml=5.3=py37h516909a_0\r\n  - readline=8.0=hf8c457e_0\r\n  - requests=2.23.0=py37_0\r\n  - requests-oauthlib=1.3.0=py_0\r\n  - rsa=4.0=py_0\r\n  - setuptools=45.2.0=py37_0\r\n  - six=1.14.0=py37_0\r\n  - sqlite=3.30.1=hcee41ef_0\r\n  - tensorboard=2.1.0=py3_0\r\n  - tk=8.6.10=hed695b0_0\r\n  - tqdm=4.43.0=py_0\r\n  - urllib3=1.25.8=py37_0\r\n  - wcwidth=0.1.8=py_0\r\n  - werkzeug=1.0.0=py_0\r\n  - wheel=0.34.2=py_1\r\n  - xz=5.2.4=h14c3975_1001\r\n  - yaml=0.2.2=h516909a_1\r\n  - zipp=3.0.0=py_0\r\n  - zlib=1.2.11=h516909a_1006\r\n  - pip:\r\n    - cython==0.29.15\r\n    - fairseq==0.9.0\r\n    - portalocker==1.5.2\r\n    - regex==2020.2.20\r\n    - sacrebleu==1.4.3\r\n    - tensorboardx==2.0\r\n    - typing==3.7.4.1\r\nprefix: /space/group/nrc_ict/project/pkgs/ubuntu18/gcc-7.3.0/miniconda3/envs/fairseq-0.9.0\r\n```\r\n\r\n### Additional context\r\n\r\nAlso, the best checkpoint is not the right one since the best checkpoint is the checkpoint produced at epoch 1.\r\n\r\n```\r\nll -rt model/\r\ntotal 7274048\r\n-rw-r----- 1 sam037 nrc_ict 620776205 Mar 24 10:42 checkpoint_best.pt\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 15:56 checkpoint394.pt\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 16:00 checkpoint395.pt\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 16:05 checkpoint396.pt\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 16:09 checkpoint397.pt\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 16:13 checkpoint398.pt\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 16:18 checkpoint399.pt\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 16:22 checkpoint400.pt\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 16:27 checkpoint401.pt\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 16:31 checkpoint402.pt\r\ndrwxr-x--- 2 sam037 nrc_ict     49152 Mar 25 16:36 valid\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 16:36 checkpoint403.pt\r\n-rw-r----- 1 sam037 nrc_ict 620776722 Mar 25 16:36 checkpoint_last.pt\r\ndrwxr-x--- 2 sam037 nrc_ict     49152 Mar 25 16:36 train\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1910/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1910/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1903", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1903/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1903/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1903/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1903", "id": 587404806, "node_id": "MDU6SXNzdWU1ODc0MDQ4MDY=", "number": 1903, "title": "CNN evaluation on translation task gives unexpected keys error ", "user": {"login": "ShreyaKhurana", "id": 8307030, "node_id": "MDQ6VXNlcjgzMDcwMzA=", "avatar_url": "https://avatars.githubusercontent.com/u/8307030?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ShreyaKhurana", "html_url": "https://github.com/ShreyaKhurana", "followers_url": "https://api.github.com/users/ShreyaKhurana/followers", "following_url": "https://api.github.com/users/ShreyaKhurana/following{/other_user}", "gists_url": "https://api.github.com/users/ShreyaKhurana/gists{/gist_id}", "starred_url": "https://api.github.com/users/ShreyaKhurana/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ShreyaKhurana/subscriptions", "organizations_url": "https://api.github.com/users/ShreyaKhurana/orgs", "repos_url": "https://api.github.com/users/ShreyaKhurana/repos", "events_url": "https://api.github.com/users/ShreyaKhurana/events{/privacy}", "received_events_url": "https://api.github.com/users/ShreyaKhurana/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-03-25T02:51:49Z", "updated_at": "2020-05-04T16:23:19Z", "closed_at": "2020-05-04T16:23:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n\r\n#### What is your question?\r\n\r\nI followed the instructions to train the` fconv_wmt_en_de` model and then was trying to evaluate the checkpoint. But I get the following error (all layers with linearized_weight are not there in state_dict):\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/skhurana/fairseq/venv/bin/fairseq-generate\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')()\r\n  File \"/home/skhurana/fairseq/fairseq/fairseq_cli/generate.py\", line 234, in cli_main\r\n    main(args)\r\n  File \"/home/skhurana/fairseq/fairseq/fairseq_cli/generate.py\", line 35, in main\r\n    return _main(args, sys.stdout)\r\n  File \"/home/skhurana/fairseq/fairseq/fairseq_cli/generate.py\", line 71, in _main\r\n    task=task,\r\n  File \"/home/skhurana/fairseq/fairseq/fairseq/checkpoint_utils.py\", line 186, in load_model_ensemble\r\n    ensemble, args, _task = load_model_ensemble_and_task(filenames, arg_overrides, task)\r\n  File \"/home/skhurana/fairseq/fairseq/fairseq/checkpoint_utils.py\", line 205, in load_model_ensemble_and_task\r\n    model.load_state_dict(state[\"model\"], strict=True, args=args)\r\n  File \"/home/skhurana/fairseq/fairseq/fairseq/models/fairseq_model.py\", line 93, in load_state_dict\r\n    return super().load_state_dict(new_state_dict, strict)\r\n  File \"/home/skhurana/fairseq/venv/lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 830, in load_state_dict\r\n    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\nRuntimeError: Error(s) in loading state_dict for FConvModel:\r\n\tUnexpected key(s) in state_dict: \"decoder.convolutions.0._linearized_weight\", \"decoder.convolutions.1._linearized_weight\", \"decoder.convolutions.2._linearized_weight\", \"decoder.convolutions.3._linearized_weight\", \"decoder.convolutions.4._linearized_weight\", \"decoder.convolutions.5._linearized_weight\", \"decoder.convolutions.6._linearized_weight\", \"decoder.convolutions.7._linearized_weight\", \"decoder.convolutions.8._linearized_weight\", \"decoder.convolutions.9._linearized_weight\", \"decoder.convolutions.10._linearized_weight\", \"decoder.convolutions.11._linearized_weight\", \"decoder.convolutions.12._linearized_weight\", \"decoder.convolutions.13._linearized_weight\", \"decoder.convolutions.14._linearized_weight\". \r\n```\r\n\r\n#### Code\r\n\r\nI train the model on a single V100 GPU using this command\r\n\r\n```\r\nfairseq-train \\\r\n    data-bin/conv-large \\\r\n    --arch fconv_wmt_en_de \\\r\n    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4096 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --lr-scheduler fixed --force-anneal 50 \\\r\n    --save-interval 1 --max-epoch 10 \\\r\n    --save-dir checkpoints/conv-large \\\r\n    --eval-bleu     --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}'     --eval-bleu-detok space     --eval-bleu-remove-bpe     --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\r\n```\r\n\r\nand my evaluation command (which I run on CPU) is:\r\n\r\n```\r\nfairseq-generate data-bin/conv-large \\\r\n    --path checkpoints/conv-large/checkpoint_best.pt \\\r\n    --beam 5 --remove-bpe --cpu\r\n```\r\n\r\n#### What have you tried?\r\n\r\nTried using different dataset and but I get the same error. Also checked to see if it's an architecture-specific error - turns out transformer evaluation is okay, just the CNN throws this error.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.4.0+cpu on inference, 1.4.0 while training\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install --editable .\r\n - Python version: 3.6.8\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1903/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1881", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1881/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1881/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1881/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1881", "id": 585760015, "node_id": "MDU6SXNzdWU1ODU3NjAwMTU=", "number": 1881, "title": "alignment indices out of range for sequence of tokens", "user": {"login": "lhk", "id": 1475537, "node_id": "MDQ6VXNlcjE0NzU1Mzc=", "avatar_url": "https://avatars.githubusercontent.com/u/1475537?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhk", "html_url": "https://github.com/lhk", "followers_url": "https://api.github.com/users/lhk/followers", "following_url": "https://api.github.com/users/lhk/following{/other_user}", "gists_url": "https://api.github.com/users/lhk/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhk/subscriptions", "organizations_url": "https://api.github.com/users/lhk/orgs", "repos_url": "https://api.github.com/users/lhk/repos", "events_url": "https://api.github.com/users/lhk/events{/privacy}", "received_events_url": "https://api.github.com/users/lhk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2020-03-22T16:40:17Z", "updated_at": "2020-03-24T07:33:33Z", "closed_at": "2020-03-23T23:21:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nI would like to use the predicted alignment to match tokens in the in- and output.\r\n\r\n### To Reproduce\r\n\r\nTo get alignments, I have modified the hub_utils and changed the following line:\r\nhttps://github.com/pytorch/fairseq/blob/42f65d65776327598a2d3ded2e92e5818c70a125/fairseq/hub_utils.py#L133\r\n\r\nI replaced the line with the following code, which will return not only the tokens, but also the alignment:\r\n`        return list(zip([self.decode(hypos[0]['tokens']) for hypos in batched_hypos], [hypos[0]['alignment'] for hypos in batched_hypos]))`\r\n\r\nWith this change, the following code will work:\r\n```\r\nimport torch\r\n\r\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\r\n\r\nen = \"The sky had changed from clear, sunny cold, to driving sleet and mist. Wrapping myself in my shaggy jacket of the cloth called bearskin, I fought my way against the stubborn storm.\"\r\nen_tokens = en2fr.tokenize(en)\r\n\r\nfr, alignment = en2fr.translate(en, beam=1, print_alignment=True)\r\nfr_tokens = en2fr.tokenize(fr)\r\n\r\nen_tokens = en_tokens.split()\r\nfr_tokens = fr_tokens.split()\r\n\r\nfor a,b in alignment:\r\n    if a>=len(en_tokens) or b>=len(fr_tokens):\r\n        print('out of range')\r\n        continue\r\n    print(en_tokens[a], fr_tokens[b])\r\n\r\n```\r\n\r\nSome of the alignments make sense. For example, it is matching:\r\n\r\n - clear clair\r\n - sunny ensoleill\u00e9\r\n\r\nBut most of the time it outputs \"out of range\"\r\n### Expected behavior\r\n\r\nAlignment indices that are not out of range for the tokens. This doesn't necessarily mean that the alignments make perfect sense. But I would expect them to be at least valid for the given data.\r\n\r\n### Environment\r\n\r\n - fairseq Version: 0.9.0 \r\n - PyTorch Version: 1.4.0\r\n - OS: Linux\r\n - How you installed fairseq: pip\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 10.1 and 7.5\r\n - GPU models and configuration: T2000 (mobile gpu)\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1881/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1880", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1880/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1880/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1880/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1880", "id": 585729801, "node_id": "MDU6SXNzdWU1ODU3Mjk4MDE=", "number": 1880, "title": "Print Alignment in hub_utils not working", "user": {"login": "tonylekhtman", "id": 7457420, "node_id": "MDQ6VXNlcjc0NTc0MjA=", "avatar_url": "https://avatars.githubusercontent.com/u/7457420?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tonylekhtman", "html_url": "https://github.com/tonylekhtman", "followers_url": "https://api.github.com/users/tonylekhtman/followers", "following_url": "https://api.github.com/users/tonylekhtman/following{/other_user}", "gists_url": "https://api.github.com/users/tonylekhtman/gists{/gist_id}", "starred_url": "https://api.github.com/users/tonylekhtman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tonylekhtman/subscriptions", "organizations_url": "https://api.github.com/users/tonylekhtman/orgs", "repos_url": "https://api.github.com/users/tonylekhtman/repos", "events_url": "https://api.github.com/users/tonylekhtman/events{/privacy}", "received_events_url": "https://api.github.com/users/tonylekhtman/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-03-22T14:24:39Z", "updated_at": "2020-06-22T22:29:07Z", "closed_at": "2020-06-22T22:29:07Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI can't use the print_alignment option when using the TransformerModel translate option.\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n```\r\nfrom fairseq.models.transformer import TransformerModel\r\nzh2en = TransformerModel.from_pretrained(\r\n  '/path/to/checkpoints',\r\n  checkpoint_file='checkpoint_best.pt',\r\n  data_name_or_path='data-bin/wmt17_zh_en_full',\r\n  bpe='subword_nmt',\r\n  bpe_codes='data-bin/wmt17_zh_en_full/zh.code'\r\n)\r\nzh2en.translate('\u4f60\u597d \u4e16\u754c', verbose=True,print_alignment=True)\r\n```\r\n\r\n\r\n\r\n\r\n#### Code sample\r\n```\r\nfrom fairseq.models.transformer import TransformerModel\r\nzh2en = TransformerModel.from_pretrained(\r\n  '/path/to/checkpoints',\r\n  checkpoint_file='checkpoint_best.pt',\r\n  data_name_or_path='data-bin/wmt17_zh_en_full',\r\n  bpe='subword_nmt',\r\n  bpe_codes='data-bin/wmt17_zh_en_full/zh.code'\r\n)\r\nzh2en.translate('\u4f60\u597d \u4e16\u754c', verbose=True,print_alignment=True)\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI want it to print the alignment row to console just as the fairseq-generate script does.\r\n\r\n### Environment\r\n\r\n - fairseq 0.9\r\n - PyTorch 1.4\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Python version: 3.6.8\r\n - CUDA/cuDNN version: cuda 10.1\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1880/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1880/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1878", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1878/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1878/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1878/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1878", "id": 585682852, "node_id": "MDU6SXNzdWU1ODU2ODI4NTI=", "number": 1878, "title": "BART multi-gpu fine-tuning error", "user": {"login": "patil-suraj", "id": 27137566, "node_id": "MDQ6VXNlcjI3MTM3NTY2", "avatar_url": "https://avatars.githubusercontent.com/u/27137566?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patil-suraj", "html_url": "https://github.com/patil-suraj", "followers_url": "https://api.github.com/users/patil-suraj/followers", "following_url": "https://api.github.com/users/patil-suraj/following{/other_user}", "gists_url": "https://api.github.com/users/patil-suraj/gists{/gist_id}", "starred_url": "https://api.github.com/users/patil-suraj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patil-suraj/subscriptions", "organizations_url": "https://api.github.com/users/patil-suraj/orgs", "repos_url": "https://api.github.com/users/patil-suraj/repos", "events_url": "https://api.github.com/users/patil-suraj/events{/privacy}", "received_events_url": "https://api.github.com/users/patil-suraj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-22T10:06:37Z", "updated_at": "2020-03-24T03:06:33Z", "closed_at": "2020-03-24T03:06:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "the train.py gives this error while fine-tuning bart on CNN/DM\r\n\r\n```\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:16775\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 4): tcp://localhost:16775\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:16775\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 6): tcp://localhost:16775\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 6\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:16775\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 3\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 7): tcp://localhost:16775\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 7\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 5): tcp://localhost:16775\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:16775\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 5\r\n2020-03-22 10:03:55 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 1\r\n2020-03-22 10:03:56 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 2\r\n2020-03-22 10:03:56 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 4\r\n2020-03-22 10:03:56 | INFO | fairseq.distributed_utils | initialized host instance-3 as rank 0\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/home/surajp815/fairseq/fairseq_cli/train.py\", line 317, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/surajp815/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/surajp815/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 107, in join\r\n    (error_index, name)\r\nException: process 0 terminated with signal SIGKILL\r\n```\r\n\r\nI've installed fairseq from master branch\r\nTraining on 8 V100 GPU's\r\ntorch version 1.4.0 ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1878/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1878/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1866", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1866/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1866/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1866/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1866", "id": 584161575, "node_id": "MDU6SXNzdWU1ODQxNjE1NzU=", "number": 1866, "title": "Can't finetune roberta on wsc with the given bash script.", "user": {"login": "Guangxuan-Xiao", "id": 40906949, "node_id": "MDQ6VXNlcjQwOTA2OTQ5", "avatar_url": "https://avatars.githubusercontent.com/u/40906949?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Guangxuan-Xiao", "html_url": "https://github.com/Guangxuan-Xiao", "followers_url": "https://api.github.com/users/Guangxuan-Xiao/followers", "following_url": "https://api.github.com/users/Guangxuan-Xiao/following{/other_user}", "gists_url": "https://api.github.com/users/Guangxuan-Xiao/gists{/gist_id}", "starred_url": "https://api.github.com/users/Guangxuan-Xiao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Guangxuan-Xiao/subscriptions", "organizations_url": "https://api.github.com/users/Guangxuan-Xiao/orgs", "repos_url": "https://api.github.com/users/Guangxuan-Xiao/repos", "events_url": "https://api.github.com/users/Guangxuan-Xiao/events{/privacy}", "received_events_url": "https://api.github.com/users/Guangxuan-Xiao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-03-19T04:54:19Z", "updated_at": "2020-03-21T18:11:25Z", "closed_at": "2020-03-21T18:11:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nWhen I tried to reproducing roberta wsc and winogrande fintuing result with the bash script given in the README, it just couldn't work. It always shows that python is unable to infer criterion arguments and it requires me to implement WSCCriterion.build_criterion, but I don't know how.\r\n#### Code\r\n```bash\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-train WSC/ \\\r\n    --restore-file $ROBERTA_PATH \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --no-epoch-checkpoints --no-last-checkpoints --no-save-optimizer-state \\\r\n    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\r\n    --valid-subset val \\\r\n    --fp16 --ddp-backend no_c10d \\\r\n    --user-dir ${FAIRSEQ_USER_DIR} \\\r\n    --task wsc --criterion wsc --wsc-cross-entropy \\\r\n    --arch roberta_base --bpe gpt2 --max-positions 512 \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 \\\r\n    --lr-scheduler polynomial_decay --lr $LR \\\r\n    --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_NUM_UPDATES \\\r\n    --max-sentences $MAX_SENTENCES \\\r\n    --max-update $TOTAL_NUM_UPDATES \\\r\n    --log-format simple --log-interval 100 \\\r\n    --seed $SEED\r\n```\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nTo search in this repository desperately and find nothing.\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):0.9.0\r\n - PyTorch Version: 1.4.0\r\n - OS (e.g., Linux): Ubuntu\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: RTX 2080ti\r\n - Any other relevant information: \r\nThe full trackback:\r\n```\r\n2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11739\r\n2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11739\r\n2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:11739\r\n2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1\r\n2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2\r\n2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:11739\r\n2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3\r\n2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0\r\n| dictionary: 50265 types\r\n| dictionary: 50265 types\r\n2020-03-19 04:45:03 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='accuracy', bpe='gpt2', broadcast_buffers=False, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='wsc', curriculum=0, data='WSC/', dataset_impl=None, ddp_backend='no_c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:11739', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gpt2_encoder_json='https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', gpt2_vocab_bpe='https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe', init_token=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, log_format='simple', log_interval=100, lr=[2e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_positions=512, max_sentences=16, max_sentences_valid=16, max_tokens=None, max_tokens_valid=None, max_update=2000, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=False, no_save=False, no_save_optimizer_state=True, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, required_batch_size_multiple=8, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/xgx/Commonsense/test/model/roberta.base/model.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, save_predictions=None, seed=1, sentence_avg=False, skip_invalid_size_inputs_valid_test=False, task='wsc', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=2000, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir='/home/xgx/fairseq/examples/roberta/wsc', valid_subset='val', validate_interval=1, warmup_updates=250, weight_decay=0.01, wsc_cross_entropy=True, wsc_margin_alpha=1.0, wsc_margin_beta=0.0)\r\n| dictionary: 50265 types\r\n| dictionary: 50265 types\r\nTraceback (most recent call last):\r\n  File \"/home/xgx/miniconda3/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/home/xgx/fairseq/fairseq_cli/train.py\", line 317, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/xgx/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/xgx/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/xgx/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/xgx/fairseq/fairseq_cli/train.py\", line 286, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/xgx/fairseq/fairseq_cli/train.py\", line 63, in main\r\n    criterion = task.build_criterion(args)\r\n  File \"/home/xgx/fairseq/fairseq/tasks/fairseq_task.py\", line 226, in build_criterion\r\n    return criterions.build_criterion(args, self)\r\n  File \"/home/xgx/fairseq/fairseq/registry.py\", line 41, in build_x\r\n    return builder(args, *extra_args, **extra_kwargs)\r\n  File \"/home/xgx/fairseq/fairseq/criterions/fairseq_criterion.py\", line 56, in build_criterion\r\n    '{}.build_criterion'.format(cls.__name__)\r\nNotImplementedError: Unable to infer Criterion arguments, please implement WSCCriterion.build_criterion\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1866/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1866/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1865", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1865/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1865/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1865/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1865", "id": 584125571, "node_id": "MDU6SXNzdWU1ODQxMjU1NzE=", "number": 1865, "title": "Get Error when Execute BART Model Finetuning Code", "user": {"login": "areomoon", "id": 33310363, "node_id": "MDQ6VXNlcjMzMzEwMzYz", "avatar_url": "https://avatars.githubusercontent.com/u/33310363?v=4", "gravatar_id": "", "url": "https://api.github.com/users/areomoon", "html_url": "https://github.com/areomoon", "followers_url": "https://api.github.com/users/areomoon/followers", "following_url": "https://api.github.com/users/areomoon/following{/other_user}", "gists_url": "https://api.github.com/users/areomoon/gists{/gist_id}", "starred_url": "https://api.github.com/users/areomoon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/areomoon/subscriptions", "organizations_url": "https://api.github.com/users/areomoon/orgs", "repos_url": "https://api.github.com/users/areomoon/repos", "events_url": "https://api.github.com/users/areomoon/events{/privacy}", "received_events_url": "https://api.github.com/users/areomoon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-19T02:48:50Z", "updated_at": "2020-03-20T03:07:14Z", "closed_at": "2020-03-20T03:07:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nHi, thanks for open source the summarization model BART.  I am following on this [tutorial](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md) to finetune the BART model on CNN-DM dataset. However, I get the following error when execute the finetuning code. \r\n```\r\n2020-03-19 02:21:14 | INFO | fairseq_cli.train | model bart_large, criterion LabelSmoothedCrossEntropyCriterion\r\n2020-03-19 02:21:14 | INFO | fairseq_cli.train | num. model params: 406290432 (num. trained: 406290432)\r\n2020-03-19 02:21:18 | INFO | fairseq_cli.train | training on 1 GPUs\r\n2020-03-19 02:21:18 | INFO | fairseq_cli.train | max tokens per GPU = 2048 and max sentences per GPU = None\r\n2020-03-19 02:21:18 | INFO | fairseq.trainer | no existing checkpoint found /home/cdsw/acoe_auditnlp_bart/finetune_weights/model.pt\r\n2020-03-19 02:21:18 | INFO | fairseq.trainer | loading train data for epoch 0\r\n2020-03-19 02:21:18 | INFO | fairseq.data.data_utils | loaded 80431 examples from: /home/cdsw/acoe_auditnlp_bart/data/newsroom/newsroom-bin/train.source-target.source\r\n2020-03-19 02:21:18 | INFO | fairseq.data.data_utils | loaded 80431 examples from: /home/cdsw/acoe_auditnlp_bart/data/newsroom/newsroom-bin/train.source-target.target\r\n2020-03-19 02:21:18 | INFO | fairseq.tasks.translation | /home/cdsw/acoe_auditnlp_bart/data/newsroom/newsroom-bin train source-target 80431 examples\r\n2020-03-19 02:21:18 | WARNING | fairseq.data.data_utils | 20 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[23528, 76618, 76582, 57396, 76508, 54278, 76718, 57490, 6449, 72644]\r\nTraceback (most recent call last):\r\n  File \"/home/cdsw/.cache/torch/hub/pytorch_fairseq_master/fairseq/data/data_utils.py\", line 221, in batch_by_size\r\n    from fairseq.data.data_utils_fast import batch_by_size_fast\r\nModuleNotFoundError: No module named 'fairseq.data.data_utils_fast'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/home/cdsw/.cache/torch/hub/pytorch_fairseq_master/fairseq_cli/train.py\", line 318, in cli_main\r\n    main(args)\r\n  File \"/home/cdsw/.cache/torch/hub/pytorch_fairseq_master/fairseq_cli/train.py\", line 81, in main\r\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\r\n  File \"/home/cdsw/.cache/torch/hub/pytorch_fairseq_master/fairseq/checkpoint_utils.py\", line 153, in load_checkpoint\r\n    epoch=0, load_dataset=True, **passthrough_args\r\n  File \"/home/cdsw/.cache/torch/hub/pytorch_fairseq_master/fairseq/trainer.py\", line 267, in get_train_iterator\r\n    epoch=epoch,\r\n  File \"/home/cdsw/.cache/torch/hub/pytorch_fairseq_master/fairseq/tasks/fairseq_task.py\", line 181, in get_batch_iterator\r\n    required_batch_size_multiple=required_batch_size_multiple,\r\n  File \"/home/cdsw/.cache/torch/hub/pytorch_fairseq_master/fairseq/data/data_utils.py\", line 224, in batch_by_size\r\n    'Please build Cython components with: `pip install --editable .` '\r\nImportError: Please build Cython components with: `pip install --editable .` or `python setup.py build_ext --inplace`\r\n```\r\nAfter getting this code, I have tried ```pip3 install cython``` , But it doesn't work.\r\n\r\n### To Reproduce\r\n\r\n#### Code sample\r\n```\r\nCUDA_VISIBLE_DEVICES=0 python3 train.py \\\r\n    /home/cdsw/data/cnndm/cnndm-bin \\\r\n    --restore-file /home/cdsw/finetune_weights/model.pt \\\r\n    --max-tokens 2048 \\\r\n    --task translation \\\r\n    --source-lang source --target-lang target \\\r\n    --truncate-source \\\r\n    --layernorm-embedding \\\r\n    --share-all-embeddings \\\r\n    --share-decoder-input-output-embed \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --required-batch-size-multiple 1 \\\r\n    --arch bart_large \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --label-smoothing 0.1 \\\r\n    --dropout 0.1 --attention-dropout 0.1 \\\r\n    --weight-decay 0.01 --optimizer adam --adam-betas \"(0.9, 0.999)\" --adam-eps 1e-08 \\\r\n    --clip-norm 0.1 \\\r\n    --lr-scheduler polynomial_decay --lr 3e-05 --total-num-update 10000 --warmup-updates 500 \\\r\n    --fp16 --update-freq 4 \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --find-unused-parameters\r\n```\r\n\r\n### Expected behavior\r\n\r\n- Run without errors \r\n\r\n### Environment\r\n\r\n - fairseq Versio== 0.9.0 \r\n - PyTorch Version==1.4.0\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): pip3\r\n - Build command you used (if compiling from source): ``` pip3 install cython```\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: Tesla V100\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1865/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1863", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1863/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1863/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1863/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1863", "id": 583835213, "node_id": "MDU6SXNzdWU1ODM4MzUyMTM=", "number": 1863, "title": "Speech recognition fails due to NaN if there is an input of size 1", "user": {"login": "mgaido91", "id": 8821783, "node_id": "MDQ6VXNlcjg4MjE3ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/8821783?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mgaido91", "html_url": "https://github.com/mgaido91", "followers_url": "https://api.github.com/users/mgaido91/followers", "following_url": "https://api.github.com/users/mgaido91/following{/other_user}", "gists_url": "https://api.github.com/users/mgaido91/gists{/gist_id}", "starred_url": "https://api.github.com/users/mgaido91/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mgaido91/subscriptions", "organizations_url": "https://api.github.com/users/mgaido91/orgs", "repos_url": "https://api.github.com/users/mgaido91/repos", "events_url": "https://api.github.com/users/mgaido91/events{/privacy}", "received_events_url": "https://api.github.com/users/mgaido91/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-18T16:01:33Z", "updated_at": "2020-05-27T14:50:45Z", "closed_at": "2020-05-27T14:50:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIf there is an input sample with a single spectrogram, the training fails because everything is converted to NaN. This is because of the normalization which is applied to each item. Despite uncommon, this situation happens in common datasets, such as mozilla-voice.\r\n\r\n### To Reproduce\r\n\r\nRun your command on a dataset which has a sample with 1 spectrogram and if distributed the command fails in the check gradient; if not, the loss becomes always `NaN`.\r\n\r\n\r\n#### Code sample\r\n```\r\n\r\n>>> t=torch.tensor([[-0.7661, -1.3889, -2.0972, -0.9134, -0.7071, -0.9765, -0.8700, -0.8283,\r\n...           0.7512,  1.3211,  2.1532,  2.1174,  1.2800,  1.2633,  1.6147,  1.6322,\r\n...           2.0723,  3.1522,  3.2852,  2.2309,  2.5569,  2.2183,  2.2862,  1.5886,\r\n...           0.8773,  0.8725,  1.2662,  0.9899,  1.1069,  1.3926,  1.2795,  1.1199,\r\n...           1.1477,  1.2687,  1.3843,  1.1903,  0.8355,  1.1367,  1.2639,  1.4707]])\r\n>>> apply_mv_norm(t)\r\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\r\n```\r\n### Expected behavior\r\n\r\nDespite such input may be unexpected/wrong, the training shouldn't fail because of a single bad sample. The training should go on successfully, and the normalization should not cause `NaN`\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.4\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install -e .\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 10\r\n - GPU models and configuration: 8 K80\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1863/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1860", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1860/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1860/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1860/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1860", "id": 583521094, "node_id": "MDU6SXNzdWU1ODM1MjEwOTQ=", "number": 1860, "title": "wmt19 model cannot run on gpu except #0.", "user": {"login": "loopdigga96", "id": 10596055, "node_id": "MDQ6VXNlcjEwNTk2MDU1", "avatar_url": "https://avatars.githubusercontent.com/u/10596055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loopdigga96", "html_url": "https://github.com/loopdigga96", "followers_url": "https://api.github.com/users/loopdigga96/followers", "following_url": "https://api.github.com/users/loopdigga96/following{/other_user}", "gists_url": "https://api.github.com/users/loopdigga96/gists{/gist_id}", "starred_url": "https://api.github.com/users/loopdigga96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loopdigga96/subscriptions", "organizations_url": "https://api.github.com/users/loopdigga96/orgs", "repos_url": "https://api.github.com/users/loopdigga96/repos", "events_url": "https://api.github.com/users/loopdigga96/events{/privacy}", "received_events_url": "https://api.github.com/users/loopdigga96/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-03-18T07:22:18Z", "updated_at": "2020-10-11T03:59:14Z", "closed_at": "2020-03-21T18:16:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI am running [tutorial](https://github.com/pytorch/fairseq/tree/master/examples/wmt19). I successfully loaded model from hub and tried to run it on second gpu (id=1). Which raised an exception that data and model are stored on different GPUs. With gpu(id=0) works fine.\r\n```\r\nUsing cache found in /home/vlad/.cache/torch/hub/pytorch_fairseq_master\r\nLoading codes from /home/vlad/.cache/torch/pytorch_fairseq/0695ef328ddefcb8cbcfabc3196182f59c0e41e0468b10cc0db2ae9c91881fcc.bb1be17de4233e13870bd7d6065bfdb03fca0a51dd0f5d0b7edf5c188eda71f1/bpecodes ...\r\nRead 30000 codes from the codes file.\r\nTraceback (most recent call last):\r\n  File \"/home/vlad/Documents/coding/experiments/paper-analyzer-Tretyak_Internship/papers/experiments/scientific_generation/scripts/paraphrase.py\", line 46, in <module>\r\n    en2de.translate(['hello'])\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 126, in translate\r\n    return self.sample(sentences, beam, verbose, **kwargs)\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 132, in sample\r\n    batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 165, in generate\r\n    translations = self.task.inference_step(generator, self.models, batch)\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/tasks/fairseq_task.py\", line 351, in inference_step\r\n    return generator.generate(models, sample, prefix_tokens=prefix_tokens)\r\n  File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\r\n    return func(*args, **kwargs)\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 93, in generate\r\n    return self._generate(model, sample, **kwargs)\r\n  File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\r\n    return func(*args, **kwargs)\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 276, in _generate\r\n    tokens[:, :step + 1], encoder_outs, temperature=self.temperature,\r\n  File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\r\n    return func(*args, **kwargs)\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 549, in forward_decoder\r\n    temperature=temperature,\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 568, in _decode_one\r\n    tokens, encoder_out=encoder_out, incremental_state=self.incremental_states[model],\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/fairseq_model.py\", line 274, in forward_decoder\r\n    return self.decoder(prev_output_tokens, **kwargs)\r\n  File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer.py\", line 704, in forward\r\n    alignment_heads=alignment_heads,\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer.py\", line 807, in extract_features\r\n    need_head_weights=bool((idx == alignment_layer)),\r\n  File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/modules/transformer_layer.py\", line 297, in forward\r\n    need_head_weights=need_head_weights,\r\n  File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/modules/multihead_attention.py\", line 325, in forward\r\n    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float(\"-inf\")\r\nRuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generic/THCTensorMasked.cu:28\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n### To Reproduce\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n```\r\nimport torch\r\n\r\nen2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de',\r\n                       checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt',\r\n                       tokenizer='moses', bpe='fastbpe').to(torch.device('cuda:1'))\r\n\r\nresult = en2de.translate(['hello'])\r\n```\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\nRun without errors\r\n\r\n### Environment\r\n\r\n - fairseq Version==0.9.0\r\n - PyTorch Version ==1.4.0\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: RTX 2080 x 2\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1860/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1860/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1853", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1853/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1853/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1853/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1853", "id": 582760841, "node_id": "MDU6SXNzdWU1ODI3NjA4NDE=", "number": 1853, "title": "Fairseq-generate not reading max_positions from --max-len-a", "user": {"login": "zeeshansayyed", "id": 543495, "node_id": "MDQ6VXNlcjU0MzQ5NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/543495?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zeeshansayyed", "html_url": "https://github.com/zeeshansayyed", "followers_url": "https://api.github.com/users/zeeshansayyed/followers", "following_url": "https://api.github.com/users/zeeshansayyed/following{/other_user}", "gists_url": "https://api.github.com/users/zeeshansayyed/gists{/gist_id}", "starred_url": "https://api.github.com/users/zeeshansayyed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zeeshansayyed/subscriptions", "organizations_url": "https://api.github.com/users/zeeshansayyed/orgs", "repos_url": "https://api.github.com/users/zeeshansayyed/repos", "events_url": "https://api.github.com/users/zeeshansayyed/events{/privacy}", "received_events_url": "https://api.github.com/users/zeeshansayyed/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-03-17T05:15:20Z", "updated_at": "2020-11-16T06:18:36Z", "closed_at": "2020-03-20T05:53:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI trained a transformer model on a simple seq2seq task. Since, my source and target sequences are much larger than 1024, I used `--max-source-positions 3000` and `--max-target-positions 3000` while training. The model trains well. But when I am using fairseq-generate to make predictions on the test set, it throws an exception saying: `Size of sample #608 is invalid (=(1012, 1092)) since max_positions=(1024, 1024), skip this example with --skip-invalid-size-inputs-valid-test`.\r\n\r\nI am using the following command for fairseq-generate:\r\n\r\n```bash\r\nfairseq-generate \\\r\n    --path experiments/$EXP_NAME/checkpoints/checkpoint_best.pt \\\r\n    --quiet \\--max\r\n    --results-path experiments/$EXP_NAME/results \\\r\n    --beam 5 \\\r\n    --max-len-a 3000 \\\r\n    --max-len-b 3000 \\\r\n    data-bin/char_level/\r\n```\r\n\r\nIs this a Bug or am I missing something?\r\n\r\nThanks", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1853/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1853/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1833", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1833/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1833/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1833/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1833", "id": 580177175, "node_id": "MDU6SXNzdWU1ODAxNzcxNzU=", "number": 1833, "title": "Calculating perplexity throws OverflowError for high loss values", "user": {"login": "prihoda", "id": 2894124, "node_id": "MDQ6VXNlcjI4OTQxMjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/2894124?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prihoda", "html_url": "https://github.com/prihoda", "followers_url": "https://api.github.com/users/prihoda/followers", "following_url": "https://api.github.com/users/prihoda/following{/other_user}", "gists_url": "https://api.github.com/users/prihoda/gists{/gist_id}", "starred_url": "https://api.github.com/users/prihoda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prihoda/subscriptions", "organizations_url": "https://api.github.com/users/prihoda/orgs", "repos_url": "https://api.github.com/users/prihoda/repos", "events_url": "https://api.github.com/users/prihoda/events{/privacy}", "received_events_url": "https://api.github.com/users/prihoda/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-03-12T19:42:38Z", "updated_at": "2020-03-17T17:08:31Z", "closed_at": "2020-03-17T17:08:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nOverflowError is thrown when calculating perplexity for high losses\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run any task with high-enough loss (e.g. `sentence_prediction` task with `--regression-target` with very high values)\r\n2. Get an `OverflowError` after first training epoch:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/SFS/user/wp/prihodad/git/oas-training/data/examples/biophysical_properties/condaenv/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/SFS/user/wp/prihodad/git/fairseq/fairseq_cli/train.py\", line 321, in cli_main\r\n    main(args)\r\n  File \"/SFS/user/wp/prihodad/git/fairseq/fairseq_cli/train.py\", line 96, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/SFS/user/wp/prihodad/git/oas-training/condaenv/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/SFS/user/wp/prihodad/git/fairseq/fairseq_cli/train.py\", line 203, in train\r\n    stats = get_training_stats(metrics.get_smoothed_values('train'))\r\n  File \"/SFS/user/wp/prihodad/git/fairseq/fairseq_cli/train.py\", line 212, in get_training_stats\r\n    stats['ppl'] = utils.get_perplexity(stats['nll_loss'])\r\n  File \"/SFS/user/wp/prihodad/git/fairseq/fairseq/utils.py\", line 349, in get_perplexity\r\n    return safe_round(base**loss, round)\r\nOverflowError: (34, 'Numerical result out of range')\r\n```\r\n\r\n#### Code sample\r\nhttps://github.com/pytorch/fairseq/blob/5028ed1b6bedd526dee27ea731284f43e87303f0/fairseq/utils.py#L346-L349\r\n\r\n### Expected behavior\r\nShow 'inf' in case ppl causes an overflow error", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1833/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1827", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1827/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1827/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1827/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1827", "id": 579827609, "node_id": "MDU6SXNzdWU1Nzk4Mjc2MDk=", "number": 1827, "title": "Validation with sacrebleu tokenizes escaped UNK-tokens", "user": {"login": "villmow", "id": 2743060, "node_id": "MDQ6VXNlcjI3NDMwNjA=", "avatar_url": "https://avatars.githubusercontent.com/u/2743060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/villmow", "html_url": "https://github.com/villmow", "followers_url": "https://api.github.com/users/villmow/followers", "following_url": "https://api.github.com/users/villmow/following{/other_user}", "gists_url": "https://api.github.com/users/villmow/gists{/gist_id}", "starred_url": "https://api.github.com/users/villmow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/villmow/subscriptions", "organizations_url": "https://api.github.com/users/villmow/orgs", "repos_url": "https://api.github.com/users/villmow/repos", "events_url": "https://api.github.com/users/villmow/events{/privacy}", "received_events_url": "https://api.github.com/users/villmow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-03-12T10:10:00Z", "updated_at": "2020-05-31T14:55:34Z", "closed_at": "2020-05-04T16:22:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI'm on a translation task and do validation with BLEU during training. If `--eval-tokenized-bleu` is not set, scoring is done using the sacrebleu.DEFAULT_TOKENIZER. See these lines:\r\nhttps://github.com/pytorch/fairseq/blob/d7b0bf7e55b48466ee7e371a959668dfb8feaaf6/fairseq/tasks/translation.py#L348-L357\r\n\r\nUNK-tokens in references are escaped with `<<UNK>>` before scoring (line 351). This may produce a reference like this `This is a new <<UNK>>.`. In hypos the UNK-tokens are not escaped, leaving a hypothesis like this `This is a new <UNK>.`\r\n\r\n**Bug:** The default tokenizer of sacrebleu breaks the UNK-tokens (which is probably not wanted behaviour) and scores each token separately, altering the bleu scores.\r\n\r\n### To Reproduce\r\n\r\nFollow any translation example.\r\n\r\n#### Code sample\r\nThis is whats currently implemented:\r\n```python\r\n>>> hypo = \"This is a new <UNK>.\"\r\n>>> ref = \"This is a new <<UNK>>.\"\r\n>>> from sacrebleu import TOKENIZERS, DEFAULT_TOKENIZER\r\n>>> TOKENIZERS[DEFAULT_TOKENIZER](hypo)\r\n'This is a new < UNK > .'\r\n>>> TOKENIZERS[DEFAULT_TOKENIZER](ref)\r\n'This is a new < < UNK > > .'\r\n>>> sacrebleu.corpus_bleu([hypo], [[ref]], tokenize=DEFAULT_TOKENIZER).score\r\n55.96526475152516\r\n```\r\n\r\nThis is how I think these sequences should be scored:\r\n```python\r\n>>> hypo = \"This is a new ABC.\"\r\n>>> ref = \"This is a new UNK.\"\r\n>>> TOKENIZERS[DEFAULT_TOKENIZER](ref)\r\n'This is a new UNK .'\r\n>>> TOKENIZERS[DEFAULT_TOKENIZER](hypo)\r\n'This is a new ABC .'\r\n>>> sacrebleu.corpus_bleu([hypo], [[ref]], tokenize=DEFAULT_TOKENIZER).score\r\n53.7284965911771\r\n```\r\n### Expected behavior\r\n\r\nThe bleu score should not be influenced by the unknown token.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.2\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): source\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1827/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1827/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1826", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1826/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1826/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1826/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1826", "id": 579695298, "node_id": "MDU6SXNzdWU1Nzk2OTUyOTg=", "number": 1826, "title": "CUDA error when training with multiple GPUs and criterion's logging_outputs_can_be_summed() is False", "user": {"login": "freewym", "id": 3506322, "node_id": "MDQ6VXNlcjM1MDYzMjI=", "avatar_url": "https://avatars.githubusercontent.com/u/3506322?v=4", "gravatar_id": "", "url": "https://api.github.com/users/freewym", "html_url": "https://github.com/freewym", "followers_url": "https://api.github.com/users/freewym/followers", "following_url": "https://api.github.com/users/freewym/following{/other_user}", "gists_url": "https://api.github.com/users/freewym/gists{/gist_id}", "starred_url": "https://api.github.com/users/freewym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/freewym/subscriptions", "organizations_url": "https://api.github.com/users/freewym/orgs", "repos_url": "https://api.github.com/users/freewym/repos", "events_url": "https://api.github.com/users/freewym/events{/privacy}", "received_events_url": "https://api.github.com/users/freewym/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-03-12T05:04:42Z", "updated_at": "2020-03-21T20:54:15Z", "closed_at": "2020-03-21T18:15:44Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nCurrently almost all subclass of `FairseqCriterion` 's `logging_outputs_can_be_summed()` returns True, and it trains OK with multiple GPUs. But if I change, for example, `CrossEntropyCriterion`'s  `logging_outputs_can_be_summed ()` to return False and train with 2 GPUs , it gives error:\r\n\r\nFile \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/ywang/fairseq6/train.py\", line 286, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/ywang/fairseq6/train.py\", line 96, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/export/b02/ywang/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/ywang/fairseq6/train.py\", line 176, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/export/b02/ywang/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/ywang/fairseq6/fairseq/trainer.py\", line 347, in train_step\r\n    logging_outputs, sample_size, ooms, ignore=is_dummy_batch,\r\n  File \"/home/ywang/fairseq6/fairseq/trainer.py\", line 616, in _aggregate_logging_outputs\r\n    logging_outputs, *extra_stats_to_sum, ignore=ignore\r\n  File \"/home/ywang/fairseq6/fairseq/trainer.py\", line 634, in _all_gather_list_sync\r\n    max_size=getattr(self.args, 'all_gather_list_size', 16384),\r\n  File \"/home/ywang/fairseq6/fairseq/distributed_utils.py\", line 176, in all_gather_list\r\n    result.append(pickle.loads(bytes(out_buffer[header_size:header_size + enc_size].tolist())))\r\n  File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/storage.py\", line 134, in _load_from_bytes\r\n    return torch.load(io.BytesIO(b))\r\n  File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py\", line 529, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py\", line 702, in _legacy_load\r\n    result = unpickler.load()\r\n  File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py\", line 665, in persistent_load\r\n    deserialized_objects[root_key] = restore_location(obj, location)\r\n  File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py\", line 156, in default_restore_location\r\n    result = fn(storage, location)\r\n  File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py\", line 136, in _cuda_deserialize\r\n    return storage_type(obj.size())\r\n  File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 480, in _lazy_new\r\n    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\r\nRuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable\r\n\r\nThe reason why I want it to return False is, I have some stats in logging_output which is a tensor not scalar, to be aggregated in `reduce_matrices()`.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Just have `CrossEntropyCriterion.logging_outputs_can_be_summed()` to return False and train with multi-GPUs\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.4\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration:  GeForce GTX 1080\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1826/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1826/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1802", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1802/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1802/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1802/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1802", "id": 577567501, "node_id": "MDU6SXNzdWU1Nzc1Njc1MDE=", "number": 1802, "title": "AttributeError: 'SentencePredictionCriterion' object has no attribute 'args'", "user": {"login": "avi-jit", "id": 11348738, "node_id": "MDQ6VXNlcjExMzQ4NzM4", "avatar_url": "https://avatars.githubusercontent.com/u/11348738?v=4", "gravatar_id": "", "url": "https://api.github.com/users/avi-jit", "html_url": "https://github.com/avi-jit", "followers_url": "https://api.github.com/users/avi-jit/followers", "following_url": "https://api.github.com/users/avi-jit/following{/other_user}", "gists_url": "https://api.github.com/users/avi-jit/gists{/gist_id}", "starred_url": "https://api.github.com/users/avi-jit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/avi-jit/subscriptions", "organizations_url": "https://api.github.com/users/avi-jit/orgs", "repos_url": "https://api.github.com/users/avi-jit/repos", "events_url": "https://api.github.com/users/avi-jit/events{/privacy}", "received_events_url": "https://api.github.com/users/avi-jit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-03-08T21:16:49Z", "updated_at": "2020-03-09T13:02:41Z", "closed_at": "2020-03-09T13:02:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "#### AttributeError: 'SentencePredictionCriterion' object has no attribute 'args'\r\nI'm trying to finetune roberta by registering a new classification head, as explained here: [github.com/pytorch/fairseq/blob/master/examples/roberta/README.custom_classification.md](github.com/pytorch/fairseq/blob/master/examples/roberta/README.custom_classification.md). I've tokenized and preprocessed my data as described but the training phase is where I'm stuck.\r\n\r\n#### Code & Output\r\n\r\nHere's the execution script and arguments:\r\n\r\n```\r\nWARMUP_UPDATES=479\r\nTOTAL_NUM_UPDATES=7812\r\nLR=1e-05\r\nHEAD_NAME=head_name\r\nNUM_CLASSES=1014\r\nMAX_SENTENCES=2\r\nROBERTA_PATH=roberta.large/model.pt \r\nCUDA_VISIBLE_DEVICES=0\r\nDATA_DIR=../data/\r\nMAX_TOKENS=4\r\n\r\npython train.py $DATA_DIR \\\r\n--restore-file $ROBERTA_PATH --max-positions 512 --max-sentences $MAX_SENTENCES \\\r\n--max-tokens $MAX_TOKENS --task sentence_prediction --reset-optimizer \\\r\n--reset-dataloader --reset-meters --required-batch-size-multiple 1 --init-token 0 \\\r\n--separator-token 2 --arch roberta_large --criterion sentence_prediction \\\r\n--classification-head-name $HEAD_NAME --num-classes $NUM_CLASSES --dropout 0.1 \\\r\n--attention-dropout 0.1 --weight-decay 0.1 --optimizer adam \\\r\n--adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 --clip-norm 0.0 \\\r\n--lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES \\\r\n--warmup-updates $WARMUP_UPDATES --fp16 --fp16-init-scale 4 \\\r\n--threshold-loss-scale 1 --fp16-scale-window 128 --max-epoch 10 \\\r\n--best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\r\n--truncate-sequence --find-unused-parameters --update-freq 4 \r\n```\r\n\r\nAnd here's the error trace:\r\n\r\n```\r\n2020-03-08 13:40:04 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, add_prev_output_tokens=False, all_gather_list_size=16384, arch='roberta_large', attention_dropout=0.1, best_checkpoint_metric='accuracy', bpe=None, broadcast_buffers=False, bucket_cap_mb=25, classification_head_name='head_name', clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='../data/', dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=24, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=128, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, log_format='simple', log_interval=100, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_positions=512, max_sentences=2, max_sentences_valid=2, max_tokens=4, max_tokens_valid=4, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_shuffle=False, num_classes=1014, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, regression_target=False, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='roberta.large/model.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, separator_token=2, skip_invalid_size_inputs_valid_test=False, task='sentence_prediction', tensorboard_logdir='', threshold_loss_scale=1.0, tokenizer=None, total_num_update=7812, train_subset='train', truncate_sequence=True, update_freq=[4], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=479, weight_decay=0.1)\r\n2020-03-08 13:40:04 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\r\n2020-03-08 13:40:04 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 1009 types\r\n2020-03-08 13:40:04 | INFO | fairseq.data.data_utils | loaded 40151 examples from: ../data/input0/valid\r\n2020-03-08 13:40:04 | INFO | fairseq.data.data_utils | loaded 40151 examples from: ../data/label/valid\r\n2020-03-08 13:40:04 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 40151\r\n2020-03-08 13:40:11 | INFO | fairseq_cli.train | RobertaModel \r\n```\r\n... I'm removing the model specs (layer wise) since it's too long ...\r\n```\r\n2020-03-08 13:40:11 | INFO | fairseq_cli.train | model roberta_large, criterion SentencePredictionCriterion\r\n2020-03-08 13:40:11 | INFO | fairseq_cli.train | num. model params: 357499983 (num. trained: 357499983)\r\n2020-03-08 13:40:11 | INFO | fairseq_cli.train | training on 1 GPUs\r\n2020-03-08 13:40:11 | INFO | fairseq_cli.train | max tokens per GPU = 4 and max sentences per GPU = 2\r\n2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.dense.weight\r\n2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.dense.bias\r\n2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.out_proj.weight\r\n2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.out_proj.bias\r\n2020-03-08 13:40:11 | INFO | fairseq.trainer | loaded checkpoint roberta.large/model.pt (epoch 1 @ 0 updates)\r\n2020-03-08 13:40:11 | INFO | fairseq.trainer | loading train data for epoch 1\r\n2020-03-08 13:40:11 | INFO | fairseq.data.data_utils | loaded 160600 examples from: ../data/input0/train\r\n2020-03-08 13:40:11 | INFO | fairseq.data.data_utils | loaded 160600 examples from: ../data/label/train\r\n2020-03-08 13:40:11 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 160600\r\n2020-03-08 13:40:11 | WARNING | fairseq.data.data_utils | 160580 samples have invalid sizes and will be skipped, max_positions=4, first few sample ids=[141335, 12811, 122058, 104432, 1925, 141500, 65517, 143950, 59283, 155828]\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/nas/home/thawani/MCS/fairseq/fairseq_cli/train.py\", line 322, in cli_main\r\n    main(args)\r\n  File \"/nas/home/thawani/MCS/fairseq/fairseq_cli/train.py\", line 100, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/nas/home/thawani/anaconda3/envs/env/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/nas/home/thawani/MCS/fairseq/fairseq_cli/train.py\", line 177, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/nas/home/thawani/anaconda3/envs/env/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/nas/home/thawani/MCS/fairseq/fairseq/trainer.py\", line 319, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/nas/home/thawani/MCS/fairseq/fairseq/tasks/fairseq_task.py\", line 337, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/nas/home/thawani/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/nas/home/thawani/MCS/fairseq/fairseq/criterions/sentence_prediction.py\", line 40, in forward\r\n    and self.args.classification_head_name in model.classification_heads\r\n  File \"/nas/home/thawani/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 576, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: 'SentencePredictionCriterion' object has no attribute 'args'\r\n\r\n```\r\n\r\n#### What have you tried?\r\n1. creating new environment and trying again from scratch: same issue.\r\n2. tracing the error through the code: I can't make heads or tails out of it. \r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: 0.9.0\r\n - PyTorch Version: 1.4.0\r\n - OS: CentOS Linux release 7.7.1908\r\n - How you installed fairseq: source\r\n - Build command used: pip install --editable .\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: NVIDIA TU102\r\n - Any other relevant information: I'm using a conda enviornment\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1802/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1802/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1794", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1794/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1794/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1794/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1794", "id": 577280672, "node_id": "MDU6SXNzdWU1NzcyODA2NzI=", "number": 1794, "title": "NameError: name '_tensorboard_writers' is not defined", "user": {"login": "Cathy-t", "id": 39083599, "node_id": "MDQ6VXNlcjM5MDgzNTk5", "avatar_url": "https://avatars.githubusercontent.com/u/39083599?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Cathy-t", "html_url": "https://github.com/Cathy-t", "followers_url": "https://api.github.com/users/Cathy-t/followers", "following_url": "https://api.github.com/users/Cathy-t/following{/other_user}", "gists_url": "https://api.github.com/users/Cathy-t/gists{/gist_id}", "starred_url": "https://api.github.com/users/Cathy-t/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Cathy-t/subscriptions", "organizations_url": "https://api.github.com/users/Cathy-t/orgs", "repos_url": "https://api.github.com/users/Cathy-t/repos", "events_url": "https://api.github.com/users/Cathy-t/events{/privacy}", "received_events_url": "https://api.github.com/users/Cathy-t/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-03-07T05:16:16Z", "updated_at": "2020-12-13T23:17:56Z", "closed_at": "2020-03-07T17:15:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\nWhen I try to run the transformer example\r\n#### Code\r\ntorch.hub.list('pytorch/fairseq')  \r\nen2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt16.en-de', tokenizer='moses', bpe='subword_nmt')\r\nen2de.eval()  # disable dropout\r\nassert isinstance(en2de.models[0], TransformerModel)\r\n\r\nen2de.translate('Hello world!')\r\n\r\nen2de.translate(['Hello world!', 'The cat sat on the mat.'])\r\n\r\nsomething wrong occured:\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/home/cat/PycharmProjects/test_pro/text_dispose/fairseq/fairseq/logging/progress_bar.py\", line 299, in _close_writers\r\n    for w in _tensorboard_writers.values():\r\nNameError: name '_tensorboard_writers' is not defined\r\n\r\n#### My environment\r\n - fairseq Version (master)\r\n - PyTorch Version (1.4.0)\r\n - OS ( Linux)\r\n - Installed fairseq by source\r\n - Python version: 3.7\r\n\r\nCould someone help me ?", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1794/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1794/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1791", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1791/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1791/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1791/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1791", "id": 577039744, "node_id": "MDU6SXNzdWU1NzcwMzk3NDQ=", "number": 1791, "title": "Sequence generation fails if the encoder outputs a sequence with lenght different from input", "user": {"login": "mgaido91", "id": 8821783, "node_id": "MDQ6VXNlcjg4MjE3ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/8821783?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mgaido91", "html_url": "https://github.com/mgaido91", "followers_url": "https://api.github.com/users/mgaido91/followers", "following_url": "https://api.github.com/users/mgaido91/following{/other_user}", "gists_url": "https://api.github.com/users/mgaido91/gists{/gist_id}", "starred_url": "https://api.github.com/users/mgaido91/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mgaido91/subscriptions", "organizations_url": "https://api.github.com/users/mgaido91/orgs", "repos_url": "https://api.github.com/users/mgaido91/repos", "events_url": "https://api.github.com/users/mgaido91/events{/privacy}", "received_events_url": "https://api.github.com/users/mgaido91/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-03-06T16:27:03Z", "updated_at": "2020-03-10T18:51:18Z", "closed_at": "2020-03-10T18:51:18Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nThe problem is present when there is a model in which the encoder outputs a sequence having a length different from the input one. This is very common in speech processing models which contain convolutions reducing the size of the input.\r\n\r\nWith such a model, the following command fails:\r\n\r\n```\r\npython generate.py  $DATA_DIR \\\r\n    --gen-subset test \\\r\n    --path $MY_MODEL_PATH \\\r\n    --max-source-positions 2000 --max-target-positions 1000 \\\r\n    --task speech_recognition --user-dir examples/speech_recognition \\\r\n    --skip-invalid-size-inputs-valid-test\r\n```\r\n\r\nwith the following stacktrace:\r\n\r\n```\r\n  File \".../fairseq/tasks/fairseq_task.py\", line 317, in inference_step\r\n    return generator.generate(models, sample, prefix_tokens=prefix_tokens)\r\n  File \".../python3.7/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\r\n    return func(*args, **kwargs)\r\n  File \".../fairseq/sequence_generator.py\", line 92, in generate\r\n    return self._generate(model, sample, **kwargs)\r\n  File \".../python3.7/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\r\n    return func(*args, **kwargs)\r\n  File .../fairseq/sequence_generator.py\", line 335, in _generate\r\n    attn[:, :, step + 1].copy_(avg_attn_scores)\r\nRuntimeError: The size of tensor a (1069) must match the size of tensor b (268) at non-singleton dimension 1\r\n```\r\n\r\n\r\n#### Code sample\r\n\r\nA very simple reproducer is provided as unit test in the PR related to this issue.\r\n\r\n### Expected behavior\r\n\r\nThe expected behavior is that the generation works fine without failures.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.4\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install -e .`\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: K80\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nIn the failure described in the command above I am using an implementation of https://www.aclweb.org/anthology/W19-6603.pdf.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1791/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1777", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1777/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1777/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1777/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1777", "id": 576005857, "node_id": "MDU6SXNzdWU1NzYwMDU4NTc=", "number": 1777, "title": "Error preprocessing multi-lang translation: prepare-iwslt17-multilingual.sh", "user": {"login": "kaiidams", "id": 34785393, "node_id": "MDQ6VXNlcjM0Nzg1Mzkz", "avatar_url": "https://avatars.githubusercontent.com/u/34785393?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kaiidams", "html_url": "https://github.com/kaiidams", "followers_url": "https://api.github.com/users/kaiidams/followers", "following_url": "https://api.github.com/users/kaiidams/following{/other_user}", "gists_url": "https://api.github.com/users/kaiidams/gists{/gist_id}", "starred_url": "https://api.github.com/users/kaiidams/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kaiidams/subscriptions", "organizations_url": "https://api.github.com/users/kaiidams/orgs", "repos_url": "https://api.github.com/users/kaiidams/repos", "events_url": "https://api.github.com/users/kaiidams/events{/privacy}", "received_events_url": "https://api.github.com/users/kaiidams/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 679100395, "node_id": "MDU6TGFiZWw2NzkxMDAzOTU=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/help%20wanted", "name": "help wanted", "color": "128A0C", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-03-05T05:39:16Z", "updated_at": "2020-03-08T00:04:55Z", "closed_at": "2020-03-08T00:04:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n`bash prepare-iwslt17-multilingual.sh` fails with errors.\r\n\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: './iwslt17.de_fr.en.bpe16k/valid.fr-en.fr'\r\n```\r\n\r\n### To Reproduce\r\n\r\n```\r\ncd examples/translation\r\nbash prepare-iwslt17-multilingual.sh\r\n```\r\n\r\n#### Code sample\r\n\r\nDid not change fairseq code\r\n\r\n### Expected behavior\r\n\r\nThe script should succeed without error\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master (commit cb2dc414c692d7de283bec4e4f9c923a66205792)\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install --editable .`\r\n - Python version: 3.7.5\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: NVIDIA K80\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nThe script produces files named such as `valid0.fr-en.fr`, `valid1.fr-en.fr`, ..., however later the script expects `valid.fr-en.fr` without an index number. \r\nThe change is by the commit 1da061f37f444e492f9cd9c7d058e35dcbf4d2c3.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1777/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1777/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1770", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1770/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1770/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1770/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1770", "id": 575321723, "node_id": "MDU6SXNzdWU1NzUzMjE3MjM=", "number": 1770, "title": "TypeError: diverse_beam_strength", "user": {"login": "taku-ito", "id": 25680590, "node_id": "MDQ6VXNlcjI1NjgwNTkw", "avatar_url": "https://avatars.githubusercontent.com/u/25680590?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taku-ito", "html_url": "https://github.com/taku-ito", "followers_url": "https://api.github.com/users/taku-ito/followers", "following_url": "https://api.github.com/users/taku-ito/following{/other_user}", "gists_url": "https://api.github.com/users/taku-ito/gists{/gist_id}", "starred_url": "https://api.github.com/users/taku-ito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taku-ito/subscriptions", "organizations_url": "https://api.github.com/users/taku-ito/orgs", "repos_url": "https://api.github.com/users/taku-ito/repos", "events_url": "https://api.github.com/users/taku-ito/events{/privacy}", "received_events_url": "https://api.github.com/users/taku-ito/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-03-04T11:16:54Z", "updated_at": "2020-03-04T20:45:18Z", "closed_at": "2020-03-04T20:45:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\ndiverse_beam_strength is a tuple, but does not currently support tuples.\r\nAre you expanding diverse beam search? Or is it wrong to be a tuple?\r\n\r\nhttps://github.com/pytorch/fairseq/blob/3335de5f441ee1b3824e16dcd98db620e40beaba/fairseq/tasks/fairseq_task.py#L247\r\n\r\nhttps://github.com/pytorch/fairseq/blob/244835d811c2c66b1de2c5e86532bac41b154c1a/fairseq/search.py#L114\r\n### Environment\r\n\r\n - fairseq Version (master):\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1770/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1770/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1766", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1766/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1766/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1766/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1766", "id": 574466982, "node_id": "MDU6SXNzdWU1NzQ0NjY5ODI=", "number": 1766, "title": "BART fine tuning pauses ", "user": {"login": "tuhinjubcse", "id": 3104771, "node_id": "MDQ6VXNlcjMxMDQ3NzE=", "avatar_url": "https://avatars.githubusercontent.com/u/3104771?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tuhinjubcse", "html_url": "https://github.com/tuhinjubcse", "followers_url": "https://api.github.com/users/tuhinjubcse/followers", "following_url": "https://api.github.com/users/tuhinjubcse/following{/other_user}", "gists_url": "https://api.github.com/users/tuhinjubcse/gists{/gist_id}", "starred_url": "https://api.github.com/users/tuhinjubcse/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tuhinjubcse/subscriptions", "organizations_url": "https://api.github.com/users/tuhinjubcse/orgs", "repos_url": "https://api.github.com/users/tuhinjubcse/repos", "events_url": "https://api.github.com/users/tuhinjubcse/events{/privacy}", "received_events_url": "https://api.github.com/users/tuhinjubcse/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-03-03T07:21:46Z", "updated_at": "2020-03-05T15:52:00Z", "closed_at": "2020-03-05T15:52:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\nTried finetuning BART and it pauses in epoch 001 a 3%\r\nscript used\r\n\r\nTOTAL_NUM_UPDATES=20000\r\nWARMUP_UPDATES=500\r\nLR=3e-05\r\nAX_TOKENS=1024\r\nUPDATE_FREQ=16\r\nBART_PATH=/nas/home/tuhinc/fairseq/bart.large/model.pt\r\n\r\npython train.py fullstory\\\r\n    --restore-file $BART_PATH \\\r\n    --max-tokens $MAX_TOKENS \\\r\n    --task translation \\\r\n    --source-lang source --target-lang target \\\r\n    --truncate-source \\\r\n    --layernorm-embedding \\\r\n    --share-all-embeddings \\\r\n    --share-decoder-input-output-embed \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --required-batch-size-multiple 1 \\\r\n    --arch bart_large \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --label-smoothing 0.1 \\\r\n    --dropout 0.1 --attention-dropout 0.1 \\\r\n    --weight-decay 0.01 --optimizer adam --adam-betas \"(0.9, 0.999)\" --adam-eps 1e-08 \\\r\n    --clip-norm 0.1 \\\r\n    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\r\n    --memory-efficient-fp16 --update-freq $UPDATE_FREQ \\\r\n    --save-dir \"checkpoint-fullstory\" \\\r\n    --ddp-backend=no_c10d  \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --find-unused-parameters;\r\n~                               \r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI have finetuned BART previously n this never happened. The length of individual samples have increased as I was training previously on truncated version of same dataset\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0) 1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.3\r\n - CUDA/cuDNN version: 10.1.243\r\n - GPU models and configuration: RTX 2080 ( 4 gpus)\r\n\r\n![image](https://user-images.githubusercontent.com/3104771/75778944-9a207d80-5d0d-11ea-8b13-dcc62cc28edb.png)\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1766/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1766/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1754", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1754/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1754/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1754/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1754", "id": 572435169, "node_id": "MDU6SXNzdWU1NzI0MzUxNjk=", "number": 1754, "title": "errors trying to decode with mbart model", "user": {"login": "mjpost", "id": 455256, "node_id": "MDQ6VXNlcjQ1NTI1Ng==", "avatar_url": "https://avatars.githubusercontent.com/u/455256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mjpost", "html_url": "https://github.com/mjpost", "followers_url": "https://api.github.com/users/mjpost/followers", "following_url": "https://api.github.com/users/mjpost/following{/other_user}", "gists_url": "https://api.github.com/users/mjpost/gists{/gist_id}", "starred_url": "https://api.github.com/users/mjpost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mjpost/subscriptions", "organizations_url": "https://api.github.com/users/mjpost/orgs", "repos_url": "https://api.github.com/users/mjpost/repos", "events_url": "https://api.github.com/users/mjpost/events{/privacy}", "received_events_url": "https://api.github.com/users/mjpost/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-02-28T00:00:27Z", "updated_at": "2020-06-09T20:22:13Z", "closed_at": "2020-02-28T15:39:17Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThanks for [releasing](https://github.com/pytorch/fairseq/commit/5e79322b3a4a9e9a11525377d3dda7ac520b921c) the mbart models! I am trying to decode with the pretrained model for use as a baseline, but am running into a few problems:\r\n\r\n- Can you clarify where the language codes go?\r\n   The paper suggests that something like `<en>` is *appended* to the encoder and (at training time) *prefixed* to the target side. Is this correct?\r\n- However, [the README](https://github.com/pytorch/fairseq/tree/master/examples/mbart#generate-on-en-ro) implies the code is more like `[en_US]` or something\r\n- How do I translate to a specific language? I would have expected to have to force-decode to the prefix (via `--prefix 1` or something) but this isn't clear. Perhaps the language code (`-t`) is used implicitly in the task?\r\n\r\nFurthermore, I cannot run the model to test this. When running with the latest fairseq, I get the following error:\r\n\r\n```\r\nRuntimeError: Error(s) in loading state_dict for BARTModel:\r\n        Unexpected key(s) in state_dict: \"encoder.layernorm_embedding.weight\", \"encoder.layernorm_embedding.bias\", \"decoder.layernorm_embedding.weight\", \"decoder.layernorm_embedding.bias\".\r\n```\r\n\r\nThis suggests to me that I am doing something wrong or that some code was not committed.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\n```bash\r\ninfile=wmt19.en\r\nreffile=wmt19.de\r\noutfile=out.wmt19.de\r\n\r\nsacrebleu -t wmt19 -l en-de --echo src | head -n 10 > $infile\r\nsacrebleu -t wmt19 -l en-de --echo ref | head -n 10 > $reffile\r\n\r\n# constants\r\nlangs=ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN\r\nMODELDIR=./cc25_pretrain\r\nDICT=$MODELDIR/dict.txt\r\nexport FAIRSEQ=~/code/fairseq\r\n# end constants\r\n\r\ntmpdir=$(mktemp -d --tmpdir=/expscratch/$USER)\r\n\r\nSRC=en_XX\r\nTRG=de_DE\r\n\r\ncat $infile | spm_encode --model $MODELDIR/sentence.bpe.model > $tmpdir/data.spm.$SRC\r\ncat $reffile | spm_encode --model $MODELDIR/sentence.bpe.model > $tmpdir/data.spm.$SRC\r\n\r\ncp $tmpdir/data.spm.$SRC $tmpdir/data.spm.$TRG\r\n\r\npython3 $FAIRSEQ/preprocess.py \\\r\n  --source-lang $SRC \\\r\n  --target-lang $TRG \\\r\n  --testpref $tmpdir/data.spm  \\\r\n  --destdir $tmpdir \\\r\n  --thresholdtgt 0 \\\r\n  --thresholdsrc 0 \\\r\n  --srcdict ${DICT} \\\r\n  --tgtdict ${DICT} \\\r\n  --workers 70\r\n\r\npython3 $FAIRSEQ/generate.py $tmpdir \\\r\n  --path $MODELDIR/model.pt \\\r\n  --task translation_from_pretrained_bart \\\r\n  --gen-subset test \\\r\n  -s $SRC \\\r\n  -t $TRG \\\r\n  --remove-bpe 'sentencepiece' \\\r\n  --max-sentences 32 \\\r\n  --langs $langs > $outfile\r\n```\r\n\r\nThis dies with the above-reported error.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): latest github\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): CentOS 7.5\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install --editable` (from within a conda env)\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 10.1 / 7.6.3\r\n - GPU models and configuration: Titan RTX", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1754/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1754/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1748", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1748/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1748/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1748/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1748", "id": 570955066, "node_id": "MDU6SXNzdWU1NzA5NTUwNjY=", "number": 1748, "title": "DenoisingTask object has no attribute build_dataset_for_inference", "user": {"login": "dugu9sword", "id": 6239743, "node_id": "MDQ6VXNlcjYyMzk3NDM=", "avatar_url": "https://avatars.githubusercontent.com/u/6239743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dugu9sword", "html_url": "https://github.com/dugu9sword", "followers_url": "https://api.github.com/users/dugu9sword/followers", "following_url": "https://api.github.com/users/dugu9sword/following{/other_user}", "gists_url": "https://api.github.com/users/dugu9sword/gists{/gist_id}", "starred_url": "https://api.github.com/users/dugu9sword/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dugu9sword/subscriptions", "organizations_url": "https://api.github.com/users/dugu9sword/orgs", "repos_url": "https://api.github.com/users/dugu9sword/repos", "events_url": "https://api.github.com/users/dugu9sword/events{/privacy}", "received_events_url": "https://api.github.com/users/dugu9sword/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "yinhanliu", "id": 39971269, "node_id": "MDQ6VXNlcjM5OTcxMjY5", "avatar_url": "https://avatars.githubusercontent.com/u/39971269?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yinhanliu", "html_url": "https://github.com/yinhanliu", "followers_url": "https://api.github.com/users/yinhanliu/followers", "following_url": "https://api.github.com/users/yinhanliu/following{/other_user}", "gists_url": "https://api.github.com/users/yinhanliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yinhanliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yinhanliu/subscriptions", "organizations_url": "https://api.github.com/users/yinhanliu/orgs", "repos_url": "https://api.github.com/users/yinhanliu/repos", "events_url": "https://api.github.com/users/yinhanliu/events{/privacy}", "received_events_url": "https://api.github.com/users/yinhanliu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yinhanliu", "id": 39971269, "node_id": "MDQ6VXNlcjM5OTcxMjY5", "avatar_url": "https://avatars.githubusercontent.com/u/39971269?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yinhanliu", "html_url": "https://github.com/yinhanliu", "followers_url": "https://api.github.com/users/yinhanliu/followers", "following_url": "https://api.github.com/users/yinhanliu/following{/other_user}", "gists_url": "https://api.github.com/users/yinhanliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yinhanliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yinhanliu/subscriptions", "organizations_url": "https://api.github.com/users/yinhanliu/orgs", "repos_url": "https://api.github.com/users/yinhanliu/repos", "events_url": "https://api.github.com/users/yinhanliu/events{/privacy}", "received_events_url": "https://api.github.com/users/yinhanliu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2020-02-26T00:27:58Z", "updated_at": "2021-07-08T23:30:11Z", "closed_at": "2020-10-22T19:46:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nWhen I run the code below, an error raises that `AttributeError: 'DenoisingTask' object has no attribute 'build_dataset_for_inference'`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n```\r\nimport torch\r\nbart = torch.hub.load('pytorch/fairseq', 'bart.large')\r\nbart.sample(['Hello world!'])\r\n```", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1748/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1748/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1734", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1734/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1734/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1734/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1734", "id": 568935904, "node_id": "MDU6SXNzdWU1Njg5MzU5MDQ=", "number": 1734, "title": "sqrt(): argument 'input' (position 1) must be Tensor, not int", "user": {"login": "slusarczyk41", "id": 36066877, "node_id": "MDQ6VXNlcjM2MDY2ODc3", "avatar_url": "https://avatars.githubusercontent.com/u/36066877?v=4", "gravatar_id": "", "url": "https://api.github.com/users/slusarczyk41", "html_url": "https://github.com/slusarczyk41", "followers_url": "https://api.github.com/users/slusarczyk41/followers", "following_url": "https://api.github.com/users/slusarczyk41/following{/other_user}", "gists_url": "https://api.github.com/users/slusarczyk41/gists{/gist_id}", "starred_url": "https://api.github.com/users/slusarczyk41/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/slusarczyk41/subscriptions", "organizations_url": "https://api.github.com/users/slusarczyk41/orgs", "repos_url": "https://api.github.com/users/slusarczyk41/repos", "events_url": "https://api.github.com/users/slusarczyk41/events{/privacy}", "received_events_url": "https://api.github.com/users/slusarczyk41/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1756261282, "node_id": "MDU6TGFiZWwxNzU2MjYxMjgy", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/unable%20to%20repro", "name": "unable to repro", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-02-21T12:48:18Z", "updated_at": "2020-02-23T09:27:21Z", "closed_at": "2020-02-23T09:27:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen using fairseq-train with RoBERTa model after 37 CUDA OOM's I get this error:\r\nTypeError: sqrt(): argument 'input' (position 1) must be Tensor, not int\r\n\r\n### Cmd I used\r\n```\r\nfairseq-train 'data-bin/agora' \\\r\n    --task masked_lm --criterion masked_lm \\\r\n    --arch roberta_base --sample-break-mode complete --tokens-per-sample 512 \\\r\n    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr 0.0005 --warmup-updates 50 \\\r\n    --total-num-update 100 \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n    --max-sentences 6 --update-freq 40 \\\r\n    --max-update 100 --log-format simple --log-interval 1 \\\r\n    --restore-file 'roberta/checkpoint_best.pt' --save-dir 'my_models/agora' \\\r\n    --skip-invalid-size-inputs-valid-test --reset-optimizer\r\n```\r\n- pretrained model downloaded from: https://github.com/sdadas/polish-nlp-resources/releases/download/roberta/roberta.zip\r\n### Traceback I got\r\n```\r\n2020-02-21 13:40:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\r\nTraceback (most recent call last):\r\n  File \"/home/jacek/anaconda3/envs/py36/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/home/jacek/Projects/fairseq/fairseq_cli/train.py\", line 307, in cli_main\r\n    main(args)\r\n  File \"/home/jacek/Projects/fairseq/fairseq_cli/train.py\", line 102, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/home/jacek/anaconda3/envs/py36/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/jacek/Projects/fairseq/fairseq_cli/train.py\", line 171, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/jacek/anaconda3/envs/py36/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/jacek/Projects/fairseq/fairseq/trainer.py\", line 369, in train_step\r\n    grad_norm = self.optimizer.clip_grad_norm(self.args.clip_norm)\r\n  File \"/home/jacek/Projects/fairseq/fairseq/optim/fairseq_optimizer.py\", line 91, in clip_grad_norm\r\n    return utils.clip_grad_norm_(self.params, max_norm)\r\n  File \"/home/jacek/Projects/fairseq/fairseq/utils.py\", line 243, in clip_grad_norm_\r\n    sum(p.grad.data.norm()**2 for p in params if p.grad is not None)\r\nTypeError: sqrt(): argument 'input' (position 1) must be Tensor, not int\r\n```\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.4.0\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): pip install --editable .\r\n - Python version: 3.6.9 Anaconda\r\n - CUDA/cuDNN version: ?\r\n - GPU models and configuration: 750 TI\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1734/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1734/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1690", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1690/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1690/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1690/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1690", "id": 562446938, "node_id": "MDU6SXNzdWU1NjI0NDY5Mzg=", "number": 1690, "title": "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn", "user": {"login": "Shilpil", "id": 8946577, "node_id": "MDQ6VXNlcjg5NDY1Nzc=", "avatar_url": "https://avatars.githubusercontent.com/u/8946577?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Shilpil", "html_url": "https://github.com/Shilpil", "followers_url": "https://api.github.com/users/Shilpil/followers", "following_url": "https://api.github.com/users/Shilpil/following{/other_user}", "gists_url": "https://api.github.com/users/Shilpil/gists{/gist_id}", "starred_url": "https://api.github.com/users/Shilpil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Shilpil/subscriptions", "organizations_url": "https://api.github.com/users/Shilpil/orgs", "repos_url": "https://api.github.com/users/Shilpil/repos", "events_url": "https://api.github.com/users/Shilpil/events{/privacy}", "received_events_url": "https://api.github.com/users/Shilpil/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-02-10T09:57:53Z", "updated_at": "2020-03-16T17:50:33Z", "closed_at": "2020-03-16T05:34:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nI am getting this error when I train wav2vec for English audio data of around 2000 hrs.(Includes datasets like Librispeech+ Common voice + Tedlium + Vox + Indian accent data as well).\r\nAlso how to estimate the number of epochs it will run for if the --max-epoch argument is not set?\r\n\r\n### To Reproduce\r\n\r\nCommand:\r\n\r\npython train.py /datadrive2/manifest --save-dir /datadrive2/model --num-workers 6 --fp16 --max-update 400000 --save-interval 1 \\--arch wav2vec --task audio_pretraining --lr 1e-06 --min-lr 1e-09 --optimizer adam --max-lr 0.005 --lr-scheduler cosine \\--conv-feature-layers \"[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]\" \\--conv-aggregator-layers \"[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]\" \\--skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion binary_cross_entropy --num-negatives 10 \\--max-sample-size 150000 --max-tokens 1500000 --skip-invalid-size-inputs-valid-test\r\n\r\n\r\n#### Error logs:\r\n\r\nepoch 001:   0%| | 15/131854 [00:30<21:28:40,  1.71it/s, loss=1.04892, wps=4.66894e+06, ups=2.48, wpb=1.9008e+06, bsz=1.9008e+06, num_updates=5, lr=5.0099e-2020-02-10 08:20:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0625\r\nepoch 001:   4%| | 4982/131854 [32:59<14:07:32,  2.49it/s, loss=0.344971, wps=4.62541e+06, ups=2.55, wpb=1.81494e+06, bsz=1.81494e+06, num_updates=4970, lr=2020-02-10 08:53:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.03125\r\nTraceback (most recent call last):                                                                                                                          \r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/datadrive2/wav2vec/fairseq/fairseq_cli/train.py\", line 303, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/datadrive2/wav2vec/env/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/datadrive2/wav2vec/env/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/datadrive2/wav2vec/env/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/datadrive2/wav2vec/fairseq/fairseq_cli/train.py\", line 270, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/datadrive2/wav2vec/fairseq/fairseq_cli/train.py\", line 102, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/usr/local/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/datadrive2/wav2vec/fairseq/fairseq_cli/train.py\", line 171, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/usr/local/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/datadrive2/wav2vec/fairseq/fairseq/trainer.py\", line 337, in train_step\r\n    raise e\r\n  File \"/datadrive2/wav2vec/fairseq/fairseq/trainer.py\", line 314, in train_step\r\n    sample, self.model, self.criterion, self.optimizer, ignore_grad\r\n  File \"/datadrive2/wav2vec/fairseq/fairseq/tasks/fairseq_task.py\", line 298, in train_step\r\n    optimizer.backward(loss)\r\n  File \"/datadrive2/wav2vec/fairseq/fairseq/optim/fp16_optimizer.py\", line 115, in backward\r\n    loss.backward()\r\n  File \"/datadrive2/wav2vec/env/lib/python3.6/site-packages/torch/tensor.py\", line 118, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/datadrive2/wav2vec/env/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n\r\n(env) 1 speech@asr-non-preemp-server:/datadrive2/wav2vec/fairseq> /usr/local/lib/python3.6/multiprocessing/semaphore_tracker.py:129: UserWarning: semaphore_tracker: There appear to be 39 leaked semaphores to clean up at shutdown\r\n  len(cache))\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) : 1.4.0\r\n - OS (e.g., Linux): Linux 16.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install --editable .\r\n - Python version: 3.6.3\r\n - CUDA/cuDNN version: 10\r\n - GPU models and configuration: 2 x Tesla V100 GPUs\r\n - Any other relevant information: Tried running with PyTorch 1.2.0 but got same error\r\n\r\n### Additional context\r\nThis error is seen around 1 hour into the training in epoch 1\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1690/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1690/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1686", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1686/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1686/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1686/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1686", "id": 562052859, "node_id": "MDU6SXNzdWU1NjIwNTI4NTk=", "number": 1686, "title": "Massive performance bug: fairseq>=0.7 is not doing incremental decoding", "user": {"login": "mingruimingrui", "id": 18568364, "node_id": "MDQ6VXNlcjE4NTY4MzY0", "avatar_url": "https://avatars.githubusercontent.com/u/18568364?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingruimingrui", "html_url": "https://github.com/mingruimingrui", "followers_url": "https://api.github.com/users/mingruimingrui/followers", "following_url": "https://api.github.com/users/mingruimingrui/following{/other_user}", "gists_url": "https://api.github.com/users/mingruimingrui/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingruimingrui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingruimingrui/subscriptions", "organizations_url": "https://api.github.com/users/mingruimingrui/orgs", "repos_url": "https://api.github.com/users/mingruimingrui/repos", "events_url": "https://api.github.com/users/mingruimingrui/events{/privacy}", "received_events_url": "https://api.github.com/users/mingruimingrui/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-08T17:37:57Z", "updated_at": "2020-02-08T18:01:36Z", "closed_at": "2020-02-08T18:01:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "[```TransformerModel.forward_decoder```](https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py#L314) function is not using `incremental_state`. Meaning all official `fairseq` scripts are not actually doing incremental decoding.\r\n\r\nI first noticed something is wrong when the inference speed dropped when upgrading from fairseq-0.6.1. cProfiler told me that fairseq-0.9.0 is doing tons of extra matmul. Digging a little into the code reveals [this](https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py#L314-L339)...\r\n\r\n```python\r\n    def forward_decoder(\r\n        self,\r\n        prev_output_tokens,\r\n        encoder_out=None,\r\n        incremental_state=None,\r\n        features_only=False,\r\n        **extra_args,\r\n    ):\r\n        attn_args = {\r\n            \"alignment_layer\": self.alignment_layer,\r\n            \"alignment_heads\": self.alignment_heads,\r\n        }\r\n        decoder_out = self.decoder(prev_output_tokens, encoder_out, **attn_args)\r\n\r\n        if self.full_context_alignment:\r\n            attn_args[\"full_context_alignment\"] = self.full_context_alignment\r\n            _, alignment_out = self.decoder(\r\n                prev_output_tokens,\r\n                encoder_out,\r\n                features_only=True,\r\n                **attn_args,\r\n                **extra_args,\r\n            )\r\n            decoder_out[1][\"attn\"] = alignment_out[\"attn\"]\r\n\r\n        return decoder_out\r\n```\r\n\r\nNotice that `incremental_state` is not passed to `self.decoder`.\r\n\r\nOn a side note, `fairseq-0.6.1` calls `model.decoder.forward` directly so there's no problem.\r\n\r\n```python\r\n### in sequence_generator.py\r\n# 0.6.1\r\ndecoder_out = list(model.decoder(tokens, encoder_out, incremental_state=incremental_states[model]))\r\n\r\n# 0.9.0\r\ndecoder_out = list(model.forward_decoder(tokens, encoder_out=encoder_out, incremental_state=self.incremental_states[model]))\r\n```\r\n\r\nI apologize for the click bait-ish title.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1686/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1686/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1683", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1683/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1683/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1683/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1683", "id": 561868400, "node_id": "MDU6SXNzdWU1NjE4Njg0MDA=", "number": 1683, "title": "using Memory-Efficiency-FP16, reload model and training cause RuntimeError: A tensor was not cuda. in fused_adam.py", "user": {"login": "miaodl", "id": 12670959, "node_id": "MDQ6VXNlcjEyNjcwOTU5", "avatar_url": "https://avatars.githubusercontent.com/u/12670959?v=4", "gravatar_id": "", "url": "https://api.github.com/users/miaodl", "html_url": "https://github.com/miaodl", "followers_url": "https://api.github.com/users/miaodl/followers", "following_url": "https://api.github.com/users/miaodl/following{/other_user}", "gists_url": "https://api.github.com/users/miaodl/gists{/gist_id}", "starred_url": "https://api.github.com/users/miaodl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/miaodl/subscriptions", "organizations_url": "https://api.github.com/users/miaodl/orgs", "repos_url": "https://api.github.com/users/miaodl/repos", "events_url": "https://api.github.com/users/miaodl/events{/privacy}", "received_events_url": "https://api.github.com/users/miaodl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-02-07T20:59:33Z", "updated_at": "2020-05-13T16:15:17Z", "closed_at": "2020-05-13T16:15:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n### To Reproduce\r\n\r\nI start to train a translation task. with arguments    \"  --memory-efficient-fp16   \".\r\nAfter training some epochs, I stop training and restart. \r\nWhen all dataset is loaded, optimizer.step() cause error below.\r\n\r\n\"A tensor was not cuda.\" \r\n\r\n  File \"/home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py\", line 30\r\n, in __call__    *args)\r\nRuntimeError: A tensor was not cuda. (multi_tensor_apply at csrc/multi_tensor_apply.cuh:60)\r\n\r\n\"\"\r\n\r\n1. Run cmd '....'\r\n2. See error\r\n\r\nTraceback (most recent call last):                                                                                                   \r\n  File \"/home/tangyue/fairseq/fairseq_cli/train_dynamic.py\", line 343, in <module>\r\n    cli_main()\r\n  File \"/home/tangyue/fairseq/fairseq_cli/train_dynamic.py\", line 335, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 7 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/tangyue/fairseq/fairseq_cli/train_dynamic.py\", line 302, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/tangyue/fairseq/fairseq_cli/train_dynamic.py\", line 104, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/home/tangyue/.pyenv/versions/3.6.8/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/tangyue/fairseq/fairseq_cli/train_dynamic.py\", line 203, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/tangyue/.pyenv/versions/3.6.8/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/tangyue/fairseq/fairseq/trainer.py\", line 412, in train_step\r\n    raise e\r\n  File \"/home/tangyue/fairseq/fairseq/trainer.py\", line 376, in train_step\r\n    self.optimizer.step()\r\n  File \"/home/tangyue/fairseq/fairseq/optim/fp16_optimizer.py\", line 374, in step\r\n    self.wrapped_optimizer.step(closure)\r\n  File \"/home/tangyue/fairseq/fairseq/optim/fairseq_optimizer.py\", line 95, in step\r\n    self.optimizer.step(closure)\r\n  File \"/home/tangyue/fairseq/fairseq/optim/fused_adam.py\", line 283, in step\r\n    group['weight_decay'])\r\n  File \"/home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py\", line 30\r\n, in __call__    *args)\r\nRuntimeError: A tensor was not cuda. (multi_tensor_apply at csrc/multi_tensor_apply.cuh:60)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7fd89d4f6813 in /home/tangyue/.pyenv/versions/torch_3.\r\n6.8/lib/python3.6/site-packages/torch/lib/libc10.so)frame #1: void multi_tensor_apply<4, AdamFunctor<float>, float, float, float, float, float, float, adamMode_t, float>(int, int, at::T\r\nensor const&, std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > > const&, AdamFunctor<float>, float, float, float, float, float, float, adamMode_t, float) + 0x24c (0x7fd89b5b5e5c in /home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/amp_C.cpython-36m-x86_64-linux-gnu.so)frame #2: multi_tensor_adam_cuda(int, at::Tensor, std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<st\r\nd::vector<at::Tensor, std::allocator<at::Tensor> > > >, float, float, float, float, int, int, int, float) + 0x6d8 (0x7fd89b5b24b8 in /home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/amp_C.cpython-36m-x86_64-linux-gnu.so)frame #3: <unknown function> + 0x22f7b (0x7fd89b571f7b in /home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/amp_C\r\n.cpython-36m-x86_64-linux-gnu.so)frame #4: <unknown function> + 0x2303e (0x7fd89b57203e in /home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/amp_C\r\n.cpython-36m-x86_64-linux-gnu.so)frame #5: <unknown function> + 0x1db21 (0x7fd89b56cb21 in /home/tangyue/.pyenv/versions/torch_3.6.8/lib/python3.6/site-packages/amp_C\r\n.cpython-36m-x86_64-linux-gnu.so)frame #6: PyCFunction_Call + 0xe9 (0x4afca9 in /home/tangyue/.pyenv/versions/torch_3.6.8/bin/python)\r\n\r\n\r\n#### Code sample\r\n\r\n\r\n### Expected behavior\r\n\r\n\r\n### Environment\r\n\r\n - fairseq  (v0.9 master 2020/2/8):\r\n - PyTorch 1.3.1\r\n - OS ubuntu 1804\r\n - installed fairseq with editable source code\r\n - Python version: 3.6.8\r\n - CUDA version: 10.2\r\n - GPU models and configuration: 8 nvidai GTX 2080ti\r\n - nvidia/apex 0.1\r\n\r\n### Additional context\r\n\r\nIt seems like I have fixed this bug:\r\n\r\n if len(state) == 0:\r\n     # Exponential moving average of gradient values\r\n    state['exp_avg'] = torch.zeros_like(p.data, dtype=torch.float)\r\n     # Exponential moving average of squared gradient values\r\n     state['exp_avg_sq'] = torch.zeros_like(p.data, dtype=torch.float)\r\n\r\n### fix here\r\nif state['exp_avg'].device!=p.device:\r\n    state['exp_avg']=state['exp_avg'].to(p.device)\r\nif state['exp_avg_sq'].device!=p.device:\r\n    state['exp_avg_sq']=state['exp_avg_sq'].to(p.device)\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1683/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1683/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1682", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1682/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1682/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1682/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1682", "id": 561706593, "node_id": "MDU6SXNzdWU1NjE3MDY1OTM=", "number": 1682, "title": "BART attn_mask unused", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-07T15:28:32Z", "updated_at": "2020-02-08T03:12:08Z", "closed_at": "2020-02-07T20:30:28Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Does anyone know why `attn_mask` is not passed to `self.self_attn` after https://github.com/pytorch/fairseq/blob/4e48c4ae5da48a5f70c969c16793e55e12db3c81/fairseq/modules/transformer_layer.py#L84?", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1682/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1679", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1679/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1679/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1679/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1679", "id": 560983857, "node_id": "MDU6SXNzdWU1NjA5ODM4NTc=", "number": 1679, "title": "Multi GPU Training Crashes", "user": {"login": "smart-patrol", "id": 7339466, "node_id": "MDQ6VXNlcjczMzk0NjY=", "avatar_url": "https://avatars.githubusercontent.com/u/7339466?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smart-patrol", "html_url": "https://github.com/smart-patrol", "followers_url": "https://api.github.com/users/smart-patrol/followers", "following_url": "https://api.github.com/users/smart-patrol/following{/other_user}", "gists_url": "https://api.github.com/users/smart-patrol/gists{/gist_id}", "starred_url": "https://api.github.com/users/smart-patrol/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smart-patrol/subscriptions", "organizations_url": "https://api.github.com/users/smart-patrol/orgs", "repos_url": "https://api.github.com/users/smart-patrol/repos", "events_url": "https://api.github.com/users/smart-patrol/events{/privacy}", "received_events_url": "https://api.github.com/users/smart-patrol/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "pipibjc", "id": 3608436, "node_id": "MDQ6VXNlcjM2MDg0MzY=", "avatar_url": "https://avatars.githubusercontent.com/u/3608436?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pipibjc", "html_url": "https://github.com/pipibjc", "followers_url": "https://api.github.com/users/pipibjc/followers", "following_url": "https://api.github.com/users/pipibjc/following{/other_user}", "gists_url": "https://api.github.com/users/pipibjc/gists{/gist_id}", "starred_url": "https://api.github.com/users/pipibjc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pipibjc/subscriptions", "organizations_url": "https://api.github.com/users/pipibjc/orgs", "repos_url": "https://api.github.com/users/pipibjc/repos", "events_url": "https://api.github.com/users/pipibjc/events{/privacy}", "received_events_url": "https://api.github.com/users/pipibjc/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "pipibjc", "id": 3608436, "node_id": "MDQ6VXNlcjM2MDg0MzY=", "avatar_url": "https://avatars.githubusercontent.com/u/3608436?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pipibjc", "html_url": "https://github.com/pipibjc", "followers_url": "https://api.github.com/users/pipibjc/followers", "following_url": "https://api.github.com/users/pipibjc/following{/other_user}", "gists_url": "https://api.github.com/users/pipibjc/gists{/gist_id}", "starred_url": "https://api.github.com/users/pipibjc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pipibjc/subscriptions", "organizations_url": "https://api.github.com/users/pipibjc/orgs", "repos_url": "https://api.github.com/users/pipibjc/repos", "events_url": "https://api.github.com/users/pipibjc/events{/privacy}", "received_events_url": "https://api.github.com/users/pipibjc/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-02-06T12:33:42Z", "updated_at": "2020-02-11T14:59:44Z", "closed_at": "2020-02-11T14:59:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "#### What is your question?\r\nI am getting a really weird error when trying to run the IWSLT 17 example on ja and zh.\r\n\r\n#### Code\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=1,2,3 fairseq-train ./data-bin/ted.ja_zh.en.bpe16k/ \\\r\n    --max-epoch 50 \\\r\n    --ddp-backend=no_c10d \\\r\n    --num-workers 32 \\\r\n    --task multilingual_translation --lang-pairs ja-en,zh-en \\\r\n    --arch multilingual_transformer_iwslt_de_en \\\r\n    --share-decoders --share-decoder-input-output-embed \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n    --lr 0.0005 --lr-scheduler inverse_sqrt --min-lr '1e-09' \\\r\n    --warmup-updates 4000 --warmup-init-lr '1e-07' \\\r\n    --label-smoothing 0.1 --criterion label_smoothed_cross_entropy \\\r\n    --dropout 0.3 --weight-decay 0.0001 \\\r\n    --save-dir checkpoints/multilingual_transformer \\\r\n    --max-tokens 4000 \r\n```\r\n\r\nWhich loads up to the epoch 001 bar but the then hangs and runs the following error:\r\n\r\n```\r\nTraceback (most recent call last):                                                                                                                    \r\n  File \"/home/ubuntu/anaconda3/envs/multi/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 297, in cli_main\r\n    torch.multiprocessing.spawn(\r\n  File \"/home/ubuntu/anaconda3/envs/multi/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/ubuntu/anaconda3/envs/multi/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/envs/multi/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 267, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 101, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/home/ubuntu/anaconda3/envs/multi/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 168, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/ubuntu/anaconda3/envs/multi/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/ubuntu/fairseq/fairseq/trainer.py\", line 347, in train_step\r\n    logging_outputs, sample_size, ooms = self._aggregate_logging_outputs(\r\n  File \"/home/ubuntu/fairseq/fairseq/trainer.py\", line 611, in _aggregate_logging_outputs\r\n    return self._fast_stat_sync_sum(logging_outputs, *extra_stats_to_sum)\r\n  File \"/home/ubuntu/fairseq/fairseq/trainer.py\", line 649, in _fast_stat_sync_sum\r\n    stats = [0.] + list(extra_stats_to_sum) + [\r\n  File \"/home/ubuntu/fairseq/fairseq/trainer.py\", line 650, in <listcomp>\r\n    sum(log.get(k, 0) for log in logging_outputs)\r\nTypeError: unsupported operand type(s) for +: 'int' and 'dict'\r\n\r\n/home/ubuntu/anaconda3/envs/multi/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 96 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```\r\n#### What have you tried?\r\n\r\nI viewed previously closed issues and modified `batch-size` and `update-freq` to see if that might help. Number of workers now set to max.\r\n\r\n*This runs perfectly fine on a single GPU*\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):  0.9.0\r\n - PyTorch Version (e.g., 1.0):1.4.0\r\n - OS (e.g., Linux): Ubuntu 16.04.6\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install e\r\n - Python version: 3.8.1\r\n - CUDA/cuDNN version: 10\r\n - GPU models and configuration: p2.8xlarge \r\n ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1679/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1679/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1652", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1652/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1652/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1652/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1652", "id": 555813468, "node_id": "MDU6SXNzdWU1NTU4MTM0Njg=", "number": 1652, "title": "Build failure when installing via pip", "user": {"login": "mortont", "id": 12803317, "node_id": "MDQ6VXNlcjEyODAzMzE3", "avatar_url": "https://avatars.githubusercontent.com/u/12803317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mortont", "html_url": "https://github.com/mortont", "followers_url": "https://api.github.com/users/mortont/followers", "following_url": "https://api.github.com/users/mortont/following{/other_user}", "gists_url": "https://api.github.com/users/mortont/gists{/gist_id}", "starred_url": "https://api.github.com/users/mortont/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mortont/subscriptions", "organizations_url": "https://api.github.com/users/mortont/orgs", "repos_url": "https://api.github.com/users/mortont/repos", "events_url": "https://api.github.com/users/mortont/events{/privacy}", "received_events_url": "https://api.github.com/users/mortont/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-27T20:27:44Z", "updated_at": "2020-01-29T19:32:32Z", "closed_at": "2020-01-29T19:32:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen installing fairseq from pip (or locally via master) the build fails at libnat.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n(from a fresh virtualenv)\r\n1. pip install fairseq\r\n\r\nor (also from a fresh virtualenv)\r\n1. git clone https://github.com/pytorch/fairseq\r\n2. cd fairseq\r\n3. pip install --editable .\r\n\r\n\r\n[Full error log](https://github.com/pytorch/fairseq/files/4119034/out.log)\r\n\r\n### Expected behavior\r\n\r\nThe install to complete without error\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Arch Linux\r\n - How you installed fairseq (`pip`, source): source (also fails with `pip`)\r\n - Build command you used (if compiling from source): pip install --editable .\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: CUDA 10.2.89 cuDNN 7.6.5.32\r\n - GPU models and configuration: GTX 1080 Ti", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1652/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1643", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1643/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1643/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1643/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1643", "id": 553898894, "node_id": "MDU6SXNzdWU1NTM4OTg4OTQ=", "number": 1643, "title": "Nested functions in CLI scripts close over local variables", "user": {"login": "erip", "id": 2348806, "node_id": "MDQ6VXNlcjIzNDg4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/2348806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erip", "html_url": "https://github.com/erip", "followers_url": "https://api.github.com/users/erip/followers", "following_url": "https://api.github.com/users/erip/following{/other_user}", "gists_url": "https://api.github.com/users/erip/gists{/gist_id}", "starred_url": "https://api.github.com/users/erip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erip/subscriptions", "organizations_url": "https://api.github.com/users/erip/orgs", "repos_url": "https://api.github.com/users/erip/repos", "events_url": "https://api.github.com/users/erip/events{/privacy}", "received_events_url": "https://api.github.com/users/erip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 679100395, "node_id": "MDU6TGFiZWw2NzkxMDAzOTU=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/help%20wanted", "name": "help wanted", "color": "128A0C", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-01-23T02:10:56Z", "updated_at": "2020-01-27T23:00:11Z", "closed_at": "2020-01-27T23:00:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWhen nested functions use a variable from some outer scope in their bodies, the variable that's used is closed over and its reference cannot be released; e.g.,\r\n\r\n```python\r\ndef foo():\r\n    x = open(\"some_file\", 'w')\r\n    def foo2(contents):\r\n        print(contents, file=x)\r\n    foo2(\"hello, world\")\r\n```\r\n\r\nBecause these nested functions are used so liberally in CLI scripts, some references to files (like those written to temporary directories in `preprocess`) cannot be garbage collected and thus appear to be in-use at test completion time. Unix is happy to clean up these temporary directories, but Windows is much less happy to do this.\r\n\r\n### To Reproduce\r\n\r\nLook at the Windows test failures. :-) \r\n\r\n#### Code sample\r\n\r\nN/A\r\n\r\n### Expected behavior\r\n\r\nTests should complete normally\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) all\r\n - OS (e.g., Linux): Windows\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): n/a\r\n - Python version: 3.6, 3.7\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1643/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1643/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1641", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1641/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1641/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1641/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1641", "id": 553745401, "node_id": "MDU6SXNzdWU1NTM3NDU0MDE=", "number": 1641, "title": "IWSLT preparation script is overwriting prepared validation data", "user": {"login": "bricksdont", "id": 12611348, "node_id": "MDQ6VXNlcjEyNjExMzQ4", "avatar_url": "https://avatars.githubusercontent.com/u/12611348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bricksdont", "html_url": "https://github.com/bricksdont", "followers_url": "https://api.github.com/users/bricksdont/followers", "following_url": "https://api.github.com/users/bricksdont/following{/other_user}", "gists_url": "https://api.github.com/users/bricksdont/gists{/gist_id}", "starred_url": "https://api.github.com/users/bricksdont/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bricksdont/subscriptions", "organizations_url": "https://api.github.com/users/bricksdont/orgs", "repos_url": "https://api.github.com/users/bricksdont/repos", "events_url": "https://api.github.com/users/bricksdont/events{/privacy}", "received_events_url": "https://api.github.com/users/bricksdont/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-01-22T19:27:22Z", "updated_at": "2020-01-24T18:33:57Z", "closed_at": "2020-01-24T18:33:57Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nThe preparation script for IWSLT 2017 multilingual data wants to preprocess several validation sets, but in fact, only the last validation set is saved for each language pair.\r\n\r\nSpecific lines of code in question: https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt17-multilingual.sh#L85-98\r\n\r\nShould all validation sets be removed except one, or would you like to keep them all?\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1641/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1641/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1632", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1632/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1632/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1632/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1632", "id": 551990533, "node_id": "MDU6SXNzdWU1NTE5OTA1MzM=", "number": 1632, "title": "Training loss is mixed with valid loss", "user": {"login": "freewym", "id": 3506322, "node_id": "MDQ6VXNlcjM1MDYzMjI=", "avatar_url": "https://avatars.githubusercontent.com/u/3506322?v=4", "gravatar_id": "", "url": "https://api.github.com/users/freewym", "html_url": "https://github.com/freewym", "followers_url": "https://api.github.com/users/freewym/followers", "following_url": "https://api.github.com/users/freewym/following{/other_user}", "gists_url": "https://api.github.com/users/freewym/gists{/gist_id}", "starred_url": "https://api.github.com/users/freewym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/freewym/subscriptions", "organizations_url": "https://api.github.com/users/freewym/orgs", "repos_url": "https://api.github.com/users/freewym/repos", "events_url": "https://api.github.com/users/freewym/events{/privacy}", "received_events_url": "https://api.github.com/users/freewym/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-01-19T21:44:08Z", "updated_at": "2020-01-21T00:34:41Z", "closed_at": "2020-01-21T00:34:41Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nAfter the latest introduction of metrics, the training loss printed as log is different from before, which I suspect is because  its value is somewhat mixed with valid loss\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nYou can reproduce it by observing the difference of training losses before/after setting `--disable-validation`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.1\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1632/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1632/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1628", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1628/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1628/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1628/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1628", "id": 551167214, "node_id": "MDU6SXNzdWU1NTExNjcyMTQ=", "number": 1628, "title": "OOM while trying to train BART", "user": {"login": "tuhinjubcse", "id": 3104771, "node_id": "MDQ6VXNlcjMxMDQ3NzE=", "avatar_url": "https://avatars.githubusercontent.com/u/3104771?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tuhinjubcse", "html_url": "https://github.com/tuhinjubcse", "followers_url": "https://api.github.com/users/tuhinjubcse/followers", "following_url": "https://api.github.com/users/tuhinjubcse/following{/other_user}", "gists_url": "https://api.github.com/users/tuhinjubcse/gists{/gist_id}", "starred_url": "https://api.github.com/users/tuhinjubcse/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tuhinjubcse/subscriptions", "organizations_url": "https://api.github.com/users/tuhinjubcse/orgs", "repos_url": "https://api.github.com/users/tuhinjubcse/repos", "events_url": "https://api.github.com/users/tuhinjubcse/events{/privacy}", "received_events_url": "https://api.github.com/users/tuhinjubcse/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-01-17T02:48:52Z", "updated_at": "2020-01-24T20:53:07Z", "closed_at": "2020-01-24T20:53:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nTried to finetune BART using same configuration mentioned here\r\nhttps://github.com/pytorch/fairseq/blob/master/examples/bart/README.cnn.md\r\n\r\nWas using 4 11GB 2080Ti , tried 3 , 2 none helps\r\nRealized the batch size used by you folks is 2048 (max_tokens) reduced it to 16 still getting OOM \r\nThe issue is OOM but due to optimization \r\n\r\n\r\nIt runs fine on 1 single GPU though\r\n\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0) 1.3.1 pytorchgpu\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: 4 11 GB 2080 TitanX\r\n\r\n![image](https://user-images.githubusercontent.com/3104771/73038919-0784e880-3e09-11ea-865c-6e0220e8b279.png)\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1628/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1622", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1622/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1622/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1622/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1622", "id": 549926089, "node_id": "MDU6SXNzdWU1NDk5MjYwODk=", "number": 1622, "title": "Division by zero in Windows builds", "user": {"login": "erip", "id": 2348806, "node_id": "MDQ6VXNlcjIzNDg4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/2348806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erip", "html_url": "https://github.com/erip", "followers_url": "https://api.github.com/users/erip/followers", "following_url": "https://api.github.com/users/erip/following{/other_user}", "gists_url": "https://api.github.com/users/erip/gists{/gist_id}", "starred_url": "https://api.github.com/users/erip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erip/subscriptions", "organizations_url": "https://api.github.com/users/erip/orgs", "repos_url": "https://api.github.com/users/erip/repos", "events_url": "https://api.github.com/users/erip/events{/privacy}", "received_events_url": "https://api.github.com/users/erip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1697206034, "node_id": "MDU6TGFiZWwxNjk3MjA2MDM0", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/windows", "name": "windows", "color": "0000ff", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-01-15T02:21:39Z", "updated_at": "2020-01-24T18:30:50Z", "closed_at": "2020-01-24T18:30:50Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nWith the introduction of the new metrics API, there are issues with division by zero.\r\n\r\n### To Reproduce\r\n\r\nTo reproduce, run the most minimal `fairseq-train` on a Windows machine. The error is also seen in the test failures from [Windows CI](https://github.com/pytorch/fairseq/pull/1595/checks?check_run_id=390286089)\r\n\r\n#### Code sample\r\n\r\nN/A\r\n\r\n### Expected behavior\r\n\r\nThere shouldn't be division by zero errors.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.3.1\r\n - OS (e.g., Linux): Windows\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): See the build script.\r\n - Python version: 3.6.7\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A\r\n\r\n### Additional context\r\n\r\nStrangely, this doesn't seem to happen in Unix builds... ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1622/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1622/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1617", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1617/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1617/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1617/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1617", "id": 549121905, "node_id": "MDU6SXNzdWU1NDkxMjE5MDU=", "number": 1617, "title": "Tokens still have spaces", "user": {"login": "mortonjt", "id": 4184797, "node_id": "MDQ6VXNlcjQxODQ3OTc=", "avatar_url": "https://avatars.githubusercontent.com/u/4184797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mortonjt", "html_url": "https://github.com/mortonjt", "followers_url": "https://api.github.com/users/mortonjt/followers", "following_url": "https://api.github.com/users/mortonjt/following{/other_user}", "gists_url": "https://api.github.com/users/mortonjt/gists{/gist_id}", "starred_url": "https://api.github.com/users/mortonjt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mortonjt/subscriptions", "organizations_url": "https://api.github.com/users/mortonjt/orgs", "repos_url": "https://api.github.com/users/mortonjt/repos", "events_url": "https://api.github.com/users/mortonjt/events{/privacy}", "received_events_url": "https://api.github.com/users/mortonjt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 679100395, "node_id": "MDU6TGFiZWw2NzkxMDAzOTU=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/help%20wanted", "name": "help wanted", "color": "128A0C", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-13T19:02:55Z", "updated_at": "2020-02-21T15:17:22Z", "closed_at": "2020-02-21T15:13:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nLong story short, I've trained roberta using a custom dictionary and now I am trying to extract features (code snippet below for reference).\r\n\r\n```python\r\nroberta = RobertaModel.from_pretrained(\r\n    path1, 'checkpoint_best.pt',\r\n    path2,\r\n    gpt2_encoder_json=custom_json,\r\n    gpt2_vocab_bpe=custom_vocab)\r\n\r\ntokens = roberta.encode('A B C D E G'))\r\n```\r\n\r\nWhen I try to run this, I get the following error below\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"attention_layers.py\", line 80, in <module>\r\n    tokens = roberta.encode(' '.join(list(s)))\r\n  File \"/home/jmorton/software/fairseq/fairseq/models/roberta/hub_interface.py\", line 57, in encode\r\n    bpe_sentence = '<s> ' + self.bpe.encode(sentence) + ' </s>'\r\n  File \"/home/jmorton/software/fairseq/fairseq/data/encoders/gpt2_bpe.py\", line 40, in encode\r\n    return ' '.join(map(str, self.bpe.encode(x)))\r\n  File \"/home/jmorton/software/fairseq/fairseq/data/encoders/gpt2_bpe_utils.py\", line 110, in encode\r\n    bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\r\n  File \"/home/jmorton/software/fairseq/fairseq/data/encoders/gpt2_bpe_utils.py\", line 110, in <genexpr>\r\n    bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\r\nKeyError: '\u0120'\r\n```\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version: 0.9.0\r\n - PyTorch Version: 1.2.0\r\n - OS: Redhat\r\n - How you installed fairseq: pip\r\n - Python version: 3.6.2\r\n - CUDA/cuDNN version: just cpu\r\n - GPU models and configuration: just cpu\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nTurns out that there is still spacing in the tokens when parsing for the particular example. The fix is presented here: https://github.com/mortonjt/fairseq/pull/1/files\r\n\r\nI'm raising this issue mainly to bring awareness around challenges of trying to plug in custom dictionaries.  Can push in PR if there is interest.\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1617/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1617/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1616", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1616/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1616/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1616/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1616", "id": 549070577, "node_id": "MDU6SXNzdWU1NDkwNzA1Nzc=", "number": 1616, "title": "Sentencepiece error during calling XLMR model.encode()", "user": {"login": "mukhal", "id": 5109053, "node_id": "MDQ6VXNlcjUxMDkwNTM=", "avatar_url": "https://avatars.githubusercontent.com/u/5109053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mukhal", "html_url": "https://github.com/mukhal", "followers_url": "https://api.github.com/users/mukhal/followers", "following_url": "https://api.github.com/users/mukhal/following{/other_user}", "gists_url": "https://api.github.com/users/mukhal/gists{/gist_id}", "starred_url": "https://api.github.com/users/mukhal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mukhal/subscriptions", "organizations_url": "https://api.github.com/users/mukhal/orgs", "repos_url": "https://api.github.com/users/mukhal/repos", "events_url": "https://api.github.com/users/mukhal/events{/privacy}", "received_events_url": "https://api.github.com/users/mukhal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1756261282, "node_id": "MDU6TGFiZWwxNzU2MjYxMjgy", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/unable%20to%20repro", "name": "unable to repro", "color": "ededed", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-13T17:17:13Z", "updated_at": "2020-01-13T17:42:47Z", "closed_at": "2020-01-13T17:42:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nwhen running \r\n```python\r\nfrom fairseq.models.roberta import XLMRModel\r\nxlmr = XLMRModel.from_pretrained('/path/to/xlmr.large', checkpoint_file='model.pt')\r\n\r\nids = xlmr.encode('Hello world!')\r\n```\r\n\r\nI get the following error\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/add/anaconda3/envs/py35/lib/python3.6/site-packages/fairseq/models/roberta/hub_interface.py\", line 57, in encode\r\n    bpe_sentence = '<s> ' + self.bpe.encode(sentence) + ' </s>'\r\n  File \"/home/add/anaconda3/envs/py35/lib/python3.6/site-packages/fairseq/data/encoders/sentencepiece_bpe.py\", line 30, in encode\r\n    return ' '.join(self.sp.EncodeAsPieces(x))\r\nTypeError: sequence item 0: expected str instance, bytes found\r\n```\r\nI am using fairseq 0.9.0 and sentencepiece 0.1.85", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1616/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1616/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1610", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1610/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1610/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1610/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1610", "id": 548190141, "node_id": "MDU6SXNzdWU1NDgxOTAxNDE=", "number": 1610, "title": "wav2vec training crashes with AssertionError", "user": {"login": "alpoktem", "id": 26279276, "node_id": "MDQ6VXNlcjI2Mjc5Mjc2", "avatar_url": "https://avatars.githubusercontent.com/u/26279276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alpoktem", "html_url": "https://github.com/alpoktem", "followers_url": "https://api.github.com/users/alpoktem/followers", "following_url": "https://api.github.com/users/alpoktem/following{/other_user}", "gists_url": "https://api.github.com/users/alpoktem/gists{/gist_id}", "starred_url": "https://api.github.com/users/alpoktem/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alpoktem/subscriptions", "organizations_url": "https://api.github.com/users/alpoktem/orgs", "repos_url": "https://api.github.com/users/alpoktem/repos", "events_url": "https://api.github.com/users/alpoktem/events{/privacy}", "received_events_url": "https://api.github.com/users/alpoktem/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alexeib", "id": 3497946, "node_id": "MDQ6VXNlcjM0OTc5NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3497946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexeib", "html_url": "https://github.com/alexeib", "followers_url": "https://api.github.com/users/alexeib/followers", "following_url": "https://api.github.com/users/alexeib/following{/other_user}", "gists_url": "https://api.github.com/users/alexeib/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexeib/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexeib/subscriptions", "organizations_url": "https://api.github.com/users/alexeib/orgs", "repos_url": "https://api.github.com/users/alexeib/repos", "events_url": "https://api.github.com/users/alexeib/events{/privacy}", "received_events_url": "https://api.github.com/users/alexeib/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2020-01-10T17:03:26Z", "updated_at": "2020-06-24T01:04:32Z", "closed_at": "2020-01-21T09:20:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nI start the training following wav2vec training guidelines. 84% through the first epoch it quits giving an AssertionError. \r\n\r\n### To Reproduce\r\n\r\n```\r\npython $FAIRSEQ/train.py $WORKDIR/manifest --save-dir $WORKDIR/models/ \\\r\n--num-workers 6 --fp16 --max-update 400000 --save-interval 1 --no-epoch-checkpoints \\\r\n--arch wav2vec --task audio_pretraining --lr 1e-06 --min-lr 1e-09 --optimizer adam \\\r\n--max-lr 0.005 --lr-scheduler cosine --conv-feature-layers \"[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]\" \\\r\n--conv-aggregator-layers \"[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]\" \\\r\n--skip-connections-agg --residual-scale 0.5 --log-compression \\\r\n--warmup-updates 500 --warmup-init-lr 1e-07 --criterion binary_cross_entropy \\\r\n--num-negatives 10 --max-sample-size 150000 --max-tokens 1500000 \\\r\n--skip-invalid-size-inputs-valid-test\r\n```\r\n\r\n#### Error output\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/twbgmy/extSW/fairseq/train.py\", line 363, in <module>\r\n    cli_main()\r\n  File \"/home/twbgmy/extSW/fairseq/train.py\", line 355, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/twbgmy/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/twbgmy/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/twbgmy/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/twbgmy/extSW/fairseq/train.py\", line 322, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/twbgmy/extSW/fairseq/train.py\", line 89, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/home/twbgmy/extSW/fairseq/train.py\", line 153, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/twbgmy/extSW/fairseq/fairseq/trainer.py\", line 327, in train_step\r\n    sample, self.model, self.criterion, self.optimizer, ignore_grad\r\n  File \"/home/twbgmy/extSW/fairseq/fairseq/tasks/fairseq_task.py\", line 290, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/home/twbgmy/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/twbgmy/extSW/fairseq/fairseq/criterions/binary_cross_entropy.py\", line 30, in forward\r\n    net_output = model(**sample['net_input'])\r\n  File \"/home/twbgmy/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/twbgmy/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 442, in forward\r\n    output = self.module(*inputs[0], **kwargs[0])\r\n  File \"/home/twbgmy/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/twbgmy/extSW/fairseq/fairseq/models/wav2vec.py\", line 183, in forward\r\n    x, targets = self.wav2vec_predictions(x, features)\r\n  File \"/home/twbgmy/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/twbgmy/extSW/fairseq/fairseq/models/wav2vec.py\", line 429, in forward\r\n      assert end == predictions.numel(), '{} != {}'.format(end, predictions.numel())\r\nAssertionError: 0 != 517\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq source installation commit #097bb73\r\n - PyTorch Version 1.2\r\n - OS Linux\r\n - Build command you used (if compiling from source): `pip install --editable .`\r\n - Python 3.7.3\r\n - CUDA 10.1/ cuDNN\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1610/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1605", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1605/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1605/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1605/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1605", "id": 547589021, "node_id": "MDU6SXNzdWU1NDc1ODkwMjE=", "number": 1605, "title": "XLM-R model output changes with batch size", "user": {"login": "ricardorei", "id": 17256847, "node_id": "MDQ6VXNlcjE3MjU2ODQ3", "avatar_url": "https://avatars.githubusercontent.com/u/17256847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ricardorei", "html_url": "https://github.com/ricardorei", "followers_url": "https://api.github.com/users/ricardorei/followers", "following_url": "https://api.github.com/users/ricardorei/following{/other_user}", "gists_url": "https://api.github.com/users/ricardorei/gists{/gist_id}", "starred_url": "https://api.github.com/users/ricardorei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ricardorei/subscriptions", "organizations_url": "https://api.github.com/users/ricardorei/orgs", "repos_url": "https://api.github.com/users/ricardorei/repos", "events_url": "https://api.github.com/users/ricardorei/events{/privacy}", "received_events_url": "https://api.github.com/users/ricardorei/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "ngoyal2707", "id": 7836935, "node_id": "MDQ6VXNlcjc4MzY5MzU=", "avatar_url": "https://avatars.githubusercontent.com/u/7836935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngoyal2707", "html_url": "https://github.com/ngoyal2707", "followers_url": "https://api.github.com/users/ngoyal2707/followers", "following_url": "https://api.github.com/users/ngoyal2707/following{/other_user}", "gists_url": "https://api.github.com/users/ngoyal2707/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngoyal2707/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngoyal2707/subscriptions", "organizations_url": "https://api.github.com/users/ngoyal2707/orgs", "repos_url": "https://api.github.com/users/ngoyal2707/repos", "events_url": "https://api.github.com/users/ngoyal2707/events{/privacy}", "received_events_url": "https://api.github.com/users/ngoyal2707/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ngoyal2707", "id": 7836935, "node_id": "MDQ6VXNlcjc4MzY5MzU=", "avatar_url": "https://avatars.githubusercontent.com/u/7836935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngoyal2707", "html_url": "https://github.com/ngoyal2707", "followers_url": "https://api.github.com/users/ngoyal2707/followers", "following_url": "https://api.github.com/users/ngoyal2707/following{/other_user}", "gists_url": "https://api.github.com/users/ngoyal2707/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngoyal2707/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngoyal2707/subscriptions", "organizations_url": "https://api.github.com/users/ngoyal2707/orgs", "repos_url": "https://api.github.com/users/ngoyal2707/repos", "events_url": "https://api.github.com/users/ngoyal2707/events{/privacy}", "received_events_url": "https://api.github.com/users/ngoyal2707/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-01-09T16:34:51Z", "updated_at": "2020-01-21T17:20:44Z", "closed_at": "2020-01-21T17:20:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nWhen using XLM-R the representations change depending on the batch size.\r\n\r\n#### Code sample\r\n```\r\nfrom fairseq.models.roberta import XLMRModel\r\nfrom torchnlp.encoders.text import stack_and_pad_tensors\r\nimport torch\r\n\r\ntorch.set_printoptions(precision=10)\r\ndef batch_encoder(samples, tokenizer):\r\n    batch = []\r\n    for sequence in samples:\r\n        batch.append(tokenizer.encode(sequence))\r\n    return stack_and_pad_tensors(batch, tokenizer.task.source_dictionary.__dict__[\"indices\"][\"<pad>\"])\r\n    \r\nxlmr = XLMRModel.from_pretrained(\r\n            \"pretrained/xlmr.base\", checkpoint_file=\"model.pt\"\r\n        )\r\nxlmr.eval()\r\n\r\nsamples = [\r\n    'the part of the regular expression within the forward slashes defines the pattern.', \r\n    'discards the current state and temporarily replaces it with the previous state.',\r\n    'to convert a smooth point to a corner point without direction lines, click the smooth point.'\r\n]\r\n\r\nwith torch.no_grad():\r\n    big_batch_tokens, bb_lengths = batch_encoder(samples, xlmr)\r\n    small_batch_tokens, sb_lengths = batch_encoder(samples[:2], xlmr)\r\n    first_sample_tokens = xlmr.encode(samples[0])\r\n\r\n    first_sample_last_layer = xlmr.extract_features(first_sample_tokens)\r\n    print (first_sample_last_layer[:, 0, :][0][:5])\r\n\r\n    small_batch_last_layer = xlmr.extract_features(tokens=small_batch_tokens)\r\n    print (small_batch_last_layer[:, 0, :][0][:5])\r\n\r\n    big_batch_last_layer = xlmr.extract_features(tokens=big_batch_tokens)\r\n    print (big_batch_last_layer[:, 0, :][0][:5])\r\n```\r\n\r\n### Expected behavior\r\n\r\n```\r\ntensor([ 0.0852593556,  0.1065494418,  0.0615975149, -0.0047241775, 0.0284897964])\r\ntensor([ 0.0852593333,  0.1065494195,  0.0615975149, -0.0047241990, 0.0284897070])\r\ntensor([ 0.0852593556,  0.1065494046,  0.0615975186, -0.0047241938, 0.0284897685])\r\n```\r\n\r\n### Additional context\r\nIf I decide to average pool overall embeddings or if I max pool these differences are even bigger.\r\n\r\nAm I doing something wrong? Is this behaviour expected?\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1605/reactions", "total_count": 2, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 2}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1605/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1591", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1591/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1591/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1591/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1591", "id": 545816802, "node_id": "MDU6SXNzdWU1NDU4MTY4MDI=", "number": 1591, "title": "OpenMP error", "user": {"login": "prashantserai", "id": 10653125, "node_id": "MDQ6VXNlcjEwNjUzMTI1", "avatar_url": "https://avatars.githubusercontent.com/u/10653125?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prashantserai", "html_url": "https://github.com/prashantserai", "followers_url": "https://api.github.com/users/prashantserai/followers", "following_url": "https://api.github.com/users/prashantserai/following{/other_user}", "gists_url": "https://api.github.com/users/prashantserai/gists{/gist_id}", "starred_url": "https://api.github.com/users/prashantserai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prashantserai/subscriptions", "organizations_url": "https://api.github.com/users/prashantserai/orgs", "repos_url": "https://api.github.com/users/prashantserai/repos", "events_url": "https://api.github.com/users/prashantserai/events{/privacy}", "received_events_url": "https://api.github.com/users/prashantserai/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-06T16:28:33Z", "updated_at": "2020-01-07T18:22:02Z", "closed_at": "2020-01-07T18:22:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nOpenMP error on running fairseq-train (even with --num-workers 0 )\r\nfairseq-generate works crash-free\r\n\r\n### To Reproduce\r\n\r\n**RUN:** \r\nDATA_FOLDER=errorsim-pw1; ARCH=fconv; FOLDER=fconv_pw1_test2 fairseq-train ./data-bin/$DATA_FOLDER --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\r\n    --arch $ARCH --save-dir ./checkpoints/$FOLDER --no-progress-bar --log-interval 50 \\\r\n    --num-workers 0 --cpu\r\n(--cpu can be removed if you have a GPU, and the error is the same with any value of --num-workers)\r\n\r\n**ERROR LOG:**\r\n OMP: Error #13: Assertion failure at z_Linux_util.cpp(2361).\r\nOMP: Hint Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see http://www.intel.com/software/products/support/.\r\nOMP: Error #13: Assertion failure at z_Linux_util.cpp(2361).\r\nOMP: Hint Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see http://www.intel.com/software/products/support/.\r\nTraceback (most recent call last):\r\n  File \"/homes/3/serai/pytorch_vib/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/homes/3/serai/fairseq_vib/fairseq_cli/train.py\", line 329, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/homes/3/serai/pytorch_vib/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/homes/3/serai/pytorch_vib/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 107, in join\r\n    (error_index, name)\r\nException: process 1 terminated with signal SIGABRT\r\n\r\n\r\n### Environment\r\n\r\n - fairseq Version: 0.8.0\r\n - PyTorch Version: 1.3.1\r\n - OS: RHEL7\r\n - How you installed fairseq (`pip`, source): Using pip, as an editable\r\n - Build command you used (if compiling from source): git clone https://github.com/pytorch/fairseq; cd fairseq; pip install --editable .\r\n - Python version: 3.7.5\r\n - CUDA/cuDNN version: \r\n - GPU models and configuration: Nvidia GeForce GTX 1080\r\n - Any other relevant information: OpenMP version is 3.1\r\n`echo |cpp -fopenmp -dM |grep -i open` prints \"#define _OPENMP 201107\"\r\n\r\nThis is a new-ish RHEL7 environment I'm trying to work with. So far whatever I've done with pytorch, fairseq, other libraries I'm using works fine.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1591/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1591/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1586", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1586/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1586/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1586/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1586", "id": 545470789, "node_id": "MDU6SXNzdWU1NDU0NzA3ODk=", "number": 1586, "title": "Results aren't written to `args.results_path` in `fairseq-generate` when specified.", "user": {"login": "erip", "id": 2348806, "node_id": "MDQ6VXNlcjIzNDg4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/2348806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erip", "html_url": "https://github.com/erip", "followers_url": "https://api.github.com/users/erip/followers", "following_url": "https://api.github.com/users/erip/following{/other_user}", "gists_url": "https://api.github.com/users/erip/gists{/gist_id}", "starred_url": "https://api.github.com/users/erip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erip/subscriptions", "organizations_url": "https://api.github.com/users/erip/orgs", "repos_url": "https://api.github.com/users/erip/repos", "events_url": "https://api.github.com/users/erip/events{/privacy}", "received_events_url": "https://api.github.com/users/erip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-05T21:41:33Z", "updated_at": "2020-01-06T17:27:10Z", "closed_at": "2020-01-06T17:27:10Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nResults aren't written to `args.results_path` in `fairseq-generate` when specified.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nAssuming a model has been trained...\r\n\r\n1. Run cmd `fairseq-generate data-bin --path checkpoints/checkpoint_best.pt --results-path out/`\r\n\r\n2. See results printed to screen and observe empty `out/`\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\nI expect generation results to be written to some file within the directory specified by `--results-path`\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0): 1.3.1\r\n - OS (e.g., Linux): Windows (but doesn't matter)\r\n - How you installed fairseq (`pip`, source): `pip`\r\n - Build command you used (if compiling from source): n/a\r\n - Python version: 3.7.x\r\n - CUDA/cuDNN version: n/a\r\n - GPU models and configuration: n/a\r\n - Any other relevant information: n/a\r\n\r\n### Additional context\r\n\r\nSee that `results_path` is unused in [`generate.py`](https://github.com/pytorch/fairseq/blob/master/generate.py).", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1586/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1581", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1581/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1581/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1581/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1581", "id": 545413923, "node_id": "MDU6SXNzdWU1NDU0MTM5MjM=", "number": 1581, "title": "The condition statement judging bos may be wrong in LanguagePairDataset?", "user": {"login": "YongfeiYan", "id": 41781351, "node_id": "MDQ6VXNlcjQxNzgxMzUx", "avatar_url": "https://avatars.githubusercontent.com/u/41781351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YongfeiYan", "html_url": "https://github.com/YongfeiYan", "followers_url": "https://api.github.com/users/YongfeiYan/followers", "following_url": "https://api.github.com/users/YongfeiYan/following{/other_user}", "gists_url": "https://api.github.com/users/YongfeiYan/gists{/gist_id}", "starred_url": "https://api.github.com/users/YongfeiYan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YongfeiYan/subscriptions", "organizations_url": "https://api.github.com/users/YongfeiYan/orgs", "repos_url": "https://api.github.com/users/YongfeiYan/repos", "events_url": "https://api.github.com/users/YongfeiYan/events{/privacy}", "received_events_url": "https://api.github.com/users/YongfeiYan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-05T12:53:29Z", "updated_at": "2020-01-05T21:10:35Z", "closed_at": "2020-01-05T21:10:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "When judging whether bos is added to source in [https://github.com/pytorch/fairseq/blob/master/fairseq/data/language_pair_dataset.py#L200](https://github.com/pytorch/fairseq/blob/master/fairseq/data/language_pair_dataset.py#L200)\r\n, `if self.src[index][-1] != bos:`should be `self.src[index][0] != bos:`?\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1581/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1581/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1559", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1559/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1559/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1559/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1559", "id": 543760341, "node_id": "MDU6SXNzdWU1NDM3NjAzNDE=", "number": 1559, "title": "Position embedding error\uff08maybe?\uff09", "user": {"login": "Dod-o", "id": 45008728, "node_id": "MDQ6VXNlcjQ1MDA4NzI4", "avatar_url": "https://avatars.githubusercontent.com/u/45008728?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dod-o", "html_url": "https://github.com/Dod-o", "followers_url": "https://api.github.com/users/Dod-o/followers", "following_url": "https://api.github.com/users/Dod-o/following{/other_user}", "gists_url": "https://api.github.com/users/Dod-o/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dod-o/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dod-o/subscriptions", "organizations_url": "https://api.github.com/users/Dod-o/orgs", "repos_url": "https://api.github.com/users/Dod-o/repos", "events_url": "https://api.github.com/users/Dod-o/events{/privacy}", "received_events_url": "https://api.github.com/users/Dod-o/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-12-30T06:20:21Z", "updated_at": "2019-12-30T06:24:33Z", "closed_at": "2019-12-30T06:24:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\nhi, \r\n\r\nI found that maybe there is something wrong with the position embedding when running the translate task.\r\n\r\n`x = embed + self.embed_positions(src_tokens)`\r\n[https://github.com/pytorch/fairseq/blob/cae55599a91d6ff21544c70f9cb92544cce6c342/fairseq/models/transformer.py#L358](url)\r\n \r\nwhy the parameter used above is src_tokens?  In my opinion, it should be a \"src_pos\".\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1559/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1559/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1550", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1550/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1550/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1550/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1550", "id": 542621546, "node_id": "MDU6SXNzdWU1NDI2MjE1NDY=", "number": 1550, "title": "'NoneType' object has no attribute 'sizes'", "user": {"login": "ehsankf", "id": 21260007, "node_id": "MDQ6VXNlcjIxMjYwMDA3", "avatar_url": "https://avatars.githubusercontent.com/u/21260007?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ehsankf", "html_url": "https://github.com/ehsankf", "followers_url": "https://api.github.com/users/ehsankf/followers", "following_url": "https://api.github.com/users/ehsankf/following{/other_user}", "gists_url": "https://api.github.com/users/ehsankf/gists{/gist_id}", "starred_url": "https://api.github.com/users/ehsankf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ehsankf/subscriptions", "organizations_url": "https://api.github.com/users/ehsankf/orgs", "repos_url": "https://api.github.com/users/ehsankf/repos", "events_url": "https://api.github.com/users/ehsankf/events{/privacy}", "received_events_url": "https://api.github.com/users/ehsankf/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 1737615870, "node_id": "MDU6TGFiZWwxNzM3NjE1ODcw", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/needs%20triage", "name": "needs triage", "color": "B7950B", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-26T17:56:54Z", "updated_at": "2021-03-17T10:58:40Z", "closed_at": "2019-12-26T18:12:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "In fairseq/tasks/translation.py\r\n\r\nI get this error\r\nsrcbert_datasets, srcbert_datasets.sizes, berttokenizer,\r\nAttributeError: 'NoneType' object has no attribute 'sizes'\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1550/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1550/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1527", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1527/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1527/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1527/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1527", "id": 540330440, "node_id": "MDU6SXNzdWU1NDAzMzA0NDA=", "number": 1527, "title": "Multihead attention without bias still tries to initalize the bias", "user": {"login": "felix-schneider", "id": 208336, "node_id": "MDQ6VXNlcjIwODMzNg==", "avatar_url": "https://avatars.githubusercontent.com/u/208336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/felix-schneider", "html_url": "https://github.com/felix-schneider", "followers_url": "https://api.github.com/users/felix-schneider/followers", "following_url": "https://api.github.com/users/felix-schneider/following{/other_user}", "gists_url": "https://api.github.com/users/felix-schneider/gists{/gist_id}", "starred_url": "https://api.github.com/users/felix-schneider/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/felix-schneider/subscriptions", "organizations_url": "https://api.github.com/users/felix-schneider/orgs", "repos_url": "https://api.github.com/users/felix-schneider/repos", "events_url": "https://api.github.com/users/felix-schneider/events{/privacy}", "received_events_url": "https://api.github.com/users/felix-schneider/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2019-12-19T13:58:17Z", "updated_at": "2022-05-30T08:19:46Z", "closed_at": "2019-12-19T16:56:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\nIf you create a `MultiheadAttention` with `bias=False`, you get the following:\r\n\r\n```\r\nTraceback (most recent call last):\r\nFile \"xxx/fairseq/fairseq/modules/multihead_attention.py\", line 56, in __init__\r\n    self.reset_parameters()\r\n  File \"xxx/fairseq/fairseq/modules/multihead_attention.py\", line 82, in reset_parameters\r\n    nn.init.constant_(self.out_proj.bias, 0.)\r\n  File \"xxx/torch/nn/init.py\", line 120, in constant_\r\n    return _no_grad_fill_(tensor, val)\r\n  File \"xxx/torch/nn/init.py\", line 24, in _no_grad_fill_\r\n    return tensor.fill_(val)\r\nAttributeError: 'NoneType' object has no attribute 'fill_'\r\n```\r\n\r\n### To Reproduce\r\n\r\n```python\r\nfrom fairseq.modules import MultiheadAttention\r\na = MultiheadAttention(1, 1, bias=False)\r\n```\r\n\r\n### Environment\r\n\r\n - fairseq Version: master\r\n - PyTorch Version: 1.3.1\r\n - OS: Linux\r\n - How you installed fairseq: source\r\n - Build command you used: `pip install -e .`\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: N/A\r\n", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1527/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1527/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1509", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1509/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1509/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1509/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1509", "id": 538472597, "node_id": "MDU6SXNzdWU1Mzg0NzI1OTc=", "number": 1509, "title": "[XLM-RoBERTa] Cannot find callable xlmr.large in hubconf", "user": {"login": "stefan-it", "id": 20651387, "node_id": "MDQ6VXNlcjIwNjUxMzg3", "avatar_url": "https://avatars.githubusercontent.com/u/20651387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefan-it", "html_url": "https://github.com/stefan-it", "followers_url": "https://api.github.com/users/stefan-it/followers", "following_url": "https://api.github.com/users/stefan-it/following{/other_user}", "gists_url": "https://api.github.com/users/stefan-it/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefan-it/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefan-it/subscriptions", "organizations_url": "https://api.github.com/users/stefan-it/orgs", "repos_url": "https://api.github.com/users/stefan-it/repos", "events_url": "https://api.github.com/users/stefan-it/events{/privacy}", "received_events_url": "https://api.github.com/users/stefan-it/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-12-16T15:11:53Z", "updated_at": "2019-12-16T16:11:05Z", "closed_at": "2019-12-16T16:11:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi,\r\n\r\nthanks for releasing the final base and large models for XLM-RoBERTa.\r\n\r\nUnfortunately, the models can't be loaded from `torch.hub`:\r\n \r\n```python\r\nimport torch\r\nxlmr = torch.hub.load('pytorch/fairseq', 'xlmr.large')\r\n```\r\n\r\nThe following error message is thrown:\r\n\r\n```bash\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-19-30bbbd5d0579> in <module>\r\n      1 import torch\r\n      2 print(torch.__version__)\r\n----> 3 xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.large')\r\n      4 xlmr.eval()  # disable dropout (or leave in train mode to finetune)\r\n\r\n/media/stefan/00eb2e8f-6826-4043-a1d7-937b9d5341c1/.venvs/flair/lib/python3.7/site-packages/torch/hub.py in load(github, model, *args, **kwargs)\r\n    355     hub_module = import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)\r\n    356 \r\n--> 357     entry = _load_entry_from_hubconf(hub_module, model)\r\n    358 \r\n    359     model = entry(*args, **kwargs)\r\n\r\n/media/stefan/00eb2e8f-6826-4043-a1d7-937b9d5341c1/.venvs/flair/lib/python3.7/site-packages/torch/hub.py in _load_entry_from_hubconf(m, model)\r\n    228 \r\n    229     if func is None or not callable(func):\r\n--> 230         raise RuntimeError('Cannot find callable {} in hubconf'.format(model))\r\n    231 \r\n    232     return func\r\n\r\nRuntimeError: Cannot find callable xlmr.large in hubconf\r\n```\r\n\r\nI'm using PyTorch 1.3.1 and I cleared the `.cache/torch` folder. \r\n\r\nBefore loading the message:\r\n\r\n```bash\r\nUsing cache found in /home/stefan/.cache/torch/hub/pytorch_fairseq_master\r\n```\r\n\r\nappears. I checked the `model.py` file in the `fairseq/models/roberta` folder and it does include the `xlmr` hub definition \ud83e\udd14\r\n\r\n(Downloading the model manually + loading it works as a workaround).", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1509/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1509/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1488", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1488/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1488/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1488/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1488", "id": 536480960, "node_id": "MDU6SXNzdWU1MzY0ODA5NjA=", "number": 1488, "title": "Arguments out of date", "user": {"login": "mayhewsw", "id": 587755, "node_id": "MDQ6VXNlcjU4Nzc1NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/587755?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mayhewsw", "html_url": "https://github.com/mayhewsw", "followers_url": "https://api.github.com/users/mayhewsw/followers", "following_url": "https://api.github.com/users/mayhewsw/following{/other_user}", "gists_url": "https://api.github.com/users/mayhewsw/gists{/gist_id}", "starred_url": "https://api.github.com/users/mayhewsw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mayhewsw/subscriptions", "organizations_url": "https://api.github.com/users/mayhewsw/orgs", "repos_url": "https://api.github.com/users/mayhewsw/repos", "events_url": "https://api.github.com/users/mayhewsw/events{/privacy}", "received_events_url": "https://api.github.com/users/mayhewsw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2019-12-11T16:27:53Z", "updated_at": "2019-12-17T01:22:20Z", "closed_at": "2019-12-17T01:22:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using `--replace-unk` in fairseq-generate, it checks if the `--raw-text` argument is present. But if the `--raw-text` argument is present, it throws a warning that it is deprecated in favor of `--dataset-impl=raw`. \r\n\r\nFix: change the check [here](https://github.com/pytorch/fairseq/blob/master/generate.py#L20) to check for `dataset-impl=raw`.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1488/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1488/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1435", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1435/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1435/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1435/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1435", "id": 529166932, "node_id": "MDU6SXNzdWU1MjkxNjY5MzI=", "number": 1435, "title": "Train a new translate model with lev_transform is wrong.", "user": {"login": "xiaoshengjun", "id": 24401657, "node_id": "MDQ6VXNlcjI0NDAxNjU3", "avatar_url": "https://avatars.githubusercontent.com/u/24401657?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiaoshengjun", "html_url": "https://github.com/xiaoshengjun", "followers_url": "https://api.github.com/users/xiaoshengjun/followers", "following_url": "https://api.github.com/users/xiaoshengjun/following{/other_user}", "gists_url": "https://api.github.com/users/xiaoshengjun/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiaoshengjun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiaoshengjun/subscriptions", "organizations_url": "https://api.github.com/users/xiaoshengjun/orgs", "repos_url": "https://api.github.com/users/xiaoshengjun/repos", "events_url": "https://api.github.com/users/xiaoshengjun/events{/privacy}", "received_events_url": "https://api.github.com/users/xiaoshengjun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2019-11-27T07:27:07Z", "updated_at": "2020-01-09T15:52:36Z", "closed_at": "2020-01-09T15:52:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I download the original  WMT'14 En-De dataset, and I processed it with scripts:\r\n\r\ncommon_params='--source-lang en --target-lang de'\r\ntrainpref='./out/data_wm_raw/train.en-de'\r\nvalidpref='./out/data_wm_raw/valid.en-de'\r\npython preprocess.py \\\r\n$common_params \\\r\n--trainpref $trainpref \\\r\n--validpref $validpref \\\r\n--destdir './out/data_wm_be/' \\\r\n--workers 4 \\ \r\n--joined-dictionary \\\r\n--thresholdtgt 5 \\ \r\n--thresholdsrc 5 \\ \r\n\r\nAnd then , I trained the model with the scripts:\r\n\r\nDATA_BIN=./out/data_wm_be/\r\nCUDA_VISIBLE_DEVICES=$device nohup python train.py $DATA_BIN \\\r\n  --save-dir $MODELS \\\r\n  --max-tokens 8000 \\\r\n  --ddp-backend=no_c10d \\\r\n  --noise random_delete \\\r\n  --share-all-embeddings \\\r\n  --task translation_lev \\\r\n  --train-subset train \\\r\n  --valid-subset valid \\\r\n  --criterion nat_loss \\\r\n  --arch levenshtein_transformer \\\r\n  --optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n  --lr 0.0005 --lr-scheduler inverse_sqrt \\\r\n  --min-lr '1e-09' --warmup-updates 10000 \\\r\n  --warmup-init-lr '1e-07' --label-smoothing 0.1 \\\r\n  --dropout 0.3 --weight-decay 0.01 \\\r\n  --decoder-learned-pos \\\r\n  --encoder-learned-pos \\\r\n  --apply-bert-init \\\r\n  --log-format 'simple' \\\r\n  --fixed-validation-seed 7 \\ \r\n  --no-progress-bar \\\r\n  --save-interval-updates 10000 \\\r\n  --max-update 300000 \\\r\n  --log-interval 1000  > $OUT/log$exp.out 2>&1 &\r\n\r\nand the error is :\r\n| model levenshtein_transformer, criterion LabelSmoothedDualImitationCriterion\r\n| num. model params: 65084416 (num. trained: 65084416)\r\n| training on 2 GPUs\r\n| max tokens per GPU = 8000 and max sentences per GPU = None\r\n| no existing checkpoint found out/models20191127wm/checkpoint_last.pt\r\n| loading train data for epoch 0\r\n| loaded 3961179 examples from: ./out/data_wm_be/train.en-de.en\r\n| loaded 3961179 examples from: ./out/data_wm_be/train.en-de.de\r\n| ./out/data_wm_be/ train en-de 3961179 examples\r\n| NOTICE: your device may support faster training with --fp16\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 337, in <module>\r\n    cli_main()\r\n  File \"train.py\", line 329, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/root/anaconda3/envs/len_transform/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/root/anaconda3/envs/len_transform/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 107, in join\r\n    (error_index, name)\r\nException: process 0 terminated with signal SIGSEGV\r\n^C\r\n\r\nCould you help me find out what makes this situation?", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1435/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1393", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1393/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1393/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1393/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1393", "id": 524961054, "node_id": "MDU6SXNzdWU1MjQ5NjEwNTQ=", "number": 1393, "title": "Error with mutilingual model and fairseq-generate", "user": {"login": "feralvam", "id": 2760680, "node_id": "MDQ6VXNlcjI3NjA2ODA=", "avatar_url": "https://avatars.githubusercontent.com/u/2760680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/feralvam", "html_url": "https://github.com/feralvam", "followers_url": "https://api.github.com/users/feralvam/followers", "following_url": "https://api.github.com/users/feralvam/following{/other_user}", "gists_url": "https://api.github.com/users/feralvam/gists{/gist_id}", "starred_url": "https://api.github.com/users/feralvam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/feralvam/subscriptions", "organizations_url": "https://api.github.com/users/feralvam/orgs", "repos_url": "https://api.github.com/users/feralvam/repos", "events_url": "https://api.github.com/users/feralvam/events{/privacy}", "received_events_url": "https://api.github.com/users/feralvam/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-11-19T11:52:28Z", "updated_at": "2019-12-19T20:43:55Z", "closed_at": "2019-12-19T20:43:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI got an error when generating with a trained multilingual model. I hope you can help me understand what went wrong and how to fix it. Some context: I'm basically trying to use the multilingual architecture as a multitask model to combine different datasets for a monolingual task (each task is a \"language\" pair).\r\n\r\nThe command used for training a many-to-one model (i.e. shared decoder) is:\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=1,2,3 fairseq-train \"${data_dir}/bin\" \\\r\n  --ddp-backend=no_c10d \\\r\n  --task multilingual_translation --lang-pairs orig-simp,complex-simp,long-simp \\\r\n  --arch multilingual_transformer \\\r\n  --share-decoders --share-decoder-input-output-embed \\\r\n  --encoder-embed-path \"${glove}\" --encoder-embed-dim 300 --encoder-ffn-embed-dim 300 \\\r\n  --decoder-embed-path \"${glove}\" --decoder-embed-dim 300 --decoder-ffn-embed-dim 300 \\\r\n  --encoder-attention-heads 5 --decoder-attention-heads 5 \\\r\n  --encoder-layers 4 --decoder-layers 4 \\\r\n  --optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n  --lr 0.0005 --lr-scheduler inverse_sqrt --min-lr '1e-09' \\\r\n  --label-smoothing 0.1 --dropout 0.3 --weight-decay 0.0001 \\\r\n  --criterion label_smoothed_cross_entropy --max-update 10000 \\\r\n  --warmup-updates 4000 --warmup-init-lr '1e-07' \\\r\n  --max-tokens 4000 --update-freq 4 \\\r\n  --save-dir \"${model_dir}\" --tensorboard-logdir \"${log_dir}\" \\\r\n```\r\n\r\nTraining proceeds without problems. Now, I want to generate the output for the 'test' subset of one of the \"language\" pairs (orig-simp) that the model was trained on. \r\n\r\n```\r\nfairseq-generate \"${data_dir}/bin\" \\\r\n  --path \"${model_dir}/${checkpoint_name}.pt\" \\\r\n  --lang-pairs orig-simp,complex-simp,long-simp \\\r\n  --task multilingual_translation --source-lang orig --target-lang simp \\\r\n  --batch-size 128 --beam 5 --remove-bpe=sentencepiece \\\r\n  --gen-subset test > \"${experiment_dir}/outputs/${output_name}.out\"\r\n```\r\n\r\nAfter running the command I get the following error:\r\n\r\n```\r\n/experiments/falva/tools/fairseq/fairseq/models/fairseq_model.py:280: UserWarning: FairseqModel is deprecated, please use FairseqEncoderDecoderModel or BaseFairseqModel instead\r\n  for key in self.keys\r\nTraceback (most recent call last):\r\n  File \"/home/falva/anaconda3/envs/mtl4ts/bin/fairseq-generate\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')()\r\n  File \"/experiments/falva/tools/fairseq/fairseq_cli/generate.py\", line 190, in cli_main\r\n    main(args)\r\n  File \"/experiments/falva/tools/fairseq/fairseq_cli/generate.py\", line 47, in main\r\n    task=task,\r\n  File \"/experiments/falva/tools/fairseq/fairseq/checkpoint_utils.py\", line 167, in load_model_ensemble\r\n    ensemble, args, _task = load_model_ensemble_and_task(filenames, arg_overrides, task)\r\n  File \"/experiments/falva/tools/fairseq/fairseq/checkpoint_utils.py\", line 186, in load_model_ensemble_and_task\r\n    model.load_state_dict(state['model'], strict=True, args=args)\r\nTypeError: load_state_dict() got an unexpected keyword argument 'args'\r\n```\r\n\r\nCould you help me understand what's going on? \r\n\r\nSome additional and perhaps useful information and questions:\r\n\r\n- For all 'language' pairs, all dataset splits (train/valid/test) were binarized before training using `fairseq-preprocess`. That's why I decided to use `fairseq-generate` instead of `fairseq-interactive`. I don't think this could be the source of the problem, right? Or is there a particular reason why, for the multilingual model, it's recommended to use `interactive` rather than `generate` as in the example you provide in your repo?\r\n- Since in this case I'm using a many-to-one model (just as in the example you provide), there is no need to use the `--encoder-langtok` or `--decoder-langtok` arguments. To my understanding, `--encoder-langtok` comes into play if I wanted to train a one-to-many model (`--encoder-langtok tgt`). But, when would `--decoder-langtok` be necessary in your experience?\r\n\r\nThank you in advance for all the help.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1393/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1393/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1343", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1343/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1343/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1343/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1343", "id": 516958424, "node_id": "MDU6SXNzdWU1MTY5NTg0MjQ=", "number": 1343, "title": "fp32 inference is faster than fp16, pytorch >= 1.2", "user": {"login": "mingruimingrui", "id": 18568364, "node_id": "MDQ6VXNlcjE4NTY4MzY0", "avatar_url": "https://avatars.githubusercontent.com/u/18568364?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingruimingrui", "html_url": "https://github.com/mingruimingrui", "followers_url": "https://api.github.com/users/mingruimingrui/followers", "following_url": "https://api.github.com/users/mingruimingrui/following{/other_user}", "gists_url": "https://api.github.com/users/mingruimingrui/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingruimingrui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingruimingrui/subscriptions", "organizations_url": "https://api.github.com/users/mingruimingrui/orgs", "repos_url": "https://api.github.com/users/mingruimingrui/repos", "events_url": "https://api.github.com/users/mingruimingrui/events{/privacy}", "received_events_url": "https://api.github.com/users/mingruimingrui/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 679100397, "node_id": "MDU6TGFiZWw2NzkxMDAzOTc=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/question", "name": "question", "color": "ededed", "default": true, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-04T04:52:23Z", "updated_at": "2019-12-24T07:00:41Z", "closed_at": "2019-12-24T07:00:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'd like to leave an interesting nugget.\r\n\r\nOn pytorch==1.0, fp16 inference is faster than fp32 but on pytorch>=1.2,\r\n\r\nOn german news-test 2014\r\n\r\n| Precision | Inference speed (sents/sec) | \r\n| --- | --- |\r\n| fp32 | 324.16 |\r\n| fp16 | 307.11 |\r\n\r\n*german news-test 2014 was pre-sorted according to the number of tokens in each sentence before batching. This is to minimize the number of padding tokens required.\r\n\r\nEnvironmental setup\r\n\r\nCPU: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\r\nGPU: V100\r\n\r\nCUDA==10.0 (conda)\r\ncuDNN==7.6.3 (conda)\r\n\r\npython==3.6\r\nfairseq==0.8.0 (pip)\r\npytorch==1.3 (conda)\r\n\r\n@myleott Could I check with you if these results are expected?", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1343/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1343/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1339", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1339/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1339/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1339/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1339", "id": 516538086, "node_id": "MDU6SXNzdWU1MTY1MzgwODY=", "number": 1339, "title": "attn_mask is calculated but not used in encoder", "user": {"login": "fuzihaofzh", "id": 1419566, "node_id": "MDQ6VXNlcjE0MTk1NjY=", "avatar_url": "https://avatars.githubusercontent.com/u/1419566?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fuzihaofzh", "html_url": "https://github.com/fuzihaofzh", "followers_url": "https://api.github.com/users/fuzihaofzh/followers", "following_url": "https://api.github.com/users/fuzihaofzh/following{/other_user}", "gists_url": "https://api.github.com/users/fuzihaofzh/gists{/gist_id}", "starred_url": "https://api.github.com/users/fuzihaofzh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fuzihaofzh/subscriptions", "organizations_url": "https://api.github.com/users/fuzihaofzh/orgs", "repos_url": "https://api.github.com/users/fuzihaofzh/repos", "events_url": "https://api.github.com/users/fuzihaofzh/events{/privacy}", "received_events_url": "https://api.github.com/users/fuzihaofzh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-02T08:56:08Z", "updated_at": "2020-02-08T02:23:46Z", "closed_at": "2020-02-08T02:23:46Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://github.com/pytorch/fairseq/blob/828c1ca7522278aa6c1eaf91fe0425b8f40dd832/fairseq/modules/transformer_layer.py#L87\r\n\r\nattn_mask is calculated in encoder layer. But it hasn't been used. Why don't we send it directly into the self_attention layer? Is there any problem if we directly send it into the self_attention layer?", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1339/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1339/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1315", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1315/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1315/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1315/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1315", "id": 513368959, "node_id": "MDU6SXNzdWU1MTMzNjg5NTk=", "number": 1315, "title": "FairseqMultimodel doesn't have forward_decoder", "user": {"login": "postrou", "id": 27743016, "node_id": "MDQ6VXNlcjI3NzQzMDE2", "avatar_url": "https://avatars.githubusercontent.com/u/27743016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/postrou", "html_url": "https://github.com/postrou", "followers_url": "https://api.github.com/users/postrou/followers", "following_url": "https://api.github.com/users/postrou/following{/other_user}", "gists_url": "https://api.github.com/users/postrou/gists{/gist_id}", "starred_url": "https://api.github.com/users/postrou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/postrou/subscriptions", "organizations_url": "https://api.github.com/users/postrou/orgs", "repos_url": "https://api.github.com/users/postrou/repos", "events_url": "https://api.github.com/users/postrou/events{/privacy}", "received_events_url": "https://api.github.com/users/postrou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-28T15:13:53Z", "updated_at": "2019-12-19T20:58:56Z", "closed_at": "2019-12-19T20:58:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I try to inference model, inheriting from `FairseqMultimodel` (I've created custom `MultilingualLSTMModel` with `MultilingualTransformerModel` as example) with either fairseq-generate or fairseq-interactive, it crashes, saying\r\n```\r\nAttributeError: 'MultilingualLSTMModel' object has no attribute 'forward_decoder'\r\n```\r\nThere is no such method both in `FairseqMultiModel` and `BaseFairseqModel` (which is parent of `FairseqMultiModel`).\r\n\r\nI suppose, the same issue will be with multilingual transformer, because both it and my class inherit from `FairseqMultiModel`.", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1315/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1315/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1274", "repository_url": "https://api.github.com/repos/facebookresearch/fairseq", "labels_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1274/labels{/name}", "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1274/comments", "events_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1274/events", "html_url": "https://github.com/facebookresearch/fairseq/issues/1274", "id": 509455200, "node_id": "MDU6SXNzdWU1MDk0NTUyMDA=", "number": 1274, "title": "Using ResamplingDataset causes trouble with filtering to large samples", "user": {"login": "villmow", "id": 2743060, "node_id": "MDQ6VXNlcjI3NDMwNjA=", "avatar_url": "https://avatars.githubusercontent.com/u/2743060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/villmow", "html_url": "https://github.com/villmow", "followers_url": "https://api.github.com/users/villmow/followers", "following_url": "https://api.github.com/users/villmow/following{/other_user}", "gists_url": "https://api.github.com/users/villmow/gists{/gist_id}", "starred_url": "https://api.github.com/users/villmow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/villmow/subscriptions", "organizations_url": "https://api.github.com/users/villmow/orgs", "repos_url": "https://api.github.com/users/villmow/repos", "events_url": "https://api.github.com/users/villmow/events{/privacy}", "received_events_url": "https://api.github.com/users/villmow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679100392, "node_id": "MDU6TGFiZWw2NzkxMDAzOTI=", "url": "https://api.github.com/repos/facebookresearch/fairseq/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2019-10-19T13:55:02Z", "updated_at": "2020-07-14T15:42:37Z", "closed_at": "2020-07-14T15:42:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, thanks for the awesome library! \r\n\r\n**Problem**: I use the ResamplingDataset to downsample frequent examples in a custom task. After every epoch a subset of the full training data is selected for the next epoch. This selection affects the filtering by size of the samples which is implemented in `get_batch_iterator`. However after the filtered indices are calculated, `set_epoch` is called again (with an incremented epoch count)! This causes a resampling, which again invalidates the filtered indices! \r\n\r\n**What happens**:\r\n1. I reset the `dataset_to_epoch_iter` to get a new iterator after each epoch.\r\n2. `FairseqTask.get_batch_iterator` is called\r\n3. Inside  `get_batch_iterator` the datasets `ResamplingDataset.set_epoch` is called, in which the resampling is implemented:\r\nhttps://github.com/pytorch/fairseq/blob/b8d024e9b8c20058dd7282f1418ebef00bfb8974/fairseq/tasks/fairseq_task.py#L135-L136\r\n4. The samples are filtered by size\r\nhttps://github.com/pytorch/fairseq/blob/b8d024e9b8c20058dd7282f1418ebef00bfb8974/fairseq/tasks/fairseq_task.py#L138-L152\r\n5. An EpochIterator is instantiated for which `next_epoch_itr` is called\r\nhttps://github.com/pytorch/fairseq/blob/b8d024e9b8c20058dd7282f1418ebef00bfb8974/fairseq/tasks/fairseq_task.py#L155-L164\r\n6. The trainer calls `next_epoch_itr` and inside of it the epoch counter is incremented and then `ResamplingDataset.set_epoch` is called again, which resamples the data (and thus the filtering indices are wrong).\r\nhttps://github.com/pytorch/fairseq/blob/b8d024e9b8c20058dd7282f1418ebef00bfb8974/fairseq/data/iterators.py#L187-L191\r\n7. This iterator is used and samples with wrong sizes are filtered.\r\n\r\nI hope I could describe the issue understandable. As a quick fix for me I uncommented the `set_epoch` call in EpochBatchIterator, which I guess is not the right approach.\r\n\r\nWhat is the correct way to handle the change of sizes after each epoch? ", "reactions": {"url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1274/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1274/timeline", "performed_via_github_app": null, "state_reason": "completed"}]