[{"url": "https://api.github.com/repos/huggingface/datasets/issues/5799", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5799/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5799/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5799/events", "html_url": "https://github.com/huggingface/datasets/issues/5799", "id": 1686334572, "node_id": "I_kwDODunzps5kg2xs", "number": 5799, "title": "Files downloaded to cache do not respect umask", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-04-27T08:06:05Z", "updated_at": "2023-04-27T09:30:17Z", "closed_at": "2023-04-27T09:30:17Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "As reported by @stas00, files downloaded to the cache do not respect umask:\r\n```bash\r\n$ ls -l /path/to/cache/datasets/downloads/\r\n-rw------- 1 uername    username    150M Apr 25 16:41 5e646c1d600f065adaeb134e536f6f2f296a6d804bd1f0e1fdcd20ee28c185c6 \r\n```\r\n\r\nRelated to:\r\n- #2065", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5799/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5785", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5785/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5785/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5785/events", "html_url": "https://github.com/huggingface/datasets/issues/5785", "id": 1680956964, "node_id": "I_kwDODunzps5kMV4k", "number": 5785, "title": "Unsupported data files raise TypeError: 'NoneType' object is not iterable", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-04-24T10:38:03Z", "updated_at": "2023-04-27T12:57:30Z", "closed_at": "2023-04-27T12:57:30Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Currently, we raise a TypeError for unsupported data files:\r\n```\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\nSee:\r\n- https://github.com/huggingface/datasets-server/issues/1073\r\n\r\nWe should give a more informative error message.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5785/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5734", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5734/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5734/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5734/events", "html_url": "https://github.com/huggingface/datasets/issues/5734", "id": 1662058028, "node_id": "I_kwDODunzps5jEP4s", "number": 5734, "title": "Remove temporary pin of fsspec", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-04-11T09:04:17Z", "updated_at": "2023-04-11T11:04:52Z", "closed_at": "2023-04-11T11:04:52Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Once root cause is found and fixed, remove the temporary pin introduced by:\r\n- #5731", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5734/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5734/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5730", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5730/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5730/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5730/events", "html_url": "https://github.com/huggingface/datasets/issues/5730", "id": 1662007926, "node_id": "I_kwDODunzps5jEDp2", "number": 5730, "title": "CI is broken: ValueError: Name (mock) already in the registry and clobber is False", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-04-11T08:29:46Z", "updated_at": "2023-04-11T08:47:56Z", "closed_at": "2023-04-11T08:47:56Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "CI is broken for `test_py310`.\r\n\r\nSee: https://github.com/huggingface/datasets/actions/runs/4665326892/jobs/8258580948\r\n```\r\n=========================== short test summary info ============================\r\nERROR tests/test_builder.py::test_builder_with_filesystem_download_and_prepare - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_builder.py::test_builder_with_filesystem_download_and_prepare_reload - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_dataset_dict.py::test_dummy_datasetdict_serialize_fs - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_file_utils.py::test_get_from_cache_fsspec - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_filesystem.py::test_is_remote_filesystem - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xexists[tmp_path/file.txt-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xexists[tmp_path/file_that_doesnt_exist.txt-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xexists[mock://top_level/second_level/date=2019-10-01/a.parquet-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xexists[mock://top_level/second_level/date=2019-10-01/file_that_doesnt_exist.parquet-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xlistdir[tmp_path-expected_paths0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xlistdir[mock://-expected_paths1] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xlistdir[mock://top_level-expected_paths2] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xlistdir[mock://top_level/second_level/date=2019-10-01-expected_paths3] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xisdir[tmp_path-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xisdir[tmp_path/file.txt-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xisdir[mock://-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xisdir[mock://top_level-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xisdir[mock://dir_that_doesnt_exist-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xisfile[tmp_path/file.txt-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xisfile[tmp_path/file_that_doesnt_exist.txt-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xisfile[mock://-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xisfile[mock://top_level/second_level/date=2019-10-01/a.parquet-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xgetsize[tmp_path/file.txt-100] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xgetsize[mock://-0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xgetsize[mock://top_level/second_level/date=2019-10-01/a.parquet-100] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xglob[tmp_path/*.txt-expected_paths0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xglob[mock://*-expected_paths1] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xglob[mock://top_*-expected_paths2] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xglob[mock://top_level/second_level/date=2019-10-0[1-4]-expected_paths3] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xglob[mock://top_level/second_level/date=2019-10-0[1-4]/*-expected_paths4] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xwalk[tmp_path-expected_outputs0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::test_xwalk[mock://top_level/second_level-expected_outputs1] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_exists[tmp_path/file.txt-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_exists[tmp_path/file_that_doesnt_exist.txt-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_exists[mock://top_level/second_level/date=2019-10-01/a.parquet-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_exists[mock://top_level/second_level/date=2019-10-01/file_that_doesnt_exist.parquet-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_glob[tmp_path-*.txt-expected_paths0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_glob[mock://-*-expected_paths1] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_glob[mock://-top_*-expected_paths2] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_glob[mock://top_level/second_level-date=2019-10-0[1-4]-expected_paths3] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_glob[mock://top_level/second_level-date=2019-10-0[1-4]/*-expected_paths4] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[tmp_path-*.txt-expected_paths0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[mock://-date=2019-10-0[1-4]-expected_paths1] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[mock://top_level-date=2019-10-0[1-4]-expected_paths2] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[mock://-date=2019-10-0[1-4]/*-expected_paths3] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[mock://top_level-date=2019-10-0[1-4]/*-expected_paths4] - ValueError: Name (mock) already in the registry and clobber is False\r\n===== 2105 passed, 18 skipped, 38 warnings, 46 errors in 236.22s (0:03:56) =====\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5730/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5728", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5728/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5728/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5728/events", "html_url": "https://github.com/huggingface/datasets/issues/5728", "id": 1661925932, "node_id": "I_kwDODunzps5jDvos", "number": 5728, "title": "The order of data split names is nondeterministic", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-04-11T07:31:25Z", "updated_at": "2023-04-26T15:05:13Z", "closed_at": "2023-04-26T15:05:13Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "After this CI error: https://github.com/huggingface/datasets/actions/runs/4639528358/jobs/8210492953?pr=5718\r\n```\r\nFAILED tests/test_data_files.py::test_get_data_files_patterns[data_file_per_split4] - AssertionError: assert ['random', 'train'] == ['train', 'random']\r\n  At index 0 diff: 'random' != 'train'\r\n  Full diff:\r\n  - ['train', 'random']\r\n  + ['random', 'train']\r\n```\r\nI have checked locally and found out that the data split order is nondeterministic.\r\n\r\nThis is caused by the use of `set` for sharded splits.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5728/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5728/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5696", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5696/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5696/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5696/events", "html_url": "https://github.com/huggingface/datasets/issues/5696", "id": 1651707008, "node_id": "I_kwDODunzps5icwyA", "number": 5696, "title": "Shuffle a sharded iterable dataset without seed can lead to duplicate data", "user": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-04-03T09:40:03Z", "updated_at": "2023-04-04T14:58:18Z", "closed_at": "2023-04-04T14:58:18Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "As reported in https://github.com/huggingface/datasets/issues/5360\r\n\r\nIf `seed=None` in `.shuffle()`, shuffled datasets don't use the same shuffling seed across nodes.\r\n\r\nBecause of that, the lists of shards is not shuffled the same way across nodes, and therefore some shards may be assigned to multiple nodes instead of exactly one.\r\n\r\nThis can happen only when you have a number of shards that is a factor of the number of nodes.\r\n\r\nThe current workaround is to always set a `seed` in `.shuffle()`", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5696/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5682", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5682/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5682/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5682/events", "html_url": "https://github.com/huggingface/datasets/issues/5682", "id": 1646000571, "node_id": "I_kwDODunzps5iG_m7", "number": 5682, "title": "ValueError when passing ignore_verifications", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-03-29T15:00:30Z", "updated_at": "2023-03-29T17:28:58Z", "closed_at": "2023-03-29T17:28:58Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "When passing `ignore_verifications=True` to `load_dataset`, we get a ValueError:\r\n```\r\nValueError: 'none' is not a valid VerificationMode\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5682/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5663", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5663/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5663/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5663/events", "html_url": "https://github.com/huggingface/datasets/issues/5663", "id": 1637173248, "node_id": "I_kwDODunzps5hlUgA", "number": 5663, "title": "CI is broken: ModuleNotFoundError: jax requires jaxlib to be installed", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-03-23T09:39:43Z", "updated_at": "2023-03-23T10:09:55Z", "closed_at": "2023-03-23T10:09:55Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "CI test_py310 is broken: see https://github.com/huggingface/datasets/actions/runs/4498945505/jobs/7916194236?pr=5662\r\n```\r\nFAILED tests/test_arrow_dataset.py::BaseDatasetTest::test_map_jax_in_memory - ModuleNotFoundError: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.\r\nFAILED tests/test_arrow_dataset.py::BaseDatasetTest::test_map_jax_on_disk - ModuleNotFoundError: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.\r\nFAILED tests/test_formatting.py::FormatterTest::test_jax_formatter - ModuleNotFoundError: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.\r\nFAILED tests/test_formatting.py::FormatterTest::test_jax_formatter_audio - ModuleNotFoundError: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.\r\nFAILED tests/test_formatting.py::FormatterTest::test_jax_formatter_device - ModuleNotFoundError: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.\r\nFAILED tests/test_formatting.py::FormatterTest::test_jax_formatter_image - ModuleNotFoundError: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.\r\nFAILED tests/test_formatting.py::FormatterTest::test_jax_formatter_jnp_array_kwargs - ModuleNotFoundError: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.\r\nFAILED tests/features/test_features.py::CastToPythonObjectsTest::test_cast_to_python_objects_jax - ModuleNotFoundError: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.\r\n===== 8 failed, 2147 passed, 10 skipped, 37 warnings in 228.69s (0:03:48) ======\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5663/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5661", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5661/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5661/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5661/events", "html_url": "https://github.com/huggingface/datasets/issues/5661", "id": 1637129445, "node_id": "I_kwDODunzps5hlJzl", "number": 5661, "title": "CI is broken: Unnecessary `dict` comprehension", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-03-23T09:13:01Z", "updated_at": "2023-03-23T09:37:51Z", "closed_at": "2023-03-23T09:37:51Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "CI check_code_quality is broken:\r\n```\r\nsrc/datasets/arrow_dataset.py:3267:35: C416 [*] Unnecessary `dict` comprehension (rewrite using `dict()`)\r\nFound 1 error.\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5661/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5616", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5616/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5616/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5616/events", "html_url": "https://github.com/huggingface/datasets/issues/5616", "id": 1612932508, "node_id": "I_kwDODunzps5gI2Wc", "number": 5616, "title": "CI is broken after fsspec-2023.3.0 release", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2023-03-07T08:06:39Z", "updated_at": "2023-03-07T08:37:29Z", "closed_at": "2023-03-07T08:37:29Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "As reported by @lhoestq, our CI is broken after `fsspec` 2023.3.0 release:\r\n```\r\nFAILED tests/test_filesystem.py::test_compression_filesystems[Bz2FileSystem] - AssertionError: assert [{'created': ...: False, ...}] == ['file.txt']\r\n  At index 0 diff: {'name': 'file.txt', 'size': 70, 'type': 'file', 'created': 1678175677.1887748, 'islink': False, 'mode': 33188, 'uid': 1001, 'gid': 123, 'mtime': 1678175677.1887748, 'ino': 286957, 'nlink': 1} != 'file.txt'\r\n  Full diff:\r\n    [\r\n  -  'file.txt',\r\n  +  {'created': 1678175677.1887748,\r\n  +   'gid': 123,\r\n  +   'ino': 286957,\r\n  +   'islink': False,\r\n  +   'mode': 33188,\r\n  +   'mtime': 1678175677.1887748,\r\n  +   'name': 'file.txt',\r\n  +   'nlink': 1,\r\n  +   'size': 70,\r\n  +   'type': 'file',\r\n  +   'uid': 1001},\r\n    ]\r\n```\r\nAlso:\r\n```\r\nFAILED tests/test_filesystem.py::test_compression_filesystems[GzipFileSystem] - AssertionError: assert [{'created': ...: False, ...}] == ['file.txt']\r\nFAILED tests/test_filesystem.py::test_compression_filesystems[Lz4FileSystem] - AssertionError: assert [{'created': ...: False, ...}] == ['file.txt']\r\nFAILED tests/test_filesystem.py::test_compression_filesystems[XzFileSystem] - AssertionError: assert [{'created': ...: False, ...}] == ['file.txt']\r\nFAILED tests/test_filesystem.py::test_compression_filesystems[ZstdFileSystem] - AssertionError: assert [{'created': ...: False, ...}] == ['file.txt']\r\n===== 5 failed, 2134 passed, 18 skipped, 38 warnings in 157.21s (0:02:37) ======\r\n```\r\n\r\nSee:\r\n- fsspec/filesystem_spec#1205", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5616/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5616/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5586", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5586/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5586/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5586/events", "html_url": "https://github.com/huggingface/datasets/issues/5586", "id": 1602961544, "node_id": "I_kwDODunzps5fi0CI", "number": 5586, "title": ".sort() is broken when used after .filter(), only in 2.10.0", "user": {"login": "MattYoon", "id": 57797966, "node_id": "MDQ6VXNlcjU3Nzk3OTY2", "avatar_url": "https://avatars.githubusercontent.com/u/57797966?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MattYoon", "html_url": "https://github.com/MattYoon", "followers_url": "https://api.github.com/users/MattYoon/followers", "following_url": "https://api.github.com/users/MattYoon/following{/other_user}", "gists_url": "https://api.github.com/users/MattYoon/gists{/gist_id}", "starred_url": "https://api.github.com/users/MattYoon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MattYoon/subscriptions", "organizations_url": "https://api.github.com/users/MattYoon/orgs", "repos_url": "https://api.github.com/users/MattYoon/repos", "events_url": "https://api.github.com/users/MattYoon/events{/privacy}", "received_events_url": "https://api.github.com/users/MattYoon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-02-28T12:18:09Z", "updated_at": "2023-02-28T18:17:26Z", "closed_at": "2023-02-28T17:21:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Describe the bug\r\n\r\nHi, thank you for your support!\r\n\r\nIt seems like the addition of multiple key sort (#5502) in 2.10.0 broke the `.sort()` method.\r\n\r\nAfter filtering a dataset with `.filter()`, the `.sort()` seems to refer to the query_table index of the previous unfiltered dataset, resulting in an IndexError.\r\n\r\nThis only happens with the 2.10.0 release.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```Python\r\nfrom datasets import load_dataset\r\n\r\n# dataset with length of 1104\r\nds = load_dataset('glue', 'ax')['test']\r\nds = ds.filter(lambda x: x['idx'] > 1100)\r\nds.sort('premise')\r\nprint('Done')\r\n```\r\n\r\n  File \"/home/dongkeun/datasets_test/test.py\", line 5, in <module>\r\n    ds.sort('premise')\r\n  File \"/home/dongkeun/miniconda3/envs/datasets_test/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 528, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/dongkeun/miniconda3/envs/datasets_test/lib/python3.9/site-packages/datasets/fingerprint.py\", line 511, in wrapper\r\n    out = func(dataset, *args, **kwargs)\r\n  File \"/home/dongkeun/miniconda3/envs/datasets_test/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3959, in sort\r\n    sort_table = query_table(\r\n  File \"/home/dongkeun/miniconda3/envs/datasets_test/lib/python3.9/site-packages/datasets/formatting/formatting.py\", line 588, in query_table\r\n    _check_valid_index_key(key, size)\r\n  File \"/home/dongkeun/miniconda3/envs/datasets_test/lib/python3.9/site-packages/datasets/formatting/formatting.py\", line 537, in _check_valid_index_key\r\n    _check_valid_index_key(max(key), size=size)\r\n  File \"/home/dongkeun/miniconda3/envs/datasets_test/lib/python3.9/site-packages/datasets/formatting/formatting.py\", line 531, in _check_valid_index_key\r\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\r\nIndexError: Invalid key: 1103 is out of bounds for size 3\r\n\r\n### Expected behavior\r\n\r\nIt should sort the dataset and print \"Done\". Which it does on 2.9.0.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.10.0\r\n- Platform: Linux-5.15.0-41-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.16\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5586/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5495", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5495/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5495/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5495/events", "html_url": "https://github.com/huggingface/datasets/issues/5495", "id": 1566803452, "node_id": "I_kwDODunzps5dY4X8", "number": 5495, "title": "to_tf_dataset fails with datetime UTC columns even if not included in columns argument", "user": {"login": "dwyatte", "id": 2512762, "node_id": "MDQ6VXNlcjI1MTI3NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/2512762?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dwyatte", "html_url": "https://github.com/dwyatte", "followers_url": "https://api.github.com/users/dwyatte/followers", "following_url": "https://api.github.com/users/dwyatte/following{/other_user}", "gists_url": "https://api.github.com/users/dwyatte/gists{/gist_id}", "starred_url": "https://api.github.com/users/dwyatte/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dwyatte/subscriptions", "organizations_url": "https://api.github.com/users/dwyatte/orgs", "repos_url": "https://api.github.com/users/dwyatte/repos", "events_url": "https://api.github.com/users/dwyatte/events{/privacy}", "received_events_url": "https://api.github.com/users/dwyatte/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892877, "node_id": "MDU6TGFiZWwxOTM1ODkyODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2023-02-01T20:47:33Z", "updated_at": "2023-02-08T14:33:19Z", "closed_at": "2023-02-08T14:33:19Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Describe the bug\r\n\r\nThere appears to be some eager behavior in `to_tf_dataset` that runs against every column in a dataset even if they aren't included in the columns argument. This is problematic with datetime UTC columns due to them not working with zero copy. If I don't have UTC information in my datetime column, then everything works as expected.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom datasets import Dataset\r\n\r\n\r\ndf = pd.DataFrame(np.random.rand(2, 1), columns=[\"x\"])\r\n# df[\"dt\"] = pd.to_datetime([\"2023-01-01\", \"2023-01-01\"])  # works fine\r\ndf[\"dt\"] = pd.to_datetime([\"2023-01-01 00:00:00.00000+00:00\", \"2023-01-01 00:00:00.00000+00:00\"])\r\ndf.to_parquet(\"test.pq\")\r\n\r\n\r\nds = Dataset.from_parquet(\"test.pq\")\r\ntf_ds = ds.to_tf_dataset(columns=[\"x\"], batch_size=2, shuffle=True)\r\n```\r\n\r\n```\r\nArrowInvalid                              Traceback (most recent call last)\r\nCell In[1], line 12\r\n      8 df.to_parquet(\"test.pq\")\r\n     11 ds = Dataset.from_parquet(\"test.pq\")\r\n---> 12 tf_ds = ds.to_tf_dataset(columns=[\"r\"], batch_size=2, shuffle=True)\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:411, in TensorflowDatasetMixin.to_tf_dataset(self, batch_size, columns, shuffle, collate_fn, drop_remainder, collate_fn_args, label_cols, prefetch, num_workers)\r\n    407     dataset = self\r\n    409 # TODO(Matt, QL): deprecate the retention of label_ids and label\r\n--> 411 output_signature, columns_to_np_types = dataset._get_output_signature(\r\n    412     dataset,\r\n    413     collate_fn=collate_fn,\r\n    414     collate_fn_args=collate_fn_args,\r\n    415     cols_to_retain=cols_to_retain,\r\n    416     batch_size=batch_size if drop_remainder else None,\r\n    417 )\r\n    419 if \"labels\" in output_signature:\r\n    420     if (\"label_ids\" in columns or \"label\" in columns) and \"labels\" not in columns:\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:254, in TensorflowDatasetMixin._get_output_signature(dataset, collate_fn, collate_fn_args, cols_to_retain, batch_size, num_test_batches)\r\n    252 for _ in range(num_test_batches):\r\n    253     indices = sample(range(len(dataset)), test_batch_size)\r\n--> 254     test_batch = dataset[indices]\r\n    255     if cols_to_retain is not None:\r\n    256         test_batch = {key: value for key, value in test_batch.items() if key in cols_to_retain}\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:2590, in Dataset.__getitem__(self, key)\r\n   2588 def __getitem__(self, key):  # noqa: F811\r\n   2589     \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 2590     return self._getitem(\r\n   2591         key,\r\n   2592     )\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:2575, in Dataset._getitem(self, key, **kwargs)\r\n   2573 formatter = get_formatter(format_type, features=self.features, **format_kwargs)\r\n   2574 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n-> 2575 formatted_output = format_table(\r\n   2576     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   2577 )\r\n   2578 return formatted_output\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:634, in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    632 python_formatter = PythonFormatter(features=None)\r\n    633 if format_columns is None:\r\n--> 634     return formatter(pa_table, query_type=query_type)\r\n    635 elif query_type == \"column\":\r\n    636     if key in format_columns:\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:410, in Formatter.__call__(self, pa_table, query_type)\r\n    408     return self.format_column(pa_table)\r\n    409 elif query_type == \"batch\":\r\n--> 410     return self.format_batch(pa_table)\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/formatting/np_formatter.py:78, in NumpyFormatter.format_batch(self, pa_table)\r\n     77 def format_batch(self, pa_table: pa.Table) -> Mapping:\r\n---> 78     batch = self.numpy_arrow_extractor().extract_batch(pa_table)\r\n     79     batch = self.python_features_decoder.decode_batch(batch)\r\n     80     batch = self.recursive_tensorize(batch)\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:164, in NumpyArrowExtractor.extract_batch(self, pa_table)\r\n    163 def extract_batch(self, pa_table: pa.Table) -> dict:\r\n--> 164     return {col: self._arrow_array_to_numpy(pa_table[col]) for col in pa_table.column_names}\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:164, in <dictcomp>(.0)\r\n    163 def extract_batch(self, pa_table: pa.Table) -> dict:\r\n--> 164     return {col: self._arrow_array_to_numpy(pa_table[col]) for col in pa_table.column_names}\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:185, in NumpyArrowExtractor._arrow_array_to_numpy(self, pa_array)\r\n    181     else:\r\n    182         zero_copy_only = _is_zero_copy_only(pa_array.type) and all(\r\n    183             not _is_array_with_nulls(chunk) for chunk in pa_array.chunks\r\n    184         )\r\n--> 185         array: List = [\r\n    186             row for chunk in pa_array.chunks for row in chunk.to_numpy(zero_copy_only=zero_copy_only)\r\n    187         ]\r\n    188 else:\r\n    189     if isinstance(pa_array.type, _ArrayXDExtensionType):\r\n    190         # don't call to_pylist() to preserve dtype of the fixed-size array\r\n\r\nFile ~/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:186, in <listcomp>(.0)\r\n    181     else:\r\n    182         zero_copy_only = _is_zero_copy_only(pa_array.type) and all(\r\n    183             not _is_array_with_nulls(chunk) for chunk in pa_array.chunks\r\n    184         )\r\n    185         array: List = [\r\n--> 186             row for chunk in pa_array.chunks for row in chunk.to_numpy(zero_copy_only=zero_copy_only)\r\n    187         ]\r\n    188 else:\r\n    189     if isinstance(pa_array.type, _ArrayXDExtensionType):\r\n    190         # don't call to_pylist() to preserve dtype of the fixed-size array\r\n\r\nFile ~/venv/lib/python3.8/site-packages/pyarrow/array.pxi:1475, in pyarrow.lib.Array.to_numpy()\r\n\r\nFile ~/venv/lib/python3.8/site-packages/pyarrow/error.pxi:100, in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Needed to copy 1 chunks with 0 nulls, but zero_copy_only was True\r\n```\r\n\r\n### Expected behavior\r\n\r\nI think there are two potential issues/fixes\r\n1. Proper handling of datetime UTC columns (perhaps there is something incorrect with zero copy handling here)\r\n2. Not eagerly running against every column in a dataset when the columns argument of `to_tf_dataset` specifies a subset of columns (although I'm not sure if this is unavoidable)\r\n\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.9.0\r\n- Platform: macOS-13.2-x86_64-i386-64bit\r\n- Python version: 3.8.12\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5495/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5495/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5445", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5445/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5445/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5445/events", "html_url": "https://github.com/huggingface/datasets/issues/5445", "id": 1550588703, "node_id": "I_kwDODunzps5cbBsf", "number": 5445, "title": "CI tests are broken: AttributeError: 'mappingproxy' object has no attribute 'target'", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-01-20T10:03:10Z", "updated_at": "2023-01-20T10:28:44Z", "closed_at": "2023-01-20T10:28:44Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "CI tests are broken, raising `AttributeError: 'mappingproxy' object has no attribute 'target'`. See: https://github.com/huggingface/datasets/actions/runs/3966497597/jobs/6797384185\r\n```\r\n...\r\nERROR tests/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[mock://top_level-date=2019-10-0[1-4]/*-expected_paths4] - AttributeError: 'mappingproxy' object has no attribute 'target'\r\n===== 2076 passed, 19 skipped, 15 warnings, 47 errors in 115.54s (0:01:55) =====\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5445/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5445/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5426", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5426/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5426/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5426/events", "html_url": "https://github.com/huggingface/datasets/issues/5426", "id": 1535158555, "node_id": "I_kwDODunzps5bgKkb", "number": 5426, "title": "CI tests are broken: SchemaInferenceError", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2023-01-16T16:02:07Z", "updated_at": "2023-01-17T07:17:12Z", "closed_at": "2023-01-16T16:49:04Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "CI is broken, raising a `SchemaInferenceError`: see https://github.com/huggingface/datasets/actions/runs/3930901593/jobs/6721492004\r\n```\r\nFAILED tests/test_beam.py::BeamBuilderTest::test_download_and_prepare_sharded - datasets.arrow_writer.SchemaInferenceError: Please pass `features` or at least one example when writing data\r\n```\r\n\r\nStack trace:\r\n```\r\n______________ BeamBuilderTest.test_download_and_prepare_sharded _______________\r\n[gw1] linux -- Python 3.7.15 /opt/hostedtoolcache/Python/3.7.15/x64/bin/python\r\n\r\nself = <tests.test_beam.BeamBuilderTest testMethod=test_download_and_prepare_sharded>\r\n\r\n    @require_beam\r\n    def test_download_and_prepare_sharded(self):\r\n        import apache_beam as beam\r\n    \r\n        original_write_parquet = beam.io.parquetio.WriteToParquet\r\n    \r\n        expected_num_examples = len(get_test_dummy_examples())\r\n        with tempfile.TemporaryDirectory() as tmp_cache_dir:\r\n            builder = DummyBeamDataset(cache_dir=tmp_cache_dir, beam_runner=\"DirectRunner\")\r\n            with patch(\"apache_beam.io.parquetio.WriteToParquet\") as write_parquet_mock:\r\n                write_parquet_mock.side_effect = partial(original_write_parquet, num_shards=2)\r\n>               builder.download_and_prepare()\r\n\r\ntests/test_beam.py:97: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/opt/hostedtoolcache/Python/3.7.15/x64/lib/python3.7/site-packages/datasets/builder.py:864: in download_and_prepare\r\n    **download_and_prepare_kwargs,\r\n/opt/hostedtoolcache/Python/3.7.15/x64/lib/python3.7/site-packages/datasets/builder.py:1976: in _download_and_prepare\r\n    num_examples, num_bytes = beam_writer.finalize(metrics.query(m_filter))\r\n/opt/hostedtoolcache/Python/3.7.15/x64/lib/python3.7/site-packages/datasets/arrow_writer.py:694: in finalize\r\n    shard_num_bytes, _ = parquet_to_arrow(source, destination)\r\n/opt/hostedtoolcache/Python/3.7.15/x64/lib/python3.7/site-packages/datasets/arrow_writer.py:740: in parquet_to_arrow\r\n    num_bytes, num_examples = writer.finalize()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <datasets.arrow_writer.ArrowWriter object at 0x7f6dcbb3e810>\r\nclose_stream = True\r\n\r\n    def finalize(self, close_stream=True):\r\n        self.write_rows_on_file()\r\n        # In case current_examples < writer_batch_size, but user uses finalize()\r\n        if self._check_duplicates:\r\n            self.check_duplicate_keys()\r\n            # Re-intializing to empty list for next batch\r\n            self.hkey_record = []\r\n        self.write_examples_on_file()\r\n        # If schema is known, infer features even if no examples were written\r\n        if self.pa_writer is None and self.schema:\r\n            self._build_writer(self.schema)\r\n        if self.pa_writer is not None:\r\n            self.pa_writer.close()\r\n            self.pa_writer = None\r\n            if close_stream:\r\n                self.stream.close()\r\n        else:\r\n            if close_stream:\r\n                self.stream.close()\r\n>           raise SchemaInferenceError(\"Please pass `features` or at least one example when writing data\")\r\nE           datasets.arrow_writer.SchemaInferenceError: Please pass `features` or at least one example when writing data\r\n\r\n/opt/hostedtoolcache/Python/3.7.15/x64/lib/python3.7/site-packages/datasets/arrow_writer.py:593: SchemaInferenceError\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5426/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5326", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5326/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5326/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5326/events", "html_url": "https://github.com/huggingface/datasets/issues/5326", "id": 1471634168, "node_id": "I_kwDODunzps5Xt1r4", "number": 5326, "title": "No documentation for main branch is built", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-12-01T16:50:58Z", "updated_at": "2022-12-02T16:26:01Z", "closed_at": "2022-12-02T16:26:01Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Since:\r\n- #5250\r\n  - Commit: 703b84311f4ead83c7f79639f2dfa739295f0be6\r\n\r\nthe docs for main branch are no longer built.\r\n\r\nThe change introduced only triggers the docs building for releases.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5326/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5326/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5298", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5298/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5298/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5298/events", "html_url": "https://github.com/huggingface/datasets/issues/5298", "id": 1464681871, "node_id": "I_kwDODunzps5XTUWP", "number": 5298, "title": "Bug in xopen with Windows pathnames", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-11-25T15:21:32Z", "updated_at": "2022-11-29T08:21:25Z", "closed_at": "2022-11-29T08:21:25Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Currently, `xopen` function has a bug with local Windows pathnames:\r\n\r\nFrom its implementation:\r\n```python\r\ndef xopen(file: str, mode=\"r\", *args, **kwargs):\r\n    file = _as_posix(PurePath(file))\r\n    main_hop, *rest_hops = file.split(\"::\")\r\n    if is_local_path(main_hop):\r\n        return open(file, mode, *args, **kwargs)\r\n```\r\n\r\nOn a Windows machine, if we pass the argument:\r\n```python\r\nxopen(\"C:\\\\Users\\\\USERNAME\\\\filename.txt\")\r\n```\r\nit returns\r\n```python\r\nopen(\"C:/Users/USERNAME/filename.txt\")\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5298/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5296", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5296/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5296/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5296/events", "html_url": "https://github.com/huggingface/datasets/issues/5296", "id": 1464553580, "node_id": "I_kwDODunzps5XS1Bs", "number": 5296, "title": "Bug in xjoin with Windows pathnames", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-11-25T13:29:33Z", "updated_at": "2022-11-29T08:05:13Z", "closed_at": "2022-11-29T08:05:13Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Currently, `xjoin` function has a bug with local Windows pathnames: instead of returning the OS-dependent join pathname, it always returns it in POSIX format.\r\n\r\n```python\r\nfrom datasets.download.streaming_download_manager import xjoin\r\n\r\npath = xjoin(\"C:\\\\Users\\\\USERNAME\", \"filename.txt\")\r\n```\r\n\r\nJoin path should be:\r\n```python\r\n\"C:\\\\Users\\\\USERNAME\\\\filename.txt\"\r\n```\r\nHowever it is:\r\n```python\r\n\"C:/Users/USERNAME/filename.txt\"\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5296/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5284", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5284/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5284/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5284/events", "html_url": "https://github.com/huggingface/datasets/issues/5284", "id": 1461519733, "node_id": "I_kwDODunzps5XHQV1", "number": 5284, "title": "Features of IterableDataset set to None by remove column", "user": {"login": "sanchit-gandhi", "id": 93869735, "node_id": "U_kgDOBZhWpw", "avatar_url": "https://avatars.githubusercontent.com/u/93869735?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sanchit-gandhi", "html_url": "https://github.com/sanchit-gandhi", "followers_url": "https://api.github.com/users/sanchit-gandhi/followers", "following_url": "https://api.github.com/users/sanchit-gandhi/following{/other_user}", "gists_url": "https://api.github.com/users/sanchit-gandhi/gists{/gist_id}", "starred_url": "https://api.github.com/users/sanchit-gandhi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sanchit-gandhi/subscriptions", "organizations_url": "https://api.github.com/users/sanchit-gandhi/orgs", "repos_url": "https://api.github.com/users/sanchit-gandhi/repos", "events_url": "https://api.github.com/users/sanchit-gandhi/events{/privacy}", "received_events_url": "https://api.github.com/users/sanchit-gandhi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 3287858981, "node_id": "MDU6TGFiZWwzMjg3ODU4OTgx", "url": "https://api.github.com/repos/huggingface/datasets/labels/streaming", "name": "streaming", "color": "fef2c0", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "alvarobartt", "id": 36760800, "node_id": "MDQ6VXNlcjM2NzYwODAw", "avatar_url": "https://avatars.githubusercontent.com/u/36760800?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alvarobartt", "html_url": "https://github.com/alvarobartt", "followers_url": "https://api.github.com/users/alvarobartt/followers", "following_url": "https://api.github.com/users/alvarobartt/following{/other_user}", "gists_url": "https://api.github.com/users/alvarobartt/gists{/gist_id}", "starred_url": "https://api.github.com/users/alvarobartt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alvarobartt/subscriptions", "organizations_url": "https://api.github.com/users/alvarobartt/orgs", "repos_url": "https://api.github.com/users/alvarobartt/repos", "events_url": "https://api.github.com/users/alvarobartt/events{/privacy}", "received_events_url": "https://api.github.com/users/alvarobartt/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alvarobartt", "id": 36760800, "node_id": "MDQ6VXNlcjM2NzYwODAw", "avatar_url": "https://avatars.githubusercontent.com/u/36760800?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alvarobartt", "html_url": "https://github.com/alvarobartt", "followers_url": "https://api.github.com/users/alvarobartt/followers", "following_url": "https://api.github.com/users/alvarobartt/following{/other_user}", "gists_url": "https://api.github.com/users/alvarobartt/gists{/gist_id}", "starred_url": "https://api.github.com/users/alvarobartt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alvarobartt/subscriptions", "organizations_url": "https://api.github.com/users/alvarobartt/orgs", "repos_url": "https://api.github.com/users/alvarobartt/repos", "events_url": "https://api.github.com/users/alvarobartt/events{/privacy}", "received_events_url": "https://api.github.com/users/alvarobartt/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 18, "created_at": "2022-11-23T10:54:59Z", "updated_at": "2023-02-02T09:05:51Z", "closed_at": "2022-11-28T12:53:24Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Describe the bug\r\n\r\nThe `remove_column` method of the IterableDataset sets the dataset features to None.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import Audio, load_dataset\r\n\r\n# load LS in streaming mode\r\ndataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True)\r\n\r\n# check original features\r\nprint(\"Original features: \", dataset.features.keys())\r\n\r\n# define features to remove: we KEEP audio and text\r\nCOLUMNS_TO_REMOVE = ['chapter_id', 'speaker_id', 'file', 'id']\r\n\r\ndataset = dataset.remove_columns(COLUMNS_TO_REMOVE)\r\n\r\n# check processed features, uh-oh!\r\nprint(\"Processed features: \", dataset.features)\r\n\r\n# streaming the first audio sample still works\r\nprint(\"First sample:\", next(iter(ds)))\r\n```\r\n\r\n**Print Output:**\r\n\r\n```\r\nOriginal features:  dict_keys(['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'])\r\nProcessed features:  None\r\nFirst sample: {'audio': {'path': '2277-149896-0000.flac', 'array': array([ 0.00186157,  0.0005188 ,  0.00024414, ..., -0.00097656,\r\n       -0.00109863, -0.00146484]), 'sampling_rate': 16000}, 'text': \"HE WAS IN A FEVERED STATE OF MIND OWING TO THE BLIGHT HIS WIFE'S ACTION THREATENED TO CAST UPON HIS ENTIRE FUTURE\"}\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe features should be those **not** removed by the `remove_column` method, i.e. audio and text.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.7.1\r\n- Platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.15\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.3.5\r\n\r\n(Running on Google Colab for a blog post: https://colab.research.google.com/drive/1ySCQREPZEl4msLfxb79pYYOWjUZhkr9y#scrollTo=8pRDGiVmH2ml)\r\n\r\ncc @polinaeterna @lhoestq ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5284/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5284/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5275", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5275/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5275/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5275/events", "html_url": "https://github.com/huggingface/datasets/issues/5275", "id": 1459358919, "node_id": "I_kwDODunzps5W_AzH", "number": 5275, "title": "YAML integer keys are not preserved Hub server-side", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2022-11-22T08:14:47Z", "updated_at": "2023-01-26T10:52:35Z", "closed_at": "2023-01-26T10:40:21Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "After an internal discussion (https://github.com/huggingface/moon-landing/issues/4563):\r\n- YAML integer keys are not preserved server-side: they are transformed to strings\r\n  - See for example this Hub PR: https://huggingface.co/datasets/acronym_identification/discussions/1/files\r\n    - Original:\r\n      ```yaml\r\n      class_label:\r\n        names:\r\n          0: B-long\r\n          1: B-short\r\n      ```\r\n    - Returned by the server:\r\n      ```yaml\r\n      class_label:\r\n        names:\r\n          '0': B-long\r\n          '1': B-short\r\n      ```\r\n- They are planning to enforce only string keys\r\n- Other projects already use interger-transformed-to string keys: e.g. `transformers` models `id2label`: https://huggingface.co/roberta-large-mnli/blob/main/config.json\r\n  ```yaml\r\n  \"id2label\": {\r\n    \"0\": \"CONTRADICTION\",\r\n    \"1\": \"NEUTRAL\",\r\n    \"2\": \"ENTAILMENT\"\r\n  }\r\n  ``` \r\n\r\nOn the other hand, at `datasets` we are currently using YAML integer keys for `dataset_info` `class_label`.\r\n\r\nPlease note (thanks @lhoestq for pointing out) that previous versions (2.6 and 2.7) of `datasets` need being patched:\r\n```python\r\nIn [18]: Features._from_yaml_list([{'dtype': {'class_label': {'names': {'0': 'neg', '1': 'pos'}}}, 'name': 'label'}])                                                                                      \r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-18-974f07eea526> in <module>\r\n----> 1 Features._from_yaml_list(ry)\r\n\r\n~/Desktop/hf/nlp/src/datasets/features/features.py in _from_yaml_list(cls, yaml_data)\r\n   1743                 raise TypeError(f\"Expected a dict or a list but got {type(obj)}: {obj}\")\r\n   1744 \r\n-> 1745         return cls.from_dict(from_yaml_inner(yaml_data))\r\n   1746 \r\n   1747     def encode_example(self, example):\r\n\r\n~/Desktop/hf/nlp/src/datasets/features/features.py in from_yaml_inner(obj)\r\n   1739             elif isinstance(obj, list):\r\n   1740                 names = [_feature.pop(\"name\") for _feature in obj]\r\n-> 1741                 return {name: from_yaml_inner(_feature) for name, _feature in zip(names, obj)}\r\n   1742             else:\r\n   1743                 raise TypeError(f\"Expected a dict or a list but got {type(obj)}: {obj}\")\r\n\r\n~/Desktop/hf/nlp/src/datasets/features/features.py in <dictcomp>(.0)\r\n   1739             elif isinstance(obj, list):\r\n   1740                 names = [_feature.pop(\"name\") for _feature in obj]\r\n-> 1741                 return {name: from_yaml_inner(_feature) for name, _feature in zip(names, obj)}\r\n   1742             else:\r\n   1743                 raise TypeError(f\"Expected a dict or a list but got {type(obj)}: {obj}\")\r\n\r\n~/Desktop/hf/nlp/src/datasets/features/features.py in from_yaml_inner(obj)\r\n   1734                             return {\"_type\": snakecase_to_camelcase(obj[\"dtype\"])}\r\n   1735                     else:\r\n-> 1736                         return from_yaml_inner(obj[\"dtype\"])\r\n   1737                 else:\r\n   1738                     return {\"_type\": snakecase_to_camelcase(_type), **unsimplify(obj)[_type]}\r\n\r\n~/Desktop/hf/nlp/src/datasets/features/features.py in from_yaml_inner(obj)\r\n   1736                         return from_yaml_inner(obj[\"dtype\"])\r\n   1737                 else:\r\n-> 1738                     return {\"_type\": snakecase_to_camelcase(_type), **unsimplify(obj)[_type]}\r\n   1739             elif isinstance(obj, list):\r\n   1740                 names = [_feature.pop(\"name\") for _feature in obj]\r\n\r\n~/Desktop/hf/nlp/src/datasets/features/features.py in unsimplify(feature)\r\n   1704             if isinstance(feature.get(\"class_label\"), dict) and isinstance(feature[\"class_label\"].get(\"names\"), dict):\r\n   1705                 label_ids = sorted(feature[\"class_label\"][\"names\"])\r\n-> 1706                 if label_ids and label_ids != list(range(label_ids[-1] + 1)):\r\n   1707                     raise ValueError(\r\n   1708                         f\"ClassLabel expected a value for all label ids [0:{label_ids[-1] + 1}] but some ids are missing.\"\r\n\r\nTypeError: can only concatenate str (not \"int\") to str\r\n```\r\n\r\nTODO:\r\n- [x] Remove YAML integer keys from `dataset_info` metadata\r\n- [x] Make a patch release for affected `datasets` versions: 2.6 and 2.7\r\n- [x] Communicate on the fix\r\n- [x] Wait for adoption\r\n- [x] Bulk edit the Hub to fix this in all canonical datasets", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5275/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5275/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5264", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5264/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5264/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5264/events", "html_url": "https://github.com/huggingface/datasets/issues/5264", "id": 1455252906, "node_id": "I_kwDODunzps5WvWWq", "number": 5264, "title": "`datasets` can't read a Parquet file in Python 3.9.13", "user": {"login": "loubnabnl", "id": 44069155, "node_id": "MDQ6VXNlcjQ0MDY5MTU1", "avatar_url": "https://avatars.githubusercontent.com/u/44069155?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loubnabnl", "html_url": "https://github.com/loubnabnl", "followers_url": "https://api.github.com/users/loubnabnl/followers", "following_url": "https://api.github.com/users/loubnabnl/following{/other_user}", "gists_url": "https://api.github.com/users/loubnabnl/gists{/gist_id}", "starred_url": "https://api.github.com/users/loubnabnl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loubnabnl/subscriptions", "organizations_url": "https://api.github.com/users/loubnabnl/orgs", "repos_url": "https://api.github.com/users/loubnabnl/repos", "events_url": "https://api.github.com/users/loubnabnl/events{/privacy}", "received_events_url": "https://api.github.com/users/loubnabnl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2022-11-18T14:44:01Z", "updated_at": "2022-11-22T11:18:08Z", "closed_at": "2022-11-22T11:18:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Describe the bug\r\n\r\nI have an error when trying to load this [dataset](https://huggingface.co/datasets/bigcode/the-stack-dedup-pjj) (it's private but I can add you to the bigcode org). `datasets` can't read one of the parquet files in the Java subset\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"bigcode/the-stack-dedup-pjj\", data_dir=\"data/java\", split=\"train\", revision=\"v1.1.a1\", use_auth_token=True)\r\n````\r\n```\r\n  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\r\n```\r\nIt seems to be an issue with new Python versions, Because it works in these two environements:\r\n```\r\n- `datasets` version: 2.6.1\r\n- Platform: Linux-5.4.0-131-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.3.4\r\n```\r\n\r\n```\r\n- `datasets` version: 2.6.1\r\n- Platform: Linux-4.19.0-22-cloud-amd64-x86_64-with-debian-10.13\r\n- Python version: 3.7.12\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.3.4\r\n```\r\nBut not in this:\r\n```\r\n- `datasets` version: 2.6.1\r\n- Platform: Linux-4.19.0-22-cloud-amd64-x86_64-with-glibc2.28\r\n- Python version: 3.9.13\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.3.4\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nLoad the dataset in python 3.9.13\r\n\r\n### Expected behavior\r\n\r\nLoad the dataset without the pyarrow error.\r\n\r\n### Environment info\r\n\r\n```\r\n- `datasets` version: 2.6.1\r\n- Platform: Linux-4.19.0-22-cloud-amd64-x86_64-with-glibc2.28\r\n- Python version: 3.9.13\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.3.4\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5264/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5264/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5202", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5202/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5202/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5202/events", "html_url": "https://github.com/huggingface/datasets/issues/5202", "id": 1435886090, "node_id": "I_kwDODunzps5VleIK", "number": 5202, "title": "CI fails after bulk edit of canonical datasets", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-11-04T10:51:20Z", "updated_at": "2023-02-16T09:11:10Z", "closed_at": "2023-02-16T09:11:10Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "```\r\n______ test_get_dataset_config_info[paws-labeled_final-expected_splits2] _______\r\n[gw0] linux -- Python 3.7.15 /opt/hostedtoolcache/Python/3.7.15/x64/bin/python\r\n\r\npath = 'paws', config_name = 'labeled_final'\r\nexpected_splits = ['train', 'test', 'validation']\r\n\r\n    @pytest.mark.parametrize(\r\n        \"path, config_name, expected_splits\",\r\n        [\r\n            (\"squad\", \"plain_text\", [\"train\", \"validation\"]),\r\n            (\"dalle-mini/wit\", \"dalle-mini--wit\", [\"train\"]),\r\n            (\"paws\", \"labeled_final\", [\"train\", \"test\", \"validation\"]),\r\n        ],\r\n    )\r\n    def test_get_dataset_config_info(path, config_name, expected_splits):\r\n        info = get_dataset_config_info(path, config_name=config_name)\r\n        assert info.config_name == config_name\r\n>       assert list(info.splits.keys()) == expected_splits\r\nE       AssertionError: assert ['test', 'tra... 'validation'] == ['train', 'te... 'validation']\r\nE         At index 0 diff: 'test' != 'train'\r\nE         Full diff:\r\nE         - ['train', 'test', 'validation']\r\nE         + ['test', 'train', 'validation']\r\n\r\ntests/test_inspect.py:45: AssertionError\r\n_ test_get_dataset_info[paws-expected_configs2-expected_splits_in_first_config2] _\r\n[gw0] linux -- Python 3.7.15 /opt/hostedtoolcache/Python/3.7.15/x64/bin/python\r\n\r\npath = 'paws'\r\nexpected_configs = ['labeled_final', 'labeled_swap', 'unlabeled_final']\r\nexpected_splits_in_first_config = ['train', 'test', 'validation']\r\n\r\n    @pytest.mark.parametrize(\r\n        \"path, expected_configs, expected_splits_in_first_config\",\r\n        [\r\n            (\"squad\", [\"plain_text\"], [\"train\", \"validation\"]),\r\n            (\"dalle-mini/wit\", [\"dalle-mini--wit\"], [\"train\"]),\r\n            (\"paws\", [\"labeled_final\", \"labeled_swap\", \"unlabeled_final\"], [\"train\", \"test\", \"validation\"]),\r\n        ],\r\n    )\r\n    def test_get_dataset_info(path, expected_configs, expected_splits_in_first_config):\r\n        infos = get_dataset_infos(path)\r\n        assert list(infos.keys()) == expected_configs\r\n        expected_config = expected_configs[0]\r\n        assert expected_config in infos\r\n        info = infos[expected_config]\r\n        assert info.config_name == expected_config\r\n>       assert list(info.splits.keys()) == expected_splits_in_first_config\r\nE       AssertionError: assert ['test', 'tra... 'validation'] == ['train', 'te... 'validation']\r\nE         At index 0 diff: 'test' != 'train'\r\nE         Full diff:\r\nE         - ['train', 'test', 'validation']\r\nE         + ['test', 'train', 'validation']\r\n\r\ntests/test_inspect.py:90: AssertionError\r\n______ test_get_dataset_split_names[paws-labeled_final-expected_splits2] _______\r\n[gw0] linux -- Python 3.7.15 /opt/hostedtoolcache/Python/3.7.15/x64/bin/python\r\n\r\npath = 'paws', expected_config = 'labeled_final'\r\nexpected_splits = ['train', 'test', 'validation']\r\n\r\n    @pytest.mark.parametrize(\r\n        \"path, expected_config, expected_splits\",\r\n        [\r\n            (\"squad\", \"plain_text\", [\"train\", \"validation\"]),\r\n            (\"dalle-mini/wit\", \"dalle-mini--wit\", [\"train\"]),\r\n            (\"paws\", \"labeled_final\", [\"train\", \"test\", \"validation\"]),\r\n        ],\r\n    )\r\n    def test_get_dataset_split_names(path, expected_config, expected_splits):\r\n        infos = get_dataset_infos(path)\r\n        assert expected_config in infos\r\n        info = infos[expected_config]\r\n        assert info.config_name == expected_config\r\n>       assert list(info.splits.keys()) == expected_splits\r\nE       AssertionError: assert ['test', 'tra... 'validation'] == ['train', 'te... 'validation']\r\nE         At index 0 diff: 'test' != 'train'\r\nE         Full diff:\r\nE         - ['train', 'test', 'validation']\r\nE         + ['test', 'train', 'validation']\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5202/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5202/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5192", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5192/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5192/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5192/events", "html_url": "https://github.com/huggingface/datasets/pull/5192", "id": 1433199790, "node_id": "PR_kwDODunzps5CD2BQ", "number": 5192, "title": "Drop labels in Image and Audio folders if files are on different levels in directory or if there is only one label", "user": {"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2022-11-02T14:01:41Z", "updated_at": "2022-11-15T16:32:53Z", "closed_at": "2022-11-15T16:31:07Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "draft": false, "pull_request": {"url": "https://api.github.com/repos/huggingface/datasets/pulls/5192", "html_url": "https://github.com/huggingface/datasets/pull/5192", "diff_url": "https://github.com/huggingface/datasets/pull/5192.diff", "patch_url": "https://github.com/huggingface/datasets/pull/5192.patch", "merged_at": "2022-11-15T16:31:07Z"}, "body": "Will close https://github.com/huggingface/datasets/issues/5153\r\n\r\nDrop labels by default (`drop_labels=None`) when:\r\n* there are files on different levels of directory hierarchy by checking their path depth\r\n* all files are in the same directory (=only one label was inferred)\r\n\r\nFirst one fixes cases like this:\r\n```\r\nrepo\r\n   image3.jpg\r\n   image4.jpg\r\n   data\r\n      image1.jpg\r\n      image2.jpg\r\n```\r\nSecond one fixes cases like this:\r\n```\r\nrepo\r\n   image1.jpg\r\n   image2.jpg\r\n   image3.jpg\r\n```\r\nThis is mostly to fix the viewer for people who just drop images in the Hub interface into the root dir.\r\n\r\nI added tests for both of the cases on local and remote files. **I also changed data files for old test on drop_labels** (`test_generate_examples_drop_labels`). The files I provide to `test_generate_examples_drop_labels` now has \"canonical\" classification structure (two dirs) in order not to change the logic of the test (=not to check these two cases addressed in the PR).\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5192/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5192/timeline", "performed_via_github_app": null, "state_reason": null}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5179", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5179/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5179/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5179/events", "html_url": "https://github.com/huggingface/datasets/issues/5179", "id": 1430826100, "node_id": "I_kwDODunzps5VSKx0", "number": 5179, "title": "`map()` fails midway due to format incompatibility ", "user": {"login": "sayakpaul", "id": 22957388, "node_id": "MDQ6VXNlcjIyOTU3Mzg4", "avatar_url": "https://avatars.githubusercontent.com/u/22957388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sayakpaul", "html_url": "https://github.com/sayakpaul", "followers_url": "https://api.github.com/users/sayakpaul/followers", "following_url": "https://api.github.com/users/sayakpaul/following{/other_user}", "gists_url": "https://api.github.com/users/sayakpaul/gists{/gist_id}", "starred_url": "https://api.github.com/users/sayakpaul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sayakpaul/subscriptions", "organizations_url": "https://api.github.com/users/sayakpaul/orgs", "repos_url": "https://api.github.com/users/sayakpaul/repos", "events_url": "https://api.github.com/users/sayakpaul/events{/privacy}", "received_events_url": "https://api.github.com/users/sayakpaul/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2022-11-01T03:57:59Z", "updated_at": "2022-11-08T11:35:26Z", "closed_at": "2022-11-08T11:35:26Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Describe the bug\n\nI am using the `emotion` dataset from Hub for sequence classification. After training the model, I am using it to generate predictions for all the entries present in the `validation` split of the dataset. \r\n\r\n```py\r\ndef get_test_accuracy(model):\r\n    def fn(batch):\r\n        inputs = {k:v.to(device) for k,v in batch.items() \r\n                  if k in tokenizer.model_input_names}\r\n        with torch.no_grad():\r\n            output = model(**inputs)\r\n            pred_label = torch.argmax(output.logits, axis=-1)\r\n        return {\"predicted_label\": pred_label.cpu().numpy()}\r\n    return fn\r\n```\r\n\r\nThis is how the `get_test_accuracy()` is being used:\r\n\r\n```py\r\n\r\nemotions = load_dataset(\"emotion\")\r\n\r\ndef tokenize(batch):\r\n    return tokenizer(batch[\"text\"], padding=True, truncation=True)\r\n\r\nemotions_encoded = emotions.map(tokenize, batched=True)\r\nemotions_encoded.set_format(\"torch\", \r\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\r\n\r\nnew_dataset = emotions_encoded[\"validation\"].map(\r\n    accuracy_fn, batched=True, batch_size=128\r\n)\r\n```\r\n\r\nComplete code is available in the Colab Notebook provided below. \r\n\r\nThe `map()` process fails midway giving:\r\n\r\n```shell\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-ad24ac288eb4> in <module>\r\n      2 \r\n      3 new_dataset = emotions_encoded[\"validation\"].map(\r\n----> 4     accuracy_fn, batched=True, batch_size=128\r\n      5 )\r\n\r\n7 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   2588                 new_fingerprint=new_fingerprint,\r\n   2589                 disable_tqdm=disable_tqdm,\r\n-> 2590                 desc=desc,\r\n   2591             )\r\n   2592         else:\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    582             self: \"Dataset\" = kwargs.pop(\"self\")\r\n    583         # apply actual function\r\n--> 584         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    585         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    586         for dataset in datasets:\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    549         }\r\n    550         # apply actual function\r\n--> 551         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    552         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    553         # re-apply format to the output\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    478             # Call actual function\r\n    479 \r\n--> 480             out = func(self, *args, **kwargs)\r\n    481 \r\n    482             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\r\n   2970                                 indices,\r\n   2971                                 check_same_num_examples=len(input_dataset.list_indexes()) > 0,\r\n-> 2972                                 offset=offset,\r\n   2973                             )\r\n   2974                         except NumExamplesMismatchError:\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples, offset)\r\n   2850             if with_rank:\r\n   2851                 additional_args += (rank,)\r\n-> 2852             processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n   2853             if update_data is None:\r\n   2854                 # Check if the function returns updated examples\r\n\r\n<ipython-input-6-4e0d280426f6> in fn(batch)\r\n      1 def get_test_accuracy(model):\r\n      2     def fn(batch):\r\n----> 3         inputs = {k:v.to(device) for k,v in batch.items() \r\n      4                   if k in tokenizer.model_input_names}\r\n      5         with torch.no_grad():\r\n\r\n<ipython-input-6-4e0d280426f6> in <dictcomp>(.0)\r\n      2     def fn(batch):\r\n      3         inputs = {k:v.to(device) for k,v in batch.items() \r\n----> 4                   if k in tokenizer.model_input_names}\r\n      5         with torch.no_grad():\r\n      6             output = model(**inputs)\r\n\r\nAttributeError: 'list' object has no attribute 'to'\r\n```\r\n\r\nAs you'd notice in the notebook, the process fails _midway_ and not at the beginning. \r\n\r\nIs this expected? \n\n### Steps to reproduce the bug\n\nColab Notebook: \r\n\r\nhttps://colab.research.google.com/gist/sayakpaul/d1570d537faf39040d02d77b1ed7de07/scratchpad.ipynb\n\n### Expected behavior\n\nThe mapping process should complete as is. If you switch the `split` to `test` it works as expected. \n\n### Environment info\n\nColab", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5179/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5153", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5153/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5153/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5153/events", "html_url": "https://github.com/huggingface/datasets/issues/5153", "id": 1420833457, "node_id": "I_kwDODunzps5UsDKx", "number": 5153, "title": "default Image/AudioFolder infers labels when there is no metadata files even if there is only one dir", "user": {"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-10-24T13:28:18Z", "updated_at": "2022-11-15T16:31:10Z", "closed_at": "2022-11-15T16:31:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Describe the bug\r\n\r\nBy default FolderBasedBuilder infers labels if there is not metadata files, even if it's meaningless (for example, they are in a single directory or in the root folder, see this repo as an example: https://huggingface.co/datasets/patrickvonplaten/audios\r\n\r\nAs this is a corner case for quick exploration of images or audios on the Hub. \r\n\r\n### Steps to reproduce the bug\r\n\r\nIf you have directory like this:\r\n\r\n```\r\nrepo\r\n   image1.jpg\r\n   image2.jpg\r\n   image3.jpg\r\n```\r\nor\r\n\r\n```\r\nrepo\r\n   data\r\n      image1.jpg\r\n      image2.jpg\r\n      image3.jpg\r\n```\r\ndoing `ds = load_dataset(repo)` would create `label` feature:\r\n```python\r\nprint(ds[\"train\"][0])\r\n>> {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x7FB5326468E0>, 'label': 0}\r\n```\r\n\r\nAlso, if you have the following structure:\r\n\r\n```\r\nrepo\r\n   data\r\n      image1.jpg\r\n      image2.jpg\r\n      image3.jpg\r\n   image4.jpg\r\n   image5.jpg\r\n   image6.jpg\r\n```\r\nit will infer two labels:\r\n\r\n```python\r\nprint(ds[\"train\"][0])\r\nprint(ds[\"train\"][-1])\r\n>> {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x7FB5326468E0>, 'label': 1}\r\n>> {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x415 at 0x7FB5326555B0>, 'label': 0}\r\n```\r\n\r\n### Expected behavior\r\n\r\nWe should have only one base feature (Image/Audio) in such cases.\r\n\r\n### Environment info\r\n\r\nall versions of `datasets`", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5153/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5133", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5133/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5133/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5133/events", "html_url": "https://github.com/huggingface/datasets/issues/5133", "id": 1413623462, "node_id": "I_kwDODunzps5UQi6m", "number": 5133, "title": "Tensor operation not functioning in dataset mapping", "user": {"login": "xinghaow99", "id": 50691954, "node_id": "MDQ6VXNlcjUwNjkxOTU0", "avatar_url": "https://avatars.githubusercontent.com/u/50691954?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xinghaow99", "html_url": "https://github.com/xinghaow99", "followers_url": "https://api.github.com/users/xinghaow99/followers", "following_url": "https://api.github.com/users/xinghaow99/following{/other_user}", "gists_url": "https://api.github.com/users/xinghaow99/gists{/gist_id}", "starred_url": "https://api.github.com/users/xinghaow99/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xinghaow99/subscriptions", "organizations_url": "https://api.github.com/users/xinghaow99/orgs", "repos_url": "https://api.github.com/users/xinghaow99/repos", "events_url": "https://api.github.com/users/xinghaow99/events{/privacy}", "received_events_url": "https://api.github.com/users/xinghaow99/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-10-18T17:53:35Z", "updated_at": "2022-10-19T04:15:45Z", "closed_at": "2022-10-19T04:15:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI'm doing a torch.mean() operation in data preprocessing, and it's not working.\r\n\r\n## Steps to reproduce the bug\r\n```\r\nfrom transformers import pipeline\r\nimport torch\r\nimport numpy as np\r\nfrom datasets import load_dataset\r\ndevice = 'cuda:0'\r\nraw_dataset = load_dataset(\"glue\", \"sst2\")\r\nfeature_extraction = pipeline('feature-extraction', 'bert-base-uncased', device=device)\r\ndef extracted_data(examples):\r\n    # feature = torch.tensor(feature_extraction(examples['sentence'], batch_size=16), device=device)\r\n    # feature = torch.mean(feature, dim=1)\r\n    feature = np.asarray(feature_extraction(examples['sentence'], batch_size=16)).squeeze().mean(1)\r\n    print(feature.shape)\r\n    return {'feature': feature}\r\n\r\nextracted_dataset = raw_dataset.map(extracted_data, batched=True, batch_size=16)\r\n```\r\n\r\n## Results\r\nWhen running with torch.mean(), the shape printed out is [16, seq_len, 768], which is exactly the same before the operation. While numpy works just fine, which gives [16, 768].\r\n\r\n\r\n## Environment info\r\n- `datasets` version: 2.6.1\r\n- Platform: Linux-4.4.0-142-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.6\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5133/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5129", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5129/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5129/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5129/events", "html_url": "https://github.com/huggingface/datasets/issues/5129", "id": 1413031664, "node_id": "I_kwDODunzps5UOSbw", "number": 5129, "title": "unexpected `cast` or `class_encode_column` result after `rename_column`", "user": {"login": "quaeast", "id": 35144675, "node_id": "MDQ6VXNlcjM1MTQ0Njc1", "avatar_url": "https://avatars.githubusercontent.com/u/35144675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/quaeast", "html_url": "https://github.com/quaeast", "followers_url": "https://api.github.com/users/quaeast/followers", "following_url": "https://api.github.com/users/quaeast/following{/other_user}", "gists_url": "https://api.github.com/users/quaeast/gists{/gist_id}", "starred_url": "https://api.github.com/users/quaeast/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/quaeast/subscriptions", "organizations_url": "https://api.github.com/users/quaeast/orgs", "repos_url": "https://api.github.com/users/quaeast/repos", "events_url": "https://api.github.com/users/quaeast/events{/privacy}", "received_events_url": "https://api.github.com/users/quaeast/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-10-18T11:15:24Z", "updated_at": "2022-10-19T03:02:26Z", "closed_at": "2022-10-19T03:02:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen invoke `cast` or `class_encode_column` to a colunm renamed by `rename_column` , it will convert all the variables in this column into one variable. I also run this script in version 2.5.2, this bug does not appear. So I switched to the older version.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"amazon_reviews_multi\", \"en\")\r\ndata = dataset['train']\r\n\r\ndata = data.remove_columns(\r\n                [\r\n                    \"review_id\",\r\n                    \"product_id\",\r\n                    \"reviewer_id\",\r\n                    \"review_title\",\r\n                    \"language\",\r\n                    \"product_category\",\r\n                ]\r\n            )\r\ndata = data.rename_column(\"review_body\", \"text\")\r\ndata1 = data.class_encode_column(\"stars\")\r\nprint(set(data1.data.columns[0]))\r\n# output: {<pyarrow.Int64Scalar: 4>, <pyarrow.Int64Scalar: 2>, <pyarrow.Int64Scalar: 3>, <pyarrow.Int64Scalar: 0>, <pyarrow.Int64Scalar: 1>}\r\n\r\n\r\ndata = data.rename_column(\"stars\", \"label\")\r\nprint(set(data.data.columns[0]))\r\n# output: {<pyarrow.Int32Scalar: 5>, <pyarrow.Int32Scalar: 4>, <pyarrow.Int32Scalar: 1>, <pyarrow.Int32Scalar: 3>, <pyarrow.Int32Scalar: 2>}\r\ndata2 = data.class_encode_column(\"label\")\r\nprint(set(data2.data.columns[0]))\r\n# output: {<pyarrow.Int64Scalar: 0>}\r\n```\r\n\r\n## Expected results\r\nthe last print should be: \r\n{<pyarrow.Int64Scalar: 4>, <pyarrow.Int64Scalar: 2>, <pyarrow.Int64Scalar: 3>, <pyarrow.Int64Scalar: 0>, <pyarrow.Int64Scalar: 1>}\r\n\r\n## Actual results\r\nbut it output:\r\n{<pyarrow.Int64Scalar: 0>}\r\n\r\n## Environment info\r\n- `datasets` version: 2.6.1\r\n- Platform: macOS-12.5.1-arm64-arm-64bit\r\n- Python version: 3.10.6\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5129/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5129/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5118", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5118/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5118/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5118/events", "html_url": "https://github.com/huggingface/datasets/issues/5118", "id": 1410547373, "node_id": "I_kwDODunzps5UEz6t", "number": 5118, "title": "Installing `datasets` on M1 computers", "user": {"login": "david1542", "id": 9879252, "node_id": "MDQ6VXNlcjk4NzkyNTI=", "avatar_url": "https://avatars.githubusercontent.com/u/9879252?v=4", "gravatar_id": "", "url": "https://api.github.com/users/david1542", "html_url": "https://github.com/david1542", "followers_url": "https://api.github.com/users/david1542/followers", "following_url": "https://api.github.com/users/david1542/following{/other_user}", "gists_url": "https://api.github.com/users/david1542/gists{/gist_id}", "starred_url": "https://api.github.com/users/david1542/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/david1542/subscriptions", "organizations_url": "https://api.github.com/users/david1542/orgs", "repos_url": "https://api.github.com/users/david1542/repos", "events_url": "https://api.github.com/users/david1542/events{/privacy}", "received_events_url": "https://api.github.com/users/david1542/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-10-16T16:50:08Z", "updated_at": "2022-10-19T09:10:08Z", "closed_at": "2022-10-19T09:10:08Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI wanted to install `datasets` dependencies on my M1 (in order to start contributing to the project). However, I got an error regarding `tensorflow`.\r\n\r\nOn M1, `tensorflow-macos` needs to be installed instead. Can we add a conditional requirement, so that `tensorflow-macos` would be installed on M1?\r\n\r\n## Steps to reproduce the bug\r\nFresh clone this project (on m1), create a virtualenv and run this:\r\n```python\r\npip install -e \".[dev]\"\r\n```\r\n\r\n## Expected results\r\nInstallation should be smooth, and all the dependencies should be installed on M1.\r\n\r\n## Actual results\r\nYou should receive an error, saying pip couldn't find a version that matches this pattern:\r\n```\r\ntensorflow>=2.3,!=2.6.0,!=2.6.1\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.6.2.dev0\r\n- Platform: macOS-12.6-arm64-arm-64bit\r\n- Python version: 3.9.6\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.5.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5118/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5117", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5117/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5117/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5117/events", "html_url": "https://github.com/huggingface/datasets/issues/5117", "id": 1409571346, "node_id": "I_kwDODunzps5UBFoS", "number": 5117, "title": "Progress bars have color red and never completed to 100%", "user": {"login": "echatzikyriakidis", "id": 63857529, "node_id": "MDQ6VXNlcjYzODU3NTI5", "avatar_url": "https://avatars.githubusercontent.com/u/63857529?v=4", "gravatar_id": "", "url": "https://api.github.com/users/echatzikyriakidis", "html_url": "https://github.com/echatzikyriakidis", "followers_url": "https://api.github.com/users/echatzikyriakidis/followers", "following_url": "https://api.github.com/users/echatzikyriakidis/following{/other_user}", "gists_url": "https://api.github.com/users/echatzikyriakidis/gists{/gist_id}", "starred_url": "https://api.github.com/users/echatzikyriakidis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/echatzikyriakidis/subscriptions", "organizations_url": "https://api.github.com/users/echatzikyriakidis/orgs", "repos_url": "https://api.github.com/users/echatzikyriakidis/repos", "events_url": "https://api.github.com/users/echatzikyriakidis/events{/privacy}", "received_events_url": "https://api.github.com/users/echatzikyriakidis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "david1542", "id": 9879252, "node_id": "MDQ6VXNlcjk4NzkyNTI=", "avatar_url": "https://avatars.githubusercontent.com/u/9879252?v=4", "gravatar_id": "", "url": "https://api.github.com/users/david1542", "html_url": "https://github.com/david1542", "followers_url": "https://api.github.com/users/david1542/followers", "following_url": "https://api.github.com/users/david1542/following{/other_user}", "gists_url": "https://api.github.com/users/david1542/gists{/gist_id}", "starred_url": "https://api.github.com/users/david1542/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/david1542/subscriptions", "organizations_url": "https://api.github.com/users/david1542/orgs", "repos_url": "https://api.github.com/users/david1542/repos", "events_url": "https://api.github.com/users/david1542/events{/privacy}", "received_events_url": "https://api.github.com/users/david1542/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "david1542", "id": 9879252, "node_id": "MDQ6VXNlcjk4NzkyNTI=", "avatar_url": "https://avatars.githubusercontent.com/u/9879252?v=4", "gravatar_id": "", "url": "https://api.github.com/users/david1542", "html_url": "https://github.com/david1542", "followers_url": "https://api.github.com/users/david1542/followers", "following_url": "https://api.github.com/users/david1542/following{/other_user}", "gists_url": "https://api.github.com/users/david1542/gists{/gist_id}", "starred_url": "https://api.github.com/users/david1542/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/david1542/subscriptions", "organizations_url": "https://api.github.com/users/david1542/orgs", "repos_url": "https://api.github.com/users/david1542/repos", "events_url": "https://api.github.com/users/david1542/events{/privacy}", "received_events_url": "https://api.github.com/users/david1542/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2022-10-14T16:12:30Z", "updated_at": "2022-10-23T12:58:41Z", "closed_at": "2022-10-23T12:58:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nProgress bars after transformative operations turn in red and never be completed to 100%\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset('rotten_tomatoes', split='test').filter(lambda o: True)\r\n```\r\n\r\n## Expected results\r\nProgress bar should be 100% and green\r\n\r\n## Actual results\r\nProgress  bar turn in red and never completed to 100%\r\n\r\n## Environment info\r\n- `datasets` version: 2.6.1\r\n- Platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.14\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5117/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5112", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5112/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5112/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5112/events", "html_url": "https://github.com/huggingface/datasets/issues/5112", "id": 1409143409, "node_id": "I_kwDODunzps5T_dJx", "number": 5112, "title": "Bug with filtered indices", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-10-14T10:35:47Z", "updated_at": "2022-10-14T13:55:03Z", "closed_at": "2022-10-14T12:11:45Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAs reported by @PartiallyTyped (and by @Muennighoff):\r\n- https://github.com/huggingface/datasets/issues/5111#issuecomment-1278652524\r\n\r\nThere is an issue with the indices of a filtered dataset.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nds = Dataset.from_dict({\"num\": [0, 1, 2, 3]})\r\nds = ds.filter(lambda num: num % 2 == 0, input_columns=\"num\", batch_size=2)\r\nassert all(item[\"num\"] % 2 == 0 for item in ds)\r\n```\r\n\r\n## Expected results\r\nThe indices of the filtered dataset should correspond to the examples with \"language\" equals to \"english\".\r\n\r\n## Actual results\r\nIndices to items with other languages are included in the filtered dataset indices\r\n\r\n## Preliminar investigation\r\nIt seems a bug introduced by:\r\n- #5030\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5112/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5112/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5111", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5111/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5111/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5111/events", "html_url": "https://github.com/huggingface/datasets/issues/5111", "id": 1408143170, "node_id": "I_kwDODunzps5T7o9C", "number": 5111, "title": "map and filter not working properly in multiprocessing with the new release 2.6.0", "user": {"login": "loubnabnl", "id": 44069155, "node_id": "MDQ6VXNlcjQ0MDY5MTU1", "avatar_url": "https://avatars.githubusercontent.com/u/44069155?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loubnabnl", "html_url": "https://github.com/loubnabnl", "followers_url": "https://api.github.com/users/loubnabnl/followers", "following_url": "https://api.github.com/users/loubnabnl/following{/other_user}", "gists_url": "https://api.github.com/users/loubnabnl/gists{/gist_id}", "starred_url": "https://api.github.com/users/loubnabnl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loubnabnl/subscriptions", "organizations_url": "https://api.github.com/users/loubnabnl/orgs", "repos_url": "https://api.github.com/users/loubnabnl/repos", "events_url": "https://api.github.com/users/loubnabnl/events{/privacy}", "received_events_url": "https://api.github.com/users/loubnabnl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2022-10-13T17:00:55Z", "updated_at": "2022-10-17T08:26:59Z", "closed_at": "2022-10-14T14:59:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen mapping is used on a dataset with more than one process, there is a weird behavior when trying to use `filter` , it's like only the samples from one worker are retrieved, one needs to specify the same `num_proc` in filter for it to work properly. This doesn't happen with `datasets` version 2.5.2\r\n\r\nIn the code below the data is filtered differently when we increase `num_proc` used in `map` although the datsets before and after mapping have identical elements.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nfrom datasets import load_dataset\r\n\r\ndef preprocess(example):\r\n    return example\r\n\r\nds = load_dataset(\"codeparrot/codeparrot-clean-valid\", split=\"train\").select([i for i in range(10)])\r\nds1 = ds.map(preprocess, num_proc=2)\r\nds2 = ds.map(preprocess)\r\n\r\n# the datasets elements are the same\r\nfor i in range(len(ds1)):\r\n    assert ds1[i]==ds2[i]\r\n\r\nprint(f'Target column before filtering {ds1[\"autogenerated\"]}')\r\nprint(f'Target column before filtering {ds2[\"autogenerated\"]}')\r\nprint(f\"datasets version {datasets.__version__}\")\r\n\r\nds_filtered_1 = ds1.filter(lambda x: not x[\"autogenerated\"])\r\nds_filtered_2 = ds2.filter(lambda x: not x[\"autogenerated\"])\r\n\r\n# all elements in Target column are false so they should all be kept, but for ds2 only the first 5=num_samples/num_proc are kept\r\nprint(ds_filtered_1)\r\nprint(ds_filtered_2)\r\n```\r\n```\r\nTarget column before filtering [False, False, False, False, False, False, False, False, False, False]\r\nTarget column before filtering [False, False, False, False, False, False, False, False, False, False]\r\n\r\nDataset({\r\n    features: ['repo_name', 'path', 'copies', 'size', 'content', 'license', 'hash', 'line_mean', 'line_max', 'alpha_frac', 'autogenerated'],\r\n    num_rows: 5\r\n})\r\nDataset({\r\n    features: ['repo_name', 'path', 'copies', 'size', 'content', 'license', 'hash', 'line_mean', 'line_max', 'alpha_frac', 'autogenerated'],\r\n    num_rows: 10\r\n})\r\n```\r\n## Expected results\r\nIncreasing `num_proc` in mapping shouldn't alter filtering. With the previous version 2.5.2 this doesn't happen\r\n\r\n## Actual results\r\nFiltering doesn't work properly when we increase `num_proc` in mapping but not when calling `filter`\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.6.0\r\n- Platform: Linux-4.19.0-22-cloud-amd64-x86_64-with-glibc2.28\r\n- Python version: 3.9.13\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5111/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5111/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5109", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5109/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5109/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5109/events", "html_url": "https://github.com/huggingface/datasets/issues/5109", "id": 1407434706, "node_id": "I_kwDODunzps5T47_S", "number": 5109, "title": "Map caching not working for some class methods", "user": {"login": "Mouhanedg56", "id": 23029765, "node_id": "MDQ6VXNlcjIzMDI5NzY1", "avatar_url": "https://avatars.githubusercontent.com/u/23029765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mouhanedg56", "html_url": "https://github.com/Mouhanedg56", "followers_url": "https://api.github.com/users/Mouhanedg56/followers", "following_url": "https://api.github.com/users/Mouhanedg56/following{/other_user}", "gists_url": "https://api.github.com/users/Mouhanedg56/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mouhanedg56/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mouhanedg56/subscriptions", "organizations_url": "https://api.github.com/users/Mouhanedg56/orgs", "repos_url": "https://api.github.com/users/Mouhanedg56/repos", "events_url": "https://api.github.com/users/Mouhanedg56/events{/privacy}", "received_events_url": "https://api.github.com/users/Mouhanedg56/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-10-13T09:12:58Z", "updated_at": "2022-10-17T10:38:45Z", "closed_at": "2022-10-17T10:38:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nThe cache loading is not working as expected for some class methods with a model stored in an attribute.\r\nThe new fingerprint for `_map_single` is not the same at each run. The hasher generate a different hash for the class method.\r\nThis comes from `dumps` function in `datasets.utils.py_utils` which generates a different dump at each run.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\r\n\r\ndataset = load_dataset(\"ethos\", \"binary\")\r\nBASE_MODELNAME = \"sentence-transformers/all-MiniLM-L6-v2\"\r\n\r\nclass Object:\r\n    def __init__(self):\r\n        config = AutoConfig.from_pretrained(BASE_MODELNAME)\r\n        self.bert = AutoModel.from_config(config=config, add_pooling_layer=False)\r\n        self.tok = AutoTokenizer.from_pretrained(BASE_MODELNAME)\r\n\r\n    def tokenize(self, examples):\r\n\r\n        tokenized_texts = self.tok(\r\n            examples[\"text\"],\r\n            padding=\"max_length\",\r\n            truncation=True,\r\n            max_length=256,\r\n        )\r\n        return tokenized_texts\r\n\r\ninstance = Object()\r\nresult = dict()\r\nfor phase in [\"train\"]:\r\n    result[phase] = dataset[phase].map(instance.tokenize, batched=True, load_from_cache_file=True, num_proc=2)\r\n\r\n```\r\n\r\n## Expected results\r\nLoad cache instead of recompute result.\r\n\r\n## Actual results\r\nResult recomputed from scratch at each run.\r\nThe cache works fine when deleting `bert` attribute.\r\n\r\n## Environment info\r\n- `datasets` version: 2.5.3.dev0\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.9.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.5.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5109/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5109/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5102", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5102/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5102/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5102/events", "html_url": "https://github.com/huggingface/datasets/issues/5102", "id": 1404746554, "node_id": "I_kwDODunzps5Turs6", "number": 5102, "title": "Error in create a dataset from a Python generator", "user": {"login": "yangxuhui", "id": 9004682, "node_id": "MDQ6VXNlcjkwMDQ2ODI=", "avatar_url": "https://avatars.githubusercontent.com/u/9004682?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yangxuhui", "html_url": "https://github.com/yangxuhui", "followers_url": "https://api.github.com/users/yangxuhui/followers", "following_url": "https://api.github.com/users/yangxuhui/following{/other_user}", "gists_url": "https://api.github.com/users/yangxuhui/gists{/gist_id}", "starred_url": "https://api.github.com/users/yangxuhui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yangxuhui/subscriptions", "organizations_url": "https://api.github.com/users/yangxuhui/orgs", "repos_url": "https://api.github.com/users/yangxuhui/repos", "events_url": "https://api.github.com/users/yangxuhui/events{/privacy}", "received_events_url": "https://api.github.com/users/yangxuhui/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892877, "node_id": "MDU6TGFiZWwxOTM1ODkyODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}, {"id": 4614514401, "node_id": "LA_kwDODunzps8AAAABEwvm4Q", "url": "https://api.github.com/repos/huggingface/datasets/labels/hacktoberfest", "name": "hacktoberfest", "color": "DF8D62", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "riccardobucco", "id": 9295277, "node_id": "MDQ6VXNlcjkyOTUyNzc=", "avatar_url": "https://avatars.githubusercontent.com/u/9295277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/riccardobucco", "html_url": "https://github.com/riccardobucco", "followers_url": "https://api.github.com/users/riccardobucco/followers", "following_url": "https://api.github.com/users/riccardobucco/following{/other_user}", "gists_url": "https://api.github.com/users/riccardobucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/riccardobucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/riccardobucco/subscriptions", "organizations_url": "https://api.github.com/users/riccardobucco/orgs", "repos_url": "https://api.github.com/users/riccardobucco/repos", "events_url": "https://api.github.com/users/riccardobucco/events{/privacy}", "received_events_url": "https://api.github.com/users/riccardobucco/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "riccardobucco", "id": 9295277, "node_id": "MDQ6VXNlcjkyOTUyNzc=", "avatar_url": "https://avatars.githubusercontent.com/u/9295277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/riccardobucco", "html_url": "https://github.com/riccardobucco", "followers_url": "https://api.github.com/users/riccardobucco/followers", "following_url": "https://api.github.com/users/riccardobucco/following{/other_user}", "gists_url": "https://api.github.com/users/riccardobucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/riccardobucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/riccardobucco/subscriptions", "organizations_url": "https://api.github.com/users/riccardobucco/orgs", "repos_url": "https://api.github.com/users/riccardobucco/repos", "events_url": "https://api.github.com/users/riccardobucco/events{/privacy}", "received_events_url": "https://api.github.com/users/riccardobucco/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-10-11T14:28:58Z", "updated_at": "2022-10-12T11:31:56Z", "closed_at": "2022-10-12T11:31:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nIn HOW-TO-GUIDES > Load > [Python generator](https://huggingface.co/docs/datasets/v2.5.2/en/loading#python-generator), the code example defines the `my_gen` function, but when creating the dataset, an undefined `my_dict` is passed in.  \r\n\r\n```Python\r\n>>> from datasets import Dataset\r\n>>> def my_gen():\r\n...    for i in range(1, 4):\r\n...        yield {\"a\": i}\r\n>>> dataset = Dataset.from_generator(my_dict)\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5102/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5102/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5099", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5099/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5099/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5099/events", "html_url": "https://github.com/huggingface/datasets/issues/5099", "id": 1404370191, "node_id": "I_kwDODunzps5TtP0P", "number": 5099, "title": "datasets doesn't support # in data paths", "user": {"login": "loubnabnl", "id": 44069155, "node_id": "MDQ6VXNlcjQ0MDY5MTU1", "avatar_url": "https://avatars.githubusercontent.com/u/44069155?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loubnabnl", "html_url": "https://github.com/loubnabnl", "followers_url": "https://api.github.com/users/loubnabnl/followers", "following_url": "https://api.github.com/users/loubnabnl/following{/other_user}", "gists_url": "https://api.github.com/users/loubnabnl/gists{/gist_id}", "starred_url": "https://api.github.com/users/loubnabnl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loubnabnl/subscriptions", "organizations_url": "https://api.github.com/users/loubnabnl/orgs", "repos_url": "https://api.github.com/users/loubnabnl/repos", "events_url": "https://api.github.com/users/loubnabnl/events{/privacy}", "received_events_url": "https://api.github.com/users/loubnabnl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892877, "node_id": "MDU6TGFiZWwxOTM1ODkyODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}, {"id": 4614514401, "node_id": "LA_kwDODunzps8AAAABEwvm4Q", "url": "https://api.github.com/repos/huggingface/datasets/labels/hacktoberfest", "name": "hacktoberfest", "color": "DF8D62", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "riccardobucco", "id": 9295277, "node_id": "MDQ6VXNlcjkyOTUyNzc=", "avatar_url": "https://avatars.githubusercontent.com/u/9295277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/riccardobucco", "html_url": "https://github.com/riccardobucco", "followers_url": "https://api.github.com/users/riccardobucco/followers", "following_url": "https://api.github.com/users/riccardobucco/following{/other_user}", "gists_url": "https://api.github.com/users/riccardobucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/riccardobucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/riccardobucco/subscriptions", "organizations_url": "https://api.github.com/users/riccardobucco/orgs", "repos_url": "https://api.github.com/users/riccardobucco/repos", "events_url": "https://api.github.com/users/riccardobucco/events{/privacy}", "received_events_url": "https://api.github.com/users/riccardobucco/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "riccardobucco", "id": 9295277, "node_id": "MDQ6VXNlcjkyOTUyNzc=", "avatar_url": "https://avatars.githubusercontent.com/u/9295277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/riccardobucco", "html_url": "https://github.com/riccardobucco", "followers_url": "https://api.github.com/users/riccardobucco/followers", "following_url": "https://api.github.com/users/riccardobucco/following{/other_user}", "gists_url": "https://api.github.com/users/riccardobucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/riccardobucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/riccardobucco/subscriptions", "organizations_url": "https://api.github.com/users/riccardobucco/orgs", "repos_url": "https://api.github.com/users/riccardobucco/repos", "events_url": "https://api.github.com/users/riccardobucco/events{/privacy}", "received_events_url": "https://api.github.com/users/riccardobucco/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2022-10-11T10:05:32Z", "updated_at": "2022-10-13T13:14:20Z", "closed_at": "2022-10-13T13:14:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\ndataset files with `#` symbol their paths aren't read correctly.\r\n\r\n## Steps to reproduce the bug\r\nThe data in folder `c#`of this [dataset](https://huggingface.co/datasets/loubnabnl/bigcode_csharp) can't be loaded. While the folder `c_sharp` with the same data is loaded properly\r\n```python\r\nds = load_dataset('loubnabnl/bigcode_csharp', split=\"train\", data_files=[\"data/c#/*\"])\r\n```\r\n```\r\nFileNotFoundError: Couldn't find file at https://huggingface.co/datasets/loubnabnl/bigcode_csharp/resolve/27a3166cff4bb18e11919cafa6f169c0f57483de/data/c#/data_0003.jsonl\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.5.2\r\n- Platform: macOS-12.2.1-arm64-arm-64bit\r\n- Python version: 3.9.13\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.3\r\n\r\ncc @lhoestq ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5099/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5097", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5097/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5097/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5097/events", "html_url": "https://github.com/huggingface/datasets/issues/5097", "id": 1403679353, "node_id": "I_kwDODunzps5TqnJ5", "number": 5097, "title": "Fatal error with pyarrow/libarrow.so", "user": {"login": "catalys1", "id": 11340846, "node_id": "MDQ6VXNlcjExMzQwODQ2", "avatar_url": "https://avatars.githubusercontent.com/u/11340846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/catalys1", "html_url": "https://github.com/catalys1", "followers_url": "https://api.github.com/users/catalys1/followers", "following_url": "https://api.github.com/users/catalys1/following{/other_user}", "gists_url": "https://api.github.com/users/catalys1/gists{/gist_id}", "starred_url": "https://api.github.com/users/catalys1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/catalys1/subscriptions", "organizations_url": "https://api.github.com/users/catalys1/orgs", "repos_url": "https://api.github.com/users/catalys1/repos", "events_url": "https://api.github.com/users/catalys1/events{/privacy}", "received_events_url": "https://api.github.com/users/catalys1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-10-10T20:29:04Z", "updated_at": "2022-10-11T06:56:01Z", "closed_at": "2022-10-11T06:56:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen using datasets, at the very end of my jobs the program crashes (see trace below).\r\nIt doesn't seem to affect anything, as it appears to happen as the program is closing down. Just importing `datasets` is enough to cause the error.\r\n\r\n## Steps to reproduce the bug\r\nThis is sufficient to reproduce the problem:\r\n\r\n```bash\r\npython -c \"import datasets\"\r\n```\r\n\r\n## Expected results\r\nProgram should run to completion without an error.\r\n\r\n## Actual results\r\n```bash\r\nFatal error condition occurred in /opt/vcpkg/buildtrees/aws-c-io/src/9e6648842a-364b708815.clean/source/event_loop.c:72: aws_thread_launch(&cleanup_thread, s_event_loop_destroy_async_thread_fn, el_group, &thread_options) == AWS_OP_SUCCESS\r\nExiting Application\r\n################################################################################\r\nStack trace:\r\n################################################################################\r\n/u/user/miniconda3/envs/env/lib/python3.10/site-packages/pyarrow/libarrow.so.900(+0x200af06) [0x150dff547f06]\r\n/u/user/miniconda3/envs/env/lib/python3.10/site-packages/pyarrow/libarrow.so.900(+0x20028e5) [0x150dff53f8e5]\r\n/u/user/miniconda3/envs/env/lib/python3.10/site-packages/pyarrow/libarrow.so.900(+0x1f27e09) [0x150dff464e09]\r\n/u/user/miniconda3/envs/env/lib/python3.10/site-packages/pyarrow/libarrow.so.900(+0x200ba3d) [0x150dff548a3d]\r\n/u/user/miniconda3/envs/env/lib/python3.10/site-packages/pyarrow/libarrow.so.900(+0x1f25948) [0x150dff462948]\r\n/u/user/miniconda3/envs/env/lib/python3.10/site-packages/pyarrow/libarrow.so.900(+0x200ba3d) [0x150dff548a3d]\r\n/u/user/miniconda3/envs/env/lib/python3.10/site-packages/pyarrow/libarrow.so.900(+0x1ee0b46) [0x150dff41db46]\r\n/u/user/miniconda3/envs/env/lib/python3.10/site-packages/pyarrow/libarrow.so.900(+0x194546a) [0x150dfee8246a]\r\n/lib64/libc.so.6(+0x39b0c) [0x150e15eadb0c]\r\n/lib64/libc.so.6(on_exit+0) [0x150e15eadc40]\r\n/u/user/miniconda3/envs/env/bin/python(+0x28db18) [0x560ae370eb18]\r\n/u/user/miniconda3/envs/env/bin/python(+0x28db4b) [0x560ae370eb4b]\r\n/u/user/miniconda3/envs/env/bin/python(+0x28db90) [0x560ae370eb90]\r\n/u/user/miniconda3/envs/env/bin/python(_PyRun_SimpleFileObject+0x1e6) [0x560ae37123e6]\r\n/u/user/miniconda3/envs/env/bin/python(_PyRun_AnyFileObject+0x44) [0x560ae37124c4]\r\n/u/user/miniconda3/envs/env/bin/python(Py_RunMain+0x35d) [0x560ae37135bd]\r\n/u/user/miniconda3/envs/env/bin/python(Py_BytesMain+0x39) [0x560ae37137d9]\r\n/lib64/libc.so.6(__libc_start_main+0xf3) [0x150e15e97493]\r\n/u/user/miniconda3/envs/env/bin/python(+0x2125d4) [0x560ae36935d4]\r\nAborted (core dumped)\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.5.1\r\n- Platform: Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.10.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.3\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5097/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5093", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5093/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5093/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5093/events", "html_url": "https://github.com/huggingface/datasets/issues/5093", "id": 1402939660, "node_id": "I_kwDODunzps5TnykM", "number": 5093, "title": "Mismatch between tutoriel and doc", "user": {"login": "clefourrier", "id": 22726840, "node_id": "MDQ6VXNlcjIyNzI2ODQw", "avatar_url": "https://avatars.githubusercontent.com/u/22726840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clefourrier", "html_url": "https://github.com/clefourrier", "followers_url": "https://api.github.com/users/clefourrier/followers", "following_url": "https://api.github.com/users/clefourrier/following{/other_user}", "gists_url": "https://api.github.com/users/clefourrier/gists{/gist_id}", "starred_url": "https://api.github.com/users/clefourrier/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clefourrier/subscriptions", "organizations_url": "https://api.github.com/users/clefourrier/orgs", "repos_url": "https://api.github.com/users/clefourrier/repos", "events_url": "https://api.github.com/users/clefourrier/events{/privacy}", "received_events_url": "https://api.github.com/users/clefourrier/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892877, "node_id": "MDU6TGFiZWwxOTM1ODkyODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}, {"id": 4614514401, "node_id": "LA_kwDODunzps8AAAABEwvm4Q", "url": "https://api.github.com/repos/huggingface/datasets/labels/hacktoberfest", "name": "hacktoberfest", "color": "DF8D62", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "riccardobucco", "id": 9295277, "node_id": "MDQ6VXNlcjkyOTUyNzc=", "avatar_url": "https://avatars.githubusercontent.com/u/9295277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/riccardobucco", "html_url": "https://github.com/riccardobucco", "followers_url": "https://api.github.com/users/riccardobucco/followers", "following_url": "https://api.github.com/users/riccardobucco/following{/other_user}", "gists_url": "https://api.github.com/users/riccardobucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/riccardobucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/riccardobucco/subscriptions", "organizations_url": "https://api.github.com/users/riccardobucco/orgs", "repos_url": "https://api.github.com/users/riccardobucco/repos", "events_url": "https://api.github.com/users/riccardobucco/events{/privacy}", "received_events_url": "https://api.github.com/users/riccardobucco/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "riccardobucco", "id": 9295277, "node_id": "MDQ6VXNlcjkyOTUyNzc=", "avatar_url": "https://avatars.githubusercontent.com/u/9295277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/riccardobucco", "html_url": "https://github.com/riccardobucco", "followers_url": "https://api.github.com/users/riccardobucco/followers", "following_url": "https://api.github.com/users/riccardobucco/following{/other_user}", "gists_url": "https://api.github.com/users/riccardobucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/riccardobucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/riccardobucco/subscriptions", "organizations_url": "https://api.github.com/users/riccardobucco/orgs", "repos_url": "https://api.github.com/users/riccardobucco/repos", "events_url": "https://api.github.com/users/riccardobucco/events{/privacy}", "received_events_url": "https://api.github.com/users/riccardobucco/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-10-10T10:23:53Z", "updated_at": "2022-10-10T17:51:15Z", "closed_at": "2022-10-10T17:51:14Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nIn the \"Process text data\" tutorial, [`map` has `return_tensors` as kwarg](https://huggingface.co/docs/datasets/main/en/nlp_process#map). It does not seem to appear in the [function documentation](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map), nor to work.\r\n\r\n## Steps to reproduce the bug\r\nMWE:\r\n```python\r\nfrom transformers import AutoTokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n\r\nfrom datasets import load_dataset\r\ndataset  = load_dataset(\"lhoestq/demo1\", split=\"train\")\r\n\r\ndataset = dataset.map(lambda examples: tokenizer(examples[\"review\"]), batched=True, return_tensors=\"pt\")\r\n\r\n```\r\n\r\n## Expected results\r\nreturn_tensors to be a valid kwarg :smiley: \r\n\r\n## Actual results\r\n```python\r\n>> TypeError: map() got an unexpected keyword argument 'return_tensors'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.14.0-1052-oem-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.3\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5093/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5090", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5090/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5090/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5090/events", "html_url": "https://github.com/huggingface/datasets/issues/5090", "id": 1401102407, "node_id": "I_kwDODunzps5TgyBH", "number": 5090, "title": "Review sync issues from GitHub to Hub", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-10-07T12:31:56Z", "updated_at": "2022-10-08T07:07:36Z", "closed_at": "2022-10-08T07:07:36Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nWe have discovered that sometimes there were sync issues between GitHub and Hub datasets, after a merge commit to main branch.\r\n\r\nFor example:\r\n- this merge commit: https://github.com/huggingface/datasets/commit/d74a9e8e4bfff1fed03a4cab99180a841d7caf4b\r\n- was not properly synced with the Hub: https://github.com/huggingface/datasets/actions/runs/3002495269/jobs/4819769684\r\n```\r\n[main 9e641de]  Add Papers with Code ID to scifact dataset (#4941)\r\n Author: Albert Villanova del Moral <albertvillanova@users.noreply.huggingface.co>\r\n 1 file changed, 42 insertions(+), 14 deletions(-)\r\npush failed !\r\nGitCommandError(['git', 'push'], 1, b'remote: ----------------------------------------------------------        \\nremote: Sorry, your push was rejected during YAML metadata verification:        \\nremote: - Error: \"license\" does not match any of the allowed types        \\nremote: ----------------------------------------------------------        \\nremote: Please find the documentation at:        \\nremote: https://huggingface.co/docs/hub/models-cards#model-card-metadata        \\nremote: ----------------------------------------------------------        \\nTo [https://huggingface.co/datasets/scifact.git\\n](https://huggingface.co/datasets/scifact.git/n) ! [remote rejected] main -> main (pre-receive hook declined)\\nerror: failed to push some refs to \\'[https://huggingface.co/datasets/scifact.git\\](https://huggingface.co/datasets/scifact.git/)'', b'')\r\n```\r\nWe are reviewing sync issues in previous commits to recover them and repushing to the Hub.\r\n\r\nTODO: Review\r\n- [x] #4941\r\n  - scifact\r\n- [x] #4931\r\n  - scifact\r\n- [x] #4753\r\n  - wikipedia \r\n- [x] #4554\r\n  - wmt17, wmt19, wmt_t2t\r\n  - Fixed with \"Release 2.4.0\" commit: https://github.com/huggingface/datasets/commit/401d4c4f9b9594cb6527c599c0e7a72ce1a0ea49\r\n    - https://huggingface.co/datasets/wmt17/commit/5c0afa83fbbd3508ff7627c07f1b27756d1379ea\r\n    - https://huggingface.co/datasets/wmt19/commit/b8ad5bf1960208a376a0ab20bc8eac9638f7b400\r\n    - https://huggingface.co/datasets/wmt_t2t/commit/b6d67191804dd0933476fede36754a436b48d1fc\r\n- [x] #4607\r\n- [x] #4416\r\n  - lccc\r\n  - Fixed with \"Release 2.3.0\" commit: https://huggingface.co/datasets/lccc/commit/8b1f8cf425b5653a0a4357a53205aac82ce038d1\r\n- [x] #4367\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5090/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5086", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5086/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5086/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5086/events", "html_url": "https://github.com/huggingface/datasets/issues/5086", "id": 1400216975, "node_id": "I_kwDODunzps5TdZ2P", "number": 5086, "title": "HTTPError: 404 Client Error: Not Found for url", "user": {"login": "km5ar", "id": 54015474, "node_id": "MDQ6VXNlcjU0MDE1NDc0", "avatar_url": "https://avatars.githubusercontent.com/u/54015474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/km5ar", "html_url": "https://github.com/km5ar", "followers_url": "https://api.github.com/users/km5ar/followers", "following_url": "https://api.github.com/users/km5ar/following{/other_user}", "gists_url": "https://api.github.com/users/km5ar/gists{/gist_id}", "starred_url": "https://api.github.com/users/km5ar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/km5ar/subscriptions", "organizations_url": "https://api.github.com/users/km5ar/orgs", "repos_url": "https://api.github.com/users/km5ar/repos", "events_url": "https://api.github.com/users/km5ar/events{/privacy}", "received_events_url": "https://api.github.com/users/km5ar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-10-06T19:48:58Z", "updated_at": "2022-10-07T15:12:01Z", "closed_at": "2022-10-07T15:12:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nI was following chap 5 from huggingface course: https://huggingface.co/course/chapter5/6?fw=tf\r\n\r\nHowever, I'm not able to download the datasets, with a 404 erros\r\n\r\n\r\n<img width=\"1160\" alt=\"iShot2022-10-06_15 54 50\" src=\"https://user-images.githubusercontent.com/54015474/194406327-ae62c2f3-1da5-4686-8631-13d879a0edee.png\">\r\n\r\n\r\n\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom huggingface_hub import hf_hub_url\r\n\r\ndata_files = hf_hub_url(\r\n    repo_id=\"lewtun/github-issues\",\r\n    filename=\"datasets-issues-with-hf-doc-builder.jsonl\",\r\n    repo_type=\"dataset\",\r\n)\r\nfrom datasets import load_dataset\r\n\r\nissues_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\r\nissues_dataset\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.5.2\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.9.12\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.4\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5086/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5085", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5085/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5085/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5085/events", "html_url": "https://github.com/huggingface/datasets/issues/5085", "id": 1400113569, "node_id": "I_kwDODunzps5TdAmh", "number": 5085, "title": "Filtering on an empty dataset returns a corrupted dataset.", "user": {"login": "gabegma", "id": 36087158, "node_id": "MDQ6VXNlcjM2MDg3MTU4", "avatar_url": "https://avatars.githubusercontent.com/u/36087158?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gabegma", "html_url": "https://github.com/gabegma", "followers_url": "https://api.github.com/users/gabegma/followers", "following_url": "https://api.github.com/users/gabegma/following{/other_user}", "gists_url": "https://api.github.com/users/gabegma/gists{/gist_id}", "starred_url": "https://api.github.com/users/gabegma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gabegma/subscriptions", "organizations_url": "https://api.github.com/users/gabegma/orgs", "repos_url": "https://api.github.com/users/gabegma/repos", "events_url": "https://api.github.com/users/gabegma/events{/privacy}", "received_events_url": "https://api.github.com/users/gabegma/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 4614514401, "node_id": "LA_kwDODunzps8AAAABEwvm4Q", "url": "https://api.github.com/repos/huggingface/datasets/labels/hacktoberfest", "name": "hacktoberfest", "color": "DF8D62", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "Mouhanedg56", "id": 23029765, "node_id": "MDQ6VXNlcjIzMDI5NzY1", "avatar_url": "https://avatars.githubusercontent.com/u/23029765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mouhanedg56", "html_url": "https://github.com/Mouhanedg56", "followers_url": "https://api.github.com/users/Mouhanedg56/followers", "following_url": "https://api.github.com/users/Mouhanedg56/following{/other_user}", "gists_url": "https://api.github.com/users/Mouhanedg56/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mouhanedg56/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mouhanedg56/subscriptions", "organizations_url": "https://api.github.com/users/Mouhanedg56/orgs", "repos_url": "https://api.github.com/users/Mouhanedg56/repos", "events_url": "https://api.github.com/users/Mouhanedg56/events{/privacy}", "received_events_url": "https://api.github.com/users/Mouhanedg56/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Mouhanedg56", "id": 23029765, "node_id": "MDQ6VXNlcjIzMDI5NzY1", "avatar_url": "https://avatars.githubusercontent.com/u/23029765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mouhanedg56", "html_url": "https://github.com/Mouhanedg56", "followers_url": "https://api.github.com/users/Mouhanedg56/followers", "following_url": "https://api.github.com/users/Mouhanedg56/following{/other_user}", "gists_url": "https://api.github.com/users/Mouhanedg56/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mouhanedg56/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mouhanedg56/subscriptions", "organizations_url": "https://api.github.com/users/Mouhanedg56/orgs", "repos_url": "https://api.github.com/users/Mouhanedg56/repos", "events_url": "https://api.github.com/users/Mouhanedg56/events{/privacy}", "received_events_url": "https://api.github.com/users/Mouhanedg56/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-10-06T18:18:49Z", "updated_at": "2022-10-07T19:06:02Z", "closed_at": "2022-10-07T18:40:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen filtering a dataset twice, where the first result is an empty dataset, the second dataset seems corrupted.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n  datasets = load_dataset(\"glue\", \"sst2\")\r\n  dataset_split = datasets['validation']\r\n  ds_filter_1 = dataset_split.filter(lambda x: False)  # Some filtering condition that leads to an empty dataset\r\n  assert ds_filter_1.num_rows == 0\r\n  sentences = ds_filter_1['sentence']\r\n  assert len(sentences) == 0\r\n  ds_filter_2 = ds_filter_1.filter(lambda x: False)  # Some other filtering condition\r\n  assert ds_filter_2.num_rows == 0\r\n  assert 'sentence' in ds_filter_2.column_names\r\n  sentences = ds_filter_2['sentence']\r\n```\r\n\r\n## Expected results\r\nThe last line should be returning an empty list, same as 4 lines above.\r\n\r\n## Actual results\r\nThe last line currently raises `IndexError: index out of bounds`.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.5.2\r\n- Platform: macOS-11.6.6-x86_64-i386-64bit\r\n- Python version: 3.9.11\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5085/reactions", "total_count": 3, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 3, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5085/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5060", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5060/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5060/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5060/events", "html_url": "https://github.com/huggingface/datasets/issues/5060", "id": 1395382940, "node_id": "I_kwDODunzps5TK9qc", "number": 5060, "title": "Unable to Use Custom Dataset Locally", "user": {"login": "zanussbaum", "id": 33707069, "node_id": "MDQ6VXNlcjMzNzA3MDY5", "avatar_url": "https://avatars.githubusercontent.com/u/33707069?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zanussbaum", "html_url": "https://github.com/zanussbaum", "followers_url": "https://api.github.com/users/zanussbaum/followers", "following_url": "https://api.github.com/users/zanussbaum/following{/other_user}", "gists_url": "https://api.github.com/users/zanussbaum/gists{/gist_id}", "starred_url": "https://api.github.com/users/zanussbaum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zanussbaum/subscriptions", "organizations_url": "https://api.github.com/users/zanussbaum/orgs", "repos_url": "https://api.github.com/users/zanussbaum/repos", "events_url": "https://api.github.com/users/zanussbaum/events{/privacy}", "received_events_url": "https://api.github.com/users/zanussbaum/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-10-03T21:55:16Z", "updated_at": "2022-10-06T14:29:18Z", "closed_at": "2022-10-06T14:29:17Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI have uploaded a [dataset](https://huggingface.co/datasets/zpn/pubchem_selfies) and followed the instructions from the [dataset_loader](https://huggingface.co/docs/datasets/dataset_script#download-data-files-and-organize-splits) tutorial. In that tutorial, it says \r\n```\r\nIf the data files live in the same folder or repository of the dataset script, \r\nyou can just pass the relative paths to the files instead of URLs.\r\n```\r\nAccordingly, I put the [relative path](https://huggingface.co/datasets/zpn/pubchem_selfies/blob/main/pubchem_selfies.py#L76) to the data to be used. I was able to test the dataset and generate the metadata locally with `datasets-cli test path/to/<your-dataset-loading-script> --save_infos --all_configs`\r\n\r\nHowever, if I try to load the data using `load_dataset`, I get the following error\r\n```\r\nwith gzip.open(filepath, mode=\"rt\") as f:\r\n  File \"/usr/local/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/gzip.py\", line 58, in open\r\n    binary_file = GzipFile(filename, gz_mode, compresslevel)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/gzip.py\", line 173, in __init__\r\n    fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\r\nFileNotFoundError: [Errno 2] No such file or directory: 'https://huggingface.co/datasets/zpn/pubchem_selfies/resolve/main/data/Compound_021000001_021500000/Compound_021000001_021500000_SELFIES.jsonl.gz'\r\n```\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"zpn/pubchem_selfies\", streaming=True)\r\n>>> t = dataset[\"train\"]\r\n>>> for item in t:\r\n......     print(item)\r\n......     break\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/zachnussbaum/env/lib/python3.9/site-packages/datasets/iterable_dataset.py\", line 723, in __iter__\r\n    for key, example in self._iter():\r\n  File \"/Users/zachnussbaum/env/lib/python3.9/site-packages/datasets/iterable_dataset.py\", line 713, in _iter\r\n    yield from ex_iterable\r\n  File \"/Users/zachnussbaum/env/lib/python3.9/site-packages/datasets/iterable_dataset.py\", line 113, in __iter__\r\n    yield from self.generate_examples_fn(**self.kwargs)\r\n  File \"/Users/zachnussbaum/.cache/huggingface/modules/datasets_modules/datasets/zpn--pubchem_selfies/d2571f35996765aea70fd3f3f8e3882d59c401fb738615c79282e2eb1d9f7a25/pubchem_selfies.py\", line 475, in _generate_examples\r\n    with gzip.open(filepath, mode=\"rt\") as f:\r\n  File \"/usr/local/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/gzip.py\", line 58, in open\r\n    binary_file = GzipFile(filename, gz_mode, compresslevel)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/gzip.py\", line 173, in __init__\r\n    fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\r\nFileNotFoundError: [Errno 2] No such file or directory: 'https://huggingface.co/datasets/zpn/pubchem_selfies/resolve/main/data/Compound_021000001_021500000/Compound_021000001_021500000_SELFIES.jsonl.gz'\r\n````\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.5.1\r\n- Platform: macOS-12.5.1-x86_64-i386-64bit\r\n- Python version: 3.9.7\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5060/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5050", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5050/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5050/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5050/events", "html_url": "https://github.com/huggingface/datasets/issues/5050", "id": 1392381882, "node_id": "I_kwDODunzps5S_g-6", "number": 5050, "title": "Restore saved format state in `load_from_disk`", "user": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892877, "node_id": "MDU6TGFiZWwxOTM1ODkyODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": {"login": "asofiaoliveira", "id": 74454835, "node_id": "MDQ6VXNlcjc0NDU0ODM1", "avatar_url": "https://avatars.githubusercontent.com/u/74454835?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asofiaoliveira", "html_url": "https://github.com/asofiaoliveira", "followers_url": "https://api.github.com/users/asofiaoliveira/followers", "following_url": "https://api.github.com/users/asofiaoliveira/following{/other_user}", "gists_url": "https://api.github.com/users/asofiaoliveira/gists{/gist_id}", "starred_url": "https://api.github.com/users/asofiaoliveira/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asofiaoliveira/subscriptions", "organizations_url": "https://api.github.com/users/asofiaoliveira/orgs", "repos_url": "https://api.github.com/users/asofiaoliveira/repos", "events_url": "https://api.github.com/users/asofiaoliveira/events{/privacy}", "received_events_url": "https://api.github.com/users/asofiaoliveira/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asofiaoliveira", "id": 74454835, "node_id": "MDQ6VXNlcjc0NDU0ODM1", "avatar_url": "https://avatars.githubusercontent.com/u/74454835?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asofiaoliveira", "html_url": "https://github.com/asofiaoliveira", "followers_url": "https://api.github.com/users/asofiaoliveira/followers", "following_url": "https://api.github.com/users/asofiaoliveira/following{/other_user}", "gists_url": "https://api.github.com/users/asofiaoliveira/gists{/gist_id}", "starred_url": "https://api.github.com/users/asofiaoliveira/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asofiaoliveira/subscriptions", "organizations_url": "https://api.github.com/users/asofiaoliveira/orgs", "repos_url": "https://api.github.com/users/asofiaoliveira/repos", "events_url": "https://api.github.com/users/asofiaoliveira/events{/privacy}", "received_events_url": "https://api.github.com/users/asofiaoliveira/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-09-30T12:40:07Z", "updated_at": "2022-10-11T16:49:24Z", "closed_at": "2022-10-11T16:49:24Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Even though we save the `format` state in `save_to_disk`, we don't restore it in `load_from_disk`. We should fix that.\r\n \r\nReported here: https://discuss.huggingface.co/t/save-to-disk-loses-formatting-information/23815", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5050/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5050/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5046", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5046/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5046/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5046/events", "html_url": "https://github.com/huggingface/datasets/issues/5046", "id": 1391372519, "node_id": "I_kwDODunzps5S7qjn", "number": 5046, "title": "Audiofolder creates empty Dataset if files same level as metadata", "user": {"login": "msis", "id": 577139, "node_id": "MDQ6VXNlcjU3NzEzOQ==", "avatar_url": "https://avatars.githubusercontent.com/u/577139?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msis", "html_url": "https://github.com/msis", "followers_url": "https://api.github.com/users/msis/followers", "following_url": "https://api.github.com/users/msis/following{/other_user}", "gists_url": "https://api.github.com/users/msis/gists{/gist_id}", "starred_url": "https://api.github.com/users/msis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msis/subscriptions", "organizations_url": "https://api.github.com/users/msis/orgs", "repos_url": "https://api.github.com/users/msis/repos", "events_url": "https://api.github.com/users/msis/events{/privacy}", "received_events_url": "https://api.github.com/users/msis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892877, "node_id": "MDU6TGFiZWwxOTM1ODkyODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}, {"id": 4614514401, "node_id": "LA_kwDODunzps8AAAABEwvm4Q", "url": "https://api.github.com/repos/huggingface/datasets/labels/hacktoberfest", "name": "hacktoberfest", "color": "DF8D62", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "riccardobucco", "id": 9295277, "node_id": "MDQ6VXNlcjkyOTUyNzc=", "avatar_url": "https://avatars.githubusercontent.com/u/9295277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/riccardobucco", "html_url": "https://github.com/riccardobucco", "followers_url": "https://api.github.com/users/riccardobucco/followers", "following_url": "https://api.github.com/users/riccardobucco/following{/other_user}", "gists_url": "https://api.github.com/users/riccardobucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/riccardobucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/riccardobucco/subscriptions", "organizations_url": "https://api.github.com/users/riccardobucco/orgs", "repos_url": "https://api.github.com/users/riccardobucco/repos", "events_url": "https://api.github.com/users/riccardobucco/events{/privacy}", "received_events_url": "https://api.github.com/users/riccardobucco/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "riccardobucco", "id": 9295277, "node_id": "MDQ6VXNlcjkyOTUyNzc=", "avatar_url": "https://avatars.githubusercontent.com/u/9295277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/riccardobucco", "html_url": "https://github.com/riccardobucco", "followers_url": "https://api.github.com/users/riccardobucco/followers", "following_url": "https://api.github.com/users/riccardobucco/following{/other_user}", "gists_url": "https://api.github.com/users/riccardobucco/gists{/gist_id}", "starred_url": "https://api.github.com/users/riccardobucco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/riccardobucco/subscriptions", "organizations_url": "https://api.github.com/users/riccardobucco/orgs", "repos_url": "https://api.github.com/users/riccardobucco/repos", "events_url": "https://api.github.com/users/riccardobucco/events{/privacy}", "received_events_url": "https://api.github.com/users/riccardobucco/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-09-29T19:17:23Z", "updated_at": "2022-10-28T13:05:07Z", "closed_at": "2022-10-28T13:05:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen audio files are at the same level as the metadata (`metadata.csv` or `metadata.jsonl` ), the `load_dataset` returns a `DatasetDict` with no rows but the correct columns.\r\n\r\nhttps://github.com/huggingface/datasets/blob/1ea4d091b7a4b83a85b2eeb8df65115d39af3766/docs/source/audio_dataset.mdx?plain=1#L88\r\n\r\n## Steps to reproduce the bug\r\n`metadata.csv`:\r\n```csv\r\nfile_name,duration,transcription\r\n./2063_fe9936e7-62b2-4e62-a276-acbd344480ce_1.wav,10.768,hello\r\n```\r\n\r\n\r\n```python\r\n>>> audio_dataset = load_dataset(\"audiofolder\", data_dir=\"/audio-data/\")\r\n>>> audio_dataset\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['audio', 'duration', 'transcription'],\r\n        num_rows: 0\r\n    })\r\n    validation: Dataset({\r\n        features: ['audio', 'duration', 'transcription'],\r\n        num_rows: 0\r\n    })\r\n})\r\n```\r\n\r\nI've tried, with no success,:\r\n- setting `split` to something else so I don't get a `DatasetDict`,\r\n- removing the `./`,\r\n- using `.jsonl`.\r\n\r\n\r\n## Expected results\r\n```\r\n Dataset({\r\n        features: ['audio', 'duration', 'transcription'],\r\n        num_rows: 1\r\n})\r\n```\r\n\r\n## Actual results\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['audio', 'duration', 'transcription'],\r\n        num_rows: 0\r\n    })\r\n    validation: Dataset({\r\n        features: ['audio', 'duration', 'transcription'],\r\n        num_rows: 0\r\n    })\r\n})\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.5.1\r\n- Platform: Linux-5.13.0-1025-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5046/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5046/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5038", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5038/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5038/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5038/events", "html_url": "https://github.com/huggingface/datasets/issues/5038", "id": 1389631122, "node_id": "I_kwDODunzps5S1BaS", "number": 5038, "title": "`Dataset.unique` showing wrong output after filtering", "user": {"login": "mxschmdt", "id": 4904985, "node_id": "MDQ6VXNlcjQ5MDQ5ODU=", "avatar_url": "https://avatars.githubusercontent.com/u/4904985?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mxschmdt", "html_url": "https://github.com/mxschmdt", "followers_url": "https://api.github.com/users/mxschmdt/followers", "following_url": "https://api.github.com/users/mxschmdt/following{/other_user}", "gists_url": "https://api.github.com/users/mxschmdt/gists{/gist_id}", "starred_url": "https://api.github.com/users/mxschmdt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mxschmdt/subscriptions", "organizations_url": "https://api.github.com/users/mxschmdt/orgs", "repos_url": "https://api.github.com/users/mxschmdt/repos", "events_url": "https://api.github.com/users/mxschmdt/events{/privacy}", "received_events_url": "https://api.github.com/users/mxschmdt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-09-28T16:20:35Z", "updated_at": "2022-09-30T15:44:25Z", "closed_at": "2022-09-30T15:44:25Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nAfter filtering a dataset, and if no samples remain, `Dataset.unique` will return the unique values of the unfiltered dataset.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import Dataset\r\n\r\ndataset = Dataset.from_dict({'id': [0]})\r\ndataset = dataset.filter(lambda _: False)\r\nprint(dataset.unique('id'))\r\n```\r\n\r\n## Expected results\r\nThe above code should return an empty list since the dataset is empty.\r\n\r\n## Actual results\r\n```bash\r\n[0]\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.5.1\r\n- Platform: Linux-5.18.19-100.fc35.x86_64-x86_64-with-glibc2.34\r\n- Python version: 3.9.14\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.3.5", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5038/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5038/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5025", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5025/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5025/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5025/events", "html_url": "https://github.com/huggingface/datasets/issues/5025", "id": 1386011239, "node_id": "I_kwDODunzps5SnNpn", "number": 5025, "title": "Custom Json Dataset Throwing Error when batch is False", "user": {"login": "jmandivarapu1", "id": 21245519, "node_id": "MDQ6VXNlcjIxMjQ1NTE5", "avatar_url": "https://avatars.githubusercontent.com/u/21245519?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmandivarapu1", "html_url": "https://github.com/jmandivarapu1", "followers_url": "https://api.github.com/users/jmandivarapu1/followers", "following_url": "https://api.github.com/users/jmandivarapu1/following{/other_user}", "gists_url": "https://api.github.com/users/jmandivarapu1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmandivarapu1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmandivarapu1/subscriptions", "organizations_url": "https://api.github.com/users/jmandivarapu1/orgs", "repos_url": "https://api.github.com/users/jmandivarapu1/repos", "events_url": "https://api.github.com/users/jmandivarapu1/events{/privacy}", "received_events_url": "https://api.github.com/users/jmandivarapu1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-09-26T12:38:39Z", "updated_at": "2022-09-27T19:50:00Z", "closed_at": "2022-09-27T19:50:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\nI tried to create my custom dataset using below code\r\n\r\n```\r\nfrom datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\r\nfrom torchvision import transforms\r\nfrom transformers import AutoProcessor\r\n# we'll use the Auto API here - it will load LayoutLMv3Processor behind the scenes,\r\n# based on the checkpoint we provide from the hub\r\nfrom datasets import load_dataset\r\n\r\ndef prepare_examples(examples):\r\n     #Some preporcessing for each image and text as all my data saved in cloud\r\n     #For this reason I couldn't set the batch to True. \r\n      encoding = processor(img_as_tensor, words, boxes=boxes, word_labels=labels,\r\n                       truncation=True, padding=\"max_length\")\r\n    # encoding['pixel_values']=np.array(encoding['pixel_values'])\r\n    return encoding\r\n\r\ndataset = load_dataset(\"json\", data_files='issues.jsonl')\r\nprocessor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\r\nfeatures = dataset[\"train\"].features\r\ncolumn_names = dataset[\"train\"].column_names\r\n# we need to define custom features for `set_format` (used later on) to work properly\r\nfeatures = Features({\r\n    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\r\n    'input_ids': Sequence(feature=Value(dtype='int64')),\r\n    'attention_mask': Sequence(Value(dtype='int64')),\r\n    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\r\n    'labels': Sequence(feature=Value(dtype='int64')),\r\n})\r\n\r\ntrain_dataset = dataset[\"train\"].map(\r\n    prepare_examples,\r\n    batched=False,\r\n    remove_columns=column_names,\r\n    features=features\r\n)\r\n```\r\n\r\nIt throws below error.\r\n```\r\n/opt/conda/lib/python3.7/site-packages/datasets/arrow_writer.py in __arrow_array__(self, type)\r\n    172                 storage = to_pyarrow_listarray(data, pa_type)\r\n--> 173                 return pa.ExtensionArray.from_storage(pa_type, storage)\r\n    174 \r\n\r\n/opt/conda/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.ExtensionArray.from_storage()\r\n\r\nTypeError: Incompatible storage type list<item: list<item: list<item: list<item: float>>>> for extension type extension<arrow.py_extension_type<Array3DExtensionType>>\r\n```\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n```\r\nrom datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\r\nfrom torchvision import transforms\r\nfrom transformers import AutoProcessor\r\n# we'll use the Auto API here - it will load LayoutLMv3Processor behind the scenes,\r\n# based on the checkpoint we provide from the hub\r\nfrom datasets import load_dataset\r\n\r\ndef prepare_examples(examples):\r\n     #Some preporcessing for each image and text as all my data saved in cloud\r\n      encoding = processor(img_as_tensor, words, boxes=boxes, word_labels=labels,\r\n                       truncation=True, padding=\"max_length\")\r\n    # encoding['pixel_values']=np.array(encoding['pixel_values'])\r\n    return encoding\r\n\r\ndataset = load_dataset(\"json\", data_files='issues.jsonl')\r\nprocessor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\r\nfeatures = dataset[\"train\"].features\r\ncolumn_names = dataset[\"train\"].column_names\r\n# we need to define custom features for `set_format` (used later on) to work properly\r\nfeatures = Features({\r\n    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\r\n    'input_ids': Sequence(feature=Value(dtype='int64')),\r\n    'attention_mask': Sequence(Value(dtype='int64')),\r\n    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\r\n    'labels': Sequence(feature=Value(dtype='int64')),\r\n})\r\n\r\ntrain_dataset = dataset[\"train\"].map(\r\n    prepare_examples,\r\n    batched=False,\r\n    remove_columns=column_names,\r\n    features=features\r\n)\r\n## Expected results\r\nA clear and concise description of the expected results.\r\nExpected would be similar to all the otherdatasets with no error.\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform: Unix\r\n- Python version: 3.9\r\n- PyArrow version: 9.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5025/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5021", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5021/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5021/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5021/events", "html_url": "https://github.com/huggingface/datasets/issues/5021", "id": 1385351250, "node_id": "I_kwDODunzps5SkshS", "number": 5021, "title": "Split is inferred from filename and overrides metadata.jsonl", "user": {"login": "float-trip", "id": 102226344, "node_id": "U_kgDOBhfZqA", "avatar_url": "https://avatars.githubusercontent.com/u/102226344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/float-trip", "html_url": "https://github.com/float-trip", "followers_url": "https://api.github.com/users/float-trip/followers", "following_url": "https://api.github.com/users/float-trip/following{/other_user}", "gists_url": "https://api.github.com/users/float-trip/gists{/gist_id}", "starred_url": "https://api.github.com/users/float-trip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/float-trip/subscriptions", "organizations_url": "https://api.github.com/users/float-trip/orgs", "repos_url": "https://api.github.com/users/float-trip/repos", "events_url": "https://api.github.com/users/float-trip/events{/privacy}", "received_events_url": "https://api.github.com/users/float-trip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892865, "node_id": "MDU6TGFiZWwxOTM1ODkyODY1", "url": "https://api.github.com/repos/huggingface/datasets/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-09-26T03:22:14Z", "updated_at": "2022-09-29T08:07:50Z", "closed_at": "2022-09-29T08:07:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nIncluding the strings \"test\" or \"train\" anywhere in a filename causes `datasets` to infer the split and silently ignore all other files.\r\n\r\nThis behavior is documented for directory names but not filenames: https://huggingface.co/docs/datasets/image_dataset#imagefolder\r\n\r\n## Steps to reproduce the bug\r\n`metadata.jsonl`\r\n```json\r\n{\"file_name\": \"photo of a cat.jpg\", \"text\": \"a photo of a cat\"}\r\n{\"file_name\": \"photo of a dog.jpg\", \"text\": \"a photo of a dog\"}\r\n{\"file_name\": \"photo of a train.jpg\", \"text\": \"a photo of a train\"}\r\n{\"file_name\": \"photo of test tubes.jpg\", \"text\": \"a photo of test tubes\"}\r\n```\r\n\r\n`bug.py`\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"dataset\")\r\n\r\nprint(dataset)\r\n# DatasetDict({\r\n#     train: Dataset({\r\n#         features: ['image', 'text'],\r\n#         num_rows: 1\r\n#     })\r\n#     test: Dataset({\r\n#         features: ['image', 'text'],\r\n#         num_rows: 1\r\n#     })\r\n# })\r\n\r\nfor split in dataset:\r\n    for n in dataset[split]:\r\n        print(n['text'])\r\n        # a photo of a train\r\n        # a photo of test tubes\r\n```\r\n\r\n## Expected results\r\nOne single dataset with all four images / a warning for unused files / documentation of this behavior\r\n\r\n## Actual results\r\nOnly the images with \"test\" or \"train\" in the name are loaded\r\n\r\n## Environment info\r\n- `datasets` version: 2.5.1\r\n- Platform: macOS-12.5.1-x86_64-i386-64bit\r\n- Python version: 3.10.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5021/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5011", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5011/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5011/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5011/events", "html_url": "https://github.com/huggingface/datasets/issues/5011", "id": 1382609587, "node_id": "I_kwDODunzps5SaPKz", "number": 5011, "title": "Audio: `encode_example` fails with IndexError", "user": {"login": "sanchit-gandhi", "id": 93869735, "node_id": "U_kgDOBZhWpw", "avatar_url": "https://avatars.githubusercontent.com/u/93869735?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sanchit-gandhi", "html_url": "https://github.com/sanchit-gandhi", "followers_url": "https://api.github.com/users/sanchit-gandhi/followers", "following_url": "https://api.github.com/users/sanchit-gandhi/following{/other_user}", "gists_url": "https://api.github.com/users/sanchit-gandhi/gists{/gist_id}", "starred_url": "https://api.github.com/users/sanchit-gandhi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sanchit-gandhi/subscriptions", "organizations_url": "https://api.github.com/users/sanchit-gandhi/orgs", "repos_url": "https://api.github.com/users/sanchit-gandhi/repos", "events_url": "https://api.github.com/users/sanchit-gandhi/events{/privacy}", "received_events_url": "https://api.github.com/users/sanchit-gandhi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-09-22T15:07:27Z", "updated_at": "2022-09-23T09:05:18Z", "closed_at": "2022-09-23T09:05:18Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading the dataset [earnings-22](https://huggingface.co/datasets/sanchit-gandhi/earnings22_split) from the Hub yields an Index Error. I created this dataset locally and then pushed to hub at the specified URL. Thus, I expect the dataset should work out-of-the-box! Indeed, the dataset viewer functions correctly, and there were no issues when I had the dataset locally.\r\n\r\nDon't think it's a sound file bug as the version matches what worked previously.\r\n\r\nUpdate: the bug appeared for me on a GPU, mysteriously on a TPU I can't repro and it downloads correctly...\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nearnings22 = load_dataset(\"sanchit-gandhi/earnings22_split\")\r\n```\r\n\r\n## Expected results\r\n```\r\n>>>  earnings22\r\nDatasetDict({\r\n    validation: Dataset({\r\n        features: ['source_id', 'audio', 'segment_id', 'sentence', 'start_ts', 'end_ts', 'id'],\r\n        num_rows: 2650\r\n    })\r\n    train: Dataset({\r\n        features: ['source_id', 'audio', 'segment_id', 'sentence', 'start_ts', 'end_ts', 'id'],\r\n        num_rows: 52006\r\n    })\r\n    test: Dataset({\r\n        features: ['source_id', 'audio', 'segment_id', 'sentence', 'start_ts', 'end_ts', 'id'],\r\n        num_rows: 2735\r\n    })\r\n})\r\n```\r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2764, in _map_single\r\n    writer.write(example)\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 451, in write\r\n    self.write_examples_on_file()\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 409, in write_examples_on_file\r\n    self.write_batch(batch_examples=batch_examples)\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 508, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow/array.pxi\", line 231, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 197, in __arrow_array__\r\n    out = cast_array_to_feature(out, type, allow_number_to_str=not self.trying_type)\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/datasets/table.py\", line 1683, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/datasets/table.py\", line 1795, in cast_array_to_feature\r\n    return feature.cast_storage(array)\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/datasets/features/audio.py\", line 190, in cast_storage\r\n    storage = pa.array([Audio().encode_example(x) if x is not None else None for x in storage.to_pylist()])\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/datasets/features/audio.py\", line 190, in <listcomp>\r\n    storage = pa.array([Audio().encode_example(x) if x is not None else None for x in storage.to_pylist()])\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/datasets/features/audio.py\", line 92, in encode_example\r\n    sf.write(buffer, value[\"array\"], value[\"sampling_rate\"], format=\"wav\")\r\n  File \"/opt/conda/envs/hf/lib/python3.8/site-packages/soundfile.py\", line 313, in write\r\n    channels = data.shape[1]\r\nIndexError: tuple index out of range\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-4.19.0-21-cloud-amd64-x86_64-with-glibc2.10\r\n- Python version: 3.8.13\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.3\r\n\r\nPlus:\r\n- SoundFile version: 0.10.3.post1\r\n\r\ncc @lhoestq @polinaeterna ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5011/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5009", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5009/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5009/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5009/events", "html_url": "https://github.com/huggingface/datasets/issues/5009", "id": 1381194067, "node_id": "I_kwDODunzps5SU1lT", "number": 5009, "title": "Error loading StonyBrookNLP/tellmewhy dataset from hub even though local copy loads correctly", "user": {"login": "ykl7", "id": 4996184, "node_id": "MDQ6VXNlcjQ5OTYxODQ=", "avatar_url": "https://avatars.githubusercontent.com/u/4996184?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ykl7", "html_url": "https://github.com/ykl7", "followers_url": "https://api.github.com/users/ykl7/followers", "following_url": "https://api.github.com/users/ykl7/following{/other_user}", "gists_url": "https://api.github.com/users/ykl7/gists{/gist_id}", "starred_url": "https://api.github.com/users/ykl7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ykl7/subscriptions", "organizations_url": "https://api.github.com/users/ykl7/orgs", "repos_url": "https://api.github.com/users/ykl7/repos", "events_url": "https://api.github.com/users/ykl7/events{/privacy}", "received_events_url": "https://api.github.com/users/ykl7/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2022-09-21T16:23:06Z", "updated_at": "2022-09-29T13:07:29Z", "closed_at": "2022-09-29T13:07:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI have added a new dataset with the identifier `StonyBrookNLP/tellmewhy` to the hub. When I load the individual files using my local copy using `dataset = datasets.load_dataset(\"json\", data_files=\"data/train.jsonl\")`, it loads the dataset correctly. However, when I try to load it from the hub, I get an error (pasted below). Additionally, `dataset = datasets.load_dataset(\"json\", data_dir=\"data/\")` throws the same error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = datasets.load_dataset('StonyBrookNLP/tellmewhy')\r\n```\r\n\r\n## Expected results\r\nSuccessfully load the `StonyBrookNLP/tellmewhy` dataset.\r\n\r\n## Actual results\r\n```\r\nUsing custom data configuration StonyBrookNLP--tellmewhy-82712924092694ff\r\nDownloading and preparing dataset json/StonyBrookNLP--tellmewhy to /home/yklal95/.cache/huggingface/datasets/StonyBrookNLP___json/StonyBrookNLP--tellmewhy-82712924092694ff/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 957.46it/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 299.14it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/yklal95/tmw-generalization/src/load_datasets.py\", line 17, in <module>\r\n    main(args)\r\n  File \"/home/yklal95/tmw-generalization/src/load_datasets.py\", line 11, in main\r\n    dataset = datasets.load_dataset(args.dataset_name)\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/load.py\", line 1746, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/builder.py\", line 704, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/builder.py\", line 793, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/builder.py\", line 1277, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/arrow_writer.py\", line 524, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/table.py\", line 2005, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/table.py\", line 1969, in cast_table_to_schema\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/table.py\", line 1969, in <listcomp>\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/table.py\", line 1681, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/table.py\", line 1681, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/table.py\", line 1822, in cast_array_to_feature\r\n    casted_values = _c(array.values, feature.feature)\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/table.py\", line 1683, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/table.py\", line 1853, in cast_array_to_feature\r\n    return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/table.py\", line 1683, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/home/yklal95/anaconda3/envs/tmw-generalization/lib/python3.9/site-packages/datasets/table.py\", line 1761, in array_cast\r\n    raise TypeError(f\"Couldn't cast array of type {array.type} to {pa_type}\")\r\nTypeError: Couldn't cast array of type int64 to null\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-4.15.0-121-generic-x86_64-with-glibc2.27\r\n- Python version: 3.9.13\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5009/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5009/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/5005", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/5005/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/5005/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/5005/events", "html_url": "https://github.com/huggingface/datasets/issues/5005", "id": 1380952960, "node_id": "I_kwDODunzps5ST6uA", "number": 5005, "title": "Release 2.5.0 breaks transformers CI", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-09-21T13:39:19Z", "updated_at": "2022-09-21T14:11:57Z", "closed_at": "2022-09-21T14:11:57Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAs reported by @lhoestq:\r\n\r\n> see https://app.circleci.com/pipelines/github/huggingface/transformers/47634/workflows/b491886b-e66e-4edb-af96-8b459e72aa25/jobs/564563\r\nthis is used here: [https://github.com/huggingface/transformers/blob/3b19c0317b6909e2d7f11b5053895ac55[\u2026]torch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py](https://github.com/huggingface/transformers/blob/3b19c0317b6909e2d7f11b5053895ac55250e7da/examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py#L482-L488)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/5005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/5005/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4990", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4990/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4990/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4990/events", "html_url": "https://github.com/huggingface/datasets/issues/4990", "id": 1378120806, "node_id": "I_kwDODunzps5SJHRm", "number": 4990, "title": "\"no-token\" is passed to `huggingface_hub` when token is `None`", "user": {"login": "Wauplin", "id": 11801849, "node_id": "MDQ6VXNlcjExODAxODQ5", "avatar_url": "https://avatars.githubusercontent.com/u/11801849?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Wauplin", "html_url": "https://github.com/Wauplin", "followers_url": "https://api.github.com/users/Wauplin/followers", "following_url": "https://api.github.com/users/Wauplin/following{/other_user}", "gists_url": "https://api.github.com/users/Wauplin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Wauplin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Wauplin/subscriptions", "organizations_url": "https://api.github.com/users/Wauplin/orgs", "repos_url": "https://api.github.com/users/Wauplin/repos", "events_url": "https://api.github.com/users/Wauplin/events{/privacy}", "received_events_url": "https://api.github.com/users/Wauplin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2022-09-19T15:14:40Z", "updated_at": "2022-09-30T09:16:00Z", "closed_at": "2022-09-30T09:16:00Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nIn the 2 lines listed below, a token is passed to `huggingface_hub` to get information from a dataset. If no token is provided, a \"no-token\" string is passed. What is the purpose of it ? If no real, I would prefer if the `None` value could be sent directly to be handle by `huggingface_hub`. I feel that here it is working because we assume the token will never be validated.\r\n\r\nhttps://github.com/huggingface/datasets/blob/5b23f58535f14cc4dd7649485bce1ccc836e7bca/src/datasets/load.py#L753\r\nhttps://github.com/huggingface/datasets/blob/5b23f58535f14cc4dd7649485bce1ccc836e7bca/src/datasets/load.py#L1121\r\n\r\n## Expected results\r\nPass `token=None` to `huggingface_hub`.\r\n\r\n## Actual results\r\n`token=\"no-token\"` is passed.\r\n\r\n## Environment info\r\n`huggingface_hub v0.10.0dev`", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4990/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4990/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4989", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4989/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4989/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4989/events", "html_url": "https://github.com/huggingface/datasets/issues/4989", "id": 1376832233, "node_id": "I_kwDODunzps5SEMrp", "number": 4989, "title": "Running add_column() seems to corrupt existing sequence-type column info", "user": {"login": "derek-rocheleau", "id": 93728165, "node_id": "U_kgDOBZYtpQ", "avatar_url": "https://avatars.githubusercontent.com/u/93728165?v=4", "gravatar_id": "", "url": "https://api.github.com/users/derek-rocheleau", "html_url": "https://github.com/derek-rocheleau", "followers_url": "https://api.github.com/users/derek-rocheleau/followers", "following_url": "https://api.github.com/users/derek-rocheleau/following{/other_user}", "gists_url": "https://api.github.com/users/derek-rocheleau/gists{/gist_id}", "starred_url": "https://api.github.com/users/derek-rocheleau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/derek-rocheleau/subscriptions", "organizations_url": "https://api.github.com/users/derek-rocheleau/orgs", "repos_url": "https://api.github.com/users/derek-rocheleau/repos", "events_url": "https://api.github.com/users/derek-rocheleau/events{/privacy}", "received_events_url": "https://api.github.com/users/derek-rocheleau/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-09-17T17:42:05Z", "updated_at": "2022-09-19T12:54:54Z", "closed_at": "2022-09-19T12:54:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a dataset that contains a column (\"foo\") that is a sequence type of length 4. So when I run .to_pandas() on it, the resulting dataframe correctly contains 4 columns - foo_0, foo_1, foo_2, foo_3. So the 1st row of the dataframe might look like:\r\n\r\nds = load_dataset(...)\r\ndf = ds.to_pandas()\r\n\r\ndf:\r\nfoo_0 | foo_1 | foo_2 | foo_3\r\n0.0 | 1.0 | 2.0 | 3.0\r\n\r\nIf I run .add_column(\"new_col\", data) on the dataset, and then .to_pandas() on the resulting new dataset, the resulting dataframe contains only 2 columns - foo, new_col. The values in column foo are lists of length 4, the 4 elements that should have been split into separate columns. Dataframe 1st row would be:\r\n\r\nds = load_dataset(...)\r\nnew_ds = ds.add_column(\"new_col\", data)\r\ndf = new_ds.to_pandas()\r\n\r\ndf:\r\nfoo | new_col\r\n[0.0, 1.0, 2.0, 3.0] | new_val\r\n\r\nI've explored the 2 datasets in a debugger and haven't noticed any changes to any attributes related to the foo column, but I can't determine why the dataframes are so different.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4989/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4989/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4982", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4982/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4982/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4982/events", "html_url": "https://github.com/huggingface/datasets/issues/4982", "id": 1375604693, "node_id": "I_kwDODunzps5R_g_V", "number": 4982, "title": "Create dataset_infos.json with VALIDATION and TEST splits", "user": {"login": "skalinin", "id": 26695348, "node_id": "MDQ6VXNlcjI2Njk1MzQ4", "avatar_url": "https://avatars.githubusercontent.com/u/26695348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skalinin", "html_url": "https://github.com/skalinin", "followers_url": "https://api.github.com/users/skalinin/followers", "following_url": "https://api.github.com/users/skalinin/following{/other_user}", "gists_url": "https://api.github.com/users/skalinin/gists{/gist_id}", "starred_url": "https://api.github.com/users/skalinin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skalinin/subscriptions", "organizations_url": "https://api.github.com/users/skalinin/orgs", "repos_url": "https://api.github.com/users/skalinin/repos", "events_url": "https://api.github.com/users/skalinin/events{/privacy}", "received_events_url": "https://api.github.com/users/skalinin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-09-16T08:21:19Z", "updated_at": "2022-09-28T07:59:39Z", "closed_at": "2022-09-28T07:59:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "The problem is described in that [issue](https://github.com/huggingface/datasets/issues/4895#issuecomment-1247975569). \r\n\r\n> When I try to create data_infos.json using datasets-cli test Peter.py --save_infos --all_configs I get an error:\r\n> ValueError: Unknown split \"test\". Should be one of ['train'].\r\n> \r\n> The data_infos.json is created perfectly fine when I use only one split - datasets.Split.TRAIN\r\n> \r\n> You can find the code here: https://huggingface.co/datasets/sberbank-ai/Peter/tree/add_splits (add_splits branch)\r\n\r\nI tried to clear the cache folder, than I got an another error. I run:\r\n\r\n```\r\ngit clone https://huggingface.co/datasets/sberbank-ai/Peter\r\ncd Peter\r\ngit checkout add_splits  # switch to a add_splits branch\r\nrm dataset_infos.json  # remove local dataset_infos.json\r\nrm -r ~/.cache/huggingface   # remove cached dataset_infos.json\r\ndatasets-cli test Peter.py --save_infos --all_configs  # trying to create new dataset_infos.json\r\n```\r\n\r\nThe error message:\r\n```\r\nUsing custom data configuration default\r\nTesting builder 'default' (1/1)\r\nDownloading and preparing dataset peter/default to /Users/kalinin/.cache/huggingface/datasets/peter/default/0.0.0/ef579519e140d6a40df2555996f26165f04c47557d7373709c8d7e7b4fd7465d...\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 5160.63it/s]\r\nExtracting data files:   0%|                                                                                                            | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/usr/local/bin/datasets-cli\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/commands/datasets_cli.py\", line 39, in main\r\n    service.run()\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/commands/test.py\", line 137, in run\r\n    builder.download_and_prepare(\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/builder.py\", line 704, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/builder.py\", line 1227, in _download_and_prepare\r\n    super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/builder.py\", line 771, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/Users/kalinin/.cache/huggingface/modules/datasets_modules/datasets/Peter/ef579519e140d6a40df2555996f26165f04c47557d7373709c8d7e7b4fd7465d/Peter.py\", line 23, in _split_generators\r\n    data_files = dl_manager.download_and_extract(_URLS)\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/download/download_manager.py\", line 431, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/download/download_manager.py\", line 403, in extract\r\n    extracted_paths = map_nested(\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 393, in map_nested\r\n    mapped = [\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 394, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True, None))\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 330, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/utils/file_utils.py\", line 213, in cached_path\r\n    output_path = ExtractManager(cache_dir=download_config.cache_dir).extract(\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/utils/extract.py\", line 46, in extract\r\n    self.extractor.extract(input_path, output_path, extractor_format)\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/utils/extract.py\", line 263, in extract\r\n    with FileLock(lock_path):\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 399, in __init__\r\n    max_filename_length = os.statvfs(os.path.dirname(lock_file)).f_namemax\r\nFileNotFoundError: [Errno 2] No such file or directory: ''\r\nException ignored in: <function BaseFileLock.__del__ at 0x11caeec10>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 328, in __del__\r\n    self.release(force=True)\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 303, in release\r\n    with self._thread_lock:\r\nAttributeError: 'UnixFileLock' object has no attribute '_thread_lock'\r\nExtracting data files:   0%|                                                                                                            | 0/4 [00:00<?, ?it/s]\r\n```\r\n\r\nCan you help me please?\r\n\r\n\r\n## Environment info\r\n- `datasets` version: 2.4.0\r\n- Platform: macOS-12.5.1-x86_64-i386-64bit\r\n- Python version: 3.9.5\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.2.4\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4982/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4961", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4961/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4961/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4961/events", "html_url": "https://github.com/huggingface/datasets/issues/4961", "id": 1368124033, "node_id": "I_kwDODunzps5Ri-qB", "number": 4961, "title": "fsspec 2022.8.2 breaks xopen in streaming mode", "user": {"login": "DCNemesis", "id": 3616964, "node_id": "MDQ6VXNlcjM2MTY5NjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/3616964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DCNemesis", "html_url": "https://github.com/DCNemesis", "followers_url": "https://api.github.com/users/DCNemesis/followers", "following_url": "https://api.github.com/users/DCNemesis/following{/other_user}", "gists_url": "https://api.github.com/users/DCNemesis/gists{/gist_id}", "starred_url": "https://api.github.com/users/DCNemesis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DCNemesis/subscriptions", "organizations_url": "https://api.github.com/users/DCNemesis/orgs", "repos_url": "https://api.github.com/users/DCNemesis/repos", "events_url": "https://api.github.com/users/DCNemesis/events{/privacy}", "received_events_url": "https://api.github.com/users/DCNemesis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-09-09T17:26:55Z", "updated_at": "2022-09-12T17:45:50Z", "closed_at": "2022-09-12T14:32:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen fsspec 2022.8.2 is installed in your environment, xopen will prematurely close files, making streaming mode inoperable.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\nimport datasets\r\n\r\ndata = datasets.load_dataset('MLCommons/ml_spoken_words', 'id_wav', split='train', streaming=True)\r\n\r\n```\r\n\r\n## Expected results\r\nDataset should load as iterator.\r\n\r\n## Actual results\r\n```\r\n[/usr/local/lib/python3.7/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1737     # Return iterable dataset in case of streaming\r\n   1738     if streaming:\r\n-> 1739         return builder_instance.as_streaming_dataset(split=split)\r\n   1740 \r\n   1741     # Some datasets are already processed on the HF google storage\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/builder.py](https://localhost:8080/#) in as_streaming_dataset(self, split, base_path)\r\n   1023         )\r\n   1024         self._check_manual_download(dl_manager)\r\n-> 1025         splits_generators = {sg.name: sg for sg in self._split_generators(dl_manager)}\r\n   1026         # By default, return all splits\r\n   1027         if split is None:\r\n\r\n[~/.cache/huggingface/modules/datasets_modules/datasets/MLCommons--ml_spoken_words/321ea853cf0a05abb7a2d7efea900692a3d8622af65a2f3ce98adb7800a5d57b/ml_spoken_words.py](https://localhost:8080/#) in _split_generators(self, dl_manager)\r\n    182                 name=datasets.Split.TRAIN,\r\n    183                 gen_kwargs={\r\n--> 184                     \"audio_archives\": [download_audio(split=\"train\", lang=lang) for lang in self.config.languages],\r\n    185                     \"local_audio_archives_paths\": [download_extract_audio(split=\"train\", lang=lang) for lang in\r\n    186                                                    self.config.languages] if not dl_manager.is_streaming else None,\r\n\r\n[~/.cache/huggingface/modules/datasets_modules/datasets/MLCommons--ml_spoken_words/321ea853cf0a05abb7a2d7efea900692a3d8622af65a2f3ce98adb7800a5d57b/ml_spoken_words.py](https://localhost:8080/#) in <listcomp>(.0)\r\n    182                 name=datasets.Split.TRAIN,\r\n    183                 gen_kwargs={\r\n--> 184                     \"audio_archives\": [download_audio(split=\"train\", lang=lang) for lang in self.config.languages],\r\n    185                     \"local_audio_archives_paths\": [download_extract_audio(split=\"train\", lang=lang) for lang in\r\n    186                                                    self.config.languages] if not dl_manager.is_streaming else None,\r\n\r\n[~/.cache/huggingface/modules/datasets_modules/datasets/MLCommons--ml_spoken_words/321ea853cf0a05abb7a2d7efea900692a3d8622af65a2f3ce98adb7800a5d57b/ml_spoken_words.py](https://localhost:8080/#) in _download_audio_archives(dl_manager, lang, format, split)\r\n    267 # for streaming case\r\n    268 def _download_audio_archives(dl_manager, lang, format, split):\r\n--> 269     archives_paths = _download_audio_archives_paths(dl_manager, lang, format, split)\r\n    270     return [dl_manager.iter_archive(archive_path) for archive_path in archives_paths]\r\n\r\n[~/.cache/huggingface/modules/datasets_modules/datasets/MLCommons--ml_spoken_words/321ea853cf0a05abb7a2d7efea900692a3d8622af65a2f3ce98adb7800a5d57b/ml_spoken_words.py](https://localhost:8080/#) in _download_audio_archives_paths(dl_manager, lang, format, split)\r\n    251     n_files_path = dl_manager.download(n_files_url)\r\n    252 \r\n--> 253     with open(n_files_path, \"r\", encoding=\"utf-8\") as file:\r\n    254         n_files = int(file.read().strip())  # the file contains a number of archives\r\n    255 \r\n\r\nValueError: I/O operation on closed file.\r\n```\r\n\r\n\r\n## Environment info\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4961/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4958", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4958/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4958/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4958/events", "html_url": "https://github.com/huggingface/datasets/issues/4958", "id": 1367695376, "node_id": "I_kwDODunzps5RhWAQ", "number": 4958, "title": "ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/2.4.0/datasets/jsonl/jsonl.py", "user": {"login": "hasakikiki", "id": 66322047, "node_id": "MDQ6VXNlcjY2MzIyMDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/66322047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hasakikiki", "html_url": "https://github.com/hasakikiki", "followers_url": "https://api.github.com/users/hasakikiki/followers", "following_url": "https://api.github.com/users/hasakikiki/following{/other_user}", "gists_url": "https://api.github.com/users/hasakikiki/gists{/gist_id}", "starred_url": "https://api.github.com/users/hasakikiki/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hasakikiki/subscriptions", "organizations_url": "https://api.github.com/users/hasakikiki/orgs", "repos_url": "https://api.github.com/users/hasakikiki/repos", "events_url": "https://api.github.com/users/hasakikiki/events{/privacy}", "received_events_url": "https://api.github.com/users/hasakikiki/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-09-09T11:29:55Z", "updated_at": "2022-09-09T11:38:44Z", "closed_at": "2022-09-09T11:38:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nWhen I use load_dataset from local jsonl files, below error happens, and I type the link into the browser prompting me `404: Not Found`. I download the other `.py` files using the same method and it works. It seems that the server is missing the appropriate file, or it is a problem with the code version.\r\n\r\n```\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/2.3.0/datasets/jsonl/jsonl.py (ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /huggingface/datasets/2.3.0/datasets/jsonl/jsonl.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x2b08342004c0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\")))\r\n\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4958/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4958/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4953", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4953/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4953/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4953/events", "html_url": "https://github.com/huggingface/datasets/issues/4953", "id": 1366356514, "node_id": "I_kwDODunzps5RcPIi", "number": 4953, "title": "CI test of TensorFlow is failing", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-09-08T13:39:29Z", "updated_at": "2022-09-08T15:14:45Z", "closed_at": "2022-09-08T15:14:45Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nThe following CI test fails: https://github.com/huggingface/datasets/runs/8246722693?check_suite_focus=true\r\n```\r\nFAILED tests/test_py_utils.py::TempSeedTest::test_tensorflow - AssertionError:\r\n```\r\n\r\nDetails:\r\n```\r\n_________________________ TempSeedTest.test_tensorflow _________________________\r\n[gw0] linux -- Python 3.7.13 /opt/hostedtoolcache/Python/3.7.13/x64/bin/python\r\n\r\nself = <tests.test_py_utils.TempSeedTest testMethod=test_tensorflow>\r\n\r\n    @require_tf\r\n    def test_tensorflow(self):\r\n        import tensorflow as tf\r\n        from tensorflow.keras import layers\r\n    \r\n        def gen_random_output():\r\n            model = layers.Dense(2)\r\n            x = tf.random.uniform((1, 3))\r\n            return model(x).numpy()\r\n    \r\n        with temp_seed(42, set_tensorflow=True):\r\n            out1 = gen_random_output()\r\n        with temp_seed(42, set_tensorflow=True):\r\n            out2 = gen_random_output()\r\n        out3 = gen_random_output()\r\n    \r\n>       np.testing.assert_equal(out1, out2)\r\nE       AssertionError: \r\nE       Arrays are not equal\r\nE       \r\nE       Mismatched elements: 2 / 2 (100%)\r\nE       Max absolute difference: 0.84619296\r\nE       Max relative difference: 16.083529\r\nE        x: array([[-0.793581,  0.333286]], dtype=float32)\r\nE        y: array([[0.052612, 0.539708]], dtype=float32)\r\n\r\ntests/test_py_utils.py:149: AssertionError\r\n```\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4953/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4953/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4945", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4945/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4945/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4945/events", "html_url": "https://github.com/huggingface/datasets/issues/4945", "id": 1364691096, "node_id": "I_kwDODunzps5RV4iY", "number": 4945, "title": "Push to hub can push splits that do not respect the regex", "user": {"login": "LysandreJik", "id": 30755778, "node_id": "MDQ6VXNlcjMwNzU1Nzc4", "avatar_url": "https://avatars.githubusercontent.com/u/30755778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LysandreJik", "html_url": "https://github.com/LysandreJik", "followers_url": "https://api.github.com/users/LysandreJik/followers", "following_url": "https://api.github.com/users/LysandreJik/following{/other_user}", "gists_url": "https://api.github.com/users/LysandreJik/gists{/gist_id}", "starred_url": "https://api.github.com/users/LysandreJik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LysandreJik/subscriptions", "organizations_url": "https://api.github.com/users/LysandreJik/orgs", "repos_url": "https://api.github.com/users/LysandreJik/repos", "events_url": "https://api.github.com/users/LysandreJik/events{/privacy}", "received_events_url": "https://api.github.com/users/LysandreJik/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-09-07T13:45:17Z", "updated_at": "2022-09-13T10:16:35Z", "closed_at": "2022-09-13T10:16:35Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThe `push_to_hub` method can push splits that do not respect the regex check that is used for downloads. Therefore, splits may be pushed but never re-used, which can be painful if the split was done after runtime preprocessing.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> from datasets import Dataset, DatasetDict, load_dataset\r\n\r\n>>> d = Dataset.from_dict({'x': [1,2,3], 'y': [1,2,3]})\r\n>>> di = DatasetDict()\r\n>>> di['identifier-with-column'] = d\r\n\r\n>>> di.push_to_hub('open-source-metrics/test')\r\nPushing split identifier-with-column to the Hub.\r\nPushing dataset shards to the dataset hub: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:04<00:00,  4.40s/it]\r\n```\r\n\r\nLoading it afterwards:\r\n```python\r\n>>> load_dataset('open-source-metrics/test')\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 610/610 [00:00<00:00, 432kB/s]\r\nUsing custom data configuration open-source-metrics--test-28b63ec7cde80488\r\nDownloading and preparing dataset None/None (download: 950 bytes, generated: 48 bytes, post-processed: Unknown size, total: 998 bytes) to /home/lysandre/.cache/huggingface/datasets/open-source-metrics___parquet/open-source-metrics--test-28b63ec7cde80488/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\r\nDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 950/950 [00:00<00:00, 1.01MB/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.48s/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2291.97it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/lysandre/.pyenv/versions/3.10.6/lib/python3.10/code.py\", line 90, in runcode\r\n    exec(code, self.locals)\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/home/lysandre/Workspaces/python/Metrics/GitHub-Metrics/.env/lib/python3.10/site-packages/datasets/load.py\", line 1746, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/lysandre/Workspaces/python/Metrics/GitHub-Metrics/.env/lib/python3.10/site-packages/datasets/builder.py\", line 704, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/lysandre/Workspaces/python/Metrics/GitHub-Metrics/.env/lib/python3.10/site-packages/datasets/builder.py\", line 771, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/lysandre/Workspaces/python/Metrics/GitHub-Metrics/.env/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 48, in _split_generators\r\n    splits.append(datasets.SplitGenerator(name=split_name, gen_kwargs={\"files\": files}))\r\n  File \"<string>\", line 5, in __init__\r\n  File \"/home/lysandre/Workspaces/python/Metrics/GitHub-Metrics/.env/lib/python3.10/site-packages/datasets/splits.py\", line 599, in __post_init__\r\n    NamedSplit(self.name)  # check that it's a valid split name\r\n  File \"/home/lysandre/Workspaces/python/Metrics/GitHub-Metrics/.env/lib/python3.10/site-packages/datasets/splits.py\", line 346, in __init__\r\n    raise ValueError(f\"Split name should match '{_split_re}' but got '{split_name}'.\")\r\nValueError: Split name should match '^\\w+(\\.\\w+)*$' but got 'identifier-with-column'.\r\n```\r\n\r\n## Expected results\r\n\r\nI would expect `push_to_hub` to stop me in my tracks if trying to upload a split that will not be working afterwards.\r\n\r\n## Actual results\r\n\r\nSee above\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-5.15.64-1-lts-x86_64-with-glibc2.36\r\n- Python version: 3.10.6\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.4\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4945/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4945/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4944", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4944/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4944/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4944/events", "html_url": "https://github.com/huggingface/datasets/issues/4944", "id": 1364313569, "node_id": "I_kwDODunzps5RUcXh", "number": 4944, "title": "larger dataset, larger GPU memory in the training phase? Is that correct?", "user": {"login": "debby1103", "id": 38886373, "node_id": "MDQ6VXNlcjM4ODg2Mzcz", "avatar_url": "https://avatars.githubusercontent.com/u/38886373?v=4", "gravatar_id": "", "url": "https://api.github.com/users/debby1103", "html_url": "https://github.com/debby1103", "followers_url": "https://api.github.com/users/debby1103/followers", "following_url": "https://api.github.com/users/debby1103/following{/other_user}", "gists_url": "https://api.github.com/users/debby1103/gists{/gist_id}", "starred_url": "https://api.github.com/users/debby1103/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/debby1103/subscriptions", "organizations_url": "https://api.github.com/users/debby1103/orgs", "repos_url": "https://api.github.com/users/debby1103/repos", "events_url": "https://api.github.com/users/debby1103/events{/privacy}", "received_events_url": "https://api.github.com/users/debby1103/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-09-07T08:46:30Z", "updated_at": "2022-09-07T12:34:58Z", "closed_at": "2022-09-07T12:34:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "    from datasets import set_caching_enabled\r\n    set_caching_enabled(False)\r\n    for ds_name in [\"squad\",\"newsqa\",\"nqopen\",\"narrativeqa\"]:\r\n            train_ds = load_from_disk(\"../../../dall/downstream/processedproqa/{}-train.hf\".format(ds_name))\r\n\r\n        break\r\n    train_ds = concatenate_datasets([train_ds,train_ds,train_ds,train_ds]) #operation 1\r\n\r\n\r\n   trainer = QuestionAnsweringTrainer( #huggingface trainer\r\n        model=model,\r\n        args=training_args,\r\n        train_dataset=train_ds,\r\n        eval_dataset= None,\r\n        eval_examples=None,\r\n        answer_column_name=answer_column,\r\n        dataset_name=\"squad\",\r\n        tokenizer=tokenizer,\r\n        data_collator=data_collator,\r\n        compute_metrics=compute_metrics if training_args.predict_with_generate else None,\r\n    )\r\n\r\nwith operation 1, the GPU memory increases from 16G to 23G", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4944/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4942", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4942/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4942/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4942/events", "html_url": "https://github.com/huggingface/datasets/issues/4942", "id": 1363869421, "node_id": "I_kwDODunzps5RSv7t", "number": 4942, "title": "Trec Dataset has incorrect labels", "user": {"login": "wmpauli", "id": 6539145, "node_id": "MDQ6VXNlcjY1MzkxNDU=", "avatar_url": "https://avatars.githubusercontent.com/u/6539145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wmpauli", "html_url": "https://github.com/wmpauli", "followers_url": "https://api.github.com/users/wmpauli/followers", "following_url": "https://api.github.com/users/wmpauli/following{/other_user}", "gists_url": "https://api.github.com/users/wmpauli/gists{/gist_id}", "starred_url": "https://api.github.com/users/wmpauli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wmpauli/subscriptions", "organizations_url": "https://api.github.com/users/wmpauli/orgs", "repos_url": "https://api.github.com/users/wmpauli/repos", "events_url": "https://api.github.com/users/wmpauli/events{/privacy}", "received_events_url": "https://api.github.com/users/wmpauli/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-09-06T22:13:40Z", "updated_at": "2022-09-08T11:12:03Z", "closed_at": "2022-09-08T11:12:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nBoth coarse and fine labels seem to be out of line.\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = \"trec\"\r\nraw_datasets = load_dataset(dataset)\r\ndf = pd.DataFrame(raw_datasets[\"test\"])\r\ndf.head()\r\n```\r\n\r\n## Expected results\r\ntext (string) | coarse_label (class label) | fine_label (class label)\r\n-- | -- | --\r\nHow far is it from Denver to Aspen ? | 5  \t(NUM) | 40  \t(NUM:dist)\r\nWhat county is Modesto , California in ? | 4  \t(LOC) | 32  \t(LOC:city)\r\nWho was Galileo ? | 3  \t(HUM) | 31  \t(HUM:desc)\r\nWhat is an atom ? | 2  \t(DESC) | 24  \t(DESC:def)\r\nWhen did Hawaii become a state ? | 5  \t(NUM) | 39  \t(NUM:date)\r\n\r\n## Actual results\r\n  index | label-coarse  |label-fine                  |                    text\r\n-- |-- | -- | --\r\n0    |         4     |     40     | How far is it from Denver to Aspen ?\r\n1      |       5        |  21  | What county is Modesto , California in ?\r\n2        |     3        |  12        |                 Who was Galileo ?\r\n3         |    0       |    7         |                What is an atom ?\r\n4         |    4       |    8       |   When did Hawaii become a state ?\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-5.4.0-1086-azure-x86_64-with-glibc2.27\r\n- Python version: 3.9.13\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.3\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4942/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4933", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4933/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4933/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4933/events", "html_url": "https://github.com/huggingface/datasets/issues/4933", "id": 1363013023, "node_id": "I_kwDODunzps5RPe2f", "number": 4933, "title": "Dataset/DatasetDict.filter() cannot have `batched=True` due to `mask` (numpy array?) being non-iterable.", "user": {"login": "tianjianjiang", "id": 4812544, "node_id": "MDQ6VXNlcjQ4MTI1NDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/4812544?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tianjianjiang", "html_url": "https://github.com/tianjianjiang", "followers_url": "https://api.github.com/users/tianjianjiang/followers", "following_url": "https://api.github.com/users/tianjianjiang/following{/other_user}", "gists_url": "https://api.github.com/users/tianjianjiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/tianjianjiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tianjianjiang/subscriptions", "organizations_url": "https://api.github.com/users/tianjianjiang/orgs", "repos_url": "https://api.github.com/users/tianjianjiang/repos", "events_url": "https://api.github.com/users/tianjianjiang/events{/privacy}", "received_events_url": "https://api.github.com/users/tianjianjiang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-09-06T09:47:48Z", "updated_at": "2022-09-06T11:44:27Z", "closed_at": "2022-09-06T11:44:27Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n`Dataset/DatasetDict.filter()` cannot have `batched=True` due to `mask` (numpy array?) being non-iterable.\r\n\r\n## Steps to reproduce the bug\r\n(In a python 3.7.12 env, I've tried 2.4.0 and 2.3.2 with both `pyarraw==9.0.0` and `pyarrow==8.0.0`.)\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n\r\nds_mc4_ja = load_dataset(\"mc4\", \"ja\")  # This will take 6+ hours... perhaps test it with a toy dataset instead?\r\nds_mc4_ja_2020 = ds_mc4_ja.filter(\r\n  lambda example: example[\"timestamp\"][:4] == \"2020\",\r\n  batched=True,\r\n)\r\n```\r\n\r\n## Expected results\r\nNo error\r\n\r\n## Actual results\r\n```python\r\n---------------------------------------------------------------------------\r\nRemoteTraceback                           Traceback (most recent call last)\r\nRemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/multiprocess/pool.py\", line 121, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 524, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py\", line 480, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2779, in _map_single\r\n    offset=offset,\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2655, in apply_function_on_filtered_inputs\r\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2347, in decorated\r\n    result = f(decorated_item, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 4946, in get_indices_from_mask_function\r\n    indices_array = [i for i, to_keep in zip(indices, mask) if to_keep]\r\nTypeError: zip argument #2 must support iteration\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/tmp/ipykernel_51348/2345782281.py in <module>\r\n      7     batched=True,\r\n      8     # batch_size=10_000,\r\n----> 9     num_proc=111,\r\n     10 )\r\n     11 # ds_mc4_ja_clean_2020 = ds_mc4_ja.filter(\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/dataset_dict.py in filter(self, function, with_indices, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, fn_kwargs, num_proc, desc)\r\n    878                     desc=desc,\r\n    879                 )\r\n--> 880                 for k, dataset in self.items()\r\n    881             }\r\n    882         )\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/dataset_dict.py in <dictcomp>(.0)\r\n    878                     desc=desc,\r\n    879                 )\r\n--> 880                 for k, dataset in self.items()\r\n    881             }\r\n    882         )\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    522         }\r\n    523         # apply actual function\r\n--> 524         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    525         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    526         # re-apply format to the output\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    478             # Call actual function\r\n    479 \r\n--> 480             out = func(self, *args, **kwargs)\r\n    481 \r\n    482             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py in filter(self, function, with_indices, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   2920             new_fingerprint=new_fingerprint,\r\n   2921             input_columns=input_columns,\r\n-> 2922             desc=desc,\r\n   2923         )\r\n   2924         new_dataset = copy.deepcopy(self)\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   2498 \r\n   2499                     for index, async_result in results.items():\r\n-> 2500                         transformed_shards[index] = async_result.get()\r\n   2501 \r\n   2502             assert (\r\n\r\n/opt/conda/lib/python3.7/site-packages/multiprocess/pool.py in get(self, timeout)\r\n    655             return self._value\r\n    656         else:\r\n--> 657             raise self._value\r\n    658 \r\n    659     def _set(self, i, obj):\r\n\r\nTypeError: zip argument #2 must support iteration\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-4.19.0-21-cloud-amd64-x86_64-with-debian-10.12\r\n- Python version: 3.7.12\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.3.5\r\n\r\n(I've tried 2.4.0 and 2.3.2 with both `pyarraw==9.0.0` and `pyarrow==8.0.0`.)", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4933/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4933/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4924", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4924/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4924/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4924/events", "html_url": "https://github.com/huggingface/datasets/issues/4924", "id": 1358611513, "node_id": "I_kwDODunzps5Q-sQ5", "number": 4924, "title": "Concatenate_datasets loads everything into RAM", "user": {"login": "louisdeneve", "id": 39416047, "node_id": "MDQ6VXNlcjM5NDE2MDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/39416047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/louisdeneve", "html_url": "https://github.com/louisdeneve", "followers_url": "https://api.github.com/users/louisdeneve/followers", "following_url": "https://api.github.com/users/louisdeneve/following{/other_user}", "gists_url": "https://api.github.com/users/louisdeneve/gists{/gist_id}", "starred_url": "https://api.github.com/users/louisdeneve/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/louisdeneve/subscriptions", "organizations_url": "https://api.github.com/users/louisdeneve/orgs", "repos_url": "https://api.github.com/users/louisdeneve/repos", "events_url": "https://api.github.com/users/louisdeneve/events{/privacy}", "received_events_url": "https://api.github.com/users/louisdeneve/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-09-01T10:25:17Z", "updated_at": "2022-09-01T11:50:54Z", "closed_at": "2022-09-01T11:50:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen loading the datasets seperately and saving them on disk, I want to concatenate them. But `concatenate_datasets` is filling up my RAM and the process gets killed. Is there a way to prevent this from happening or is this intended behaviour? Thanks in advance\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ngcs = gcsfs.GCSFileSystem(project='project')\r\ndatasets = [load_from_disk(f'path/to/slice/of/data/{i}', fs=gcs, keep_in_memory=False) for i in range(10)]\r\n\r\ndataset = concatenate_datasets(datasets)\r\n```\r\n\r\n## Expected results\r\nA concatenated dataset which is stored on my disk.\r\n\r\n## Actual results\r\nConcatenated dataset gets loaded into RAM and overflows it which gets the process killed.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-4.19.0-21-cloud-amd64-x86_64-with-glibc2.10\r\n- Python version: 3.8.13\r\n- PyArrow version: 8.0.1\r\n- Pandas version: 1.4.3", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4924/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4922", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4922/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4922/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4922/events", "html_url": "https://github.com/huggingface/datasets/issues/4922", "id": 1357684018, "node_id": "I_kwDODunzps5Q7J0y", "number": 4922, "title": "I/O error on Google Colab in streaming mode", "user": {"login": "jotterbach", "id": 5595043, "node_id": "MDQ6VXNlcjU1OTUwNDM=", "avatar_url": "https://avatars.githubusercontent.com/u/5595043?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jotterbach", "html_url": "https://github.com/jotterbach", "followers_url": "https://api.github.com/users/jotterbach/followers", "following_url": "https://api.github.com/users/jotterbach/following{/other_user}", "gists_url": "https://api.github.com/users/jotterbach/gists{/gist_id}", "starred_url": "https://api.github.com/users/jotterbach/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jotterbach/subscriptions", "organizations_url": "https://api.github.com/users/jotterbach/orgs", "repos_url": "https://api.github.com/users/jotterbach/repos", "events_url": "https://api.github.com/users/jotterbach/events{/privacy}", "received_events_url": "https://api.github.com/users/jotterbach/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-08-31T18:08:26Z", "updated_at": "2022-08-31T18:15:48Z", "closed_at": "2022-08-31T18:15:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen trying to load a streaming dataset in Google Colab the loading fails with an I/O error\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nfrom datasets import load_dataset\r\nhf_ds = load_dataset(path='wmt19', name='cs-en', streaming=True, split=datasets.Split.VALIDATION)\r\nlist(hf_ds.take(5))\r\n```\r\n\r\n## Expected results\r\nIt should load five data points\r\n\r\n## Actual results\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[<ipython-input-13-7b5b8b1e7e58>](https://localhost:8080/#) in <module>\r\n      2 from datasets import load_dataset\r\n      3 hf_ds = load_dataset(path='wmt19', name='cs-en', streaming=True, split=datasets.Split.VALIDATION)\r\n----> 4 list(hf_ds.take(5))\r\n\r\n6 frames\r\n[/usr/local/lib/python3.7/dist-packages/datasets/iterable_dataset.py](https://localhost:8080/#) in __iter__(self)\r\n    716 \r\n    717     def __iter__(self):\r\n--> 718         for key, example in self._iter():\r\n    719             if self.features:\r\n    720                 # `IterableDataset` automatically fills missing columns with None.\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/iterable_dataset.py](https://localhost:8080/#) in _iter(self)\r\n    706         else:\r\n    707             ex_iterable = self._ex_iterable\r\n--> 708         yield from ex_iterable\r\n    709 \r\n    710     def _iter_shard(self, shard_idx: int):\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/iterable_dataset.py](https://localhost:8080/#) in __iter__(self)\r\n    582 \r\n    583     def __iter__(self):\r\n--> 584         yield from islice(self.ex_iterable, self.n)\r\n    585 \r\n    586     def shuffle_data_sources(self, generator: np.random.Generator) -> \"TakeExamplesIterable\":\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/iterable_dataset.py](https://localhost:8080/#) in __iter__(self)\r\n    110 \r\n    111     def __iter__(self):\r\n--> 112         yield from self.generate_examples_fn(**self.kwargs)\r\n    113 \r\n    114     def shuffle_data_sources(self, generator: np.random.Generator) -> \"ExamplesIterable\":\r\n\r\n[~/.cache/huggingface/modules/datasets_modules/datasets/wmt19/aeadcbe9f1cbf9969e603239d33d3e43670cf250c1158edf74f5f6e74d4f21d0/wmt_utils.py](https://localhost:8080/#) in _generate_examples(self, split_subsets, extraction_map, with_translation)\r\n    845                 raise ValueError(\"Invalid number of files: %d\" % len(files))\r\n    846 \r\n--> 847             for sub_key, ex in sub_generator(*sub_generator_args):\r\n    848                 if not all(ex.values()):\r\n    849                     continue\r\n\r\n[~/.cache/huggingface/modules/datasets_modules/datasets/wmt19/aeadcbe9f1cbf9969e603239d33d3e43670cf250c1158edf74f5f6e74d4f21d0/wmt_utils.py](https://localhost:8080/#) in _parse_parallel_sentences(f1, f2, filename1, filename2)\r\n    923         l2_sentences, l2 = parse_file(f2_i, filename2)\r\n    924 \r\n--> 925         for line_id, (s1, s2) in enumerate(zip(l1_sentences, l2_sentences)):\r\n    926             key = f\"{f_id}/{line_id}\"\r\n    927             yield key, {l1: s1, l2: s2}\r\n\r\n[~/.cache/huggingface/modules/datasets_modules/datasets/wmt19/aeadcbe9f1cbf9969e603239d33d3e43670cf250c1158edf74f5f6e74d4f21d0/wmt_utils.py](https://localhost:8080/#) in gen()\r\n    895 \r\n    896         def gen():\r\n--> 897             with open(path, encoding=\"utf-8\") as f:\r\n    898                 for line in f:\r\n    899                     seg_match = re.match(seg_re, line)\r\n\r\nValueError: I/O operation on closed file.\r\n```\r\n\r\n## Environment info\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 9.0.0. (the same error happened with PyArrow version 6.0.0)\r\n- Pandas version: 1.3.5\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4922/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4922/timeline", "performed_via_github_app": null, "state_reason": "not_planned"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4920", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4920/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4920/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4920/events", "html_url": "https://github.com/huggingface/datasets/issues/4920", "id": 1357564589, "node_id": "I_kwDODunzps5Q6sqt", "number": 4920, "title": "Unable to load local tsv files through load_dataset method", "user": {"login": "DataNoob0723", "id": 44038517, "node_id": "MDQ6VXNlcjQ0MDM4NTE3", "avatar_url": "https://avatars.githubusercontent.com/u/44038517?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DataNoob0723", "html_url": "https://github.com/DataNoob0723", "followers_url": "https://api.github.com/users/DataNoob0723/followers", "following_url": "https://api.github.com/users/DataNoob0723/following{/other_user}", "gists_url": "https://api.github.com/users/DataNoob0723/gists{/gist_id}", "starred_url": "https://api.github.com/users/DataNoob0723/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DataNoob0723/subscriptions", "organizations_url": "https://api.github.com/users/DataNoob0723/orgs", "repos_url": "https://api.github.com/users/DataNoob0723/repos", "events_url": "https://api.github.com/users/DataNoob0723/events{/privacy}", "received_events_url": "https://api.github.com/users/DataNoob0723/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-08-31T16:13:39Z", "updated_at": "2022-09-01T05:31:30Z", "closed_at": "2022-09-01T05:31:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nUnable to load local tsv files through load_dataset method.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\ndata_files = {\r\n    'train': 'train.tsv',\r\n    'test': 'test.tsv'\r\n}\r\nraw_datasets = load_dataset('tsv', data_files=data_files)\r\n\r\n## Expected results\r\nI am pretty sure the data files exist in the current directory. The above code should load them as Datasets, but threw exceptions.\r\n\r\n## Actual results\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n[<ipython-input-9-24207899c1af>](https://localhost:8080/#) in <module>\r\n----> 1 raw_datasets = load_dataset('tsv', data_files='train.tsv')\r\n\r\n2 frames\r\n[/usr/local/lib/python3.7/dist-packages/datasets/load.py](https://localhost:8080/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1244                         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1245                         f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n-> 1246                     ) from None\r\n   1247                 raise e1 from None\r\n   1248     else:\r\n\r\nFileNotFoundError: Couldn't find a dataset script at /content/tsv/tsv.py or any data file in the same directory. Couldn't find 'tsv' on the Hugging Face Hub either: FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/main/datasets/tsv/tsv.py\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4920/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4916", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4916/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4916/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4916/events", "html_url": "https://github.com/huggingface/datasets/issues/4916", "id": 1357076940, "node_id": "I_kwDODunzps5Q41nM", "number": 4916, "title": "Apache Beam unable to write the downloaded wikipedia dataset", "user": {"login": "Shilpac20", "id": 71849081, "node_id": "MDQ6VXNlcjcxODQ5MDgx", "avatar_url": "https://avatars.githubusercontent.com/u/71849081?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Shilpac20", "html_url": "https://github.com/Shilpac20", "followers_url": "https://api.github.com/users/Shilpac20/followers", "following_url": "https://api.github.com/users/Shilpac20/following{/other_user}", "gists_url": "https://api.github.com/users/Shilpac20/gists{/gist_id}", "starred_url": "https://api.github.com/users/Shilpac20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Shilpac20/subscriptions", "organizations_url": "https://api.github.com/users/Shilpac20/orgs", "repos_url": "https://api.github.com/users/Shilpac20/repos", "events_url": "https://api.github.com/users/Shilpac20/events{/privacy}", "received_events_url": "https://api.github.com/users/Shilpac20/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-08-31T09:39:25Z", "updated_at": "2022-08-31T10:53:19Z", "closed_at": "2022-08-31T10:53:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nHi, I am currently trying to download wikipedia dataset using\r\nload_dataset(\"wikipedia\", language=\"aa\", date=\"20220401\", split=\"train\",beam_runner='DirectRunner'). However, I end up in getting filenotfound error. I get this error for any language I try to download. It downloads the file but while saving it in hugging face cache it fails to write. This happens for any available date of any language in wikipedia dump. I had raised another issue earlier #4915 but probably was not that clear and the solution provider misunderstood my problem. Hence raising one more issue. Any help is appreciated.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"wikipedia\", language=\"aa\", date=\"20220401\", split=\"train\",beam_runner='DirectRunner')\r\n```\r\n\r\n## Expected results\r\nto load the dataset\r\n\r\n## Actual results\r\nI am pasting the error trace here:\r\nDownloading builder script: 35.9kB [00:00, ?B/s]\r\nDownloading metadata: 30.4kB [00:00, 1.94MB/s]\r\nUsing custom data configuration 20220401.aa-date=20220401,language=aa\r\nDownloading and preparing dataset wikipedia/20220401.aa to C:\\Users\\Shilpa.cache\\huggingface\\datasets\\wikipedia\\20220401.aa-date=20220401,language=aa\\2.0.0\\aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.1k/11.1k [00:00<00:00, 712kB/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00, 2.82s/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<?, ?it/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35.6k/35.6k [00:00<00:00, 84.3kB/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00, 2.93s/it]\r\nTraceback (most recent call last):\r\nFile \"apache_beam\\runners\\common.py\", line 1417, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 837, in apache_beam.runners.common.PerWindowInvoker.invoke_process\r\nFile \"apache_beam\\runners\\common.py\", line 981, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\r\nFile \"apache_beam\\runners\\common.py\", line 1571, in apache_beam.runners.common._OutputHandler.handle_process_outputs\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\iobase.py\", line 1193, in process\r\nself.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\options\\value_provider.py\", line 193, in _f\r\nreturn fnc(self, *args, **kwargs)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\filebasedsink.py\", line 202, in open_writer\r\nreturn FileBasedSinkWriter(self, writer_path)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\filebasedsink.py\", line 419, in init\r\nself.temp_handle = self.sink.open(temp_shard_path)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\parquetio.py\", line 553, in open\r\nself._file_handle = super().open(temp_path)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\options\\value_provider.py\", line 193, in _f\r\nreturn fnc(self, *args, **kwargs)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\filebasedsink.py\", line 139, in open\r\ntemp_path, self.mime_type, self.compression_type)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\filesystems.py\", line 224, in create\r\nreturn filesystem.create(path, mime_type, compression_type)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\localfilesystem.py\", line 163, in create\r\nreturn self._path_open(path, 'wb', mime_type, compression_type)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\localfilesystem.py\", line 140, in _path_open\r\nraw_file = io.open(path, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\Shilpa\\.cache\\huggingface\\datasets\\wikipedia\\20220401.aa-date=20220401,language=aa\\2.0.0\\aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559.incomplete\\beam-temp-wikipedia-train-880233e8287e11edaf9d3ca067f2714e\\20a05238-6106-4420-a713-4eca6dd5959a.wikipedia-train'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"G:/abc/temp.py\", line 32, in\r\nbeam_runner='DirectRunner')\r\nFile \"G:\\Python3.7\\lib\\site-packages\\datasets\\load.py\", line 1751, in load_dataset\r\nuse_auth_token=use_auth_token,\r\nFile \"G:\\Python3.7\\lib\\site-packages\\datasets\\builder.py\", line 705, in download_and_prepare\r\ndl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\nFile \"G:\\Python3.7\\lib\\site-packages\\datasets\\builder.py\", line 1394, in _download_and_prepare\r\npipeline_results = pipeline.run()\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\pipeline.py\", line 574, in run\r\nreturn self.runner.run_pipeline(self, self._options)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\direct\\direct_runner.py\", line 131, in run_pipeline\r\nreturn runner.run_pipeline(pipeline, options)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\", line 201, in run_pipeline\r\noptions)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\", line 212, in run_via_runner_api\r\nreturn self.run_stages(stage_context, stages)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\", line 443, in run_stages\r\nrunner_execution_context, bundle_context_manager, bundle_input)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\", line 776, in _execute_bundle\r\nbundle_manager))\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\", line 1000, in _run_bundle\r\ndata_input, data_output, input_timers, expected_timer_output)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\", line 1309, in process_bundle\r\nresult_future = self._worker_handler.control_conn.push(process_bundle_req)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\worker_handlers.py\", line 380, in push\r\nresponse = self.worker.do_instruction(request)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\worker\\sdk_worker.py\", line 598, in do_instruction\r\ngetattr(request, request_type), request.instruction_id)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\worker\\sdk_worker.py\", line 635, in process_bundle\r\nbundle_processor.process_bundle(instruction_id))\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\worker\\bundle_processor.py\", line 1004, in process_bundle\r\nelement.data)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\runners\\worker\\bundle_processor.py\", line 227, in process_encoded\r\nself.output(decoded_value)\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 526, in apache_beam.runners.worker.operations.Operation.output\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 528, in apache_beam.runners.worker.operations.Operation.output\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 237, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 907, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 908, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\common.py\", line 1419, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 1491, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\nFile \"apache_beam\\runners\\common.py\", line 1417, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 623, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\nFile \"apache_beam\\runners\\common.py\", line 1581, in apache_beam.runners.common._OutputHandler.handle_process_outputs\r\nFile \"apache_beam\\runners\\common.py\", line 1694, in apache_beam.runners.common._OutputHandler._write_value_to_tag\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 907, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 908, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\common.py\", line 1419, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 1491, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\nFile \"apache_beam\\runners\\common.py\", line 1417, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 623, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\nFile \"apache_beam\\runners\\common.py\", line 1581, in apache_beam.runners.common._OutputHandler.handle_process_outputs\r\nFile \"apache_beam\\runners\\common.py\", line 1694, in apache_beam.runners.common._OutputHandler._write_value_to_tag\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 907, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 908, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\common.py\", line 1419, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 1491, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\nFile \"apache_beam\\runners\\common.py\", line 1417, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 837, in apache_beam.runners.common.PerWindowInvoker.invoke_process\r\nFile \"apache_beam\\runners\\common.py\", line 981, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\r\nFile \"apache_beam\\runners\\common.py\", line 1581, in apache_beam.runners.common._OutputHandler.handle_process_outputs\r\nFile \"apache_beam\\runners\\common.py\", line 1694, in apache_beam.runners.common._OutputHandler._write_value_to_tag\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 907, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 908, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\common.py\", line 1419, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 1491, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\nFile \"apache_beam\\runners\\common.py\", line 1417, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 623, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\nFile \"apache_beam\\runners\\common.py\", line 1581, in apache_beam.runners.common._OutputHandler.handle_process_outputs\r\nFile \"apache_beam\\runners\\common.py\", line 1694, in apache_beam.runners.common._OutputHandler._write_value_to_tag\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 324, in apache_beam.runners.worker.operations.GeneralPurposeConsumerSet.receive\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 905, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 907, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 908, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\common.py\", line 1419, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 1491, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\nFile \"apache_beam\\runners\\common.py\", line 1417, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 623, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\nFile \"apache_beam\\runners\\common.py\", line 1581, in apache_beam.runners.common._OutputHandler.handle_process_outputs\r\nFile \"apache_beam\\runners\\common.py\", line 1694, in apache_beam.runners.common._OutputHandler._write_value_to_tag\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 907, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 908, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\common.py\", line 1419, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 1491, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\nFile \"apache_beam\\runners\\common.py\", line 1417, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 837, in apache_beam.runners.common.PerWindowInvoker.invoke_process\r\nFile \"apache_beam\\runners\\common.py\", line 981, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\r\nFile \"apache_beam\\runners\\common.py\", line 1581, in apache_beam.runners.common._OutputHandler.handle_process_outputs\r\nFile \"apache_beam\\runners\\common.py\", line 1694, in apache_beam.runners.common._OutputHandler._write_value_to_tag\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 907, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\worker\\operations.py\", line 908, in apache_beam.runners.worker.operations.DoOperation.process\r\nFile \"apache_beam\\runners\\common.py\", line 1419, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 1507, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\nFile \"apache_beam\\runners\\common.py\", line 1417, in apache_beam.runners.common.DoFnRunner.process\r\nFile \"apache_beam\\runners\\common.py\", line 837, in apache_beam.runners.common.PerWindowInvoker.invoke_process\r\nFile \"apache_beam\\runners\\common.py\", line 981, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\r\nFile \"apache_beam\\runners\\common.py\", line 1571, in apache_beam.runners.common._OutputHandler.handle_process_outputs\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\iobase.py\", line 1193, in process\r\nself.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\options\\value_provider.py\", line 193, in _f\r\nreturn fnc(self, *args, **kwargs)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\filebasedsink.py\", line 202, in open_writer\r\nreturn FileBasedSinkWriter(self, writer_path)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\filebasedsink.py\", line 419, in init\r\nself.temp_handle = self.sink.open(temp_shard_path)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\parquetio.py\", line 553, in open\r\nself._file_handle = super().open(temp_path)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\options\\value_provider.py\", line 193, in _f\r\nreturn fnc(self, *args, **kwargs)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\filebasedsink.py\", line 139, in open\r\ntemp_path, self.mime_type, self.compression_type)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\filesystems.py\", line 224, in create\r\nreturn filesystem.create(path, mime_type, compression_type)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\localfilesystem.py\", line 163, in create\r\nreturn self._path_open(path, 'wb', mime_type, compression_type)\r\nFile \"G:\\Python3.7\\lib\\site-packages\\apache_beam\\io\\localfilesystem.py\", line 140, in _path_open\r\nraw_file = io.open(path, mode)\r\nRuntimeError: FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\Shilpa\\.cache\\huggingface\\datasets\\wikipedia\\20220401.aa-date=20220401,language=aa\\2.0.0\\aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559.incomplete\\beam-temp-wikipedia-train-880233e8287e11edaf9d3ca067f2714e\\20a05238-6106-4420-a713-4eca6dd5959a.wikipedia-train' [while running 'train/Save to parquet/Write/WriteImpl/WriteBundles']\r\n\r\n## Environment info\r\nPython: 3.7.6\r\nWindows 10 Pro\r\ndatasets :2.4.0\r\napache_beam: 2.41.0\r\nmwparserfromhell: 0.6.4", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4916/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4907", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4907/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4907/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4907/events", "html_url": "https://github.com/huggingface/datasets/issues/4907", "id": 1353808348, "node_id": "I_kwDODunzps5QsXnc", "number": 4907, "title": "None Type error for swda datasets", "user": {"login": "hannan72", "id": 8229163, "node_id": "MDQ6VXNlcjgyMjkxNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8229163?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hannan72", "html_url": "https://github.com/hannan72", "followers_url": "https://api.github.com/users/hannan72/followers", "following_url": "https://api.github.com/users/hannan72/following{/other_user}", "gists_url": "https://api.github.com/users/hannan72/gists{/gist_id}", "starred_url": "https://api.github.com/users/hannan72/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hannan72/subscriptions", "organizations_url": "https://api.github.com/users/hannan72/orgs", "repos_url": "https://api.github.com/users/hannan72/repos", "events_url": "https://api.github.com/users/hannan72/events{/privacy}", "received_events_url": "https://api.github.com/users/hannan72/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-08-29T07:05:20Z", "updated_at": "2022-08-30T14:43:41Z", "closed_at": "2022-08-30T14:43:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI got `'NoneType' object is not callable` error while calling the swda datasets.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"swda\")\r\n```\r\n\r\n## Expected results\r\nRun without error\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Python version: 3.8.10\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4907/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4907/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4906", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4906/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4906/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4906/events", "html_url": "https://github.com/huggingface/datasets/issues/4906", "id": 1353223925, "node_id": "I_kwDODunzps5QqI71", "number": 4906, "title": "Can't import datasets AttributeError: partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)", "user": {"login": "OPterminator", "id": 63536981, "node_id": "MDQ6VXNlcjYzNTM2OTgx", "avatar_url": "https://avatars.githubusercontent.com/u/63536981?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OPterminator", "html_url": "https://github.com/OPterminator", "followers_url": "https://api.github.com/users/OPterminator/followers", "following_url": "https://api.github.com/users/OPterminator/following{/other_user}", "gists_url": "https://api.github.com/users/OPterminator/gists{/gist_id}", "starred_url": "https://api.github.com/users/OPterminator/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OPterminator/subscriptions", "organizations_url": "https://api.github.com/users/OPterminator/orgs", "repos_url": "https://api.github.com/users/OPterminator/repos", "events_url": "https://api.github.com/users/OPterminator/events{/privacy}", "received_events_url": "https://api.github.com/users/OPterminator/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-08-28T02:23:24Z", "updated_at": "2023-04-11T16:31:05Z", "closed_at": "2022-10-03T12:22:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\nNot able to import datasets \r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nimport os\r\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\r\nimport numpy as np\r\nimport random\r\nimport sklearn\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nimport sys\r\nimport tensorflow as tf\r\nimport plotly.express as px\r\nimport transformers\r\nimport tokenizers\r\nimport nlp as nlp\r\nimport utils\r\nimport datasets\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\nimport should work normal\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-21-b3b5b0b62103> in <module>\r\n     13 import nlp as nlp\r\n     14 import utils\r\n---> 15 import datasets\r\n\r\n~\\anaconda3\\lib\\site-packages\\datasets\\__init__.py in <module>\r\n     44 from .fingerprint import disable_caching, enable_caching, is_caching_enabled, set_caching_enabled\r\n     45 from .info import DatasetInfo, MetricInfo\r\n---> 46 from .inspect import (\r\n     47     get_dataset_config_info,\r\n     48     get_dataset_config_names,\r\n\r\n~\\anaconda3\\lib\\site-packages\\datasets\\inspect.py in <module>\r\n     28 from .download.streaming_download_manager import StreamingDownloadManager\r\n     29 from .info import DatasetInfo\r\n---> 30 from .load import dataset_module_factory, import_main_class, load_dataset_builder, metric_module_factory\r\n     31 from .utils.file_utils import relative_to_absolute_path\r\n     32 from .utils.logging import get_logger\r\n\r\n~\\anaconda3\\lib\\site-packages\\datasets\\load.py in <module>\r\n     53 from .iterable_dataset import IterableDataset\r\n     54 from .metric import Metric\r\n---> 55 from .packaged_modules import (\r\n     56     _EXTENSION_TO_MODULE,\r\n     57     _MODULE_SUPPORTS_METADATA,\r\n\r\n~\\anaconda3\\lib\\site-packages\\datasets\\packaged_modules\\__init__.py in <module>\r\n      4 from typing import List\r\n      5 \r\n----> 6 from .csv import csv\r\n      7 from .imagefolder import imagefolder\r\n      8 from .json import json\r\n\r\n~\\anaconda3\\lib\\site-packages\\datasets\\packaged_modules\\csv\\csv.py in <module>\r\n     13 \r\n     14 \r\n---> 15 logger = datasets.utils.logging.get_logger(__name__)\r\n     16 \r\n     17 _PANDAS_READ_CSV_NO_DEFAULT_PARAMETERS = [\"names\", \"prefix\"]\r\n\r\nAttributeError: partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.4.0\r\n- Platform: Windows-10-10.0.22000-SP0\r\n- Python version: 3.8.8\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.2.4\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4906/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4906/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4897", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4897/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4897/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4897/events", "html_url": "https://github.com/huggingface/datasets/issues/4897", "id": 1351784727, "node_id": "I_kwDODunzps5QkpkX", "number": 4897, "title": "datasets generate large arrow file", "user": {"login": "osayes", "id": 18533904, "node_id": "MDQ6VXNlcjE4NTMzOTA0", "avatar_url": "https://avatars.githubusercontent.com/u/18533904?v=4", "gravatar_id": "", "url": "https://api.github.com/users/osayes", "html_url": "https://github.com/osayes", "followers_url": "https://api.github.com/users/osayes/followers", "following_url": "https://api.github.com/users/osayes/following{/other_user}", "gists_url": "https://api.github.com/users/osayes/gists{/gist_id}", "starred_url": "https://api.github.com/users/osayes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/osayes/subscriptions", "organizations_url": "https://api.github.com/users/osayes/orgs", "repos_url": "https://api.github.com/users/osayes/repos", "events_url": "https://api.github.com/users/osayes/events{/privacy}", "received_events_url": "https://api.github.com/users/osayes/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-08-26T05:51:16Z", "updated_at": "2022-09-18T05:07:52Z", "closed_at": "2022-09-18T05:07:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Checking the large file in disk, and found the large cache file in the cifar10 data directory:\r\n\r\n![image](https://user-images.githubusercontent.com/18533904/186830449-ba96cdeb-0fe8-4543-994d-2abe7145933f.png)\r\n\r\nAs we know, the size of cifar10 dataset is ~130MB, but the cache file has almost 30GB size, there may be some problems here.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4897/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4897/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4895", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4895/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4895/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4895/events", "html_url": "https://github.com/huggingface/datasets/issues/4895", "id": 1350798527, "node_id": "I_kwDODunzps5Qg4y_", "number": 4895, "title": "load_dataset method returns Unknown split \"validation\" even if this dir exists", "user": {"login": "SamSamhuns", "id": 13418507, "node_id": "MDQ6VXNlcjEzNDE4NTA3", "avatar_url": "https://avatars.githubusercontent.com/u/13418507?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SamSamhuns", "html_url": "https://github.com/SamSamhuns", "followers_url": "https://api.github.com/users/SamSamhuns/followers", "following_url": "https://api.github.com/users/SamSamhuns/following{/other_user}", "gists_url": "https://api.github.com/users/SamSamhuns/gists{/gist_id}", "starred_url": "https://api.github.com/users/SamSamhuns/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SamSamhuns/subscriptions", "organizations_url": "https://api.github.com/users/SamSamhuns/orgs", "repos_url": "https://api.github.com/users/SamSamhuns/repos", "events_url": "https://api.github.com/users/SamSamhuns/events{/privacy}", "received_events_url": "https://api.github.com/users/SamSamhuns/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2022-08-25T12:11:00Z", "updated_at": "2022-10-06T17:49:28Z", "closed_at": "2022-09-29T08:07:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe `datasets.load_dataset` returns a `ValueError: Unknown split \"validation\". Should be one of ['train', 'test'].` when running `load_dataset(local_data_dir_path, split=\"validation\")` even if the `validation` sub-directory exists in the local data path.\r\n\r\nThe data directories are as follows and attached to this issue:\r\n```\r\ntest_data1\r\n              |_ train\r\n                  |_ 1012.png\r\n                  |_ metadata.jsonl\r\n                  ...\r\n              |_ test\r\n                  ...\r\n              |_ validation\r\n                  |_ 234.png\r\n                  |_ metadata.jsonl\r\n                  ...\r\ntest_data2\r\n              |_ train\r\n                  |_ train_1012.png\r\n                  |_ metadata.jsonl\r\n                  ...\r\n              |_ test\r\n                  ...\r\n              |_ validation\r\n                  |_ val_234.png\r\n                  |_ metadata.jsonl\r\n                  ...\r\n```\r\n\r\nThey contain the same image files and `metadata.jsonl` but the images in `test_data2` have the split names prepended i.e.\r\n`train_1012.png, val_234.png` and the images in  `test_data1` do not have the split names prepended to the image names i.e. `1012.png, 234.png`\r\n\r\nI actually saw in another issue `val` was not recognized as a split name but here I would expect the files to take the split from the parent directory name i.e. val should become part of the validation split?\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\ndatasets.logging.set_verbosity_error()\r\nfrom datasets import load_dataset, get_dataset_split_names\r\n\r\n\r\n# the following only finds train, validation and test splits correctly\r\npath = \"./test_data1\"\r\nprint(\"######################\", get_dataset_split_names(path), \"######################\")\r\n\r\ndataset_list = []\r\nfor spt in [\"train\", \"test\", \"validation\"]:\r\n    dataset = load_dataset(path, split=spt)\r\n    dataset_list.append(dataset)\r\n\r\n\r\n# the following only finds train and test splits\r\npath = \"./test_data2\"\r\nprint(\"######################\", get_dataset_split_names(path), \"######################\")\r\n\r\ndataset_list = []\r\nfor spt in [\"train\", \"test\", \"validation\"]:\r\n    dataset = load_dataset(path, split=spt)\r\n    dataset_list.append(dataset)\r\n```\r\n\r\n\r\n## Expected results\r\n```\r\n###################### ['train', 'test', 'validation'] ######################\r\n###################### ['train', 'test', 'validation'] ######################\r\n```\r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_data_loader.py\", line 11, in <module>\r\n\r\n    dataset = load_dataset(path, split=spt)\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/load.py\", line 1758, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/builder.py\", line 893, in as_dataset\r\n    datasets = map_nested(\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 385, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/builder.py\", line 924, in _build_single_dataset\r\n    ds = self._as_dataset(\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/builder.py\", line 993, in _as_dataset\r\n    dataset_kwargs = ArrowReader(self._cache_dir, self.info).read(\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/arrow_reader.py\", line 211, in read\r\n    files = self.get_file_instructions(name, instructions, split_infos)\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/arrow_reader.py\", line 184, in get_file_instructions\r\n    file_instructions = make_file_instructions(\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/arrow_reader.py\", line 107, in make_file_instructions\r\n    absolute_instructions = instruction.to_absolute(name2len)\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/arrow_reader.py\", line 616, in to_absolute\r\n    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/arrow_reader.py\", line 616, in <listcomp>\r\n    return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]\r\n  File \"/home/venv/lib/python3.8/site-packages/datasets/arrow_reader.py\", line 433, in _rel_to_abs_instr\r\n    raise ValueError(f'Unknown split \"{split}\". Should be one of {list(name2len)}.')\r\nValueError: Unknown split \"validation\". Should be one of ['train', 'test'].\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform: Linux Ubuntu 18.04\r\n- Python version: 3.8.12\r\n- PyArrow version: 9.0.0\r\n\r\nData files\r\n\r\n[test_data1.zip](https://github.com/huggingface/datasets/files/9424463/test_data1.zip)\r\n[test_data2.zip](https://github.com/huggingface/datasets/files/9424468/test_data2.zip)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4895/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4895/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4889", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4889/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4889/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4889/events", "html_url": "https://github.com/huggingface/datasets/issues/4889", "id": 1349758525, "node_id": "I_kwDODunzps5Qc649", "number": 4889, "title": "torchaudio 11.0 yields different results than torchaudio 12.1 when loading MP3", "user": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-08-24T16:54:43Z", "updated_at": "2023-03-02T15:33:05Z", "closed_at": "2023-03-02T15:33:04Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen loading Common Voice with torchaudio 0.11.0 the results are different to 0.12.1 which leads to problems in transformers see: https://github.com/huggingface/transformers/pull/18749\r\n\r\n## Steps to reproduce the bug\r\n\r\nIf you run the following code once with `torchaudio==0.11.0+cu102` and `torchaudio==0.12.1+cu102` you can see that the tensors differ. This is a pretty big breaking change and makes some integration tests fail in Transformers.\r\n\r\n```python\r\n#!/usr/bin/env python3\r\nfrom datasets import load_dataset\r\nimport datasets\r\nimport numpy as np\r\nimport torch\r\nimport torchaudio\r\nprint(\"torch vesion\", torch.__version__)\r\nprint(\"torchaudio vesion\", torchaudio.__version__)\r\n\r\nsave_audio = True\r\nload_audios = False\r\n\r\nif save_audio:\r\n    ds = load_dataset(\"common_voice\", \"en\", split=\"train\", streaming=True)\r\n    ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\r\n    ds_iter = iter(ds)\r\n    sample = next(ds_iter)\r\n\r\n    np.save(f\"audio_sample_{torch.__version__}\", sample[\"audio\"][\"array\"])\r\n    print(sample[\"audio\"][\"array\"])\r\n\r\nif load_audios:\r\n    array_torch_11 = np.load(\"/home/patrick/audio_sample_1.11.0+cu102.npy\")\r\n    print(\"Array 11 Shape\", array_torch_11.shape)\r\n    print(\"Array 11 abs sum\", np.sum(np.abs(array_torch_11)))\r\n    array_torch_12 = np.load(\"/home/patrick/audio_sample_1.12.1+cu102.npy\")\r\n    print(\"Array 12 Shape\", array_torch_12.shape)\r\n    print(\"Array 12 abs sum\", np.sum(np.abs(array_torch_12)))\r\n```\r\n\r\nHaving saved the tensors the print output yields:\r\n\r\n```\r\ntorch vesion 1.12.1+cu102\r\ntorchaudio vesion 0.12.1+cu102\r\nArray 11 Shape (122880,)\r\nArray 11 abs sum 1396.4988\r\nArray 12 Shape (123264,)\r\nArray 12 abs sum 1396.5193\r\n```\r\n\r\n## Expected results\r\ntorchaudio 11.0 and 12.1 should yield same results.\r\n\r\n## Actual results\r\nSee above.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.1.1.dev0\r\n- Platform: Linux-5.18.10-76051810-generic-x86_64-with-glibc2.34\r\n- Python version: 3.9.7\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4889/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4862", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4862/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4862/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4862/events", "html_url": "https://github.com/huggingface/datasets/issues/4862", "id": 1343464699, "node_id": "I_kwDODunzps5QE6T7", "number": 4862, "title": "Got \"AttributeError: 'xPath' object has no attribute 'read'\" when loading an excel dataset with my own code", "user": {"login": "yana-xuyan", "id": 38536635, "node_id": "MDQ6VXNlcjM4NTM2NjM1", "avatar_url": "https://avatars.githubusercontent.com/u/38536635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yana-xuyan", "html_url": "https://github.com/yana-xuyan", "followers_url": "https://api.github.com/users/yana-xuyan/followers", "following_url": "https://api.github.com/users/yana-xuyan/following{/other_user}", "gists_url": "https://api.github.com/users/yana-xuyan/gists{/gist_id}", "starred_url": "https://api.github.com/users/yana-xuyan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yana-xuyan/subscriptions", "organizations_url": "https://api.github.com/users/yana-xuyan/orgs", "repos_url": "https://api.github.com/users/yana-xuyan/repos", "events_url": "https://api.github.com/users/yana-xuyan/events{/privacy}", "received_events_url": "https://api.github.com/users/yana-xuyan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-08-18T18:36:14Z", "updated_at": "2022-08-31T09:25:08Z", "closed_at": "2022-08-31T09:25:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n# The dataset function is as follows\uff1a\r\nfrom pathlib import Path\r\nfrom typing import Dict, List, Tuple\r\n\r\nimport datasets\r\nimport pandas as pd\r\n\r\n_CITATION = \"\"\"\\\r\n\"\"\"\r\n\r\n_DATASETNAME = \"jadi_ide\"\r\n\r\n_DESCRIPTION = \"\"\"\\\r\n\"\"\"\r\n\r\n_HOMEPAGE = \"\"\r\n_LICENSE = \"Unknown\"\r\n_URLS = {\r\n    _DATASETNAME: \"https://github.com/fathanick/Javanese-Dialect-Identification-from-Twitter-Data/raw/main/Update 16K_Dataset.xlsx\",\r\n}\r\n_SOURCE_VERSION = \"1.0.0\"\r\n\r\n\r\nclass JaDi_Ide(datasets.GeneratorBasedBuilder):\r\n\r\n    SOURCE_VERSION = datasets.Version(_SOURCE_VERSION)\r\n\r\n    BUILDER_CONFIGS = [\r\n        NusantaraConfig(\r\n            name=\"jadi_ide_source\",\r\n            version=SOURCE_VERSION,\r\n            description=\"JaDi-Ide source schema\",\r\n            schema=\"source\",\r\n            subset_id=\"jadi_ide\",\r\n        ),\r\n    ]\r\n\r\n    DEFAULT_CONFIG_NAME = \"source\"\r\n\r\n    def _info(self) -> datasets.DatasetInfo:\r\n        if self.config.schema == \"source\":\r\n            features = datasets.Features(\r\n                {\r\n                    \"id\": datasets.Value(\"string\"), \r\n                    \"text\": datasets.Value(\"string\"), \r\n                    \"label\": datasets.Value(\"string\")\r\n                }\r\n            )\r\n\r\n        return datasets.DatasetInfo(\r\n            description=_DESCRIPTION,\r\n            features=features,\r\n            homepage=_HOMEPAGE,\r\n            license=_LICENSE,\r\n            citation=_CITATION,\r\n        )\r\n\r\n    def _split_generators(self, dl_manager: datasets.DownloadManager) -> List[datasets.SplitGenerator]:\r\n        \"\"\"Returns SplitGenerators.\"\"\"\r\n        # Dataset does not have predetermined split, putting all as TRAIN\r\n        urls = _URLS[_DATASETNAME]\r\n        base_dir = Path(dl_manager.download_and_extract(urls))\r\n        data_files = {\"train\": base_dir}\r\n\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN,\r\n                gen_kwargs={\r\n                    \"filepath\": data_files[\"train\"],\r\n                    \"split\": \"train\",\r\n                },\r\n            ),\r\n        ]\r\n\r\n    def _generate_examples(self, filepath: Path, split: str) -> Tuple[int, Dict]:\r\n        \"\"\"Yields examples as (key, example) tuples.\"\"\"\r\n        df = pd.read_excel(filepath, engine='openpyxl')\r\n        df.columns = [\"id\", \"text\", \"label\"]\r\n\r\n        if self.config.schema == \"source\":\r\n            for row in df.itertuples():\r\n                ex = {\r\n                    \"id\": str(row.id),\r\n                    \"text\": row.text,\r\n                    \"label\": row.label,\r\n                }\r\n                yield row.id, ex\r\n\r\n```\r\n\r\n## Expected results\r\nExpecting to load the dataset smoothly.\r\n\r\n## Actual results\r\n  File \"/home/xuyan/anaconda3/lib/python3.7/site-packages/datasets/load.py\", line 1751, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/xuyan/anaconda3/lib/python3.7/site-packages/datasets/builder.py\", line 705, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/xuyan/anaconda3/lib/python3.7/site-packages/datasets/builder.py\", line 1227, in _download_and_prepare\r\n    super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n  File \"/home/xuyan/anaconda3/lib/python3.7/site-packages/datasets/builder.py\", line 793, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/xuyan/anaconda3/lib/python3.7/site-packages/datasets/builder.py\", line 1216, in _prepare_split\r\n    desc=f\"Generating {split_info.name} split\",\r\n  File \"/home/xuyan/anaconda3/lib/python3.7/site-packages/tqdm/std.py\", line 1195, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/xuyan/.cache/huggingface/modules/datasets_modules/datasets/jadi_ide/7a539f2b6f726defea8fbe36ceda17bae66c370f6d6c418e3a08d760ebef7519/jadi_ide.py\", line 107, in _generate_examples\r\n    df = pd.read_excel(filepath, engine='openpyxl')\r\n  File \"/home/xuyan/anaconda3/lib/python3.7/site-packages/datasets/download/streaming_download_manager.py\", line 701, in xpandas_read_excel\r\n    return pd.read_excel(BytesIO(filepath_or_buffer.read()), **kwargs)\r\nAttributeError: 'xPath' object has no attribute 'read'\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-4.15.0-142-generic-x86_64-with-debian-stretch-sid\r\n- Python version: 3.7.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 0.25.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4862/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4862/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4858", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4858/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4858/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4858/events", "html_url": "https://github.com/huggingface/datasets/issues/4858", "id": 1340859853, "node_id": "I_kwDODunzps5P6-XN", "number": 4858, "title": "map() function removes columns when input_columns is not None", "user": {"login": "pramodith", "id": 16939722, "node_id": "MDQ6VXNlcjE2OTM5NzIy", "avatar_url": "https://avatars.githubusercontent.com/u/16939722?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pramodith", "html_url": "https://github.com/pramodith", "followers_url": "https://api.github.com/users/pramodith/followers", "following_url": "https://api.github.com/users/pramodith/following{/other_user}", "gists_url": "https://api.github.com/users/pramodith/gists{/gist_id}", "starred_url": "https://api.github.com/users/pramodith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pramodith/subscriptions", "organizations_url": "https://api.github.com/users/pramodith/orgs", "repos_url": "https://api.github.com/users/pramodith/repos", "events_url": "https://api.github.com/users/pramodith/events{/privacy}", "received_events_url": "https://api.github.com/users/pramodith/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-08-16T20:42:30Z", "updated_at": "2022-09-22T13:55:24Z", "closed_at": "2022-09-22T13:55:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe map function, removes features from the dataset that are not present in the _input_columns_ list of columns, despite the columns being removed not mentioned in the _remove_columns_ argument.\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import Dataset\r\nds = Dataset.from_dict({\"a\" : [1,2,3],\"b\" : [0,1,0], \"c\" : [2,4,5]})\r\n\r\ndef double(x,y):\r\n  x = x*2\r\n  y = y*2\r\n  return {\"d\" : x, \"e\" : y}\r\n\r\nds.map(double, input_columns=[\"a\",\"c\"])\r\n```\r\n\r\n## Expected results\r\n```\r\nDataset({\r\n    features: ['a', 'b', 'c', 'd', 'e'],\r\n    num_rows: 3\r\n})\r\n```\r\n## Actual results\r\n```\r\nDataset({\r\n    features: ['a', 'c', 'd', 'e'],\r\n    num_rows: 3\r\n})\r\n```\r\n\r\nIn this specific example feature **b** should not be removed.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: linux (colab)\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4858/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4858/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4857", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4857/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4857/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4857/events", "html_url": "https://github.com/huggingface/datasets/issues/4857", "id": 1340397153, "node_id": "I_kwDODunzps5P5NZh", "number": 4857, "title": "No preprocessed wikipedia is working on huggingface/datasets", "user": {"login": "aninrusimha", "id": 30733039, "node_id": "MDQ6VXNlcjMwNzMzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/30733039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aninrusimha", "html_url": "https://github.com/aninrusimha", "followers_url": "https://api.github.com/users/aninrusimha/followers", "following_url": "https://api.github.com/users/aninrusimha/following{/other_user}", "gists_url": "https://api.github.com/users/aninrusimha/gists{/gist_id}", "starred_url": "https://api.github.com/users/aninrusimha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aninrusimha/subscriptions", "organizations_url": "https://api.github.com/users/aninrusimha/orgs", "repos_url": "https://api.github.com/users/aninrusimha/repos", "events_url": "https://api.github.com/users/aninrusimha/events{/privacy}", "received_events_url": "https://api.github.com/users/aninrusimha/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-08-16T13:55:33Z", "updated_at": "2022-08-17T13:35:08Z", "closed_at": "2022-08-17T13:35:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n20220301 wikipedia dump has been deprecated, so now there is no working wikipedia dump on huggingface\r\n\r\nhttps://huggingface.co/datasets/wikipedia\r\n\r\nhttps://dumps.wikimedia.org/enwiki/\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4857/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4856", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4856/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4856/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4856/events", "html_url": "https://github.com/huggingface/datasets/issues/4856", "id": 1339779957, "node_id": "I_kwDODunzps5P22t1", "number": 4856, "title": "file missing when load_dataset with openwebtext on windows", "user": {"login": "kingstarcraft", "id": 10361976, "node_id": "MDQ6VXNlcjEwMzYxOTc2", "avatar_url": "https://avatars.githubusercontent.com/u/10361976?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kingstarcraft", "html_url": "https://github.com/kingstarcraft", "followers_url": "https://api.github.com/users/kingstarcraft/followers", "following_url": "https://api.github.com/users/kingstarcraft/following{/other_user}", "gists_url": "https://api.github.com/users/kingstarcraft/gists{/gist_id}", "starred_url": "https://api.github.com/users/kingstarcraft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kingstarcraft/subscriptions", "organizations_url": "https://api.github.com/users/kingstarcraft/orgs", "repos_url": "https://api.github.com/users/kingstarcraft/repos", "events_url": "https://api.github.com/users/kingstarcraft/events{/privacy}", "received_events_url": "https://api.github.com/users/kingstarcraft/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-08-16T04:04:22Z", "updated_at": "2023-01-04T03:39:12Z", "closed_at": "2023-01-04T03:39:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n0015896-b1054262f7da52a0518521e29c8e352c.txt is missing when I run run_mlm.py with openwebtext. I check the cache_path and can not find 0015896-b1054262f7da52a0518521e29c8e352c.txt. but I can find this file in the 17ecf461bfccd469a1fbc264ccb03731f8606eea7b3e2e8b86e13d18040bf5b3/urlsf_subset00-16_data.xz  with 7-zip.\r\n\r\n## Steps to reproduce the bug\r\n```sh\r\npython run_mlm.py --model_type roberta --tokenizer_name roberta-base --dataset_name openwebtext --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --output_dir F:/model/roberta-base\r\n```\r\nor \r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"openwebtext\", None, cache_dir=None, use_auth_token=None)\r\n```\r\n\r\n## Expected results\r\nLoading is successful\r\n## Actual results\r\nTraceback (most recent call last):\r\n  File \"D:\\Python\\v3.8.5\\lib\\site-packages\\datasets\\builder.py\", line 704, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"D:\\Python\\v3.8.5\\lib\\site-packages\\datasets\\builder.py\", line 1227, in _download_and_prepare\r\n    super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n  File \"D:\\Python\\v3.8.5\\lib\\site-packages\\datasets\\builder.py\", line 795, in _download_and_prepare\r\n    raise OSError(\r\nOSError: Cannot find data file. \r\nOriginal error:\r\n[Errno 22] Invalid argument: 'F://huggingface/datasets/downloads/extracted/0901d27f43b7e9ac0577da0d0061c8c632ba0b70ecd1b4bfb21562d9b7486faa/0015896-b1054262f7da52a0518521e29c8e352c.txt'\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: windows\r\n- Python version: 3.8.5 \r\n- PyArrow version: 9.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4856/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4852", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4852/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4852/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4852/events", "html_url": "https://github.com/huggingface/datasets/issues/4852", "id": 1339450991, "node_id": "I_kwDODunzps5P1mZv", "number": 4852, "title": "Bug in multilingual_with_para config of exams dataset and checksums error", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-08-15T20:14:52Z", "updated_at": "2022-09-16T09:50:55Z", "closed_at": "2022-08-16T06:29:07Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nThere is a bug for \"multilingual_with_para\" config in exams dataset:\r\n```python\r\nds = load_dataset(\"./datasets/exams\", split=\"train\")\r\n```\r\nraises:\r\n```\r\nKeyError: 'choices'\r\n```\r\n\r\nMoreover, there is a  NonMatchingChecksumError:\r\n```\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://github.com/mhardalov/exams-qa/raw/main/data/exams/multilingual/with_paragraphs/train_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/multilingual/with_paragraphs/dev_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/multilingual/with_paragraphs/test_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/test_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_bg_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_bg_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_hr_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_hr_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_hu_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_hu_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_it_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_it_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_mk_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_mk_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_pl_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_pl_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_pt_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_pt_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_sq_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_sq_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_sr_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_sr_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_tr_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_tr_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/train_vi_with_para.jsonl.tar.gz', 'https://github.com/mhardalov/exams-qa/raw/main/data/exams/cross-lingual/with_paragraphs/dev_vi_with_para.jsonl.tar.gz']\r\n```\r\n\r\nCC: @thesofakillers", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4852/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4852/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4820", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4820/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4820/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4820/events", "html_url": "https://github.com/huggingface/datasets/issues/4820", "id": 1335117132, "node_id": "I_kwDODunzps5PlEVM", "number": 4820, "title": "Terminating: fork() called from a process already using GNU OpenMP, this is unsafe.", "user": {"login": "talhaanwarch", "id": 37379131, "node_id": "MDQ6VXNlcjM3Mzc5MTMx", "avatar_url": "https://avatars.githubusercontent.com/u/37379131?v=4", "gravatar_id": "", "url": "https://api.github.com/users/talhaanwarch", "html_url": "https://github.com/talhaanwarch", "followers_url": "https://api.github.com/users/talhaanwarch/followers", "following_url": "https://api.github.com/users/talhaanwarch/following{/other_user}", "gists_url": "https://api.github.com/users/talhaanwarch/gists{/gist_id}", "starred_url": "https://api.github.com/users/talhaanwarch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/talhaanwarch/subscriptions", "organizations_url": "https://api.github.com/users/talhaanwarch/orgs", "repos_url": "https://api.github.com/users/talhaanwarch/repos", "events_url": "https://api.github.com/users/talhaanwarch/events{/privacy}", "received_events_url": "https://api.github.com/users/talhaanwarch/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-08-10T19:42:33Z", "updated_at": "2022-08-10T19:53:10Z", "closed_at": "2022-08-10T19:53:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, when i try to run prepare_dataset function in [fine tuning ASR tutorial 4](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb) , i got this error.\r\nI got this error\r\nTerminating: fork() called from a process already using GNU OpenMP, this is unsafe.\r\nThere is no other logs available, so i have no clue what is the cause of it.\r\n```\r\n\r\ndef prepare_dataset(batch):\r\n    audio = batch[\"path\"]\r\n    # batched output is \"un-batched\"\r\n    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\r\n    batch[\"input_length\"] = len(batch[\"input_values\"])\r\n    with processor.as_target_processor():\r\n        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\r\n    return batch\r\n\r\ndata = data.map(prepare_dataset, remove_columns=data.column_names[\"train\"],\r\n                      num_proc=4)\r\n```\r\n\r\n\r\nSpecify the actual results or traceback.\r\nThere is no traceback except\r\n`Terminating: fork() called from a process already using GNU OpenMP, this is unsafe.`\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-5.15.0-43-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.3\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4820/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4817", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4817/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4817/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4817/events", "html_url": "https://github.com/huggingface/datasets/issues/4817", "id": 1334572163, "node_id": "I_kwDODunzps5Pi_SD", "number": 4817, "title": "Outdated Link for mkqa Dataset", "user": {"login": "liaeh", "id": 52380283, "node_id": "MDQ6VXNlcjUyMzgwMjgz", "avatar_url": "https://avatars.githubusercontent.com/u/52380283?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liaeh", "html_url": "https://github.com/liaeh", "followers_url": "https://api.github.com/users/liaeh/followers", "following_url": "https://api.github.com/users/liaeh/following{/other_user}", "gists_url": "https://api.github.com/users/liaeh/gists{/gist_id}", "starred_url": "https://api.github.com/users/liaeh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liaeh/subscriptions", "organizations_url": "https://api.github.com/users/liaeh/orgs", "repos_url": "https://api.github.com/users/liaeh/repos", "events_url": "https://api.github.com/users/liaeh/events{/privacy}", "received_events_url": "https://api.github.com/users/liaeh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-08-10T12:45:45Z", "updated_at": "2022-08-11T09:37:52Z", "closed_at": "2022-08-11T09:37:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe URL used to download the mkqa dataset is outdated. It seems the URL to download the dataset is currently https://github.com/apple/ml-mkqa/blob/main/dataset/mkqa.jsonl.gz instead of https://github.com/apple/ml-mkqa/raw/master/dataset/mkqa.jsonl.gz (master branch has been renamed to main).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"mkqa\")\r\n```\r\n\r\n## Expected results\r\ndownloads the dataset\r\n\r\n## Actual results\r\n```python\r\nDownloading builder script:\r\n4.79k/? [00:00<00:00, 201kB/s]\r\nDownloading metadata:\r\n13.2k/? [00:00<00:00, 504kB/s]\r\n\r\nDownloading and preparing dataset mkqa/mkqa (download: 11.35 MiB, generated: 34.29 MiB, post-processed: Unknown size, total: 45.65 MiB) to /home/lhr/.cache/huggingface/datasets/mkqa/mkqa/1.0.0/5401489c674c81257cf563417aaaa5de2c7e26a1090ce9b10eb0404f10003d4d...\r\n\r\nDownloading data files: 0%\r\n0/1 [00:00<?, ?it/s]\r\n\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\nInput In [3], in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n----> 3 dataset = load_dataset(\"mkqa\")\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/load.py:1746, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1743 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   1745 # Download and prepare data\r\n-> 1746 builder_instance.download_and_prepare(\r\n   1747     download_config=download_config,\r\n   1748     download_mode=download_mode,\r\n   1749     ignore_verifications=ignore_verifications,\r\n   1750     try_from_hf_gcs=try_from_hf_gcs,\r\n   1751     use_auth_token=use_auth_token,\r\n   1752 )\r\n   1754 # Build dataset for splits\r\n   1755 keep_in_memory = (\r\n   1756     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   1757 )\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/builder.py:704, in DatasetBuilder.download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    702         logger.warning(\"HF google storage unreachable. Downloading and preparing it from source\")\r\n    703 if not downloaded_from_gcs:\r\n--> 704     self._download_and_prepare(\r\n    705         dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    706     )\r\n    707 # Sync info\r\n    708 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/builder.py:1227, in GeneratorBasedBuilder._download_and_prepare(self, dl_manager, verify_infos)\r\n   1226 def _download_and_prepare(self, dl_manager, verify_infos):\r\n-> 1227     super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/builder.py:771, in DatasetBuilder._download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    769 split_dict = SplitDict(dataset_name=self.name)\r\n    770 split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 771 split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    773 # Checksums verification\r\n    774 if verify_infos and dl_manager.record_checksums:\r\n\r\nFile ~/.cache/huggingface/modules/datasets_modules/datasets/mkqa/5401489c674c81257cf563417aaaa5de2c7e26a1090ce9b10eb0404f10003d4d/mkqa.py:130, in Mkqa._split_generators(self, dl_manager)\r\n    128 # download and extract URLs\r\n    129 urls_to_download = _URLS\r\n--> 130 downloaded_files = dl_manager.download_and_extract(urls_to_download)\r\n    132 return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": downloaded_files[\"train\"]})]\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/download/download_manager.py:431, in DownloadManager.download_and_extract(self, url_or_urls)\r\n    415 def download_and_extract(self, url_or_urls):\r\n    416     \"\"\"Download and extract given url_or_urls.\r\n    417 \r\n    418     Is roughly equivalent to:\r\n   (...)\r\n    429         extracted_path(s): `str`, extracted paths of given URL(s).\r\n    430     \"\"\"\r\n--> 431     return self.extract(self.download(url_or_urls))\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/download/download_manager.py:309, in DownloadManager.download(self, url_or_urls)\r\n    306 download_func = partial(self._download, download_config=download_config)\r\n    308 start_time = datetime.now()\r\n--> 309 downloaded_path_or_paths = map_nested(\r\n    310     download_func,\r\n    311     url_or_urls,\r\n    312     map_tuple=True,\r\n    313     num_proc=download_config.num_proc,\r\n    314     disable_tqdm=not is_progress_bar_enabled(),\r\n    315     desc=\"Downloading data files\",\r\n    316 )\r\n    317 duration = datetime.now() - start_time\r\n    318 logger.info(f\"Downloading took {duration.total_seconds() // 60} min\")\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/utils/py_utils.py:393, in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm, desc)\r\n    391     num_proc = 1\r\n    392 if num_proc <= 1 or len(iterable) <= num_proc:\r\n--> 393     mapped = [\r\n    394         _single_map_nested((function, obj, types, None, True, None))\r\n    395         for obj in logging.tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n    396     ]\r\n    397 else:\r\n    398     split_kwds = []  # We organize the splits ourselve (contiguous splits)\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/utils/py_utils.py:394, in <listcomp>(.0)\r\n    391     num_proc = 1\r\n    392 if num_proc <= 1 or len(iterable) <= num_proc:\r\n    393     mapped = [\r\n--> 394         _single_map_nested((function, obj, types, None, True, None))\r\n    395         for obj in logging.tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n    396     ]\r\n    397 else:\r\n    398     split_kwds = []  # We organize the splits ourselve (contiguous splits)\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/utils/py_utils.py:330, in _single_map_nested(args)\r\n    328 # Singleton first to spare some computation\r\n    329 if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 330     return function(data_struct)\r\n    332 # Reduce logging to keep things readable in multiprocessing with tqdm\r\n    333 if rank is not None and logging.get_verbosity() < logging.WARNING:\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/download/download_manager.py:335, in DownloadManager._download(self, url_or_filename, download_config)\r\n    332 if is_relative_path(url_or_filename):\r\n    333     # append the relative path to the base_path\r\n    334     url_or_filename = url_or_path_join(self._base_path, url_or_filename)\r\n--> 335 return cached_path(url_or_filename, download_config=download_config)\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/utils/file_utils.py:185, in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    181     url_or_filename = str(url_or_filename)\r\n    183 if is_remote_url(url_or_filename):\r\n    184     # URL, so get it from the cache (downloading if necessary)\r\n--> 185     output_path = get_from_cache(\r\n    186         url_or_filename,\r\n    187         cache_dir=cache_dir,\r\n    188         force_download=download_config.force_download,\r\n    189         proxies=download_config.proxies,\r\n    190         resume_download=download_config.resume_download,\r\n    191         user_agent=download_config.user_agent,\r\n    192         local_files_only=download_config.local_files_only,\r\n    193         use_etag=download_config.use_etag,\r\n    194         max_retries=download_config.max_retries,\r\n    195         use_auth_token=download_config.use_auth_token,\r\n    196         ignore_url_params=download_config.ignore_url_params,\r\n    197         download_desc=download_config.download_desc,\r\n    198     )\r\n    199 elif os.path.exists(url_or_filename):\r\n    200     # File, and it exists.\r\n    201     output_path = url_or_filename\r\n\r\nFile ~/repos/punc-cap/venv/lib/python3.9/site-packages/datasets/utils/file_utils.py:530, in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, download_desc)\r\n    525     raise FileNotFoundError(\r\n    526         f\"Cannot find the requested files in the cached path at {cache_path} and outgoing traffic has been\"\r\n    527         \" disabled. To enable file online look-ups, set 'local_files_only' to False.\"\r\n    528     )\r\n    529 elif response is not None and response.status_code == 404:\r\n--> 530     raise FileNotFoundError(f\"Couldn't find file at {url}\")\r\n    531 _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\r\n    532 if head_error is not None:\r\n\r\nFileNotFoundError: Couldn't find file at https://github.com/apple/ml-mkqa/raw/master/dataset/mkqa.jsonl.gz\r\n\r\n\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-5.13.0-40-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4817/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4817/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4811", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4811/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4811/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4811/events", "html_url": "https://github.com/huggingface/datasets/issues/4811", "id": 1333043421, "node_id": "I_kwDODunzps5PdKDd", "number": 4811, "title": "Bug in function validate_type for Python >= 3.9", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-08-09T10:25:21Z", "updated_at": "2022-08-12T13:27:05Z", "closed_at": "2022-08-12T13:27:05Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nThe function `validate_type` assumes that the type `typing.Optional[str]` is automatically transformed to `typing.Union[str, NoneType]`.\r\n```python\r\nIn [4]: typing.Optional[str]\r\nOut[4]: typing.Union[str, NoneType]\r\n```\r\n\r\nHowever, this is not the case for Python 3.9:\r\n```python\r\nIn [3]: typing.Optional[str]\r\nOut[3]: typing.Optional[str]\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4811/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4811/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4805", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4805/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4805/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4805/events", "html_url": "https://github.com/huggingface/datasets/issues/4805", "id": 1332653531, "node_id": "I_kwDODunzps5Pbq3b", "number": 4805, "title": "Wrong example in opus_gnome dataset card", "user": {"login": "gojiteji", "id": 38291975, "node_id": "MDQ6VXNlcjM4MjkxOTc1", "avatar_url": "https://avatars.githubusercontent.com/u/38291975?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gojiteji", "html_url": "https://github.com/gojiteji", "followers_url": "https://api.github.com/users/gojiteji/followers", "following_url": "https://api.github.com/users/gojiteji/following{/other_user}", "gists_url": "https://api.github.com/users/gojiteji/gists{/gist_id}", "starred_url": "https://api.github.com/users/gojiteji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gojiteji/subscriptions", "organizations_url": "https://api.github.com/users/gojiteji/orgs", "repos_url": "https://api.github.com/users/gojiteji/repos", "events_url": "https://api.github.com/users/gojiteji/events{/privacy}", "received_events_url": "https://api.github.com/users/gojiteji/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-08-09T03:21:27Z", "updated_at": "2022-08-09T11:52:05Z", "closed_at": "2022-08-09T11:52:05Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI found that [the example on opus_gone dataset ](https://github.com/huggingface/datasets/tree/main/datasets/opus_gnome#dataset-summary) doesn't work.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n load_dataset(\"gnome\", lang1=\"it\", lang2=\"pl\")\r\n```\r\n`\"gnome\"` should be `\"opus_gnome\"`\r\n\r\n## Expected results\r\n```bash\r\n100%\r\n1/1 [00:00<00:00, 42.09it/s]\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'translation'],\r\n        num_rows: 8368\r\n    })\r\n})\r\n```\r\n\r\n## Actual results\r\n```bash\r\n Couldn't find 'gnome' on the Hugging Face Hub either: FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/main/datasets/gnome/gnome.py\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-5.4.0-120-generic-x86_64-with-glibc2.27\r\n- Python version: 3.9.13\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.3\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4805/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4805/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4795", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4795/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4795/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4795/events", "html_url": "https://github.com/huggingface/datasets/issues/4795", "id": 1329525732, "node_id": "I_kwDODunzps5PPvPk", "number": 4795, "title": "Missing MBPP splits", "user": {"login": "stadlerb", "id": 2452384, "node_id": "MDQ6VXNlcjI0NTIzODQ=", "avatar_url": "https://avatars.githubusercontent.com/u/2452384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stadlerb", "html_url": "https://github.com/stadlerb", "followers_url": "https://api.github.com/users/stadlerb/followers", "following_url": "https://api.github.com/users/stadlerb/following{/other_user}", "gists_url": "https://api.github.com/users/stadlerb/gists{/gist_id}", "starred_url": "https://api.github.com/users/stadlerb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stadlerb/subscriptions", "organizations_url": "https://api.github.com/users/stadlerb/orgs", "repos_url": "https://api.github.com/users/stadlerb/repos", "events_url": "https://api.github.com/users/stadlerb/events{/privacy}", "received_events_url": "https://api.github.com/users/stadlerb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-08-05T06:51:01Z", "updated_at": "2022-09-13T12:27:24Z", "closed_at": "2022-09-13T12:27:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "(@albertvillanova)\r\nThe [MBPP dataset on the Hub](https://huggingface.co/datasets/mbpp) has only a test split for both its \"full\" and its \"sanitized\" subset, while the [paper](https://arxiv.org/abs/2108.07732) states in subsection 2.1 regarding the full split:\r\n> In the experiments described later in the paper, we hold out 10 problems for **few-shot prompting**, another 500 as our **test** dataset (which is used to evaluate both few-shot inference and fine-tuned models), 374 problems for **fine-tuning**, and the rest for **validation**.\r\n\r\nIf the dataset on the Hub should reproduce most closely what the original authors use, I guess this four-way split should be reflected. \r\n\r\nThe paper doesn't explicitly state the task_id ranges of the splits, but the [GitHub readme](https://github.com/google-research/google-research/tree/master/mbpp) referenced in the paper specifies exact task_id ranges, although it misstates the total number of samples:\r\n> We specify a train and test split to use for evaluation. Specifically:\r\n> \r\n> * Task IDs 11-510 are used for evaluation.\r\n> * Task IDs 1-10 and 511-1000 are used for training and/or prompting. We typically used 1-10 for few-shot prompting, although you can feel free to use any of the training examples.\r\n\r\nI.e. the few-shot, train and validation splits are combined into one split, with a soft suggestion of using the first ten for few-shot prompting. It is not explicitly stated whether the 374 fine-tuning samples mentioned in the paper have task_id 511 to 784 or 601 to 974 or are randomly sampled from task_id 511 to 974.\r\n\r\nRegarding the \"sanitized\" split the paper states the following:\r\n> For evaluations involving the edited dataset, we perform comparisons with 100 problems that appear in both the original and edited dataset, using the same held out 10 problems for few-shot prompting and 374 problems for fine-tuning. \r\n\r\nThe statement doesn't appear to be very precise, as among the 10 few-shot problems, those with task_id 1, 5 and 10 are not even part of the sanitized variant, and many from the task_id range from 511 to 974 are missing (e.g. task_id 511 to 553). I suppose the idea the task_id ranges for each split remain the same, even if some of the task_ids are not present. That would result in 7 few-shot, 257 test, 141 train and 22 validation examples in the sanitized split.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4795/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4790", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4790/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4790/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4790/events", "html_url": "https://github.com/huggingface/datasets/issues/4790", "id": 1328546904, "node_id": "I_kwDODunzps5PMARY", "number": 4790, "title": "Issue with fine classes in trec dataset", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-08-04T12:28:51Z", "updated_at": "2022-08-22T16:14:16Z", "closed_at": "2022-08-22T16:14:16Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAccording to their paper, the TREC dataset contains 2 kinds of classes:\r\n- 6 coarse classes: TREC-6\r\n- 50 fine classes: TREC-50\r\n\r\nHowever, our implementation only has 47 (instead of 50) fine classes. The reason for this is that we only considered the last segment of the label, which is repeated for several coarse classes:\r\n- We have one `desc` fine label instead of 2:\r\n  - `DESC:desc`\r\n  - `HUM:desc`\r\n- We have one `other` fine label instead of 3:\r\n  - `ENTY:other`\r\n  - `LOC:other`\r\n  - `NUM:other`\r\n\r\nFrom their paper:\r\n> We define a two-layered taxonomy, which represents a natural semantic classification for typical answers in the TREC task. The hierarchy contains 6 coarse classes and 50 fine classes,\r\n\r\n> Each coarse class contains a non-overlapping set of fine classes.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4790/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4787", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4787/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4787/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4787/events", "html_url": "https://github.com/huggingface/datasets/issues/4787", "id": 1328243911, "node_id": "I_kwDODunzps5PK2TH", "number": 4787, "title": "NonMatchingChecksumError in mbpp dataset", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-08-04T08:15:51Z", "updated_at": "2022-08-04T17:21:01Z", "closed_at": "2022-08-04T17:21:01Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAs reported on the Hub [Fix Checksum Mismatch](https://huggingface.co/datasets/mbpp/discussions/1), there is a `NonMatchingChecksumError` when loading mbpp dataset\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nds = load_dataset(\"mbpp\", \"full\")\r\n```\r\n\r\n## Expected results\r\nLoading of the dataset without any exception raised.\r\n\r\n## Actual results\r\n```\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-1-a3fbdd3ed82e> in <module>\r\n----> 1 ds = load_dataset(\"mbpp\", \"full\")\r\n\r\n.../huggingface/datasets/src/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1791 \r\n   1792     # Download and prepare data\r\n-> 1793     builder_instance.download_and_prepare(\r\n   1794         download_config=download_config,\r\n   1795         download_mode=download_mode,\r\n.../huggingface/datasets/src/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    702                             logger.warning(\"HF google storage unreachable. Downloading and preparing it from source\")\r\n    703                     if not downloaded_from_gcs:\r\n--> 704                         self._download_and_prepare(\r\n    705                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    706                         )\r\n\r\n.../huggingface/datasets/src/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos)\r\n   1225 \r\n   1226     def _download_and_prepare(self, dl_manager, verify_infos):\r\n-> 1227         super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n   1228 \r\n   1229     def _get_examples_iterable_for_split(self, split_generator: SplitGenerator) -> ExamplesIterable:\r\n\r\n.../huggingface/datasets/src/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    773         # Checksums verification\r\n    774         if verify_infos and dl_manager.record_checksums:\r\n--> 775             verify_checksums(\r\n    776                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n    777             )\r\n\r\n.../huggingface/datasets/src/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://raw.githubusercontent.com/google-research/google-research/master/mbpp/mbpp.jsonl']\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4787/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4787/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4786", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4786/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4786/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4786/events", "html_url": "https://github.com/huggingface/datasets/issues/4786", "id": 1327340828, "node_id": "I_kwDODunzps5PHZ0c", "number": 4786, "title": ".save_to_disk('path', fs=s3) TypeError ", "user": {"login": "h-k-dev", "id": 110547763, "node_id": "U_kgDOBpbTMw", "avatar_url": "https://avatars.githubusercontent.com/u/110547763?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h-k-dev", "html_url": "https://github.com/h-k-dev", "followers_url": "https://api.github.com/users/h-k-dev/followers", "following_url": "https://api.github.com/users/h-k-dev/following{/other_user}", "gists_url": "https://api.github.com/users/h-k-dev/gists{/gist_id}", "starred_url": "https://api.github.com/users/h-k-dev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h-k-dev/subscriptions", "organizations_url": "https://api.github.com/users/h-k-dev/orgs", "repos_url": "https://api.github.com/users/h-k-dev/repos", "events_url": "https://api.github.com/users/h-k-dev/events{/privacy}", "received_events_url": "https://api.github.com/users/h-k-dev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-08-03T14:49:29Z", "updated_at": "2022-08-03T15:23:00Z", "closed_at": "2022-08-03T15:23:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "The following code:\r\n```python\r\nimport datasets\r\n\r\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=[\"train\", \"test\"])\r\ns3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\r\ntrain_dataset.save_to_disk(\"s3://datasets/\", fs=s3)\r\n\r\n```\r\nproduces following traceback:\r\n\r\n```shell\r\nFile \"C:\\Users\\Hong Knop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\botocore\\auth.py\", line 374, in scope\r\n    return '/'.join(scope)\r\n\r\n```\r\nI invoke print(scope) in <auth.py> (line 373) and find this:\r\n\r\n```python\r\n[('4VA08VLL3VTKQJKCAI8M',), '20220803', 'us-east-1', 's3', 'aws4_request']\r\n\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4786/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4786/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4782", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4782/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4782/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4782/events", "html_url": "https://github.com/huggingface/datasets/issues/4782", "id": 1326247158, "node_id": "I_kwDODunzps5PDOz2", "number": 4782, "title": "pyarrow.lib.ArrowCapacityError: array cannot contain more than 2147483646 bytes, have 2147483648", "user": {"login": "conceptofmind", "id": 25208228, "node_id": "MDQ6VXNlcjI1MjA4MjI4", "avatar_url": "https://avatars.githubusercontent.com/u/25208228?v=4", "gravatar_id": "", "url": "https://api.github.com/users/conceptofmind", "html_url": "https://github.com/conceptofmind", "followers_url": "https://api.github.com/users/conceptofmind/followers", "following_url": "https://api.github.com/users/conceptofmind/following{/other_user}", "gists_url": "https://api.github.com/users/conceptofmind/gists{/gist_id}", "starred_url": "https://api.github.com/users/conceptofmind/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/conceptofmind/subscriptions", "organizations_url": "https://api.github.com/users/conceptofmind/orgs", "repos_url": "https://api.github.com/users/conceptofmind/repos", "events_url": "https://api.github.com/users/conceptofmind/events{/privacy}", "received_events_url": "https://api.github.com/users/conceptofmind/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-08-02T18:36:05Z", "updated_at": "2022-08-22T09:46:28Z", "closed_at": "2022-08-20T02:11:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nFollowing the example in CodeParrot, I receive an array size limitation error when deduplicating larger datasets.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset_name = \"the_pile\"\r\nds = load_dataset(dataset_name, split=\"train\")\r\nds = ds.map(preprocess, num_proc=num_workers)\r\nuniques = set(ds.unique(\"hash\"))\r\n```\r\nGists for minimum reproducible example:\r\nhttps://gist.github.com/conceptofmind/c5804428ea1bd89767815f9cd5f02d9a\r\nhttps://gist.github.com/conceptofmind/feafb07e236f28d79c2d4b28ffbdb6e2\r\n\r\n## Expected results\r\nChunking and writing out a deduplicated dataset. \r\n\r\n## Actual results\r\n```\r\nreturn dataset._data.column(column).unique().to_pylist()\r\nFile \"pyarrow/table.pxi\", line 394, in pyarrow.lib.ChunkedArray.unique\r\nFile \"pyarrow/_compute.pyx\", line 531, in pyarrow._compute.call_function\r\nFile \"pyarrow/_compute.pyx\", line 330, in pyarrow._compute.Function.call\r\nFile \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\nFile \"pyarrow/error.pxi\", line 124, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowCapacityError: array cannot contain more than 2147483646 bytes, have 2147483648\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4782/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4779", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4779/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4779/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4779/events", "html_url": "https://github.com/huggingface/datasets/issues/4779", "id": 1325997225, "node_id": "I_kwDODunzps5PCRyp", "number": 4779, "title": "Loading natural_questions requires apache_beam even with existing preprocessed data", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-08-02T15:06:57Z", "updated_at": "2022-08-02T16:03:18Z", "closed_at": "2022-08-02T16:03:18Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen loading \"natural_questions\", the package \"apache_beam\" is required:\r\n```\r\nImportError: To be able to use natural_questions, you need to install the following dependency: apache_beam.\r\nPlease install it using 'pip install apache_beam' for instance'\r\n```\r\n\r\nThis requirement is unnecessary, once there exists preprocessed data and the script just needs to download it.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset(\"natural_questions\", \"dev\", split=\"validation\", revision=\"main\")\r\n```\r\n\r\n## Expected results\r\nNo ImportError raised.\r\n\r\n## Actual results\r\n```\r\nImportError                               Traceback (most recent call last)\r\n[<ipython-input-3-c938e7c05d02>](https://localhost:8080/#) in <module>()\r\n----> 1 from datasets import load_dataset; ds = load_dataset(\"natural_questions\", \"dev\", split=\"validation\", revision=\"main\")\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1732         revision=revision,\r\n   1733         use_auth_token=use_auth_token,\r\n-> 1734         **config_kwargs,\r\n   1735     )\r\n   1736 \r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\r\n   1504         download_mode=download_mode,\r\n   1505         data_dir=data_dir,\r\n-> 1506         data_files=data_files,\r\n   1507     )\r\n   1508 \r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/load.py](https://localhost:8080/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1245                         f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n   1246                     ) from None\r\n-> 1247                 raise e1 from None\r\n   1248     else:\r\n   1249         raise FileNotFoundError(\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/load.py](https://localhost:8080/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1180                     download_config=download_config,\r\n   1181                     download_mode=download_mode,\r\n-> 1182                     dynamic_modules_path=dynamic_modules_path,\r\n   1183                 ).get_module()\r\n   1184             elif path.count(\"/\") == 1:  # community dataset on the Hub\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/load.py](https://localhost:8080/#) in get_module(self)\r\n    490             base_path=hf_github_url(path=self.name, name=\"\", revision=revision),\r\n    491             imports=imports,\r\n--> 492             download_config=self.download_config,\r\n    493         )\r\n    494         additional_files = [(config.DATASETDICT_INFOS_FILENAME, dataset_infos_path)] if dataset_infos_path else []\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/load.py](https://localhost:8080/#) in _download_additional_modules(name, base_path, imports, download_config)\r\n    214         _them_str = \"them\" if len(needs_to_be_installed) > 1 else \"it\"\r\n    215         raise ImportError(\r\n--> 216             f\"To be able to use {name}, you need to install the following {_depencencies_str}: \"\r\n    217             f\"{', '.join(needs_to_be_installed)}.\\nPlease install {_them_str} using 'pip install \"\r\n    218             f\"{' '.join(needs_to_be_installed.values())}' for instance'\"\r\n\r\nImportError: To be able to use natural_questions, you need to install the following dependency: apache_beam.\r\nPlease install it using 'pip install apache_beam' for instance'\r\n```\r\n\r\n## Environment info\r\nColab notebook.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4779/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4772", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4772/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4772/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4772/events", "html_url": "https://github.com/huggingface/datasets/issues/4772", "id": 1322693123, "node_id": "I_kwDODunzps5O1rID", "number": 4772, "title": "AssertionError when using label_cols in to_tf_dataset ", "user": {"login": "lehrig", "id": 9555494, "node_id": "MDQ6VXNlcjk1NTU0OTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/9555494?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lehrig", "html_url": "https://github.com/lehrig", "followers_url": "https://api.github.com/users/lehrig/followers", "following_url": "https://api.github.com/users/lehrig/following{/other_user}", "gists_url": "https://api.github.com/users/lehrig/gists{/gist_id}", "starred_url": "https://api.github.com/users/lehrig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lehrig/subscriptions", "organizations_url": "https://api.github.com/users/lehrig/orgs", "repos_url": "https://api.github.com/users/lehrig/repos", "events_url": "https://api.github.com/users/lehrig/events{/privacy}", "received_events_url": "https://api.github.com/users/lehrig/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-07-29T21:32:12Z", "updated_at": "2022-09-12T11:24:46Z", "closed_at": "2022-09-12T11:24:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nAn incorrect `AssertionError` is raised when using `label_cols` in `to_tf_dataset` and the label's key name is `label`.\r\n\r\nThe assertion is in this line:\r\nhttps://github.com/huggingface/datasets/blob/2.4.0/src/datasets/arrow_dataset.py#L475\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfrom transformers import DefaultDataCollator\r\n\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\n\r\ntf_dataset = dataset.to_tf_dataset(\r\n    columns=[\"sentence1\", \"sentence2\", \"idx\"],\r\n    label_cols=[\"label\"],\r\n    batch_size=16,\r\n    collate_fn=DefaultDataCollator(return_tensors=\"tf\"),\r\n)\r\n```\r\n\r\n## Expected results\r\nNo assertion error.\r\n\r\n## Actual results\r\n```\r\nAssertionError: in user code:\r\n\r\n    File \"/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 475, in split_features_and_labels  *\r\n        assert set(features.keys()).union(labels.keys()) == set(input_batch.keys())\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-4.18.0-305.45.1.el8_4.ppc64le-ppc64le-with-glibc2.17\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.3\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4772/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4742", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4742/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4742/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4742/events", "html_url": "https://github.com/huggingface/datasets/issues/4742", "id": 1317260663, "node_id": "I_kwDODunzps5Og813", "number": 4742, "title": "Dummy data nowhere to be found", "user": {"login": "BramVanroy", "id": 2779410, "node_id": "MDQ6VXNlcjI3Nzk0MTA=", "avatar_url": "https://avatars.githubusercontent.com/u/2779410?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BramVanroy", "html_url": "https://github.com/BramVanroy", "followers_url": "https://api.github.com/users/BramVanroy/followers", "following_url": "https://api.github.com/users/BramVanroy/following{/other_user}", "gists_url": "https://api.github.com/users/BramVanroy/gists{/gist_id}", "starred_url": "https://api.github.com/users/BramVanroy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BramVanroy/subscriptions", "organizations_url": "https://api.github.com/users/BramVanroy/orgs", "repos_url": "https://api.github.com/users/BramVanroy/repos", "events_url": "https://api.github.com/users/BramVanroy/events{/privacy}", "received_events_url": "https://api.github.com/users/BramVanroy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-07-25T19:18:42Z", "updated_at": "2022-11-04T14:04:24Z", "closed_at": "2022-11-04T14:04:10Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nTo finalize my dataset, I wanted to create dummy data as per the guide and I ran \r\n\r\n```shell\r\n datasets-cli dummy_data datasets/hebban-reviews --auto_generate\r\n```\r\n\r\nwhere hebban-reviews is [this repo](https://huggingface.co/datasets/BramVanroy/hebban-reviews). And even though the scripts runs and shows a message at the end that it succeeded, I cannot find the dummy data anywhere. Where is it?\r\n\r\n## Expected results\r\n\r\nTo see the dummy data in the datasets' folder or in the folder where I ran the command.\r\n\r\n## Actual results\r\n\r\nI see the following message but I cannot find the dummy data anywhere.\r\n\r\n```\r\nDummy data generation done and dummy data test succeeded for config 'filtered''.\r\nAutomatic dummy data generation succeeded for all configs of '.\\datasets\\hebban-reviews\\'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.4.1.dev0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.8.8\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.3\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4742/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4737", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4737/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4737/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4737/events", "html_url": "https://github.com/huggingface/datasets/issues/4737", "id": 1315011004, "node_id": "I_kwDODunzps5OYXm8", "number": 4737, "title": "Download error on scene_parse_150", "user": {"login": "juliensimon", "id": 3436143, "node_id": "MDQ6VXNlcjM0MzYxNDM=", "avatar_url": "https://avatars.githubusercontent.com/u/3436143?v=4", "gravatar_id": "", "url": "https://api.github.com/users/juliensimon", "html_url": "https://github.com/juliensimon", "followers_url": "https://api.github.com/users/juliensimon/followers", "following_url": "https://api.github.com/users/juliensimon/following{/other_user}", "gists_url": "https://api.github.com/users/juliensimon/gists{/gist_id}", "starred_url": "https://api.github.com/users/juliensimon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/juliensimon/subscriptions", "organizations_url": "https://api.github.com/users/juliensimon/orgs", "repos_url": "https://api.github.com/users/juliensimon/repos", "events_url": "https://api.github.com/users/juliensimon/events{/privacy}", "received_events_url": "https://api.github.com/users/juliensimon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-07-22T13:28:28Z", "updated_at": "2022-09-01T15:37:11Z", "closed_at": "2022-09-01T15:37:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"scene_parse_150\", \"scene_parsing\")\r\n\r\nFileNotFoundError: Couldn't find file at http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4737/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4734", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4734/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4734/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4734/events", "html_url": "https://github.com/huggingface/datasets/issues/4734", "id": 1314495382, "node_id": "I_kwDODunzps5OWZuW", "number": 4734, "title": "Package rouge-score cannot be imported", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-07-22T07:15:05Z", "updated_at": "2022-07-22T07:45:19Z", "closed_at": "2022-07-22T07:45:18Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAfter the today release of `rouge_score-0.0.7` it seems no longer importable. Our CI fails: https://github.com/huggingface/datasets/runs/7463218591?check_suite_focus=true\r\n```\r\nFAILED tests/test_dataset_common.py::LocalDatasetTest::test_builder_class_bigbench\r\nFAILED tests/test_dataset_common.py::LocalDatasetTest::test_builder_configs_bigbench\r\nFAILED tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_bigbench\r\nFAILED tests/test_metric_common.py::LocalMetricTest::test_load_metric_rouge\r\n```\r\nwith errors:\r\n```\r\n>   from rouge_score import rouge_scorer\r\nE   ModuleNotFoundError: No module named 'rouge_score'\r\n```\r\n```\r\nE           ImportError: To be able to use rouge, you need to install the following dependency: rouge_score.\r\nE           Please install it using 'pip install rouge_score' for instance'\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4734/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4734/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4733", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4733/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4733/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4733/events", "html_url": "https://github.com/huggingface/datasets/issues/4733", "id": 1314479616, "node_id": "I_kwDODunzps5OWV4A", "number": 4733, "title": "rouge metric", "user": {"login": "asking28", "id": 29248466, "node_id": "MDQ6VXNlcjI5MjQ4NDY2", "avatar_url": "https://avatars.githubusercontent.com/u/29248466?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asking28", "html_url": "https://github.com/asking28", "followers_url": "https://api.github.com/users/asking28/followers", "following_url": "https://api.github.com/users/asking28/following{/other_user}", "gists_url": "https://api.github.com/users/asking28/gists{/gist_id}", "starred_url": "https://api.github.com/users/asking28/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asking28/subscriptions", "organizations_url": "https://api.github.com/users/asking28/orgs", "repos_url": "https://api.github.com/users/asking28/repos", "events_url": "https://api.github.com/users/asking28/events{/privacy}", "received_events_url": "https://api.github.com/users/asking28/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-07-22T07:06:51Z", "updated_at": "2022-07-22T09:08:02Z", "closed_at": "2022-07-22T09:05:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\nLoading  Rouge metric gives error after latest rouge-score==0.0.7 release.\r\nDowngrading rougemetric==0.0.4 works fine.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\nfrom rouge_score import rouge_scorer, scoring \r\nshould run\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\nFile \"/root/.cache/huggingface/modules/datasets_modules/metrics/rouge/0ffdb60f436bdb8884d5e4d608d53dbe108e82dac4f494a66f80ef3f647c104f/rouge.py\", line 21, in <module>\r\n    from rouge_score import rouge_scorer, scoring\r\nImportError: cannot import name 'rouge_scorer' from 'rouge_score' (unknown location)\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform: Linux\r\n- Python version:3.9\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4733/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4730", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4730/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4730/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4730/events", "html_url": "https://github.com/huggingface/datasets/issues/4730", "id": 1313421263, "node_id": "I_kwDODunzps5OSTfP", "number": 4730, "title": "Loading imagenet-1k validation split takes much more RAM than expected", "user": {"login": "fxmarty", "id": 9808326, "node_id": "MDQ6VXNlcjk4MDgzMjY=", "avatar_url": "https://avatars.githubusercontent.com/u/9808326?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fxmarty", "html_url": "https://github.com/fxmarty", "followers_url": "https://api.github.com/users/fxmarty/followers", "following_url": "https://api.github.com/users/fxmarty/following{/other_user}", "gists_url": "https://api.github.com/users/fxmarty/gists{/gist_id}", "starred_url": "https://api.github.com/users/fxmarty/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fxmarty/subscriptions", "organizations_url": "https://api.github.com/users/fxmarty/orgs", "repos_url": "https://api.github.com/users/fxmarty/repos", "events_url": "https://api.github.com/users/fxmarty/events{/privacy}", "received_events_url": "https://api.github.com/users/fxmarty/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-07-21T15:14:06Z", "updated_at": "2022-07-21T16:41:04Z", "closed_at": "2022-07-21T16:41:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nLoading into memory the validation split of imagenet-1k takes much more RAM than expected. Assuming ImageNet-1k is 150 GB, split is 50000 validation images and 1,281,167 train images, I would expect only about 6 GB loaded in RAM.\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import  load_dataset\r\n\r\ndataset = load_dataset(\"imagenet-1k\", split=\"validation\")\r\n\r\nprint(dataset)\r\n\r\n\"\"\"prints\r\nDataset({\r\n    features: ['image', 'label'],\r\n    num_rows: 50000\r\n})\r\n\"\"\"\r\n\r\npipe_inputs = dataset[\"image\"]\r\n# and wait :-)\r\n```\r\n\r\n## Expected results\r\nUse only < 10 GB RAM when loading the images.\r\n\r\n## Actual results\r\n![image](https://user-images.githubusercontent.com/9808326/180249183-62f75ca4-d127-402a-9330-f12825a22b0a.png)\r\n\r\n```\r\nUsing custom data configuration default\r\nReusing dataset imagenet-1k (/home/fxmarty/.cache/huggingface/datasets/imagenet-1k/default/1.0.0/a1e9bfc56c3a7350165007d1176b15e9128fcaf9ab972147840529aed3ae52bc)\r\nKilled\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.3.3.dev0\r\n- Platform: Linux-5.15.0-41-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.12\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.3.5\r\n- datasets commit: 4e4222f1b6362c2788aec0dd2cd8cede6dd17b80\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4730/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4725", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4725/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4725/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4725/events", "html_url": "https://github.com/huggingface/datasets/issues/4725", "id": 1311907096, "node_id": "I_kwDODunzps5OMh0Y", "number": 4725, "title": "the_pile datasets URL broken. ", "user": {"login": "TrentBrick", "id": 12433427, "node_id": "MDQ6VXNlcjEyNDMzNDI3", "avatar_url": "https://avatars.githubusercontent.com/u/12433427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TrentBrick", "html_url": "https://github.com/TrentBrick", "followers_url": "https://api.github.com/users/TrentBrick/followers", "following_url": "https://api.github.com/users/TrentBrick/following{/other_user}", "gists_url": "https://api.github.com/users/TrentBrick/gists{/gist_id}", "starred_url": "https://api.github.com/users/TrentBrick/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TrentBrick/subscriptions", "organizations_url": "https://api.github.com/users/TrentBrick/orgs", "repos_url": "https://api.github.com/users/TrentBrick/repos", "events_url": "https://api.github.com/users/TrentBrick/events{/privacy}", "received_events_url": "https://api.github.com/users/TrentBrick/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-07-20T20:57:30Z", "updated_at": "2022-07-22T06:09:46Z", "closed_at": "2022-07-21T07:38:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://github.com/huggingface/datasets/pull/3627 changed the Eleuther AI Pile dataset URL from https://the-eye.eu/ to https://mystic.the-eye.eu/ but the latter is now broken and the former works again. \r\n\r\nNote that when I git clone the repo and use `pip install -e .` and then edit the URL back the codebase doesn't seem to use this edit so the mystic URL is also cached somewhere else that I can't find? ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4725/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4725/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4696", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4696/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4696/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4696/events", "html_url": "https://github.com/huggingface/datasets/issues/4696", "id": 1307183099, "node_id": "I_kwDODunzps5N6gf7", "number": 4696, "title": "Cannot load LinCE dataset", "user": {"login": "finiteautomata", "id": 167943, "node_id": "MDQ6VXNlcjE2Nzk0Mw==", "avatar_url": "https://avatars.githubusercontent.com/u/167943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/finiteautomata", "html_url": "https://github.com/finiteautomata", "followers_url": "https://api.github.com/users/finiteautomata/followers", "following_url": "https://api.github.com/users/finiteautomata/following{/other_user}", "gists_url": "https://api.github.com/users/finiteautomata/gists{/gist_id}", "starred_url": "https://api.github.com/users/finiteautomata/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/finiteautomata/subscriptions", "organizations_url": "https://api.github.com/users/finiteautomata/orgs", "repos_url": "https://api.github.com/users/finiteautomata/repos", "events_url": "https://api.github.com/users/finiteautomata/events{/privacy}", "received_events_url": "https://api.github.com/users/finiteautomata/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-07-17T19:01:54Z", "updated_at": "2022-07-18T09:20:40Z", "closed_at": "2022-07-18T07:24:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nCannot load LinCE dataset due to a connection error\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"lince\", \"ner_spaeng\")\r\n```\r\n\r\nA notebook with this code and corresponding error can be found at https://colab.research.google.com/drive/1pgX3bNB9amuUwAVfPFm-XuMV5fEg-cD2\r\n\r\n## Expected results\r\n\r\nIt should load the dataset\r\n\r\n## Actual results\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nConnectionError                           Traceback (most recent call last)\r\n<ipython-input-2-fc551ddcebef> in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 dataset = load_dataset(\"lince\", \"ner_spaeng\")\r\n\r\n10 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1682         ignore_verifications=ignore_verifications,\r\n   1683         try_from_hf_gcs=try_from_hf_gcs,\r\n-> 1684         use_auth_token=use_auth_token,\r\n   1685     )\r\n   1686 \r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    703                     if not downloaded_from_gcs:\r\n    704                         self._download_and_prepare(\r\n--> 705                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    706                         )\r\n    707                     # Sync info\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos)\r\n   1219 \r\n   1220     def _download_and_prepare(self, dl_manager, verify_infos):\r\n-> 1221         super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n   1222 \r\n   1223     def _get_examples_iterable_for_split(self, split_generator: SplitGenerator) -> ExamplesIterable:\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    769         split_dict = SplitDict(dataset_name=self.name)\r\n    770         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 771         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    772 \r\n    773         # Checksums verification\r\n\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/lince/10d41747f55f0849fa84ac579ea1acfa7df49aa2015b60426bc459c111b3d589/lince.py in _split_generators(self, dl_manager)\r\n    481     def _split_generators(self, dl_manager):\r\n    482         \"\"\"Returns SplitGenerators.\"\"\"\r\n--> 483         lince_dir = dl_manager.download_and_extract(f\"{_LINCE_URL}/{self.config.name}.zip\")\r\n    484         data_dir = os.path.join(lince_dir, self.config.data_dir)\r\n    485         return [\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/download/download_manager.py in download_and_extract(self, url_or_urls)\r\n    429             extracted_path(s): `str`, extracted paths of given URL(s).\r\n    430         \"\"\"\r\n--> 431         return self.extract(self.download(url_or_urls))\r\n    432 \r\n    433     def get_recorded_sizes_checksums(self):\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/download/download_manager.py in download(self, url_or_urls)\r\n    313             num_proc=download_config.num_proc,\r\n    314             disable_tqdm=not is_progress_bar_enabled(),\r\n--> 315             desc=\"Downloading data files\",\r\n    316         )\r\n    317         duration = datetime.now() - start_time\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm, desc)\r\n    346     # Singleton\r\n    347     if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 348         return function(data_struct)\r\n    349 \r\n    350     disable_tqdm = disable_tqdm or not logging.is_progress_bar_enabled()\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/download/download_manager.py in _download(self, url_or_filename, download_config)\r\n    333             # append the relative path to the base_path\r\n    334             url_or_filename = url_or_path_join(self._base_path, url_or_filename)\r\n--> 335         return cached_path(url_or_filename, download_config=download_config)\r\n    336 \r\n    337     def iter_archive(self, path_or_buf: Union[str, io.BufferedReader]):\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    195             use_auth_token=download_config.use_auth_token,\r\n    196             ignore_url_params=download_config.ignore_url_params,\r\n--> 197             download_desc=download_config.download_desc,\r\n    198         )\r\n    199     elif os.path.exists(url_or_filename):\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, download_desc)\r\n    531         _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\r\n    532         if head_error is not None:\r\n--> 533             raise ConnectionError(f\"Couldn't reach {url} ({repr(head_error)})\")\r\n    534         elif response is not None:\r\n    535             raise ConnectionError(f\"Couldn't reach {url} (error {response.status_code})\")\r\n\r\nConnectionError: Couldn't reach https://ritual.uh.edu/lince/libaccess/eyJ1c2VybmFtZSI6ICJodWdnaW5nZmFjZSBubHAiLCAidXNlcl9pZCI6IDExMSwgImVtYWlsIjogImR1bW15QGVtYWlsLmNvbSJ9/ner_spaeng.zip (ConnectTimeout(MaxRetryError(\"HTTPSConnectionPool(host='ritual.uh.edu', port=443): Max retries exceeded with url: /lince/libaccess/eyJ1c2VybmFtZSI6ICJodWdnaW5nZmFjZSBubHAiLCAidXNlcl9pZCI6IDExMSwgImVtYWlsIjogImR1bW15QGVtYWlsLmNvbSJ9/ner_spaeng.zip (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7feb1c45a690>, 'Connection to ritual.uh.edu timed out. (connect timeout=100)'))\")))\r\n```\r\n\r\n## Environment info\r\n\r\n\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4696/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4692", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4692/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4692/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4692/events", "html_url": "https://github.com/huggingface/datasets/issues/4692", "id": 1306609680, "node_id": "I_kwDODunzps5N4UgQ", "number": 4692, "title": "Unable to cast a column with `Image()` by using the `cast_column()` feature", "user": {"login": "skrishnan99", "id": 28833916, "node_id": "MDQ6VXNlcjI4ODMzOTE2", "avatar_url": "https://avatars.githubusercontent.com/u/28833916?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skrishnan99", "html_url": "https://github.com/skrishnan99", "followers_url": "https://api.github.com/users/skrishnan99/followers", "following_url": "https://api.github.com/users/skrishnan99/following{/other_user}", "gists_url": "https://api.github.com/users/skrishnan99/gists{/gist_id}", "starred_url": "https://api.github.com/users/skrishnan99/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skrishnan99/subscriptions", "organizations_url": "https://api.github.com/users/skrishnan99/orgs", "repos_url": "https://api.github.com/users/skrishnan99/repos", "events_url": "https://api.github.com/users/skrishnan99/events{/privacy}", "received_events_url": "https://api.github.com/users/skrishnan99/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-07-15T22:56:03Z", "updated_at": "2022-07-19T13:36:24Z", "closed_at": "2022-07-19T13:36:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\nWhen I create a dataset, then add a column to the created dataset through the `dataset.add_column` feature and then try to cast a column of the dataset (this column contains image paths) with `Image()` by using the `cast_column()` feature, I get the following error - ``` TypeError: Couldn't cast array of type\r\nstring\r\nto\r\n{'bytes': Value(dtype='binary', id=None), 'path': Value(dtype='string', id=None)} ```\r\n\r\nWhen I try and cast the same column, but without doing the `add_column` in the previous step, it works as expected.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import Dataset, Image\r\n\r\n\r\ndata_dict = {\r\n    \"img_path\": [\"https://picsum.photos/200/300\"]\r\n}\r\n\r\ndataset = Dataset.from_dict(data_dict)\r\n\r\n#NOTE Comment out this line and use cast_column and it works properly\r\ndataset = dataset.add_column(\"yeet\", [1])\r\n\r\n#NOTE This line fails to execute properly if `add_column` is called before\r\ndataset = dataset.cast_column(\"img_path\", Image())\r\n\r\n# #NOTE This is my current workaround. This seems to work fine with/without `add_column`. While\r\n# # running this, make sure to comment out the `cast_column` line\r\n# new_features = dataset.features.copy()\r\n# new_features[\"img_path\"] = Image()\r\n# dataset = dataset.cast(new_features)\r\n\r\n\r\nprint(dataset)\r\nprint(dataset.features)\r\nprint(dataset[0])\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\nAble to successfully use `cast_column` to cast a column containing img_paths to now be Image() features after modifying the dataset using `add_column` in a previous step\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/surya/Desktop/hf_bug_test.py\", line 14, in <module>\r\n    dataset = dataset.cast_column(\"img_path\", Image())\r\n  File \"/home/surya/anaconda3/envs/snap_test/lib/python3.9/site-packages/datasets/fingerprint.py\", line 458, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/surya/anaconda3/envs/snap_test/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 1580, in cast_column\r\n    dataset._data = dataset._data.cast(dataset.features.arrow_schema)\r\n  File \"/home/surya/anaconda3/envs/snap_test/lib/python3.9/site-packages/datasets/table.py\", line 1487, in cast\r\n    new_tables.append(subtable.cast(subschema, *args, **kwargs))\r\n  File \"/home/surya/anaconda3/envs/snap_test/lib/python3.9/site-packages/datasets/table.py\", line 834, in cast\r\n    return InMemoryTable(table_cast(self.table, *args, **kwargs))\r\n  File \"/home/surya/anaconda3/envs/snap_test/lib/python3.9/site-packages/datasets/table.py\", line 1897, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"/home/surya/anaconda3/envs/snap_test/lib/python3.9/site-packages/datasets/table.py\", line 1880, in cast_table_to_schema\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"/home/surya/anaconda3/envs/snap_test/lib/python3.9/site-packages/datasets/table.py\", line 1880, in <listcomp>\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"/home/surya/anaconda3/envs/snap_test/lib/python3.9/site-packages/datasets/table.py\", line 1673, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/home/surya/anaconda3/envs/snap_test/lib/python3.9/site-packages/datasets/table.py\", line 1673, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/home/surya/anaconda3/envs/snap_test/lib/python3.9/site-packages/datasets/table.py\", line 1846, in cast_array_to_feature\r\n    raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\nTypeError: Couldn't cast array of type\r\nstring\r\nto\r\n{'bytes': Value(dtype='binary', id=None), 'path': Value(dtype='string', id=None)}\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.3.2\r\n- Platform: Ubuntu 20.04.3 LTS\r\n- Python version: 3.9.7\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4692/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4692/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4681", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4681/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4681/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4681/events", "html_url": "https://github.com/huggingface/datasets/issues/4681", "id": 1304617484, "node_id": "I_kwDODunzps5NwuIM", "number": 4681, "title": "IndexError when loading ImageFolder", "user": {"login": "johko", "id": 2843485, "node_id": "MDQ6VXNlcjI4NDM0ODU=", "avatar_url": "https://avatars.githubusercontent.com/u/2843485?v=4", "gravatar_id": "", "url": "https://api.github.com/users/johko", "html_url": "https://github.com/johko", "followers_url": "https://api.github.com/users/johko/followers", "following_url": "https://api.github.com/users/johko/following{/other_user}", "gists_url": "https://api.github.com/users/johko/gists{/gist_id}", "starred_url": "https://api.github.com/users/johko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/johko/subscriptions", "organizations_url": "https://api.github.com/users/johko/orgs", "repos_url": "https://api.github.com/users/johko/repos", "events_url": "https://api.github.com/users/johko/events{/privacy}", "received_events_url": "https://api.github.com/users/johko/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-07-14T10:57:55Z", "updated_at": "2022-07-25T12:37:54Z", "closed_at": "2022-07-25T12:37:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading an image dataset with `imagefolder` throws `IndexError: list index out of range` when the given folder contains a non-image file (like a csv).\r\n\r\n## Steps to reproduce the bug\r\nPut a csv file in a folder with images and load it:\r\n```python\r\nimport datasets\r\n datasets.load_dataset(\"imagefolder\", data_dir=path/to/folder)\r\n```\r\n\r\n## Expected results\r\nI would expect a better error message, like `Unsupported file` or even the dataset loader just ignoring every file that is not an image in that case.\r\n\r\n## Actual results\r\nHere is the whole traceback:\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.11.0-051100-generic-x86_64-with-glibc2.27\r\n- Python version: 3.9.9\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.3\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4681/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4677", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4677/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4677/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4677/events", "html_url": "https://github.com/huggingface/datasets/issues/4677", "id": 1302258440, "node_id": "I_kwDODunzps5NnuMI", "number": 4677, "title": "Random 400 Client Error when pushing dataset", "user": {"login": "msis", "id": 577139, "node_id": "MDQ6VXNlcjU3NzEzOQ==", "avatar_url": "https://avatars.githubusercontent.com/u/577139?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msis", "html_url": "https://github.com/msis", "followers_url": "https://api.github.com/users/msis/followers", "following_url": "https://api.github.com/users/msis/following{/other_user}", "gists_url": "https://api.github.com/users/msis/gists{/gist_id}", "starred_url": "https://api.github.com/users/msis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msis/subscriptions", "organizations_url": "https://api.github.com/users/msis/orgs", "repos_url": "https://api.github.com/users/msis/repos", "events_url": "https://api.github.com/users/msis/events{/privacy}", "received_events_url": "https://api.github.com/users/msis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-07-12T15:56:44Z", "updated_at": "2023-02-07T13:54:10Z", "closed_at": "2023-02-07T13:54:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen pushing a dataset, the client errors randomly with `Bad Request for url:...`.\r\nAt the next call, a new parquet file is created for each shard.\r\nThe client may fail at any random shard.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset.push_to_hub(\"ORG/DATASET\", private=True, branch=\"main\")\r\n```\r\n\r\n## Expected results\r\nPush all the dataset to the Hub with no duplicates. \r\nIf it fails, it should retry or fail, but continue from the last failed shard.\r\n\r\n## Actual results\r\n```\r\n---------------------------------------------------------------------------\r\nHTTPError                                 Traceback (most recent call last)\r\ntesting.ipynb Cell 29 in <cell line: 1>()\r\n----> [1](testing.ipynb?line=0) dataset.push_to_hub(\"ORG/DATASET\", private=True, branch=\"main\")\r\n\r\nFile ~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py:4297, in Dataset.push_to_hub(self, repo_id, split, private, token, branch, max_shard_size, shard_size, embed_external_files)\r\n   4291     warnings.warn(\r\n   4292         \"'shard_size' was renamed to 'max_shard_size' in version 2.1.1 and will be removed in 2.4.0.\",\r\n   4293         FutureWarning,\r\n   4294     )\r\n   4295     max_shard_size = shard_size\r\n-> 4297 repo_id, split, uploaded_size, dataset_nbytes, repo_files, deleted_size = self._push_parquet_shards_to_hub(\r\n   4298     repo_id=repo_id,\r\n   4299     split=split,\r\n   4300     private=private,\r\n   4301     token=token,\r\n   4302     branch=branch,\r\n   4303     max_shard_size=max_shard_size,\r\n   4304     embed_external_files=embed_external_files,\r\n   4305 )\r\n   4306 organization, dataset_name = repo_id.split(\"/\")\r\n   4307 info_to_dump = self.info.copy()\r\n\r\nFile ~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py:4195, in Dataset._push_parquet_shards_to_hub(self, repo_id, split, private, token, branch, max_shard_size, embed_external_files)\r\n   4193         shard.to_parquet(buffer)\r\n   4194         uploaded_size += buffer.tell()\r\n-> 4195         _retry(\r\n   4196             api.upload_file,\r\n   4197             func_kwargs=dict(\r\n   4198                 path_or_fileobj=buffer.getvalue(),\r\n   4199                 path_in_repo=shard_path_in_repo,\r\n   4200                 repo_id=repo_id,\r\n   4201                 token=token,\r\n   4202                 repo_type=\"dataset\",\r\n   4203                 revision=branch,\r\n   4204                 identical_ok=False,\r\n   4205             ),\r\n   4206             exceptions=HTTPError,\r\n   4207             status_codes=[504],\r\n   4208             base_wait_time=2.0,\r\n   4209             max_retries=5,\r\n   4210             max_wait_time=20.0,\r\n   4211         )\r\n   4212     shards_path_in_repo.append(shard_path_in_repo)\r\n   4214 # Cleanup to remove unused files\r\n\r\nFile ~/.local/lib/python3.9/site-packages/datasets/utils/file_utils.py:284, in _retry(func, func_args, func_kwargs, exceptions, status_codes, max_retries, base_wait_time, max_wait_time)\r\n    282 except exceptions as err:\r\n    283     if retry >= max_retries or (status_codes and err.response.status_code not in status_codes):\r\n--> 284         raise err\r\n    285     else:\r\n    286         sleep_time = min(max_wait_time, base_wait_time * 2**retry)  # Exponential backoff\r\n\r\nFile ~/.local/lib/python3.9/site-packages/datasets/utils/file_utils.py:281, in _retry(func, func_args, func_kwargs, exceptions, status_codes, max_retries, base_wait_time, max_wait_time)\r\n    279 while True:\r\n    280     try:\r\n--> 281         return func(*func_args, **func_kwargs)\r\n    282     except exceptions as err:\r\n    283         if retry >= max_retries or (status_codes and err.response.status_code not in status_codes):\r\n\r\nFile ~/.local/lib/python3.9/site-packages/huggingface_hub/hf_api.py:1967, in HfApi.upload_file(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, identical_ok, commit_message, commit_description, create_pr)\r\n   1957 commit_message = (\r\n   1958     commit_message\r\n   1959     if commit_message is not None\r\n   1960     else f\"Upload {path_in_repo} with huggingface_hub\"\r\n   1961 )\r\n   1962 operation = CommitOperationAdd(\r\n   1963     path_or_fileobj=path_or_fileobj,\r\n   1964     path_in_repo=path_in_repo,\r\n   1965 )\r\n-> 1967 pr_url = self.create_commit(\r\n   1968     repo_id=repo_id,\r\n   1969     repo_type=repo_type,\r\n   1970     operations=[operation],\r\n   1971     commit_message=commit_message,\r\n   1972     commit_description=commit_description,\r\n   1973     token=token,\r\n   1974     revision=revision,\r\n   1975     create_pr=create_pr,\r\n   1976 )\r\n   1977 if pr_url is not None:\r\n   1978     re_match = re.match(REGEX_DISCUSSION_URL, pr_url)\r\n\r\nFile ~/.local/lib/python3.9/site-packages/huggingface_hub/hf_api.py:1844, in HfApi.create_commit(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads)\r\n   1836 commit_url = f\"{self.endpoint}/api/{repo_type}s/{repo_id}/commit/{revision}\"\r\n   1838 commit_resp = requests.post(\r\n   1839     url=commit_url,\r\n   1840     headers={\"Authorization\": f\"Bearer {token}\"},\r\n   1841     json=commit_payload,\r\n   1842     params={\"create_pr\": 1} if create_pr else None,\r\n   1843 )\r\n-> 1844 _raise_for_status(commit_resp)\r\n   1845 return commit_resp.json().get(\"pullRequestUrl\", None)\r\n\r\nFile ~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:84, in _raise_for_status(request)\r\n     76 if request.status_code == 401:\r\n     77     # The repo was not found and the user is not Authenticated\r\n     78     raise RepositoryNotFoundError(\r\n     79         f\"401 Client Error: Repository Not Found for url: {request.url}. If the\"\r\n     80         \" repo is private, make sure you are authenticated. (Request ID:\"\r\n     81         f\" {request_id})\"\r\n     82     )\r\n---> 84 _raise_with_request_id(request)\r\n\r\nFile ~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:95, in _raise_with_request_id(request)\r\n     92 if request_id is not None and len(e.args) > 0 and isinstance(e.args[0], str):\r\n     93     e.args = (e.args[0] + f\" (Request ID: {request_id})\",) + e.args[1:]\r\n---> 95 raise e\r\n\r\nFile ~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:90, in _raise_with_request_id(request)\r\n     88 request_id = request.headers.get(\"X-Request-Id\")\r\n     89 try:\r\n---> 90     request.raise_for_status()\r\n     91 except Exception as e:\r\n     92     if request_id is not None and len(e.args) > 0 and isinstance(e.args[0], str):\r\n\r\nFile ~/.local/lib/python3.9/site-packages/requests/models.py:1021, in Response.raise_for_status(self)\r\n   1016     http_error_msg = (\r\n   1017         f\"{self.status_code} Server Error: {reason} for url: {self.url}\"\r\n   1018     )\r\n   1020 if http_error_msg:\r\n-> 1021     raise HTTPError(http_error_msg, response=self)\r\n\r\nHTTPError: 400 Client Error: Bad Request for url: https://huggingface.co/api/datasets/ORG/DATASET/commit/main (Request ID: a_F0IQAHJdxGKVRYyu1cF)\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.13.0-1025-aws-x86_64-with-glibc2.31\r\n- Python version: 3.9.4\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.3\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4677/timeline", "performed_via_github_app": null, "state_reason": "not_planned"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4676", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4676/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4676/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4676/events", "html_url": "https://github.com/huggingface/datasets/issues/4676", "id": 1302202028, "node_id": "I_kwDODunzps5Nngas", "number": 4676, "title": "Dataset.map gets stuck on _cast_to_python_objects", "user": {"login": "srobertjames", "id": 662612, "node_id": "MDQ6VXNlcjY2MjYxMg==", "avatar_url": "https://avatars.githubusercontent.com/u/662612?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srobertjames", "html_url": "https://github.com/srobertjames", "followers_url": "https://api.github.com/users/srobertjames/followers", "following_url": "https://api.github.com/users/srobertjames/following{/other_user}", "gists_url": "https://api.github.com/users/srobertjames/gists{/gist_id}", "starred_url": "https://api.github.com/users/srobertjames/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srobertjames/subscriptions", "organizations_url": "https://api.github.com/users/srobertjames/orgs", "repos_url": "https://api.github.com/users/srobertjames/repos", "events_url": "https://api.github.com/users/srobertjames/events{/privacy}", "received_events_url": "https://api.github.com/users/srobertjames/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892877, "node_id": "MDU6TGFiZWwxOTM1ODkyODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": {"login": "szmoro", "id": 5697926, "node_id": "MDQ6VXNlcjU2OTc5MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/5697926?v=4", "gravatar_id": "", "url": "https://api.github.com/users/szmoro", "html_url": "https://github.com/szmoro", "followers_url": "https://api.github.com/users/szmoro/followers", "following_url": "https://api.github.com/users/szmoro/following{/other_user}", "gists_url": "https://api.github.com/users/szmoro/gists{/gist_id}", "starred_url": "https://api.github.com/users/szmoro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/szmoro/subscriptions", "organizations_url": "https://api.github.com/users/szmoro/orgs", "repos_url": "https://api.github.com/users/szmoro/repos", "events_url": "https://api.github.com/users/szmoro/events{/privacy}", "received_events_url": "https://api.github.com/users/szmoro/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "szmoro", "id": 5697926, "node_id": "MDQ6VXNlcjU2OTc5MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/5697926?v=4", "gravatar_id": "", "url": "https://api.github.com/users/szmoro", "html_url": "https://github.com/szmoro", "followers_url": "https://api.github.com/users/szmoro/followers", "following_url": "https://api.github.com/users/szmoro/following{/other_user}", "gists_url": "https://api.github.com/users/szmoro/gists{/gist_id}", "starred_url": "https://api.github.com/users/szmoro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/szmoro/subscriptions", "organizations_url": "https://api.github.com/users/szmoro/orgs", "repos_url": "https://api.github.com/users/szmoro/repos", "events_url": "https://api.github.com/users/szmoro/events{/privacy}", "received_events_url": "https://api.github.com/users/szmoro/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2022-07-12T15:09:58Z", "updated_at": "2022-10-03T13:01:04Z", "closed_at": "2022-10-03T13:01:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n`Dataset.map`, when fed a Huggingface Tokenizer as its map func, can sometimes spend huge amounts of time doing casts.  A minimal example follows.\r\n\r\nNot all usages suffer from this.  For example, I profiled the preprocessor at https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb , and it did _not_ have this problem.  However, I'm at a loss to figure out how it avoids it, as the example below is simple and minimal and still has this problem.\r\n\r\nThis casting, where it occurs, causes the `Dataset.map` to run approximately 7x slower than it runs for code which does not cause this casting.\r\n\r\nThis may be related to https://github.com/huggingface/datasets/issues/1046 .  However, the tokenizer is _not_ set to return Tensors.\r\n\r\n## Steps to reproduce the bug\r\nA minimal, self-contained example to reproduce is below:\r\n```python\r\nimport transformers\r\nfrom transformers import AutoTokenizer\r\nfrom datasets import load_dataset\r\nimport torch\r\nimport cProfile\r\n\r\npretrained = 'distilbert-base-uncased'\r\ntokenizer = AutoTokenizer.from_pretrained(pretrained)\r\n\r\nsquad = load_dataset('squad')\r\nsquad_train = squad['train']\r\nsquad_tiny = squad_train.select(range(5000))\r\n\r\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\r\n\r\ndef tokenize(ds):\r\n        tokens = tokenizer(text=ds['question'],\r\n                                text_pair=ds['context'],\r\n                                add_special_tokens=True,\r\n                                padding='max_length',\r\n                                truncation='only_second',\r\n                                max_length=160,\r\n                                stride=32,\r\n                                return_overflowing_tokens=True,\r\n                                return_offsets_mapping=True,\r\n                                )\r\n        return tokens\r\n\r\ncmd = 'squad_tiny.map(tokenize, batched=True, remove_columns=squad_tiny.column_names)'\r\ncProfile.run(cmd, sort='tottime')\r\n```\r\n\r\n## Actual results\r\nThe code works, but takes 10-25 sec per batch (about 7x slower than non-casting code), with the following profile.  Note that `_cast_to_python_objects` is the culprit.\r\n```\r\n    63524075 function calls (58206482 primitive calls) in 121.836 seconds\r\n\r\n   Ordered by: internal time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n5274034/40   68.751    0.000  111.060    2.776 features.py:262(_cast_to_python_objects)\r\n 42223832   24.077    0.000   33.310    0.000 {built-in method builtins.isinstance}\r\n 16338/20    5.121    0.000  111.053    5.553 features.py:361(<listcomp>)\r\n  5274135    4.747    0.000    4.749    0.000 {built-in method _abc._abc_instancecheck}\r\n    80/40    4.731    0.059  116.292    2.907 {pyarrow.lib.array}\r\n  5274135    4.485    0.000    9.234    0.000 abc.py:96(__instancecheck__)\r\n2661564/2645196    2.959    0.000    4.298    0.000 features.py:1081(_check_non_null_non_empty_recursive)\r\n        5    2.786    0.557    2.786    0.557 {method 'encode_batch' of 'tokenizers.Tokenizer' objects}\r\n  2668052    0.930    0.000    0.930    0.000 {built-in method builtins.len}\r\n     5000    0.930    0.000    0.938    0.000 tokenization_utils_fast.py:187(_convert_encoding)\r\n        5    0.750    0.150    0.808    0.162 {method 'to_pydict' of 'pyarrow.lib.Table' objects}\r\n        1    0.444    0.444  121.749  121.749 arrow_dataset.py:2501(_map_single)\r\n       40    0.375    0.009  116.291    2.907 arrow_writer.py:151(__arrow_array__)\r\n       10    0.066    0.007    0.066    0.007 {method 'write_batch' of 'pyarrow.lib._CRecordBatchWriter' objects}\r\n        1    0.060    0.060  121.835  121.835 fingerprint.py:409(wrapper)\r\n11387/5715    0.049    0.000    0.175    0.000 {built-in method builtins.getattr}\r\n       36    0.049    0.001    0.049    0.001 {pyarrow._compute.call_function}\r\n    15000    0.040    0.000    0.040    0.000 _collections_abc.py:719(__iter__)\r\n        3    0.023    0.008    0.023    0.008 {built-in method _imp.create_dynamic}\r\n       77    0.020    0.000    0.020    0.000 {built-in method builtins.dir}\r\n       37    0.019    0.001    0.019    0.001 socket.py:543(send)\r\n       15    0.017    0.001    0.017    0.001 tokenization_utils_fast.py:460(<listcomp>)\r\n  432/421    0.015    0.000    0.024    0.000 traitlets.py:1388(_notify_observers)\r\n     5000    0.015    0.000    0.018    0.000 _collections_abc.py:672(keys)\r\n       51    0.014    0.000    0.042    0.001 traitlets.py:276(getmembers)\r\n        5    0.014    0.003    3.775    0.755 tokenization_utils_fast.py:392(_batch_encode_plus)\r\n      3/1    0.014    0.005    0.035    0.035 {built-in method _imp.exec_dynamic}\r\n        5    0.012    0.002    0.950    0.190 tokenization_utils_fast.py:438(<listcomp>)\r\n    31626    0.012    0.000    0.012    0.000 {method 'append' of 'list' objects}\r\n1532/1001    0.011    0.000    0.189    0.000 traitlets.py:643(get)\r\n        5    0.009    0.002    3.796    0.759 arrow_dataset.py:2631(apply_function_on_filtered_inputs)\r\n       51    0.009    0.000    0.062    0.001 traitlets.py:1766(traits)\r\n        5    0.008    0.002    3.784    0.757 tokenization_utils_base.py:2632(batch_encode_plus)\r\n      368    0.007    0.000    0.044    0.000 traitlets.py:1715(_get_trait_default_generator)\r\n       26    0.007    0.000    0.022    0.001 traitlets.py:1186(setup_instance)\r\n       51    0.006    0.000    0.010    0.000 traitlets.py:1781(<listcomp>)\r\n    80/32    0.006    0.000    0.052    0.002 table.py:1758(cast_array_to_feature)\r\n      684    0.006    0.000    0.007    0.000 {method 'items' of 'dict' objects}\r\n4344/1794    0.006    0.000    0.192    0.000 traitlets.py:675(__get__)\r\n...\r\n```\r\n## Environment info\r\nI observed this on both Google colab and my local workstation:\r\n\r\n### Google colab\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n\r\n### Local\r\n- `datasets` version: 2.3.2\r\n- Platform: Windows-7-6.1.7601-SP1\r\n- Python version: 3.8.10\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.3\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4676/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4676/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4674", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4674/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4674/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4674/events", "html_url": "https://github.com/huggingface/datasets/issues/4674", "id": 1301294844, "node_id": "I_kwDODunzps5NkC78", "number": 4674, "title": "Issue loading datasets -- pyarrow.lib has no attribute", "user": {"login": "margotwagner", "id": 39107794, "node_id": "MDQ6VXNlcjM5MTA3Nzk0", "avatar_url": "https://avatars.githubusercontent.com/u/39107794?v=4", "gravatar_id": "", "url": "https://api.github.com/users/margotwagner", "html_url": "https://github.com/margotwagner", "followers_url": "https://api.github.com/users/margotwagner/followers", "following_url": "https://api.github.com/users/margotwagner/following{/other_user}", "gists_url": "https://api.github.com/users/margotwagner/gists{/gist_id}", "starred_url": "https://api.github.com/users/margotwagner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/margotwagner/subscriptions", "organizations_url": "https://api.github.com/users/margotwagner/orgs", "repos_url": "https://api.github.com/users/margotwagner/repos", "events_url": "https://api.github.com/users/margotwagner/events{/privacy}", "received_events_url": "https://api.github.com/users/margotwagner/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-07-11T22:10:44Z", "updated_at": "2023-02-28T18:06:55Z", "closed_at": "2023-02-28T18:06:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to load sentiment analysis datasets from huggingface, but any dataset I try to use via load_dataset, I get the same error:\r\n`AttributeError: module 'pyarrow.lib' has no attribute 'IpcReadOptions'`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = load_dataset(\"glue\", \"cola\")\r\n```\r\n\r\n## Expected results\r\nDownload datasets without issue.\r\n\r\n## Actual results\r\n`AttributeError: module 'pyarrow.lib' has no attribute 'IpcReadOptions'`\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.3.2\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.1.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4674/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4673", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4673/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4673/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4673/events", "html_url": "https://github.com/huggingface/datasets/issues/4673", "id": 1301010331, "node_id": "I_kwDODunzps5Ni9eb", "number": 4673, "title": "load_datasets on csv returns everything as a string ", "user": {"login": "courtneysprouse", "id": 25102613, "node_id": "MDQ6VXNlcjI1MTAyNjEz", "avatar_url": "https://avatars.githubusercontent.com/u/25102613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/courtneysprouse", "html_url": "https://github.com/courtneysprouse", "followers_url": "https://api.github.com/users/courtneysprouse/followers", "following_url": "https://api.github.com/users/courtneysprouse/following{/other_user}", "gists_url": "https://api.github.com/users/courtneysprouse/gists{/gist_id}", "starred_url": "https://api.github.com/users/courtneysprouse/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/courtneysprouse/subscriptions", "organizations_url": "https://api.github.com/users/courtneysprouse/orgs", "repos_url": "https://api.github.com/users/courtneysprouse/repos", "events_url": "https://api.github.com/users/courtneysprouse/events{/privacy}", "received_events_url": "https://api.github.com/users/courtneysprouse/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-07-11T17:30:24Z", "updated_at": "2022-07-12T13:33:09Z", "closed_at": "2022-07-12T13:33:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nIf you use:\r\n\r\n`conll_dataset.to_csv(\"ner_conll.csv\")`\r\n\r\nIt will create a csv file with all of your data as expected, however when you load it with:\r\n\r\n`conll_dataset = load_dataset(\"csv\", data_files=\"ner_conll.csv\")` \r\n\r\neverything is read in as a string. For example if I look at everything in 'ner_tags' I get back `['[3 0 7 0 0 0 7 0 0]', '[1 2]', '[5 0]']` instead of what I originally saved which was `[[3, 0, 7, 0, 0, 0, 7, 0, 0], [1, 2], [5, 0]]`\r\n\r\nI think maybe there is something funky going on with the csv delimiter \r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n#load original conll dataset\r\norig_conll = load_dataset(\"conll2003\")\r\n\r\n#save original conll as a csv \r\norig_conll.to_csv(\"ner_conll.csv\")\r\n\r\n#reload conll data as a csv\r\nnew_conll = load_dataset(\"csv\", data_files=\"ner_conll.csv\")`\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\nI would expect the data be returned as the data type I saved it as. I.e. if I save a list of ints \r\n[[3, 0, 7, 0, 0, 0, 7, 0, 0]], I shouldnt get back a string ['[3 0 7 0 0 0 7 0 0]']\r\nI also get back a string when I pass a list of strings ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\r\n\r\n## Actual results\r\nA list of strings `['[3 0 7 0 0 0 7 0 0]', '[1 2]', '[5 0]']`\r\nA string \"['EU' 'rejects' 'German' 'call' 'to' 'boycott' 'British' 'lamb' '.']\"\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.4.0-121-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.13\r\n- PyArrow version: 8.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4673/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4673/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4670", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4670/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4670/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4670/events", "html_url": "https://github.com/huggingface/datasets/issues/4670", "id": 1299984246, "node_id": "I_kwDODunzps5NfC92", "number": 4670, "title": "Can't extract files from `.7z` zipfile using `download_and_extract`", "user": {"login": "bhavitvyamalik", "id": 19718818, "node_id": "MDQ6VXNlcjE5NzE4ODE4", "avatar_url": "https://avatars.githubusercontent.com/u/19718818?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bhavitvyamalik", "html_url": "https://github.com/bhavitvyamalik", "followers_url": "https://api.github.com/users/bhavitvyamalik/followers", "following_url": "https://api.github.com/users/bhavitvyamalik/following{/other_user}", "gists_url": "https://api.github.com/users/bhavitvyamalik/gists{/gist_id}", "starred_url": "https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bhavitvyamalik/subscriptions", "organizations_url": "https://api.github.com/users/bhavitvyamalik/orgs", "repos_url": "https://api.github.com/users/bhavitvyamalik/repos", "events_url": "https://api.github.com/users/bhavitvyamalik/events{/privacy}", "received_events_url": "https://api.github.com/users/bhavitvyamalik/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-07-10T18:16:49Z", "updated_at": "2022-07-15T13:02:07Z", "closed_at": "2022-07-15T13:02:07Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI'm adding a new dataset which is a `.7z` zip file in Google drive and contains 3 json files inside. I'm able to download the data files using `download_and_extract` but after downloading it throws this error:\r\n```\r\n>>> dataset = load_dataset(\"./datasets/mantis/\")\r\nUsing custom data configuration default\r\nDownloading and preparing dataset mantis/default to /Users/bhavitvyamalik/.cache/huggingface/datasets/mantis/default/1.1.0/611affa804ec53e2055a335cc1b8b213bb5a0b5142d919967729d5ee23c6bab4...\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77.2M/77.2M [00:23<00:00, 3.28MB/s]\r\n/Users/bhavitvyamalik/.cache/huggingface/datasets/downloads/fc3d70123c9de8407587a59aa426c37819cf2bf016795d33270e8a1d558a34e6\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/bhavitvyamalik/Desktop/work/hf/datasets/src/datasets/load.py\", line 1745, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/Users/bhavitvyamalik/Desktop/work/hf/datasets/src/datasets/builder.py\", line 595, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/Users/bhavitvyamalik/Desktop/work/hf/datasets/src/datasets/builder.py\", line 690, in _download_and_prepare\r\n    ) from None\r\nOSError: Cannot find data file. \r\nOriginal error:\r\n[Errno 20] Not a directory: '/Users/bhavitvyamalik/.cache/huggingface/datasets/downloads/fc3d70123c9de8407587a59aa426c37819cf2bf016795d33270e8a1d558a34e6/merged_train.json'\r\n```\r\njust before generating the splits. I checked `fc3d70123c9de8407587a59aa426c37819cf2bf016795d33270e8a1d558a34e6` file and it's `7z` zip file (similar to downloaded Google drive file) which means it didn't get unzip. Do I need to unzip it separately and then pass the paths for train,dev,test files in `SplitGenerator`?\r\n\r\n## Environment info\r\n- `datasets` version: 1.18.4.dev0\r\n- Platform: Darwin-19.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.8\r\n- PyArrow version: 5.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4670/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4670/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4669", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4669/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4669/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4669/events", "html_url": "https://github.com/huggingface/datasets/issues/4669", "id": 1299848003, "node_id": "I_kwDODunzps5NehtD", "number": 4669, "title": "loading oscar-corpus/OSCAR-2201  raises an error", "user": {"login": "vitalyshalumov", "id": 33824221, "node_id": "MDQ6VXNlcjMzODI0MjIx", "avatar_url": "https://avatars.githubusercontent.com/u/33824221?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vitalyshalumov", "html_url": "https://github.com/vitalyshalumov", "followers_url": "https://api.github.com/users/vitalyshalumov/followers", "following_url": "https://api.github.com/users/vitalyshalumov/following{/other_user}", "gists_url": "https://api.github.com/users/vitalyshalumov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vitalyshalumov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vitalyshalumov/subscriptions", "organizations_url": "https://api.github.com/users/vitalyshalumov/orgs", "repos_url": "https://api.github.com/users/vitalyshalumov/repos", "events_url": "https://api.github.com/users/vitalyshalumov/events{/privacy}", "received_events_url": "https://api.github.com/users/vitalyshalumov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-07-10T07:09:30Z", "updated_at": "2022-07-11T09:27:49Z", "closed_at": "2022-07-11T09:27:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nload_dataset('oscar-2201', 'af')\r\n\r\nraises an error:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/code.py\", line 90, in runcode\r\n    exec(code, self.locals)\r\n  File \"<input>\", line 1, in <module>\r\n  File \"..python3.8/site-packages/datasets/load.py\", line 1656, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \".../lib/python3.8/site-packages/datasets/load.py\", line 1439, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \".../lib/python3.8/site-packages/datasets/load.py\", line 1189, in dataset_module_factory\r\n    raise FileNotFoundError(\r\nFileNotFoundError: Couldn't find a dataset script at .../oscar-2201/oscar-2201.py or any data file in the same directory. Couldn't find 'oscar-2201' on the Hugging Face Hub either: FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/oscar-2201/oscar-2201.py\r\n\r\n\r\nI've tried other permutations such as : \r\noscar_22 = load_dataset('oscar-2201', 'af',use_auth_token=True)\r\noscar_22 = load_dataset('oscar-corpus/OSCAR-2201', 'af',use_auth_token=True)\r\noscar_22 = load_dataset('oscar-2201', 'af')\r\noscar_22 = load_dataset('oscar-corpus/OSCAR-2201')\r\n\r\n\r\nwith the same unfortunate result.\r\n\r\n\r\n## Steps to reproduce the bug\r\noscar_22 = load_dataset('oscar-2201', 'af',use_auth_token=True)\r\noscar_22 = load_dataset('oscar-corpus/OSCAR-2201', 'af',use_auth_token=True)\r\noscar_22 = load_dataset('oscar-2201', 'af')\r\noscar_22 = load_dataset('oscar-corpus/OSCAR-2201')\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nloaded data\r\n\r\n## Actual results\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/code.py\", line 90, in runcode\r\n    exec(code, self.locals)\r\n  File \"<input>\", line 1, in <module>\r\n  File \"..python3.8/site-packages/datasets/load.py\", line 1656, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \".../lib/python3.8/site-packages/datasets/load.py\", line 1439, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \".../lib/python3.8/site-packages/datasets/load.py\", line 1189, in dataset_module_factory\r\n    raise FileNotFoundError(\r\nFileNotFoundError: Couldn't find a dataset script at .../oscar-2201/oscar-2201.py or any data file in the same directory. Couldn't find 'oscar-2201' on the Hugging Face Hub either: FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/oscar-2201/oscar-2201.py\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.13.0-37-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.3\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4669/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4669/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4666", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4666/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4666/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4666/events", "html_url": "https://github.com/huggingface/datasets/issues/4666", "id": 1299732238, "node_id": "I_kwDODunzps5NeFcO", "number": 4666, "title": "Issues with concatenating datasets", "user": {"login": "ChenghaoMou", "id": 32014649, "node_id": "MDQ6VXNlcjMyMDE0NjQ5", "avatar_url": "https://avatars.githubusercontent.com/u/32014649?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChenghaoMou", "html_url": "https://github.com/ChenghaoMou", "followers_url": "https://api.github.com/users/ChenghaoMou/followers", "following_url": "https://api.github.com/users/ChenghaoMou/following{/other_user}", "gists_url": "https://api.github.com/users/ChenghaoMou/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChenghaoMou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChenghaoMou/subscriptions", "organizations_url": "https://api.github.com/users/ChenghaoMou/orgs", "repos_url": "https://api.github.com/users/ChenghaoMou/repos", "events_url": "https://api.github.com/users/ChenghaoMou/events{/privacy}", "received_events_url": "https://api.github.com/users/ChenghaoMou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-07-09T17:45:14Z", "updated_at": "2022-07-12T17:16:15Z", "closed_at": "2022-07-12T17:16:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nIt is impossible to concatenate datasets if a feature is sequence of dict in one dataset and a dict of sequence in another. But based on the document, it should be automatically converted.\r\n\r\n> A [datasets.Sequence](https://huggingface.co/docs/datasets/v2.3.2/en/package_reference/main_classes#datasets.Sequence) with a internal dictionary feature will be automatically converted into a dictionary of lists. This behavior is implemented to have a compatilbity layer with the TensorFlow Datasets library but may be un-wanted in some cases. If you don\u2019t want this behavior, you can use a python list instead of the [datasets.Sequence](https://huggingface.co/docs/datasets/v2.3.2/en/package_reference/main_classes#datasets.Sequence).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import concatenate_datasets, load_dataset\r\n\r\nsquad = load_dataset(\"squad_v2\")\r\nsquad[\"train\"].to_json(\"output.jsonl\", lines=True)\r\n\r\ntemp = load_dataset(\"json\", data_files={\"train\": \"output.jsonl\"})\r\nconcatenate_datasets([temp[\"train\"], squad[\"train\"]])\r\n```\r\n\r\n## Expected results\r\nNo error executing that code\r\n\r\n## Actual results\r\n```\r\nValueError: The features can't be aligned because the key answers of features {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)} has unexpected type - Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None) (expected either {'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'answer_start': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)} or Value(\"null\").\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.3.2\r\n- Platform: macOS-12.4-arm64-arm-64bit\r\n- Python version: 3.8.11\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4666/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4666/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4665", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4665/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4665/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4665/events", "html_url": "https://github.com/huggingface/datasets/issues/4665", "id": 1299652638, "node_id": "I_kwDODunzps5NdyAe", "number": 4665, "title": "Unable to create dataset having Python dataset script only", "user": {"login": "aleSuglia", "id": 1479733, "node_id": "MDQ6VXNlcjE0Nzk3MzM=", "avatar_url": "https://avatars.githubusercontent.com/u/1479733?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aleSuglia", "html_url": "https://github.com/aleSuglia", "followers_url": "https://api.github.com/users/aleSuglia/followers", "following_url": "https://api.github.com/users/aleSuglia/following{/other_user}", "gists_url": "https://api.github.com/users/aleSuglia/gists{/gist_id}", "starred_url": "https://api.github.com/users/aleSuglia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aleSuglia/subscriptions", "organizations_url": "https://api.github.com/users/aleSuglia/orgs", "repos_url": "https://api.github.com/users/aleSuglia/repos", "events_url": "https://api.github.com/users/aleSuglia/events{/privacy}", "received_events_url": "https://api.github.com/users/aleSuglia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-07-09T11:45:46Z", "updated_at": "2022-07-11T07:10:09Z", "closed_at": "2022-07-11T07:10:01Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nHi there,\r\n\r\nI'm trying to add the following dataset to Huggingface datasets: https://huggingface.co/datasets/Heriot-WattUniversity/dialog-babi/blob/\r\n\r\nI'm trying to do so using the CLI commands but seems that this command generates the wrong `dataset_info.json` file (you can find it in the repo already):\r\n```\r\ndatasets-cli test Heriot-WattUniversity/dialog-babi/dialog_babi.py --save_infos --all-configs\r\n```\r\nwhile it errors when I remove the python script:\r\n```\r\ndatasets-cli test Heriot-WattUniversity/dialog-babi/ --save_infos --all-configs \r\n```\r\nThe error message is the following:\r\n```\r\nFileNotFoundError: Unable to resolve any data file that matches '['**']' at /Users/as2180/workspace/Heriot-WattUniversity/dialog-babi with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'zip']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.3.2\r\n- Platform: macOS-12.4-arm64-arm-64bit\r\n- Python version: 3.9.9\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.3", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4665/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4665/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4621", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4621/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4621/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4621/events", "html_url": "https://github.com/huggingface/datasets/issues/4621", "id": 1293030128, "node_id": "I_kwDODunzps5NEhLw", "number": 4621, "title": "ImageFolder raises an error with parameters drop_metadata=True and drop_labels=False when metadata.jsonl is present", "user": {"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-07-04T11:21:44Z", "updated_at": "2022-07-15T14:24:24Z", "closed_at": "2022-07-15T14:24:24Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nIf you pass `drop_metadata=True` and `drop_labels=False` when a `data_dir` contains at least one `matadata.jsonl` file, you will get a KeyError. This is probably not a very useful case but we shouldn't get an error anyway. Asking users to move metadata files manually outside `data_dir` or pass features manually (when there is a tool that can infer them automatically) don't look like a good idea to me either.\r\n\r\n## Steps to reproduce the bug\r\n### Clone an example dataset from the Hub\r\n```bash\r\ngit clone https://huggingface.co/datasets/nateraw/test-imagefolder-metadata\r\n```\r\n### Try to load it\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"test-imagefolder-metadata\", drop_metadata=True, drop_labels=False)\r\n```\r\nor even just\r\n```python\r\nds = load_dataset(\"test-imagefolder-metadata\", drop_metadata=True)\r\n```\r\nas `drop_labels=False` is a default value.\r\n\r\n## Expected results\r\nA DatasetDict object with two features: `\"image\"` and `\"label\"`.\r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/polina/workspace/datasets/debug.py\", line 18, in <module>\r\n    ds = load_dataset(\r\n  File \"/home/polina/workspace/datasets/src/datasets/load.py\", line 1732, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/polina/workspace/datasets/src/datasets/builder.py\", line 704, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/polina/workspace/datasets/src/datasets/builder.py\", line 1227, in _download_and_prepare\r\n    super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n  File \"/home/polina/workspace/datasets/src/datasets/builder.py\", line 793, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/polina/workspace/datasets/src/datasets/builder.py\", line 1218, in _prepare_split\r\n    example = self.info.features.encode_example(record)\r\n  File \"/home/polina/workspace/datasets/src/datasets/features/features.py\", line 1596, in encode_example\r\n    return encode_nested_example(self, example)\r\n  File \"/home/polina/workspace/datasets/src/datasets/features/features.py\", line 1165, in encode_nested_example\r\n    {\r\n  File \"/home/polina/workspace/datasets/src/datasets/features/features.py\", line 1165, in <dictcomp>\r\n    {\r\n  File \"/home/polina/workspace/datasets/src/datasets/utils/py_utils.py\", line 249, in zip_dict\r\n    yield key, tuple(d[key] for d in dicts)\r\n  File \"/home/polina/workspace/datasets/src/datasets/utils/py_utils.py\", line 249, in <genexpr>\r\n    yield key, tuple(d[key] for d in dicts)\r\nKeyError: 'label'\r\n```\r\n\r\n## Environment info\r\n`datasets` master branch \r\n\r\n- `datasets` version: 2.3.3.dev0\r\n- Platform: Linux-5.14.0-1042-oem-x86_64-with-glibc2.17\r\n- Python version: 3.8.12\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.4.1\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4621/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4621/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4620", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4620/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4620/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4620/events", "html_url": "https://github.com/huggingface/datasets/issues/4620", "id": 1292797878, "node_id": "I_kwDODunzps5NDoe2", "number": 4620, "title": "Data type is not recognized when using datetime.time", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-07-04T08:13:38Z", "updated_at": "2022-07-07T13:57:11Z", "closed_at": "2022-07-07T13:57:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nCreating a dataset from a pandas dataframe with `datetime.time` format generates an error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport pandas as pd\r\nfrom datetime import time\r\nfrom datasets import Dataset\r\ndf = pd.DataFrame({\"feature_name\": [time(1, 1, 1)]})\r\ndataset = Dataset.from_pandas(df)\r\n```\r\n\r\n## Expected results\r\n\r\nThe dataset should be created.\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 823, in from_pandas\r\n    return cls(table, info=info, split=split)\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 679, in __init__\r\n    inferred_features = Features.from_arrow_schema(arrow_table.schema)\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1551, in from_arrow_schema\r\n    obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1551, in <dictcomp>\r\n    obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1315, in generate_from_arrow_type\r\n    return Value(dtype=_arrow_to_datasets_dtype(pa_type))\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 83, in _arrow_to_datasets_dtype\r\n    return f\"time64[{arrow_type.unit}]\"\r\nAttributeError: 'pyarrow.lib.DataType' object has no attribute 'unit'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 2.3.3.dev0\r\n- Platform: Linux-5.13.0-1031-aws-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4620/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4620/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4612", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4612/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4612/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4612/events", "html_url": "https://github.com/huggingface/datasets/issues/4612", "id": 1290984660, "node_id": "I_kwDODunzps5M8tzU", "number": 4612, "title": "Release 2.3.0 broke custom iterable datasets", "user": {"login": "aapot", "id": 19529125, "node_id": "MDQ6VXNlcjE5NTI5MTI1", "avatar_url": "https://avatars.githubusercontent.com/u/19529125?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aapot", "html_url": "https://github.com/aapot", "followers_url": "https://api.github.com/users/aapot/followers", "following_url": "https://api.github.com/users/aapot/following{/other_user}", "gists_url": "https://api.github.com/users/aapot/gists{/gist_id}", "starred_url": "https://api.github.com/users/aapot/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aapot/subscriptions", "organizations_url": "https://api.github.com/users/aapot/orgs", "repos_url": "https://api.github.com/users/aapot/repos", "events_url": "https://api.github.com/users/aapot/events{/privacy}", "received_events_url": "https://api.github.com/users/aapot/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-07-01T06:46:07Z", "updated_at": "2022-07-05T15:08:21Z", "closed_at": "2022-07-05T15:08:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nTrying to iterate examples from custom iterable dataset fails to bug introduced in `torch_iterable_dataset.py` since the release of 2.3.0. \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nnext(iter(custom_iterable_dataset))\r\n```\r\n\r\n## Expected results\r\n`next(iter(custom_iterable_dataset))` should return examples from the dataset\r\n\r\n## Actual results\r\n```\r\n/usr/local/lib/python3.7/dist-packages/datasets/formatting/dataset_wrappers/torch_iterable_dataset.py in _set_fsspec_for_multiprocess()\r\n     16     See https://github.com/fsspec/gcsfs/issues/379\r\n     17     \"\"\"\r\n---> 18     fsspec.asyn.iothread[0] = None\r\n     19     fsspec.asyn.loop[0] = None\r\n     20 \r\n\r\nAttributeError: module 'fsspec' has no attribute 'asyn'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.3.0\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.3.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4612/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4610", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4610/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4610/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4610/events", "html_url": "https://github.com/huggingface/datasets/issues/4610", "id": 1290603827, "node_id": "I_kwDODunzps5M7Q0z", "number": 4610, "title": "codeparrot/github-code failing to load ", "user": {"login": "PyDataBlog", "id": 29863388, "node_id": "MDQ6VXNlcjI5ODYzMzg4", "avatar_url": "https://avatars.githubusercontent.com/u/29863388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PyDataBlog", "html_url": "https://github.com/PyDataBlog", "followers_url": "https://api.github.com/users/PyDataBlog/followers", "following_url": "https://api.github.com/users/PyDataBlog/following{/other_user}", "gists_url": "https://api.github.com/users/PyDataBlog/gists{/gist_id}", "starred_url": "https://api.github.com/users/PyDataBlog/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PyDataBlog/subscriptions", "organizations_url": "https://api.github.com/users/PyDataBlog/orgs", "repos_url": "https://api.github.com/users/PyDataBlog/repos", "events_url": "https://api.github.com/users/PyDataBlog/events{/privacy}", "received_events_url": "https://api.github.com/users/PyDataBlog/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2022-06-30T20:24:48Z", "updated_at": "2022-07-05T14:24:13Z", "closed_at": "2022-07-05T09:19:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\ncodeparrot/github-code fails to load with a `TypeError: get_patterns_in_dataset_repository() missing 1 required positional argument: 'base_path'`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n```\r\n\r\n## Expected results\r\nloaded dataset object\r\n\r\n## Actual results\r\n```python\r\n [3]: dataset = load_dataset(\"codeparrot/github-code\")\r\nNo config specified, defaulting to: github-code/all-all\r\nDownloading and preparing dataset github-code/all-all to /home/bebr/.cache/huggingface/datasets/codeparrot___github-code/all-all/0.0.0/a55513bc0f81db773f9896c7aac225af0cff5b323bb9d2f68124f0a8cc3fb817...\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nInput In [3], in <cell line: 1>()\r\n----> 1 dataset = load_dataset(\"codeparrot/github-code\")\r\n\r\nFile ~/miniconda3/envs/fastapi-kube/lib/python3.10/site-packages/datasets/load.py:1679, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1676 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   1678 # Download and prepare data\r\n-> 1679 builder_instance.download_and_prepare(\r\n   1680     download_config=download_config,\r\n   1681     download_mode=download_mode,\r\n   1682     ignore_verifications=ignore_verifications,\r\n   1683     try_from_hf_gcs=try_from_hf_gcs,\r\n   1684     use_auth_token=use_auth_token,\r\n   1685 )\r\n   1687 # Build dataset for splits\r\n   1688 keep_in_memory = (\r\n   1689     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   1690 )\r\n\r\nFile ~/miniconda3/envs/fastapi-kube/lib/python3.10/site-packages/datasets/builder.py:704, in DatasetBuilder.download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    702         logger.warning(\"HF google storage unreachable. Downloading and preparing it from source\")\r\n    703 if not downloaded_from_gcs:\r\n--> 704     self._download_and_prepare(\r\n    705         dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    706     )\r\n    707 # Sync info\r\n    708 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~/miniconda3/envs/fastapi-kube/lib/python3.10/site-packages/datasets/builder.py:1221, in GeneratorBasedBuilder._download_and_prepare(self, dl_manager, verify_infos)\r\n   1220 def _download_and_prepare(self, dl_manager, verify_infos):\r\n-> 1221     super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n\r\nFile ~/miniconda3/envs/fastapi-kube/lib/python3.10/site-packages/datasets/builder.py:771, in DatasetBuilder._download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    769 split_dict = SplitDict(dataset_name=self.name)\r\n    770 split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 771 split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    773 # Checksums verification\r\n    774 if verify_infos and dl_manager.record_checksums:\r\n\r\nFile ~/.cache/huggingface/modules/datasets_modules/datasets/codeparrot--github-code/a55513bc0f81db773f9896c7aac225af0cff5b323bb9d2f68124f0a8cc3fb817/github-code.py:169, in GithubCode._split_generators(self, dl_manager)\r\n    162 def _split_generators(self, dl_manager):\r\n    164     hfh_dataset_info = HfApi(datasets.config.HF_ENDPOINT).dataset_info(\r\n    165         _REPO_NAME,\r\n    166         timeout=100.0,\r\n    167     )\r\n--> 169     patterns = datasets.data_files.get_patterns_in_dataset_repository(hfh_dataset_info)\r\n    170     data_files = datasets.data_files.DataFilesDict.from_hf_repo(\r\n    171         patterns,\r\n    172         dataset_info=hfh_dataset_info,\r\n    173     )\r\n    175     files = dl_manager.download_and_extract(data_files[\"train\"])\r\n\r\nTypeError: get_patterns_in_dataset_repository() missing 1 required positional argument: 'base_path'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.18.7-arch1-1-x86_64-with-glibc2.35\r\n- Python version: 3.10.5\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4610/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4609", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4609/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4609/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4609/events", "html_url": "https://github.com/huggingface/datasets/issues/4609", "id": 1290392083, "node_id": "I_kwDODunzps5M6dIT", "number": 4609, "title": "librispeech dataset has to download whole subset when specifing the split to use", "user": {"login": "sunhaozhepy", "id": 73462159, "node_id": "MDQ6VXNlcjczNDYyMTU5", "avatar_url": "https://avatars.githubusercontent.com/u/73462159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sunhaozhepy", "html_url": "https://github.com/sunhaozhepy", "followers_url": "https://api.github.com/users/sunhaozhepy/followers", "following_url": "https://api.github.com/users/sunhaozhepy/following{/other_user}", "gists_url": "https://api.github.com/users/sunhaozhepy/gists{/gist_id}", "starred_url": "https://api.github.com/users/sunhaozhepy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sunhaozhepy/subscriptions", "organizations_url": "https://api.github.com/users/sunhaozhepy/orgs", "repos_url": "https://api.github.com/users/sunhaozhepy/repos", "events_url": "https://api.github.com/users/sunhaozhepy/events{/privacy}", "received_events_url": "https://api.github.com/users/sunhaozhepy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-06-30T16:38:24Z", "updated_at": "2022-07-12T21:44:32Z", "closed_at": "2022-07-12T21:44:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nlibrispeech dataset has to download whole subset when specifing the split to use\r\n\r\n## Steps to reproduce the bug\r\nsee below\r\n# Sample code to reproduce the bug\r\n```\r\n!pip install datasets\r\nfrom datasets import load_dataset\r\nraw_dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"train.100\")\r\n```\r\n\r\n## Expected results\r\nThe split \"train.clean.100\" is downloaded.\r\n\r\n## Actual results\r\nAll four splits in \"clean\" subset is downloaded.\r\n\r\n## Environment info\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4609/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4609/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4603", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4603/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4603/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4603/events", "html_url": "https://github.com/huggingface/datasets/issues/4603", "id": 1289963331, "node_id": "I_kwDODunzps5M40dD", "number": 4603, "title": "CI fails recurrently and randomly on Windows", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-06-30T10:59:58Z", "updated_at": "2022-06-30T13:22:25Z", "closed_at": "2022-06-30T13:22:25Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "As reported by @lhoestq,\r\n\r\nThe windows CI is currently flaky: some dependencies like `aiobotocore`, `multiprocess` and `seqeval` sometimes fail to install.\r\nIn particular it seems that building the wheels fail. Here is an example of logs:\r\n\r\n```\r\nBuilding wheel for seqeval (setup.py): started\r\n  Running command 'C:\\tools\\miniconda3\\envs\\py37\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\circleci\\\\AppData\\\\Local\\\\Temp\\\\pip-install-h55pfgbv\\\\seqeval_d6cdb9d23ff6490b98b6c4bcaecb516e\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\circleci\\\\AppData\\\\Local\\\\Temp\\\\pip-install-h55pfgbv\\\\seqeval_d6cdb9d23ff6490b98b6c4bcaecb516e\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\circleci\\AppData\\Local\\Temp\\pip-wheel-x3cc8ym6'\r\n  No parent package detected, impossible to derive `name`\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  package init file 'seqeval\\__init__.py' not found (or not a regular file)\r\n  package init file 'seqeval\\metrics\\__init__.py' not found (or not a regular file)\r\n  C:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\setuptools\\command\\install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n    setuptools.SetuptoolsDeprecationWarning,\r\n  installing to build\\bdist.win-amd64\\wheel\r\n  running install\r\n  running install_lib\r\n  warning: install_lib: 'build\\lib' does not exist -- no Python modules to install\r\n\r\n  running install_egg_info\r\n  running egg_info\r\n  creating UNKNOWN.egg-info\r\n  writing UNKNOWN.egg-info\\PKG-INFO\r\n  writing dependency_links to UNKNOWN.egg-info\\dependency_links.txt\r\n  writing top-level names to UNKNOWN.egg-info\\top_level.txt\r\n  writing manifest file 'UNKNOWN.egg-info\\SOURCES.txt'\r\n  reading manifest file 'UNKNOWN.egg-info\\SOURCES.txt'\r\n  writing manifest file 'UNKNOWN.egg-info\\SOURCES.txt'\r\n  Copying UNKNOWN.egg-info to build\\bdist.win-amd64\\wheel\\.\\UNKNOWN-0.0.0-py3.7.egg-info\r\n  running install_scripts\r\n  creating build\\bdist.win-amd64\\wheel\\UNKNOWN-0.0.0.dist-info\\WHEEL\r\n  creating 'C:\\Users\\circleci\\AppData\\Local\\Temp\\pip-wheel-x3cc8ym6\\UNKNOWN-0.0.0-py3-none-any.whl' and adding 'build\\bdist.win-amd64\\wheel' to it\r\n  adding 'UNKNOWN-0.0.0.dist-info/METADATA'\r\n  adding 'UNKNOWN-0.0.0.dist-info/WHEEL'\r\n  adding 'UNKNOWN-0.0.0.dist-info/top_level.txt'\r\n  adding 'UNKNOWN-0.0.0.dist-info/RECORD'\r\n  removing build\\bdist.win-amd64\\wheel\r\n  Building wheel for seqeval (setup.py): finished with status 'done'\r\n  Created wheel for seqeval: filename=UNKNOWN-0.0.0-py3-none-any.whl size=963 sha256=67eb93a6e1ff4796c5882a13f9fa25bb0d3d103796e2525f9cecf3b2ef26d4b1\r\n  Stored in directory: c:\\users\\circleci\\appdata\\local\\pip\\cache\\wheels\\05\\96\\ee\\7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\r\n  WARNING: Built wheel for seqeval is invalid: Wheel has unexpected file name: expected 'seqeval', got 'UNKNOWN'\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4603/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4594", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4594/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4594/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4594/events", "html_url": "https://github.com/huggingface/datasets/issues/4594", "id": 1288070023, "node_id": "I_kwDODunzps5MxmOH", "number": 4594, "title": "load_from_disk suggests incorrect fix when used to load DatasetDict", "user": {"login": "dvsth", "id": 11157811, "node_id": "MDQ6VXNlcjExMTU3ODEx", "avatar_url": "https://avatars.githubusercontent.com/u/11157811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dvsth", "html_url": "https://github.com/dvsth", "followers_url": "https://api.github.com/users/dvsth/followers", "following_url": "https://api.github.com/users/dvsth/following{/other_user}", "gists_url": "https://api.github.com/users/dvsth/gists{/gist_id}", "starred_url": "https://api.github.com/users/dvsth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dvsth/subscriptions", "organizations_url": "https://api.github.com/users/dvsth/orgs", "repos_url": "https://api.github.com/users/dvsth/repos", "events_url": "https://api.github.com/users/dvsth/events{/privacy}", "received_events_url": "https://api.github.com/users/dvsth/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-06-29T01:40:01Z", "updated_at": "2022-06-29T04:03:44Z", "closed_at": "2022-06-29T04:03:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Edit: Please feel free to remove this issue. The problem was not the error message but the fact that the DatasetDict.load_from_disk does not support loading nested splits, i.e. if one of the splits is itself a DatasetDict. If nesting splits is an antipattern, perhaps the load_from_disk function can throw a warning indicating that?", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4594/timeline", "performed_via_github_app": null, "state_reason": "not_planned"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4591", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4591/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4591/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4591/events", "html_url": "https://github.com/huggingface/datasets/issues/4591", "id": 1288021332, "node_id": "I_kwDODunzps5MxaVU", "number": 4591, "title": "Can't push Images to hub with manual Dataset", "user": {"login": "cceyda", "id": 15624271, "node_id": "MDQ6VXNlcjE1NjI0Mjcx", "avatar_url": "https://avatars.githubusercontent.com/u/15624271?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cceyda", "html_url": "https://github.com/cceyda", "followers_url": "https://api.github.com/users/cceyda/followers", "following_url": "https://api.github.com/users/cceyda/following{/other_user}", "gists_url": "https://api.github.com/users/cceyda/gists{/gist_id}", "starred_url": "https://api.github.com/users/cceyda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cceyda/subscriptions", "organizations_url": "https://api.github.com/users/cceyda/orgs", "repos_url": "https://api.github.com/users/cceyda/repos", "events_url": "https://api.github.com/users/cceyda/events{/privacy}", "received_events_url": "https://api.github.com/users/cceyda/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-06-29T00:01:23Z", "updated_at": "2022-07-08T12:01:36Z", "closed_at": "2022-07-08T12:01:35Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nIf I create a dataset including an 'Image' feature manually, when pushing to hub decoded images are not pushed, \r\ninstead it looks for image where image local path is/used to be.\r\nThis doesn't (at least didn't used to) happen with imagefolder. I want to build dataset manually because it is complicated.\r\n\r\nThis happens even though the dataset is looking like decoded images:\r\n![image](https://user-images.githubusercontent.com/15624271/176322689-2cc819cf-9d5c-4a8f-9f3d-83ae8ec06f20.png)\r\nand I use `embed_external_files=True` while `push_to_hub` (same with false)\r\n## Steps to reproduce the bug\r\n```python\r\n\r\nfrom PIL import Image\r\nfrom datasets import Image as ImageFeature\r\nfrom datasets import Features,Dataset\r\n#manually create dataset\r\nfeats=Features(\r\n    {\r\n        \"images\": [ImageFeature()], #same even if explicitly ImageFeature(decode=True)\r\n        \"input_image\": ImageFeature(),\r\n    }\r\n)\r\n\r\ntest_data={\"images\":[[Image.open(\"test.jpg\"),Image.open(\"test.jpg\"),Image.open(\"test.jpg\")]], \"input_image\":[Image.open(\"test.jpg\")]}\r\ntest_dataset=Dataset.from_dict(test_data,features=feats)\r\nprint(test_dataset)\r\n\r\ntest_dataset.push_to_hub(\"ceyda/image_test_public\",private=False,token=\"\",embed_external_files=True)\r\n\r\n# clear cache rm -r ~/.cache/huggingface\r\n# remove \"test.jpg\" # remove to see that it is looking for image on the local path\r\n\r\ntest_dataset=load_dataset(\"ceyda/image_test_public\",use_auth_token=\"\")\r\nprint(test_dataset)\r\nprint(test_dataset['train'][0])\r\n```\r\n\r\n## Expected results\r\nshould be able to push image bytes if dataset has `Image(decode=True)`\r\n\r\n## Actual results\r\n\r\nerrors because it is trying to decode file from the non existing local path.\r\n```\r\n---->  print(test_dataset['train'][0])\r\n\r\nFile ~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:2154, in Dataset.__getitem__(self, key)\r\n   2152 def __getitem__(self, key):  # noqa: F811\r\n   2153     \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 2154     return self._getitem(\r\n   2155         key,\r\n   2156     )\r\n\r\nFile ~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:2139, in Dataset._getitem(self, key, decoded, **kwargs)\r\n   2137 formatter = get_formatter(format_type, features=self.features, decoded=decoded, **format_kwargs)\r\n   2138 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n-> 2139 formatted_output = format_table(\r\n   2140     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   2141 )\r\n   2142 return formatted_output\r\n\r\nFile ~/.local/lib/python3.8/site-packages/datasets/formatting/formatting.py:532, in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    530 python_formatter = PythonFormatter(features=None)\r\n    531 if format_columns is None:\r\n...\r\n-> 3068     fp = builtins.open(filename, \"rb\")\r\n   3069     exclusive_fp = True\r\n   3071 try:\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'test.jpg'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.4.0-1074-azure-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4591/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4591/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4589", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4589/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4589/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4589/events", "html_url": "https://github.com/huggingface/datasets/issues/4589", "id": 1287600029, "node_id": "I_kwDODunzps5Mvzed", "number": 4589, "title": "Permission denied: '/home/.cache' when load_dataset with local script", "user": {"login": "jiangh0", "id": 24559732, "node_id": "MDQ6VXNlcjI0NTU5NzMy", "avatar_url": "https://avatars.githubusercontent.com/u/24559732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiangh0", "html_url": "https://github.com/jiangh0", "followers_url": "https://api.github.com/users/jiangh0/followers", "following_url": "https://api.github.com/users/jiangh0/following{/other_user}", "gists_url": "https://api.github.com/users/jiangh0/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiangh0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiangh0/subscriptions", "organizations_url": "https://api.github.com/users/jiangh0/orgs", "repos_url": "https://api.github.com/users/jiangh0/repos", "events_url": "https://api.github.com/users/jiangh0/events{/privacy}", "received_events_url": "https://api.github.com/users/jiangh0/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-06-28T16:26:03Z", "updated_at": "2022-06-29T06:26:28Z", "closed_at": "2022-06-29T06:25:08Z", "author_association": "NONE", "active_lock_reason": null, "body": null, "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4589/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4589/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4575", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4575/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4575/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4575/events", "html_url": "https://github.com/huggingface/datasets/issues/4575", "id": 1285446700, "node_id": "I_kwDODunzps5Mnlws", "number": 4575, "title": "Problem about wmt17 zh-en dataset", "user": {"login": "winterfell2021", "id": 85819194, "node_id": "MDQ6VXNlcjg1ODE5MTk0", "avatar_url": "https://avatars.githubusercontent.com/u/85819194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/winterfell2021", "html_url": "https://github.com/winterfell2021", "followers_url": "https://api.github.com/users/winterfell2021/followers", "following_url": "https://api.github.com/users/winterfell2021/following{/other_user}", "gists_url": "https://api.github.com/users/winterfell2021/gists{/gist_id}", "starred_url": "https://api.github.com/users/winterfell2021/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/winterfell2021/subscriptions", "organizations_url": "https://api.github.com/users/winterfell2021/orgs", "repos_url": "https://api.github.com/users/winterfell2021/repos", "events_url": "https://api.github.com/users/winterfell2021/events{/privacy}", "received_events_url": "https://api.github.com/users/winterfell2021/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-06-27T08:35:42Z", "updated_at": "2022-08-23T10:01:02Z", "closed_at": "2022-08-23T10:00:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "It seems that in subset casia2015, some samples are like `{'c[hn]':'xxx', 'en': 'aa'}`.\r\nSo when using `data = load_dataset('wmt17', \"zh-en\")` to load the wmt17 zh-en dataset, which will raise the exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 78, in <module>\r\n    data = load_dataset(args.dataset, \"zh-en\")\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 1684, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/builder.py\", line 705, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/builder.py\", line 1221, in _download_and_prepare\r\n    super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/builder.py\", line 793, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/builder.py\", line 1215, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py\", line 533, in finalize\r\n    self.write_examples_on_file()\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py\", line 410, in write_examples_on_file\r\n    self.write_batch(batch_examples=batch_examples)\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py\", line 503, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow/array.pxi\", line 230, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py\", line 198, in __arrow_array__\r\n    out = cast_array_to_feature(out, type, allow_number_to_str=not self.trying_type)\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/table.py\", line 1675, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/table.py\", line 1846, in cast_array_to_feature\r\n    return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/table.py\", line 1675, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/datasets/table.py\", line 1756, in array_cast\r\n    raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{pa_type}\")\r\nTypeError: Couldn't cast array of type\r\nstruct<c[hn]: string, en: string, zh: string>\r\nto\r\nstruct<en: string, zh: string>\r\n```\r\n\r\nSo the solution of this problem is to change the original array manually:\r\n```\r\nif 'c[hn]' in str(array.type):\r\n        py_array = array.to_pylist()\r\n        data_list = []\r\n        for vo in py_array:\r\n            tmp = {\r\n                'en': vo['en'],\r\n            }\r\n            if 'zh' not in vo:\r\n                tmp['zh'] = vo['c[hn]']\r\n            else:\r\n                tmp['zh'] = vo['zh']\r\n            data_list.append(tmp)\r\n        array = pa.array(data_list, type=pa.struct([\r\n            pa.field('en', pa.string()),\r\n            pa.field('zh', pa.string()),\r\n        ]))\r\n```\r\n\r\nTherefore, maybe a correct version of original casia2015 file need to be updated", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4575/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4570", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4570/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4570/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4570/events", "html_url": "https://github.com/huggingface/datasets/issues/4570", "id": 1284846168, "node_id": "I_kwDODunzps5MlTJY", "number": 4570, "title": "Dataset sharding non-contiguous?", "user": {"login": "cakiki", "id": 3664563, "node_id": "MDQ6VXNlcjM2NjQ1NjM=", "avatar_url": "https://avatars.githubusercontent.com/u/3664563?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cakiki", "html_url": "https://github.com/cakiki", "followers_url": "https://api.github.com/users/cakiki/followers", "following_url": "https://api.github.com/users/cakiki/following{/other_user}", "gists_url": "https://api.github.com/users/cakiki/gists{/gist_id}", "starred_url": "https://api.github.com/users/cakiki/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cakiki/subscriptions", "organizations_url": "https://api.github.com/users/cakiki/orgs", "repos_url": "https://api.github.com/users/cakiki/repos", "events_url": "https://api.github.com/users/cakiki/events{/privacy}", "received_events_url": "https://api.github.com/users/cakiki/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-06-26T08:34:05Z", "updated_at": "2022-06-30T11:00:47Z", "closed_at": "2022-06-26T14:36:20Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI'm not sure if this is a bug; more likely normal behavior but i wanted to double check.\r\nIs it normal that `datasets.shard` does not produce chunks that, when concatenated produce the original ordering of the sharded dataset? \r\n\r\nThis might be related to this pull request (https://github.com/huggingface/datasets/pull/4466) but I have to admit I did not properly look into the changes made.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nmax_shard_size = convert_file_size_to_int('300MB')\r\ndataset_nbytes = dataset.data.nbytes\r\nnum_shards = int(dataset_nbytes / max_shard_size) + 1\r\nnum_shards = max(num_shards, 1)\r\nprint(f\"{num_shards=}\")\r\nfor shard_index in range(num_shards):\r\n    shard = dataset.shard(num_shards=num_shards, index=shard_index)\r\n    shard.to_parquet(f\"tokenized/tokenized-{shard_index:03d}.parquet\")\r\nos.listdir('tokenized/')\r\n```\r\n\r\n## Expected results\r\nI expected the shards to match the order of the data of the original dataset; i.e. `dataset[10]` being the same as `shard_1[10]` for example\r\n\r\n## Actual results\r\nOnly the first element is the same; i.e. `dataset[0]` is the same as `shard_1[0]`\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-4.15.0-176-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.4\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4570/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4570/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4568", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4568/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4568/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4568/events", "html_url": "https://github.com/huggingface/datasets/issues/4568", "id": 1284655624, "node_id": "I_kwDODunzps5MkkoI", "number": 4568, "title": "XNLI cache reload is very slow", "user": {"login": "Muennighoff", "id": 62820084, "node_id": "MDQ6VXNlcjYyODIwMDg0", "avatar_url": "https://avatars.githubusercontent.com/u/62820084?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Muennighoff", "html_url": "https://github.com/Muennighoff", "followers_url": "https://api.github.com/users/Muennighoff/followers", "following_url": "https://api.github.com/users/Muennighoff/following{/other_user}", "gists_url": "https://api.github.com/users/Muennighoff/gists{/gist_id}", "starred_url": "https://api.github.com/users/Muennighoff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Muennighoff/subscriptions", "organizations_url": "https://api.github.com/users/Muennighoff/orgs", "repos_url": "https://api.github.com/users/Muennighoff/repos", "events_url": "https://api.github.com/users/Muennighoff/events{/privacy}", "received_events_url": "https://api.github.com/users/Muennighoff/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-06-25T16:43:56Z", "updated_at": "2022-07-04T14:29:40Z", "closed_at": "2022-07-04T14:29:40Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Reproduce\r\n\r\nUsing `2.3.3.dev0`\r\n\r\n`from datasets import load_dataset`\r\n`load_dataset(\"xnli\", \"en\")`\r\nTurn off Internet\r\n`load_dataset(\"xnli\", \"en\")`\r\n\r\nI cancelled the second `load_dataset` eventually cuz it took super long. It would be great to have something to specify e.g. `only_load_from_cache`  and avoid the library trying to download when there is no Internet. If I leave it running it works but takes way longer than when there is Internet. I would expect loading from cache to take the same amount of time regardless of whether there is Internet.\r\n```\r\n---------------------------------------------------------------------------\r\ngaierror                                  Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in _new_conn(self)\r\n    174             conn = connection.create_connection(\r\n--> 175                 (self._dns_host, self.port), self.timeout, **extra_kw\r\n    176             )\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)\r\n     71 \r\n---> 72     for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\r\n     73         af, socktype, proto, canonname, sa = res\r\n\r\n/opt/conda/lib/python3.7/socket.py in getaddrinfo(host, port, family, type, proto, flags)\r\n    751     addrlist = []\r\n--> 752     for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\n    753         af, socktype, proto, canonname, sa = res\r\n\r\ngaierror: [Errno -3] Temporary failure in name resolution\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nKeyboardInterrupt                         Traceback (most recent call last)\r\n/tmp/ipykernel_33/3594208039.py in <module>\r\n----> 1 load_dataset(\"xnli\", \"en\")\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1673         revision=revision,\r\n   1674         use_auth_token=use_auth_token,\r\n-> 1675         **config_kwargs,\r\n   1676     )\r\n   1677 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/load.py in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\r\n   1494         download_mode=download_mode,\r\n   1495         data_dir=data_dir,\r\n-> 1496         data_files=data_files,\r\n   1497     )\r\n   1498 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/load.py in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1182                     download_config=download_config,\r\n   1183                     download_mode=download_mode,\r\n-> 1184                     dynamic_modules_path=dynamic_modules_path,\r\n   1185                 ).get_module()\r\n   1186             elif path.count(\"/\") == 1:  # community dataset on the Hub\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/load.py in __init__(self, name, revision, download_config, download_mode, dynamic_modules_path)\r\n    506         self.dynamic_modules_path = dynamic_modules_path\r\n    507         assert self.name.count(\"/\") == 0\r\n--> 508         increase_load_count(name, resource_type=\"dataset\")\r\n    509 \r\n    510     def download_loading_script(self, revision: Optional[str]) -> str:\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/load.py in increase_load_count(name, resource_type)\r\n    166     if not config.HF_DATASETS_OFFLINE and config.HF_UPDATE_DOWNLOAD_COUNTS:\r\n    167         try:\r\n--> 168             head_hf_s3(name, filename=name + \".py\", dataset=(resource_type == \"dataset\"))\r\n    169         except Exception:\r\n    170             pass\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/utils/file_utils.py in head_hf_s3(identifier, filename, use_cdn, dataset, max_retries)\r\n     93     return http_head(\r\n     94         hf_bucket_url(identifier=identifier, filename=filename, use_cdn=use_cdn, dataset=dataset),\r\n---> 95         max_retries=max_retries,\r\n     96     )\r\n     97 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/utils/file_utils.py in http_head(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\r\n    445         allow_redirects=allow_redirects,\r\n    446         timeout=timeout,\r\n--> 447         max_retries=max_retries,\r\n    448     )\r\n    449     return response\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/utils/file_utils.py in _request_with_retry(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\r\n    366         tries += 1\r\n    367         try:\r\n--> 368             response = requests.request(method=method.upper(), url=url, timeout=timeout, **params)\r\n    369             success = True\r\n    370         except (requests.exceptions.ConnectTimeout, requests.exceptions.ConnectionError) as err:\r\n\r\n/opt/conda/lib/python3.7/site-packages/requests/api.py in request(method, url, **kwargs)\r\n     59     # cases, and look like a memory leak in others.\r\n     60     with sessions.Session() as session:\r\n---> 61         return session.request(method=method, url=url, **kwargs)\r\n     62 \r\n     63 \r\n\r\n/opt/conda/lib/python3.7/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\r\n    527         }\r\n    528         send_kwargs.update(settings)\r\n--> 529         resp = self.send(prep, **send_kwargs)\r\n    530 \r\n    531         return resp\r\n\r\n/opt/conda/lib/python3.7/site-packages/requests/sessions.py in send(self, request, **kwargs)\r\n    643 \r\n    644         # Send the request\r\n--> 645         r = adapter.send(request, **kwargs)\r\n    646 \r\n    647         # Total elapsed time of the request (approximately)\r\n\r\n/opt/conda/lib/python3.7/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\r\n    448                     decode_content=False,\r\n    449                     retries=self.max_retries,\r\n--> 450                     timeout=timeout\r\n    451                 )\r\n    452 \r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    708                 body=body,\r\n    709                 headers=headers,\r\n--> 710                 chunked=chunked,\r\n    711             )\r\n    712 \r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\r\n    384         # Trigger any extra validation we need to do.\r\n    385         try:\r\n--> 386             self._validate_conn(conn)\r\n    387         except (SocketTimeout, BaseSSLError) as e:\r\n    388             # Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn)\r\n   1038         # Force connect early to allow us to validate the connection.\r\n   1039         if not getattr(conn, \"sock\", None):  # AppEngine might not have  `.sock`\r\n-> 1040             conn.connect()\r\n   1041 \r\n   1042         if not conn.is_verified:\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in connect(self)\r\n    356     def connect(self):\r\n    357         # Add certificate verification\r\n--> 358         self.sock = conn = self._new_conn()\r\n    359         hostname = self.host\r\n    360         tls_in_tls = False\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in _new_conn(self)\r\n    173         try:\r\n    174             conn = connection.create_connection(\r\n--> 175                 (self._dns_host, self.port), self.timeout, **extra_kw\r\n    176             )\r\n    177 \r\n\r\nKeyboardInterrupt: \r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4568/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4568/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4566", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4566/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4566/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4566/events", "html_url": "https://github.com/huggingface/datasets/issues/4566", "id": 1284397594, "node_id": "I_kwDODunzps5Mjloa", "number": 4566, "title": "Document link #load_dataset_enhancing_performance points to nowhere", "user": {"login": "subercui", "id": 11674033, "node_id": "MDQ6VXNlcjExNjc0MDMz", "avatar_url": "https://avatars.githubusercontent.com/u/11674033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/subercui", "html_url": "https://github.com/subercui", "followers_url": "https://api.github.com/users/subercui/followers", "following_url": "https://api.github.com/users/subercui/following{/other_user}", "gists_url": "https://api.github.com/users/subercui/gists{/gist_id}", "starred_url": "https://api.github.com/users/subercui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/subercui/subscriptions", "organizations_url": "https://api.github.com/users/subercui/orgs", "repos_url": "https://api.github.com/users/subercui/repos", "events_url": "https://api.github.com/users/subercui/events{/privacy}", "received_events_url": "https://api.github.com/users/subercui/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-06-25T01:18:19Z", "updated_at": "2023-01-24T16:33:40Z", "closed_at": "2023-01-24T16:33:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n![image](https://user-images.githubusercontent.com/11674033/175752806-5b066b92-9d28-4771-9112-5c8606f07741.png)\r\n\r\n\r\nThe [load_dataset_enhancing_performance](https://huggingface.co/docs/datasets/v2.3.2/en/package_reference/main_classes#load_dataset_enhancing_performance) link [here](https://huggingface.co/docs/datasets/v2.3.2/en/package_reference/main_classes#datasets.Dataset.load_from_disk.keep_in_memory) points to nowhere, I guess it should point to https://huggingface.co/docs/datasets/v2.3.2/en/cache#improve-performance?\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4566/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4566/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4550", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4550/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4550/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4550/events", "html_url": "https://github.com/huggingface/datasets/issues/4550", "id": 1282374441, "node_id": "I_kwDODunzps5Mb3sp", "number": 4550, "title": "imdb source error", "user": {"login": "Muhtasham", "id": 20128202, "node_id": "MDQ6VXNlcjIwMTI4MjAy", "avatar_url": "https://avatars.githubusercontent.com/u/20128202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Muhtasham", "html_url": "https://github.com/Muhtasham", "followers_url": "https://api.github.com/users/Muhtasham/followers", "following_url": "https://api.github.com/users/Muhtasham/following{/other_user}", "gists_url": "https://api.github.com/users/Muhtasham/gists{/gist_id}", "starred_url": "https://api.github.com/users/Muhtasham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Muhtasham/subscriptions", "organizations_url": "https://api.github.com/users/Muhtasham/orgs", "repos_url": "https://api.github.com/users/Muhtasham/repos", "events_url": "https://api.github.com/users/Muhtasham/events{/privacy}", "received_events_url": "https://api.github.com/users/Muhtasham/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-06-23T13:02:52Z", "updated_at": "2022-06-23T13:47:05Z", "closed_at": "2022-06-23T13:47:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nimdb dataset not loading\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"imdb\")\r\n```\r\n\r\n\r\n## Expected results\r\n\r\n\r\n## Actual results\r\n```bash\r\n06/23/2022 14:45:18 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\r\n06/23/2022 14:46:34 - INFO - datasets.utils.file_utils - HEAD request to http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz timed out, retrying... [1.0]\r\n.....\r\nConnectionError: Couldn't reach http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz (ConnectTimeout(MaxRetryError(\"HTTPConnectionPool(host='ai.stanford.edu', port=80): Max retries exceeded with url: /~amaas/data/sentiment/aclImdb_v1.tar.gz (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f2d750cf690>, 'Connection to ai.stanford.edu timed out. (connect timeout=100)'))\")))\r\n```\r\n## Environment info\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4550/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4550/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4549", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4549/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4549/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4549/events", "html_url": "https://github.com/huggingface/datasets/issues/4549", "id": 1282312975, "node_id": "I_kwDODunzps5MbosP", "number": 4549, "title": "FileNotFoundError when passing a data_file inside a directory starting with double underscores", "user": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-06-23T12:19:24Z", "updated_at": "2022-06-30T14:38:18Z", "closed_at": "2022-06-30T14:38:18Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Bug experienced in the `accelerate` CI: https://github.com/huggingface/accelerate/runs/7016055148?check_suite_focus=true\r\n\r\nThis is related to https://github.com/huggingface/datasets/pull/4505 and the changes from https://github.com/huggingface/datasets/pull/4412", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4549/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4549/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4528", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4528/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4528/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4528/events", "html_url": "https://github.com/huggingface/datasets/issues/4528", "id": 1276679155, "node_id": "I_kwDODunzps5MGJPz", "number": 4528, "title": "Memory leak when iterating a Dataset", "user": {"login": "NouamaneTazi", "id": 29777165, "node_id": "MDQ6VXNlcjI5Nzc3MTY1", "avatar_url": "https://avatars.githubusercontent.com/u/29777165?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NouamaneTazi", "html_url": "https://github.com/NouamaneTazi", "followers_url": "https://api.github.com/users/NouamaneTazi/followers", "following_url": "https://api.github.com/users/NouamaneTazi/following{/other_user}", "gists_url": "https://api.github.com/users/NouamaneTazi/gists{/gist_id}", "starred_url": "https://api.github.com/users/NouamaneTazi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NouamaneTazi/subscriptions", "organizations_url": "https://api.github.com/users/NouamaneTazi/orgs", "repos_url": "https://api.github.com/users/NouamaneTazi/repos", "events_url": "https://api.github.com/users/NouamaneTazi/events{/privacy}", "received_events_url": "https://api.github.com/users/NouamaneTazi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-06-20T10:03:14Z", "updated_at": "2022-09-12T08:51:39Z", "closed_at": "2022-09-12T08:51:39Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "e## Describe the bug\r\nIt seems that memory never gets freed after iterating a `Dataset` (using `.map()` or a simple `for` loop)\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport gc\r\nimport logging\r\nimport time\r\nimport pyarrow\r\nfrom datasets import load_dataset\r\nfrom tqdm import trange\r\nimport os, psutil\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\nlogger = logging.getLogger(__name__)\r\nprocess = psutil.Process(os.getpid())\r\n\r\nprint(process.memory_info().rss)  # output: 633507840 bytes\r\n\r\ncorpus = load_dataset(\"BeIR/msmarco\", 'corpus', keep_in_memory=False, streaming=False)['corpus'] # or \"BeIR/trec-covid\" for a smaller dataset\r\n\r\nprint(process.memory_info().rss)  # output: 698601472 bytes\r\n\r\nlogger.info(\"Applying method to all examples in all splits\")\r\nfor i in trange(0, len(corpus), 1000):\r\n    batch = corpus[i:i+1000]\r\n    data = pyarrow.total_allocated_bytes()\r\n    if data > 0:\r\n        logger.info(f\"{i}/{len(corpus)}: {data}\")\r\n\r\nprint(process.memory_info().rss)  # output: 3788247040 bytes\r\n\r\ndel batch\r\ngc.collect()\r\n\r\nprint(process.memory_info().rss)  # output: 3788247040 bytes\r\n\r\nlogger.info(\"Done...\")\r\ntime.sleep(100)\r\n```\r\n\r\n## Expected results\r\nLimited memory usage, and memory to be freed after processing\r\n\r\n## Actual results\r\nMemory leak\r\n![test](https://user-images.githubusercontent.com/29777165/174578276-f2c37e6c-b5d8-4985-b4d8-8413eb2b3241.png)\r\nYou can see how the memory allocation keeps increasing until it reaches a steady state when we hit the `time.sleep(100)`, which showcases that even the garbage collector couldn't free the allocated memory\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.4.0-90-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4528/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4521", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4521/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4521/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4521/events", "html_url": "https://github.com/huggingface/datasets/issues/4521", "id": 1274919437, "node_id": "I_kwDODunzps5L_boN", "number": 4521, "title": "Datasets method `.map` not hashing", "user": {"login": "sanchit-gandhi", "id": 93869735, "node_id": "U_kgDOBZhWpw", "avatar_url": "https://avatars.githubusercontent.com/u/93869735?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sanchit-gandhi", "html_url": "https://github.com/sanchit-gandhi", "followers_url": "https://api.github.com/users/sanchit-gandhi/followers", "following_url": "https://api.github.com/users/sanchit-gandhi/following{/other_user}", "gists_url": "https://api.github.com/users/sanchit-gandhi/gists{/gist_id}", "starred_url": "https://api.github.com/users/sanchit-gandhi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sanchit-gandhi/subscriptions", "organizations_url": "https://api.github.com/users/sanchit-gandhi/orgs", "repos_url": "https://api.github.com/users/sanchit-gandhi/repos", "events_url": "https://api.github.com/users/sanchit-gandhi/events{/privacy}", "received_events_url": "https://api.github.com/users/sanchit-gandhi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-06-17T11:31:10Z", "updated_at": "2022-08-04T12:08:16Z", "closed_at": "2022-06-28T13:23:05Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nDatasets method `.map` not hashing, even with an empty no-op function\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# download 9MB dummy dataset\r\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\")\r\n\r\ndef prepare_dataset(batch):\r\n     return(batch)\r\n\r\nds = ds.map(\r\n    prepare_dataset,\r\n    num_proc=1,\r\n    desc=\"preprocess train dataset\",\r\n)\r\n```\r\n\r\n## Expected results\r\nHashed and cached dataset preprocessing\r\n\r\n## Actual results\r\nDoes not hash properly:\r\n```\r\nParameter 'function'=<function prepare_dataset at 0x7fccb68e9280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.3.3.dev0\r\n- Platform: Linux-5.11.0-1028-gcp-x86_64-with-glibc2.31\r\n- Python version: 3.9.12\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n\r\ncc @lhoestq \r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4521/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4521/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4520", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4520/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4520/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4520/events", "html_url": "https://github.com/huggingface/datasets/issues/4520", "id": 1274879180, "node_id": "I_kwDODunzps5L_RzM", "number": 4520, "title": "Failure to  hash `dataclasses` - results in functions that cannot be hashed or cached in `.map`", "user": {"login": "sanchit-gandhi", "id": 93869735, "node_id": "U_kgDOBZhWpw", "avatar_url": "https://avatars.githubusercontent.com/u/93869735?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sanchit-gandhi", "html_url": "https://github.com/sanchit-gandhi", "followers_url": "https://api.github.com/users/sanchit-gandhi/followers", "following_url": "https://api.github.com/users/sanchit-gandhi/following{/other_user}", "gists_url": "https://api.github.com/users/sanchit-gandhi/gists{/gist_id}", "starred_url": "https://api.github.com/users/sanchit-gandhi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sanchit-gandhi/subscriptions", "organizations_url": "https://api.github.com/users/sanchit-gandhi/orgs", "repos_url": "https://api.github.com/users/sanchit-gandhi/repos", "events_url": "https://api.github.com/users/sanchit-gandhi/events{/privacy}", "received_events_url": "https://api.github.com/users/sanchit-gandhi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-06-17T10:47:17Z", "updated_at": "2022-06-28T14:47:17Z", "closed_at": "2022-06-28T14:04:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Dataclasses cannot be hashed. As a result, they cannot be hashed or cached if used in the `.map` method. Dataclasses are used extensively in Transformers examples scripts: (c.f. [CTC example](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py)). Since dataclasses cannot be hashed, one has to define separate variables prior to passing dataclass attributes to the `.map` method:\r\n```python\r\nphoneme_language = data_args.phoneme_language\r\n```\r\nin the example https://github.com/huggingface/transformers/blob/3c7e56fbb11f401de2528c1dcf0e282febc031cd/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py#L603-L630\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom dataclasses import dataclass, field\r\nfrom datasets.fingerprint import Hasher\r\n\r\n@dataclass\r\nclass DataTrainingArguments:\r\n    \"\"\"\r\n    Arguments pertaining to what data we are going to input our model for training and eval.\r\n    \"\"\"\r\n\r\n    phoneme_language: str = field(\r\n        default=None, metadata={\"help\": \"The name of the phoneme language to use.\"}\r\n    )\r\n\r\ndata_args = DataTrainingArguments(phoneme_language =\"foo\")\r\n\r\nHasher.hash(data_args)\r\n\r\nphoneme_language = data_args.phoneme_language\r\n\r\nHasher.hash(phoneme_language)\r\n```\r\n\r\n## Expected results\r\nA hash.\r\n## Actual results\r\n<details>\r\n<summary> Traceback </summary>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\nInput In [1], in <cell line: 16>()\r\n     10     phoneme_language: str = field(\r\n     11         default=None, metadata={\"help\": \"The name of the phoneme language to use.\"}\r\n     12     )\r\n     14 data_args = DataTrainingArguments(phoneme_language =\"foo\")\r\n---> 16 Hasher.hash(data_args)\r\n     18 phoneme_language = data_args. phoneme_language\r\n     20 Hasher.hash(phoneme_language)\r\n\r\nFile ~/datasets/src/datasets/fingerprint.py:237, in Hasher.hash(cls, value)\r\n    235     return cls.dispatch[type(value)](cls, value)\r\n    236 else:\r\n--> 237     return cls.hash_default(value)\r\n\r\nFile ~/datasets/src/datasets/fingerprint.py:230, in Hasher.hash_default(cls, value)\r\n    228 @classmethod\r\n    229 def hash_default(cls, value: Any) -> str:\r\n--> 230     return cls.hash_bytes(dumps(value))\r\n\r\nFile ~/datasets/src/datasets/utils/py_utils.py:564, in dumps(obj)\r\n    562 file = StringIO()\r\n    563 with _no_cache_fields(obj):\r\n--> 564     dump(obj, file)\r\n    565 return file.getvalue()\r\n\r\nFile ~/datasets/src/datasets/utils/py_utils.py:539, in dump(obj, file)\r\n    537 def dump(obj, file):\r\n    538     \"\"\"pickle an object to a file\"\"\"\r\n--> 539     Pickler(file, recurse=True).dump(obj)\r\n    540     return\r\n\r\nFile ~/hf/lib/python3.8/site-packages/dill/_dill.py:620, in Pickler.dump(self, obj)\r\n    618     raise PicklingError(msg)\r\n    619 else:\r\n--> 620     StockPickler.dump(self, obj)\r\n    621 return\r\n\r\nFile /usr/lib/python3.8/pickle.py:487, in _Pickler.dump(self, obj)\r\n    485 if self.proto >= 4:\r\n    486     self.framer.start_framing()\r\n--> 487 self.save(obj)\r\n    488 self.write(STOP)\r\n    489 self.framer.end_framing()\r\n\r\nFile /usr/lib/python3.8/pickle.py:603, in _Pickler.save(self, obj, save_persistent_id)\r\n    599     raise PicklingError(\"Tuple returned by %s must have \"\r\n    600                         \"two to six elements\" % reduce)\r\n    602 # Save the reduce() output and finally memoize the object\r\n--> 603 self.save_reduce(obj=obj, *rv)\r\n\r\nFile /usr/lib/python3.8/pickle.py:687, in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\r\n    684     raise PicklingError(\r\n    685         \"args[0] from __newobj__ args has the wrong class\")\r\n    686 args = args[1:]\r\n--> 687 save(cls)\r\n    688 save(args)\r\n    689 write(NEWOBJ)\r\n\r\nFile /usr/lib/python3.8/pickle.py:560, in _Pickler.save(self, obj, save_persistent_id)\r\n    558 f = self.dispatch.get(t)\r\n    559 if f is not None:\r\n--> 560     f(self, obj)  # Call unbound method with explicit self\r\n    561     return\r\n    563 # Check private dispatch table if any, or else\r\n    564 # copyreg.dispatch_table\r\n\r\nFile ~/hf/lib/python3.8/site-packages/dill/_dill.py:1838, in save_type(pickler, obj, postproc_list)\r\n   1836             postproc_list = []\r\n   1837         postproc_list.append((setattr, (obj, '__qualname__', obj_name)))\r\n-> 1838     _save_with_postproc(pickler, (_create_type, (\r\n   1839         type(obj), obj.__name__, obj.__bases__, _dict\r\n   1840     )), obj=obj, postproc_list=postproc_list)\r\n   1841     log.info(\"# %s\" % _t)\r\n   1842 else:\r\n\r\nFile ~/hf/lib/python3.8/site-packages/dill/_dill.py:1140, in _save_with_postproc(pickler, reduction, is_pickler_dill, obj, postproc_list)\r\n   1137     pickler._postproc[id(obj)] = postproc_list\r\n   1139 # TODO: Use state_setter in Python 3.8 to allow for faster cPickle implementations\r\n-> 1140 pickler.save_reduce(*reduction, obj=obj)\r\n   1142 if is_pickler_dill:\r\n   1143     # pickler.x -= 1\r\n   1144     # print(pickler.x*' ', 'pop', obj, id(obj))\r\n   1145     postproc = pickler._postproc.pop(id(obj))\r\n\r\nFile /usr/lib/python3.8/pickle.py:692, in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\r\n    690 else:\r\n    691     save(func)\r\n--> 692     save(args)\r\n    693     write(REDUCE)\r\n    695 if obj is not None:\r\n    696     # If the object is already in the memo, this means it is\r\n    697     # recursive. In this case, throw away everything we put on the\r\n    698     # stack, and fetch the object back from the memo.\r\n\r\nFile /usr/lib/python3.8/pickle.py:560, in _Pickler.save(self, obj, save_persistent_id)\r\n    558 f = self.dispatch.get(t)\r\n    559 if f is not None:\r\n--> 560     f(self, obj)  # Call unbound method with explicit self\r\n    561     return\r\n    563 # Check private dispatch table if any, or else\r\n    564 # copyreg.dispatch_table\r\n\r\nFile /usr/lib/python3.8/pickle.py:901, in _Pickler.save_tuple(self, obj)\r\n    899 write(MARK)\r\n    900 for element in obj:\r\n--> 901     save(element)\r\n    903 if id(obj) in memo:\r\n    904     # Subtle.  d was not in memo when we entered save_tuple(), so\r\n    905     # the process of saving the tuple's elements must have saved\r\n   (...)\r\n    909     # could have been done in the \"for element\" loop instead, but\r\n    910     # recursive tuples are a rare thing.\r\n    911     get = self.get(memo[id(obj)][0])\r\n\r\nFile /usr/lib/python3.8/pickle.py:560, in _Pickler.save(self, obj, save_persistent_id)\r\n    558 f = self.dispatch.get(t)\r\n    559 if f is not None:\r\n--> 560     f(self, obj)  # Call unbound method with explicit self\r\n    561     return\r\n    563 # Check private dispatch table if any, or else\r\n    564 # copyreg.dispatch_table\r\n\r\nFile ~/hf/lib/python3.8/site-packages/dill/_dill.py:1251, in save_module_dict(pickler, obj)\r\n   1248     if is_dill(pickler, child=False) and pickler._session:\r\n   1249         # we only care about session the first pass thru\r\n   1250         pickler._first_pass = False\r\n-> 1251     StockPickler.save_dict(pickler, obj)\r\n   1252     log.info(\"# D2\")\r\n   1253 return\r\n\r\nFile /usr/lib/python3.8/pickle.py:971, in _Pickler.save_dict(self, obj)\r\n    968     self.write(MARK + DICT)\r\n    970 self.memoize(obj)\r\n--> 971 self._batch_setitems(obj.items())\r\n\r\nFile /usr/lib/python3.8/pickle.py:997, in _Pickler._batch_setitems(self, items)\r\n    995     for k, v in tmp:\r\n    996         save(k)\r\n--> 997         save(v)\r\n    998     write(SETITEMS)\r\n    999 elif n:\r\n\r\nFile /usr/lib/python3.8/pickle.py:560, in _Pickler.save(self, obj, save_persistent_id)\r\n    558 f = self.dispatch.get(t)\r\n    559 if f is not None:\r\n--> 560     f(self, obj)  # Call unbound method with explicit self\r\n    561     return\r\n    563 # Check private dispatch table if any, or else\r\n    564 # copyreg.dispatch_table\r\n\r\nFile ~/datasets/src/datasets/utils/py_utils.py:862, in save_function(pickler, obj)\r\n    859     if state_dict:\r\n    860         state = state, state_dict\r\n--> 862     dill._dill._save_with_postproc(\r\n    863         pickler,\r\n    864         (\r\n    865             dill._dill._create_function,\r\n    866             (obj.__code__, globs, obj.__name__, obj.__defaults__, closure),\r\n    867             state,\r\n    868         ),\r\n    869         obj=obj,\r\n    870         postproc_list=postproc_list,\r\n    871     )\r\n    872 else:\r\n    873     closure = obj.func_closure\r\n\r\nFile ~/hf/lib/python3.8/site-packages/dill/_dill.py:1153, in _save_with_postproc(pickler, reduction, is_pickler_dill, obj, postproc_list)\r\n   1151 dest, source = reduction[1]\r\n   1152 if source:\r\n-> 1153     pickler.write(pickler.get(pickler.memo[id(dest)][0]))\r\n   1154     pickler._batch_setitems(iter(source.items()))\r\n   1155 else:\r\n   1156     # Updating with an empty dictionary. Same as doing nothing.\r\n\r\nKeyError: 140434581781568\r\n```\r\n\r\n</details>\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.3.3.dev0\r\n- Platform: Linux-5.11.0-1028-gcp-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n\r\ncc @lhoestq ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4520/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4520/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4514", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4514/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4514/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4514/events", "html_url": "https://github.com/huggingface/datasets/issues/4514", "id": 1273505230, "node_id": "I_kwDODunzps5L6CXO", "number": 4514, "title": "Allow .JPEG as a file extension", "user": {"login": "DiGyt", "id": 34550289, "node_id": "MDQ6VXNlcjM0NTUwMjg5", "avatar_url": "https://avatars.githubusercontent.com/u/34550289?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DiGyt", "html_url": "https://github.com/DiGyt", "followers_url": "https://api.github.com/users/DiGyt/followers", "following_url": "https://api.github.com/users/DiGyt/following{/other_user}", "gists_url": "https://api.github.com/users/DiGyt/gists{/gist_id}", "starred_url": "https://api.github.com/users/DiGyt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DiGyt/subscriptions", "organizations_url": "https://api.github.com/users/DiGyt/orgs", "repos_url": "https://api.github.com/users/DiGyt/repos", "events_url": "https://api.github.com/users/DiGyt/events{/privacy}", "received_events_url": "https://api.github.com/users/DiGyt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-06-16T12:36:20Z", "updated_at": "2022-06-20T08:18:46Z", "closed_at": "2022-06-16T17:11:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen loading image data, HF datasets seems to recognize `.jpg` and `.jpeg` file extensions, but not e.g. .JPEG. As the naming convention .JPEG is used in important datasets such as imagenet, I would welcome if according extensions like .JPEG or .JPG would be allowed.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# use bash to create 2 sham datasets with jpeg and JPEG ext\r\n!mkdir dataset_a\r\n!mkdir dataset_b\r\n!wget https://upload.wikimedia.org/wikipedia/commons/7/71/Dsc_%28179253513%29.jpeg -O example_img.jpeg\r\n!cp example_img.jpeg ./dataset_a/\r\n!mv example_img.jpeg ./dataset_b/example_img.JPEG\r\n\r\nfrom datasets import load_dataset\r\n\r\n# working\r\ndf1 = load_dataset(\"./dataset_a\", ignore_verifications=True)\r\n\r\n#not working\r\ndf2 = load_dataset(\"./dataset_b\", ignore_verifications=True)\r\n\r\n# show\r\nprint(df1, df2)\r\n```\r\n\r\n## Expected results\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 1\r\n    })\r\n}) DatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 1\r\n    })\r\n})\r\n```\r\n\r\n## Actual results\r\n```\r\nFileNotFoundError: Unable to resolve any data file that matches '['**']' at /..PATH../dataset_b with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'zip']\r\n```\r\n\r\nI know that it can be annoying to allow seemingly arbitrary numbers of file extensions. But I think this one would be really welcome.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4514/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4514/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4508", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4508/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4508/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4508/events", "html_url": "https://github.com/huggingface/datasets/issues/4508", "id": 1272718921, "node_id": "I_kwDODunzps5L3CZJ", "number": 4508, "title": "cast_storage method from datasets.features", "user": {"login": "romainremyb", "id": 67968596, "node_id": "MDQ6VXNlcjY3OTY4NTk2", "avatar_url": "https://avatars.githubusercontent.com/u/67968596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/romainremyb", "html_url": "https://github.com/romainremyb", "followers_url": "https://api.github.com/users/romainremyb/followers", "following_url": "https://api.github.com/users/romainremyb/following{/other_user}", "gists_url": "https://api.github.com/users/romainremyb/gists{/gist_id}", "starred_url": "https://api.github.com/users/romainremyb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/romainremyb/subscriptions", "organizations_url": "https://api.github.com/users/romainremyb/orgs", "repos_url": "https://api.github.com/users/romainremyb/repos", "events_url": "https://api.github.com/users/romainremyb/events{/privacy}", "received_events_url": "https://api.github.com/users/romainremyb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-06-15T20:47:22Z", "updated_at": "2022-06-16T13:54:07Z", "closed_at": "2022-06-16T13:54:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA bug occurs when mapping a function to a dataset object. I ran the same code with the same data yesterday and it worked just fine. It works when i run locally on an old version of datasets.\r\n\r\n## Steps to reproduce the bug\r\nSteps are:\r\n- load whatever datset\r\n- write a preprocessing function such as \"tokenize_and_align_labels\" written in https://huggingface.co/docs/transformers/tasks/token_classification\r\n- map the function on dataset and get \"ValueError: Class label -100 less than -1\" from cast_storage method from datasets.features\r\n\r\n# Sample code to reproduce the bug\r\ndef tokenize_and_align_labels(examples):\r\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=38,padding=\"max_length\")\r\n\r\n    labels = []\r\n    for i, label in enumerate(examples[f\"labels\"]):\r\n        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\r\n        previous_word_idx = None\r\n        label_ids = []\r\n        for word_idx in word_ids:  # Set the special tokens to -100.\r\n            if word_idx is None:\r\n                label_ids.append(-100)\r\n            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\r\n                label_ids.append(label[word_idx])\r\n            else:\r\n                label_ids.append(-100)\r\n            previous_word_idx = word_idx\r\n        labels.append(label_ids)\r\n\r\n    tokenized_inputs[\"labels\"] = labels\r\n    return tokenized_inputs\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\r\ndt = dataset.map(tokenize_and_align_labels, batched=True)\r\n\r\n## Expected results\r\nNew dataset objects should load and do on older versions.\r\n\r\n## Actual results\r\n\"ValueError: Class label -100 less than -1\" from cast_storage method from datasets.features\r\n\r\n## Environment info\r\neverything works fine on older installations of datasets/transformers\r\n\r\nIssue arises when installing datasets on google collab under python3.7\r\nI can't manage to find the exact output you're requirering but version printed is datasets-2.3.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4508/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4508/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4506", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4506/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4506/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4506/events", "html_url": "https://github.com/huggingface/datasets/issues/4506", "id": 1272516895, "node_id": "I_kwDODunzps5L2REf", "number": 4506, "title": "Failure to hash (and cache) a `.map(...)` (almost always) - using this method can produce incorrect results", "user": {"login": "DrMatters", "id": 22641583, "node_id": "MDQ6VXNlcjIyNjQxNTgz", "avatar_url": "https://avatars.githubusercontent.com/u/22641583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DrMatters", "html_url": "https://github.com/DrMatters", "followers_url": "https://api.github.com/users/DrMatters/followers", "following_url": "https://api.github.com/users/DrMatters/following{/other_user}", "gists_url": "https://api.github.com/users/DrMatters/gists{/gist_id}", "starred_url": "https://api.github.com/users/DrMatters/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DrMatters/subscriptions", "organizations_url": "https://api.github.com/users/DrMatters/orgs", "repos_url": "https://api.github.com/users/DrMatters/repos", "events_url": "https://api.github.com/users/DrMatters/events{/privacy}", "received_events_url": "https://api.github.com/users/DrMatters/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-06-15T17:11:31Z", "updated_at": "2023-02-16T03:14:32Z", "closed_at": "2022-06-28T13:23:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nSometimes I get messages about not being able to hash a method:\r\n`Parameter 'function'=<function StupidDataModule._separate_speaker_id_from_dialogue at 0x7f1b27180d30> of the transform datasets.arrow_dataset.Dataset.\r\n_map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.`\r\nWhilst the function looks like this:\r\n```python\r\n@staticmethod\r\ndef _separate_speaker_id_from_dialogue(example: arrow_dataset.Example):\r\n    speaker_id, dialogue = tuple(zip(*(example[\"dialogue\"])))\r\n    example[\"speaker_id\"] = speaker_id\r\n    example[\"dialogue\"] = dialogue\r\n    return example\r\n```\r\nThis is the first step in my preprocessing pipeline, but sometimes the message about failure to hash is not appearing on the first step, but then appears on a later step.\r\nThis error is sometimes causing a failure to use cached data, instead of re-running all steps again.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport copy\r\nimport datasets\r\nfrom datasets import arrow_dataset\r\n\r\ndef main():\r\n    dataset = datasets.load_dataset(\"blended_skill_talk\")\r\n    res = dataset.map(method)\r\n    print(res)\r\n\r\ndef method(example: arrow_dataset.Example):\r\n    example['previous_utterance_copy'] = copy.deepcopy(example['previous_utterance'])\r\n    return example\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nRun with:\r\n```\r\npython -m reproduce_error\r\n```\r\n\r\n## Expected results\r\nDataset is mapped and cached correctly.\r\n\r\n## Actual results\r\nThe code outputs this at some point:\r\n`Parameter 'function'=<function method at 0x7faa83d2a160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.`\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform: Ubuntu 20.04.3\r\n- Python version: 3.9.12\r\n- PyArrow version: 8.0.0\r\n- Datasets version: 2.3.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4506/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4506/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4498", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4498/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4498/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4498/events", "html_url": "https://github.com/huggingface/datasets/issues/4498", "id": 1272100549, "node_id": "I_kwDODunzps5L0rbF", "number": 4498, "title": "WER and CER > 1", "user": {"login": "sadrasabouri", "id": 43045767, "node_id": "MDQ6VXNlcjQzMDQ1NzY3", "avatar_url": "https://avatars.githubusercontent.com/u/43045767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sadrasabouri", "html_url": "https://github.com/sadrasabouri", "followers_url": "https://api.github.com/users/sadrasabouri/followers", "following_url": "https://api.github.com/users/sadrasabouri/following{/other_user}", "gists_url": "https://api.github.com/users/sadrasabouri/gists{/gist_id}", "starred_url": "https://api.github.com/users/sadrasabouri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sadrasabouri/subscriptions", "organizations_url": "https://api.github.com/users/sadrasabouri/orgs", "repos_url": "https://api.github.com/users/sadrasabouri/repos", "events_url": "https://api.github.com/users/sadrasabouri/events{/privacy}", "received_events_url": "https://api.github.com/users/sadrasabouri/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-06-15T11:35:12Z", "updated_at": "2022-06-15T16:38:05Z", "closed_at": "2022-06-15T16:38:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nIt seems that in some cases in which the `prediction` is longer than the `reference` we may have word/character error rate higher than 1 which is a bit odd.\r\n\r\nIf it's a real bug I think I can solve it with a PR changing [this](https://github.com/huggingface/datasets/blob/master/metrics/wer/wer.py#L105) line to\r\n```python\r\nreturn min(incorrect / total, 1.0)\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_metric\r\nwer = load_metric(\"wer\")\r\nwer_value = wer.compute(predictions=[\"Hi World vka\"], references=[\"Hello\"])\r\nprint(wer_value)\r\n```\r\n\r\n## Expected results\r\n```\r\n1.0\r\n```\r\n\r\n## Actual results\r\n```\r\n3.0\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.3.0\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4498/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4483", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4483/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4483/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4483/events", "html_url": "https://github.com/huggingface/datasets/issues/4483", "id": 1269253840, "node_id": "I_kwDODunzps5Lp0bQ", "number": 4483, "title": "Dataset.map throws pyarrow.lib.ArrowNotImplementedError when converting from list of empty lists", "user": {"login": "sanderland", "id": 48946947, "node_id": "MDQ6VXNlcjQ4OTQ2OTQ3", "avatar_url": "https://avatars.githubusercontent.com/u/48946947?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sanderland", "html_url": "https://github.com/sanderland", "followers_url": "https://api.github.com/users/sanderland/followers", "following_url": "https://api.github.com/users/sanderland/following{/other_user}", "gists_url": "https://api.github.com/users/sanderland/gists{/gist_id}", "starred_url": "https://api.github.com/users/sanderland/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sanderland/subscriptions", "organizations_url": "https://api.github.com/users/sanderland/orgs", "repos_url": "https://api.github.com/users/sanderland/repos", "events_url": "https://api.github.com/users/sanderland/events{/privacy}", "received_events_url": "https://api.github.com/users/sanderland/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-06-13T10:47:52Z", "updated_at": "2022-06-14T13:34:14Z", "closed_at": "2022-06-14T13:34:14Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nDataset.map throws pyarrow.lib.ArrowNotImplementedError: Unsupported cast from int64 to null using function cast_null when converting from a type of 'empty lists' to 'lists with some type'.\r\n\r\nThis appears to be due to the interaction of arrow internals and some assumptions made by datasets.\r\n\r\nThe bug appeared when binarizing some labels, and then adding a dataset which had all these labels absent (to force the model to not label empty strings such with anything)\r\nParticularly the fact that this only happens in batched mode is strange.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport numpy as np\r\nds = Dataset.from_dict(\r\n    {\r\n        \"text\": [\"the lazy dog jumps over the quick fox\", \"another sentence\"],\r\n        \"label\": [[], []],\r\n    }\r\n)\r\ndef mapper(features):\r\n    features['label'] = [\r\n        [0,0,0] for l in features['label']\r\n    ]\r\n    return features\r\nds_mapped = ds.map(mapper,batched=True)\r\n```\r\n\r\n## Expected results\r\nNot crashing\r\n\r\n## Actual results\r\n```\r\n../.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:2346: in map\r\n    return self._map_single(\r\n../.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:532: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:499: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../.venv/lib/python3.8/site-packages/datasets/fingerprint.py:458: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:2751: in _map_single\r\n    writer.write_batch(batch)\r\n../.venv/lib/python3.8/site-packages/datasets/arrow_writer.py:503: in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\npyarrow/array.pxi:230: in pyarrow.lib.array\r\n    ???\r\npyarrow/array.pxi:110: in pyarrow.lib._handle_arrow_array_protocol\r\n    ???\r\n../.venv/lib/python3.8/site-packages/datasets/arrow_writer.py:198: in __arrow_array__\r\n    out = cast_array_to_feature(out, type, allow_number_to_str=not self.trying_type)\r\n../.venv/lib/python3.8/site-packages/datasets/table.py:1675: in wrapper\r\n    return func(array, *args, **kwargs)\r\n../.venv/lib/python3.8/site-packages/datasets/table.py:1812: in cast_array_to_feature\r\n    casted_values = _c(array.values, feature.feature)\r\n../.venv/lib/python3.8/site-packages/datasets/table.py:1675: in wrapper\r\n    return func(array, *args, **kwargs)\r\n../.venv/lib/python3.8/site-packages/datasets/table.py:1843: in cast_array_to_feature\r\n    return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n../.venv/lib/python3.8/site-packages/datasets/table.py:1675: in wrapper\r\n    return func(array, *args, **kwargs)\r\n../.venv/lib/python3.8/site-packages/datasets/table.py:1752: in array_cast\r\n    return array.cast(pa_type)\r\npyarrow/array.pxi:915: in pyarrow.lib.Array.cast\r\n    ???\r\n../.venv/lib/python3.8/site-packages/pyarrow/compute.py:376: in cast\r\n    return call_function(\"cast\", [arr], options)\r\npyarrow/_compute.pyx:542: in pyarrow._compute.call_function\r\n    ???\r\npyarrow/_compute.pyx:341: in pyarrow._compute.Function.call\r\n    ???\r\npyarrow/error.pxi:144: in pyarrow.lib.pyarrow_internal_check_status\r\n    ???\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n>   ???\r\nE   pyarrow.lib.ArrowNotImplementedError: Unsupported cast from int64 to null using function cast_null\r\npyarrow/error.pxi:121: ArrowNotImplementedError\r\n```\r\n\r\n## Workarounds\r\n\r\n* Not using batched=True\r\n* Using an np.array([],dtype=float) or similar instead of [] in the input\r\n* Naming the output column differently from the input column\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.2\r\n- Platform: Ubuntu\r\n- Python version: 3.8\r\n- PyArrow version: 8.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4483/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4480", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4480/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4480/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4480/events", "html_url": "https://github.com/huggingface/datasets/issues/4480", "id": 1268921567, "node_id": "I_kwDODunzps5LojTf", "number": 4480, "title": "Bigbench tensorflow GPU dependency", "user": {"login": "cceyda", "id": 15624271, "node_id": "MDQ6VXNlcjE1NjI0Mjcx", "avatar_url": "https://avatars.githubusercontent.com/u/15624271?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cceyda", "html_url": "https://github.com/cceyda", "followers_url": "https://api.github.com/users/cceyda/followers", "following_url": "https://api.github.com/users/cceyda/following{/other_user}", "gists_url": "https://api.github.com/users/cceyda/gists{/gist_id}", "starred_url": "https://api.github.com/users/cceyda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cceyda/subscriptions", "organizations_url": "https://api.github.com/users/cceyda/orgs", "repos_url": "https://api.github.com/users/cceyda/repos", "events_url": "https://api.github.com/users/cceyda/events{/privacy}", "received_events_url": "https://api.github.com/users/cceyda/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-06-13T05:24:06Z", "updated_at": "2022-06-14T19:45:24Z", "closed_at": "2022-06-14T19:45:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading bigbech\r\n```py\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"bigbench\",\"swedish_to_german_proverbs\")\r\n```\r\ntries to use gpu and fails with OOM with the following error\r\n\r\n```\r\nDownloading and preparing dataset bigbench/swedish_to_german_proverbs (download: Unknown size, generated: 68.92 KiB, post-processed: Unknown size, total: 68.92 KiB) to /home/ceyda/.cache/huggingface/datasets/bigbench/swedish_to_german_proverbs/1.0.0/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0...\r\nGenerating default split:   0%|                                                                                                                        | 0/72 [00:00<?, ? examples/s]2022-06-13 14:11:04.154469: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-06-13 14:11:05.133600: F tensorflow/core/platform/statusor.cc:33] Attempting to fetch value instead of handling error INTERNAL: failed initializing StreamExecutor for CUDA device ordinal 3: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 25396838400\r\nAborted (core dumped)\r\n```\r\nI think this is because bigbench dependency (below) installs tensorflow (GPU version) and dataloading tries to use GPU as default.\r\n\r\n`pip install bigbench@https://storage.googleapis.com/public_research_data/bigbench/bigbench-0.0.1.tar.gz`\r\n\r\nwhile just doing 'pip install bigbench' results in following error \r\n```\r\nFile \"/home/ceyda/.local/lib/python3.7/site-packages/datasets/load.py\", line 109, in import_main_class\r\n    module = importlib.import_module(module_path)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/ceyda/.cache/huggingface/modules/datasets_modules/datasets/bigbench/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0/bigbench.py\", line 118, in <module>\r\n    class Bigbench(datasets.GeneratorBasedBuilder):\r\n  File \"/home/ceyda/.cache/huggingface/modules/datasets_modules/datasets/bigbench/7d2f6e537fa937dfaac8b1c1df782f2055071d3fd8e4f4ae93d28012a354ced0/bigbench.py\", line 127, in Bigbench\r\n    BigBenchConfig(name=name, version=datasets.Version(\"1.0.0\")) for name in bb_utils.get_all_json_task_names()\r\nAttributeError: module 'bigbench.api.util' has no attribute 'get_all_json_task_names'\r\n```\r\n\r\n## Steps to avoid the bug\r\nNot ideal but can solve with (since I don't really use tensorflow elsewhere)\r\n`pip uninstall tensorflow` \r\n`pip install tensorflow-cpu`\r\n\r\n\r\n## Environment info\r\n- datasets @ master\r\n- Python version: 3.7\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4480/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4471", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4471/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4471/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4471/events", "html_url": "https://github.com/huggingface/datasets/issues/4471", "id": 1267475268, "node_id": "I_kwDODunzps5LjCNE", "number": 4471, "title": "CI error with repo lhoestq/_dummy", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-06-10T12:26:06Z", "updated_at": "2022-06-10T13:24:53Z", "closed_at": "2022-06-10T13:24:53Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nCI is failing because of repo \"lhoestq/_dummy\". See: https://app.circleci.com/pipelines/github/huggingface/datasets/12461/workflows/1b040b45-9578-4ab9-8c44-c643c4eb8691/jobs/74269\r\n```\r\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/datasets/lhoestq/_dummy?full=true\r\n```\r\n\r\nThe repo seems to no longer exist: https://huggingface.co/api/datasets/lhoestq/_dummy\r\n```\r\nerror: \"Repository not found\"\r\n```\r\n\r\nCC: @lhoestq ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4471/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4467", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4467/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4467/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4467/events", "html_url": "https://github.com/huggingface/datasets/issues/4467", "id": 1266218358, "node_id": "I_kwDODunzps5LePV2", "number": 4467, "title": "Transcript string 'null' converted to [None] by load_dataset()", "user": {"login": "mbarnig", "id": 1360633, "node_id": "MDQ6VXNlcjEzNjA2MzM=", "avatar_url": "https://avatars.githubusercontent.com/u/1360633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mbarnig", "html_url": "https://github.com/mbarnig", "followers_url": "https://api.github.com/users/mbarnig/followers", "following_url": "https://api.github.com/users/mbarnig/following{/other_user}", "gists_url": "https://api.github.com/users/mbarnig/gists{/gist_id}", "starred_url": "https://api.github.com/users/mbarnig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mbarnig/subscriptions", "organizations_url": "https://api.github.com/users/mbarnig/orgs", "repos_url": "https://api.github.com/users/mbarnig/repos", "events_url": "https://api.github.com/users/mbarnig/events{/privacy}", "received_events_url": "https://api.github.com/users/mbarnig/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-06-09T14:26:00Z", "updated_at": "2022-06-09T17:55:37Z", "closed_at": "2022-06-09T16:29:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Issue\r\nI am training a luxembourgish speech-recognition model in Colab with a custom dataset, including a dictionary of luxembourgish words, for example the speaken numbers 0 to 9. When preparing the dataset with the script \r\n\r\n`ds_train1 = mydataset.map(prepare_dataset)` \r\n\r\nthe following error was issued:\r\n\r\n```   \r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-69-1e8f2b37f5bc> in <module>()\r\n----> 1 ds_train = mydataset_train.map(prepare_dataset)\r\n\r\n11 frames\r\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in __call__(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\r\n   2450         if not _is_valid_text_input(text):\r\n   2451             raise ValueError(\r\n-> 2452                 \"text input must of type str (single example), List[str] (batch or single pretokenized example) \"\r\n   2453                 \"or List[List[str]] (batch of pretokenized examples).\"\r\n   2454             )\r\n\r\nValueError: text input must of type str (single example), List[str] (batch or single pretokenized example) or List[List[str]] (batch of pretokenized examples).\r\n```\r\nDebugging this problem was not easy, all transcriptions in the dataset are correct strings. Finally I discovered that the transcription string 'null' is interpreted as [None] by the `load_dataset()` script. By deleting this row in the dataset the  training worked fine.\r\n\r\n## Expected result: \r\ntranscription 'null' interpreted as 'str' instead of 'None'.\r\n\r\n## Reproduction\r\nHere is the code to reproduce the error with a one-row-dataset.\r\n\r\n```  \r\nwith open(\"null-test.csv\") as f:\r\n    reader = csv.reader(f)\r\n    for row in reader:\r\n        print(row)\r\n```  \r\n\r\n['wav_filename', 'wav_filesize', 'transcript']\r\n['wavs/female/NULL1.wav', '17530', 'null']\r\n\r\n```\r\ndataset = load_dataset('csv', data_files={'train': 'null-test.csv'}) \r\n```  \r\n\r\nUsing custom data configuration default-81ac0c0e27af3514\r\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-81ac0c0e27af3514/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\r\nDownloading data files: 100%\r\n1/1 [00:00<00:00, 29.55it/s]\r\nExtracting data files: 100%\r\n1/1 [00:00<00:00, 23.66it/s]\r\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-81ac0c0e27af3514/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\r\n100%\r\n1/1 [00:00<00:00, 25.84it/s]\r\n\r\n```  \r\nprint(dataset['train']['transcript'])\r\n``` \r\n\r\n[None]\r\n\r\n## Environment info\r\n```\r\n!pip install datasets==2.2.2\r\n!pip install transformers==4.19.2\r\n```  ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4467/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4467/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4461", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4461/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4461/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4461/events", "html_url": "https://github.com/huggingface/datasets/issues/4461", "id": 1264800451, "node_id": "I_kwDODunzps5LY1LD", "number": 4461, "title": "AttributeError: module 'datasets' has no attribute 'load_dataset'", "user": {"login": "AlexNLP", "id": 59248970, "node_id": "MDQ6VXNlcjU5MjQ4OTcw", "avatar_url": "https://avatars.githubusercontent.com/u/59248970?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AlexNLP", "html_url": "https://github.com/AlexNLP", "followers_url": "https://api.github.com/users/AlexNLP/followers", "following_url": "https://api.github.com/users/AlexNLP/following{/other_user}", "gists_url": "https://api.github.com/users/AlexNLP/gists{/gist_id}", "starred_url": "https://api.github.com/users/AlexNLP/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AlexNLP/subscriptions", "organizations_url": "https://api.github.com/users/AlexNLP/orgs", "repos_url": "https://api.github.com/users/AlexNLP/repos", "events_url": "https://api.github.com/users/AlexNLP/events{/privacy}", "received_events_url": "https://api.github.com/users/AlexNLP/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-06-08T13:59:20Z", "updated_at": "2022-06-08T14:41:00Z", "closed_at": "2022-06-08T14:41:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI have piped install datasets, but this package doesn't have these attributes: load_dataset, load_metric.\r\n\r\n## Environment info\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-5.13.0-44-generic-x86_64-with-debian-bullseye-sid\r\n- Python version: 3.6.13\r\n- PyArrow version: 6.0.1\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4461/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4461/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4452", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4452/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4452/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4452/events", "html_url": "https://github.com/huggingface/datasets/issues/4452", "id": 1262529654, "node_id": "I_kwDODunzps5LQKx2", "number": 4452, "title": "Trying to load FEVER dataset results in NonMatchingChecksumError", "user": {"login": "santhnm2", "id": 5347982, "node_id": "MDQ6VXNlcjUzNDc5ODI=", "avatar_url": "https://avatars.githubusercontent.com/u/5347982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/santhnm2", "html_url": "https://github.com/santhnm2", "followers_url": "https://api.github.com/users/santhnm2/followers", "following_url": "https://api.github.com/users/santhnm2/following{/other_user}", "gists_url": "https://api.github.com/users/santhnm2/gists{/gist_id}", "starred_url": "https://api.github.com/users/santhnm2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/santhnm2/subscriptions", "organizations_url": "https://api.github.com/users/santhnm2/orgs", "repos_url": "https://api.github.com/users/santhnm2/repos", "events_url": "https://api.github.com/users/santhnm2/events{/privacy}", "received_events_url": "https://api.github.com/users/santhnm2/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-06-06T23:13:15Z", "updated_at": "2022-12-15T13:36:40Z", "closed_at": "2022-06-08T07:16:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nTrying to load the `fever` dataset fails with `datasets.utils.info_utils.NonMatchingChecksumError`.\r\n\r\nI tried with `download_mode=\"force_redownload\"` but that did not fix the error. I also tried with `ignore_verification=True` but then that raised a `json.decoder.JSONDecodeError`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('fever', 'v1.0') # Fails with NonMatchingChecksumError\r\ndataset = load_dataset('fever', 'v1.0', download_mode=\"force_redownload\") # Fails with NonMatchingChecksumError\r\ndataset = load_dataset('fever', 'v1.0', ignore_verification=True)` # Fails with JSONDecodeError\r\n```\r\n\r\n## Expected results\r\nI expect this call to return with no error raised.\r\n\r\n## Actual results\r\nWith `ignore_verification=False`:\r\n```\r\n*** datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://s3-eu-west-1.amazonaws.com/fever.public/train.jsonl', 'https://s3-eu-west-1.amazonaws.com/fever.public/shared_task_dev.jsonl', 'https://s3-eu-west-1.amazonaws.com/fever.public/shared_task_dev_public.jsonl', 'https://s3-eu-west-1.amazonaws.com/fever.public/shared_task_test.jsonl', 'https://s3-eu-west-1.amazonaws.com/fever.public/paper_dev.jsonl', 'https://s3-eu-west-1.amazonaws.com/fever.public/paper_test.jsonl']\r\n```\r\nWith `ignore_verification=True`:\r\n```\r\n*** json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.3.dev0\r\n- Platform: Linux-4.15.0-50-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.13\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4452/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4439", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4439/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4439/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4439/events", "html_url": "https://github.com/huggingface/datasets/issues/4439", "id": 1258434111, "node_id": "I_kwDODunzps5LAi4_", "number": 4439, "title": "TIMIT won't load after manual download: Errors about files that don't exist", "user": {"login": "drscotthawley", "id": 13925685, "node_id": "MDQ6VXNlcjEzOTI1Njg1", "avatar_url": "https://avatars.githubusercontent.com/u/13925685?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drscotthawley", "html_url": "https://github.com/drscotthawley", "followers_url": "https://api.github.com/users/drscotthawley/followers", "following_url": "https://api.github.com/users/drscotthawley/following{/other_user}", "gists_url": "https://api.github.com/users/drscotthawley/gists{/gist_id}", "starred_url": "https://api.github.com/users/drscotthawley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drscotthawley/subscriptions", "organizations_url": "https://api.github.com/users/drscotthawley/orgs", "repos_url": "https://api.github.com/users/drscotthawley/repos", "events_url": "https://api.github.com/users/drscotthawley/events{/privacy}", "received_events_url": "https://api.github.com/users/drscotthawley/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-06-02T16:35:56Z", "updated_at": "2022-06-03T08:44:17Z", "closed_at": "2022-06-03T08:44:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nI get the message from HuggingFace that it must be downloaded manually. From the URL provided in the message, I got to UPenn page for manual download. (UPenn apparently want $250? for the dataset??)   ...So, ok, I obtained a copy from a friend and also a smaller version from Kaggle. But in both cases the HF dataloader fails; it is looking for files that don't exist anywhere in the dataset: it is looking for files with lower-case letters like \"**test*\" (all the filenames in both my copies are uppercase) and certain file extensions that exclude the .DOC which is provided in TIMIT:\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndata = load_dataset('timit_asr', 'clean')['train']\r\n```\r\n\r\n## Expected results\r\nThe dataset should load with no errors. \r\n\r\n## Actual results\r\nThis error message:\r\n```\r\n  File \"/home/ubuntu/envs/data2vec/lib/python3.9/site-packages/datasets/data_files.py\", line 201, in resolve_patterns_locally_or_by_urls\r\n    raise FileNotFoundError(error_msg)\r\nFileNotFoundError: Unable to resolve any data file that matches '['**test*', '**eval*']' at /home/ubuntu/datasets/timit with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'zip']\r\n```\r\n\r\nBut this is a strange sort of error: why is it looking for lower-case file names when all the TIMIT dataset filenames are uppercase?  Why does it exclude .DOC files when the only parts of the TIMIT data set with \"TEST\" in them have \".DOC\" extensions?   ...I wonder, how was anyone able to get this to work in the first place?\r\n\r\nThe files in the dataset look like the following: \r\n```\r\n\u00b3       PHONCODE.DOC\r\n\u00b3       PROMPTS.TXT\r\n\u00b3       SPKRINFO.TXT\r\n\u00b3       SPKRSENT.TXT\r\n\u00b3       TESTSET.DOC\r\n```\r\n...so why are these being excluded by the dataset loader? \r\n\r\n\r\n## Environment info\r\n- `datasets` version: 2.2.2\r\n- Platform: Linux-5.4.0-1060-aws-x86_64-with-glibc2.27\r\n- Python version: 3.9.9\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4439/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4439/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4435", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4435/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4435/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4435/events", "html_url": "https://github.com/huggingface/datasets/issues/4435", "id": 1257496552, "node_id": "I_kwDODunzps5K89_o", "number": 4435, "title": "Load a local cached dataset that has been modified", "user": {"login": "mihail911", "id": 2789441, "node_id": "MDQ6VXNlcjI3ODk0NDE=", "avatar_url": "https://avatars.githubusercontent.com/u/2789441?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mihail911", "html_url": "https://github.com/mihail911", "followers_url": "https://api.github.com/users/mihail911/followers", "following_url": "https://api.github.com/users/mihail911/following{/other_user}", "gists_url": "https://api.github.com/users/mihail911/gists{/gist_id}", "starred_url": "https://api.github.com/users/mihail911/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mihail911/subscriptions", "organizations_url": "https://api.github.com/users/mihail911/orgs", "repos_url": "https://api.github.com/users/mihail911/repos", "events_url": "https://api.github.com/users/mihail911/events{/privacy}", "received_events_url": "https://api.github.com/users/mihail911/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-06-02T01:51:49Z", "updated_at": "2022-06-02T23:59:26Z", "closed_at": "2022-06-02T23:59:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI have loaded a dataset as follows:\r\n```\r\nd = load_dataset(\"emotion\", split=\"validation\")\r\n```\r\nAfterwards I make some modifications to the dataset via a `map` call:\r\n```\r\nd.map(some_update_func, cache_file_name=modified_dataset)\r\n```\r\nThis generates a cached version of the dataset on my local system in the same directory as the original download of the data (/path/to/cache). Running an `ls` returns:\r\n```\r\nmodified_dataset\r\ndataset_info.json  \r\nemotion-test.arrow  \r\nemotion-train.arrow  \r\nemotion-validation.arrow\r\n```\r\nas expected. However, when I try to load up the modified cached dataset via a call to \r\n```\r\nmodified = load_dataset(\"emotion\", split=\"validation\", data_files=\"/path/to/cache/modified_dataset\") \r\n```\r\nit simply redownloads a new version of the dataset and dumps to a new cache rather than loading up the original modified dataset:\r\n```\r\nUsing custom data configuration validation-cdbf51685638421b\r\nDownloading and preparing dataset emotion/validation to ...\r\n```\r\n\r\nHow am I supposed to load the original modified local cache copy of the dataset?\r\n\r\n## Environment info\r\n- `datasets` version: 2.2.2\r\n- Platform: Linux-5.4.0-113-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.13\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4435/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4435/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4428", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4428/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4428/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4428/events", "html_url": "https://github.com/huggingface/datasets/issues/4428", "id": 1254092818, "node_id": "I_kwDODunzps5Kv_AS", "number": 4428, "title": "Errors when building dummy data if you use nested _URLS", "user": {"login": "silverriver", "id": 2529049, "node_id": "MDQ6VXNlcjI1MjkwNDk=", "avatar_url": "https://avatars.githubusercontent.com/u/2529049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/silverriver", "html_url": "https://github.com/silverriver", "followers_url": "https://api.github.com/users/silverriver/followers", "following_url": "https://api.github.com/users/silverriver/following{/other_user}", "gists_url": "https://api.github.com/users/silverriver/gists{/gist_id}", "starred_url": "https://api.github.com/users/silverriver/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/silverriver/subscriptions", "organizations_url": "https://api.github.com/users/silverriver/orgs", "repos_url": "https://api.github.com/users/silverriver/repos", "events_url": "https://api.github.com/users/silverriver/events{/privacy}", "received_events_url": "https://api.github.com/users/silverriver/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-05-31T16:10:57Z", "updated_at": "2022-06-07T09:24:09Z", "closed_at": "2022-06-07T09:24:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen making dummy data with the `datasets-cli dummy_data` tool,\r\nan error will be raised if you use a nested _URLS in your dataset script.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/name/LCCC/datasets/src/datasets/commands/datasets_cli.py\", line 43, in <module>\r\n    main()\r\n  File \"/home/name/LCCC/datasets/src/datasets/commands/datasets_cli.py\", line 39, in main\r\n    service.run()\r\n  File \"/home/name/LCCC/datasets/src/datasets/commands/dummy_data.py\", line 311, in run\r\n    self._autogenerate_dummy_data(\r\n  File \"/home/name/LCCC/datasets/src/datasets/commands/dummy_data.py\", line 337, in _autogenerate_dummy_data\r\n    dataset_builder._split_generators(dl_manager)\r\n  File \"/home/name/.cache/huggingface/modules/datasets_modules/datasets/personal_dialog/559332bced5eeafa7f7efc2a7c10ce02cee2a8116bbab4611c35a50ba2715b77/personal_dialog.py\", line 108, in _split_generators\r\n    data_dir = dl_manager.download_and_extract(urls)\r\n  File \"/home/name/LCCC/datasets/src/datasets/commands/dummy_data.py\", line 56, in download_and_extract\r\n    dummy_output = self.mock_download_manager.download(url_or_urls)\r\n  File \"/home/name/LCCC/datasets/src/datasets/download/mock_download_manager.py\", line 130, in download\r\n    return self.download_and_extract(data_url)\r\n  File \"/home/name/LCCC/datasets/src/datasets/download/mock_download_manager.py\", line 122, in download_and_extract\r\n    return self.create_dummy_data_dict(dummy_file, data_url)\r\n  File \"/home/name/LCCC/datasets/src/datasets/download/mock_download_manager.py\", line 165, in create_dummy_data_dict\r\n    if isinstance(first_value, str) and len(set(dummy_data_dict.values())) < len(dummy_data_dict.values()):\r\nTypeError: unhashable type: 'list'\r\n\r\n\r\n## Steps to reproduce the bug\r\n\r\nYou can use my dataset script implemented here:\r\nhttps://github.com/silverriver/datasets/blob/2ecd36760c40b8e29b1137cd19b5bad0e19c76fd/datasets/personal_dialog/personal_dialog.py\r\n\r\n```python\r\ndatasets_cli dummy_data datasets/personal_dialog --auto_generate\r\n```\r\n\r\nYou can change https://github.com/silverriver/datasets/blob/2ecd36760c40b8e29b1137cd19b5bad0e19c76fd/datasets/personal_dialog/personal_dialog.py#L54\r\nto \r\n\r\n```\r\n\"train\": \"https://huggingface.co/datasets/silver/personal_dialog/resolve/main/dev_random.jsonl.gz\"\r\n```\r\n\r\nbefore runing the above script to avoid downloading a large training data.\r\n\r\n\r\n## Expected results\r\nThe dummy data should be generated\r\n\r\n## Actual results\r\nAn error is raised.\r\n\r\nIt seems that in https://github.com/huggingface/datasets/blob/12540dd75015678ec6019f258d811ee107439a73/src/datasets/download/mock_download_manager.py#L165\r\nWe only check if the first item of dummy_data_dict.values() is str.\r\nHowever, dummy_data_dict.values() may have the type of [str, list, list].\r\nA simple fix would be changing https://github.com/huggingface/datasets/blob/12540dd75015678ec6019f258d811ee107439a73/src/datasets/download/mock_download_manager.py#L165 to\r\n\r\n```python\r\nif all([isinstance(value, str) for value in dummy_data_dict.values()]) and len(set(dummy_data_dict.values())) < len(dummy_data_dict.values()):\r\n```\r\n\r\nBut I don't know if this kinds of change may bring any side effect since I am not sure about the detail logic here.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform: Linux\r\n- Python version: Python 3.9.10\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4428/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4422", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4422/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4422/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4422/events", "html_url": "https://github.com/huggingface/datasets/issues/4422", "id": 1253146511, "node_id": "I_kwDODunzps5KsX-P", "number": 4422, "title": "Cannot load timit_asr data set", "user": {"login": "bhaddow", "id": 992795, "node_id": "MDQ6VXNlcjk5Mjc5NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/992795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bhaddow", "html_url": "https://github.com/bhaddow", "followers_url": "https://api.github.com/users/bhaddow/followers", "following_url": "https://api.github.com/users/bhaddow/following{/other_user}", "gists_url": "https://api.github.com/users/bhaddow/gists{/gist_id}", "starred_url": "https://api.github.com/users/bhaddow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bhaddow/subscriptions", "organizations_url": "https://api.github.com/users/bhaddow/orgs", "repos_url": "https://api.github.com/users/bhaddow/repos", "events_url": "https://api.github.com/users/bhaddow/events{/privacy}", "received_events_url": "https://api.github.com/users/bhaddow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2022-05-30T22:00:22Z", "updated_at": "2022-06-02T06:34:05Z", "closed_at": "2022-05-31T13:42:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to load the timit_asr data set. I have tried with a copy from the LDC, and a copy from deepai. In both cases they fail with a \"duplicate key\" error. With the LDC version I have to convert the file extensions all to upper-case before I can load it at all.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ntimit = datasets.load_dataset(\"timit_asr\", data_dir = \"/path/to/dataset\")\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\n\r\nThe data set should load without error. It worked for me before the LDC url change.\r\n\r\n## Actual results\r\n```\r\ndatasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: SA1\r\nKeys should be unique and deterministic in nature\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- `datasets` version: 2.2.2\r\n- Platform: Linux-5.4.0-90-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.12\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4422/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4405", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4405/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4405/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4405/events", "html_url": "https://github.com/huggingface/datasets/issues/4405", "id": 1248574087, "node_id": "I_kwDODunzps5Ka7qH", "number": 4405, "title": "[TypeError: Couldn't cast array of type] Cannot process dataset in v2.2.2", "user": {"login": "jiangwy99", "id": 39762734, "node_id": "MDQ6VXNlcjM5NzYyNzM0", "avatar_url": "https://avatars.githubusercontent.com/u/39762734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiangwy99", "html_url": "https://github.com/jiangwy99", "followers_url": "https://api.github.com/users/jiangwy99/followers", "following_url": "https://api.github.com/users/jiangwy99/following{/other_user}", "gists_url": "https://api.github.com/users/jiangwy99/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiangwy99/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiangwy99/subscriptions", "organizations_url": "https://api.github.com/users/jiangwy99/orgs", "repos_url": "https://api.github.com/users/jiangwy99/repos", "events_url": "https://api.github.com/users/jiangwy99/events{/privacy}", "received_events_url": "https://api.github.com/users/jiangwy99/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-05-25T18:56:43Z", "updated_at": "2022-06-07T14:27:20Z", "closed_at": "2022-06-07T14:27:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to process the [conll2012_ontonotesv5](https://huggingface.co/datasets/conll2012_ontonotesv5) dataset in `datasets` v2.2.2 and am running into a type error when casting the features.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport os\r\nfrom typing import (\r\n    List,\r\n    Dict,\r\n)\r\nfrom collections import (\r\n    defaultdict,\r\n)\r\nfrom dataclasses import (\r\n    dataclass,\r\n)\r\nfrom datasets import (\r\n    load_dataset,\r\n)\r\n\r\n\r\n@dataclass\r\nclass ConllConverter:\r\n\r\n    path: str\r\n    name: str\r\n    cache_dir: str\r\n\r\n    def __post_init__(\r\n        self,\r\n    ):\r\n        self.dataset = load_dataset(\r\n            path=self.path,\r\n            name=self.name,\r\n            cache_dir=self.cache_dir,\r\n        )\r\n\r\n    def convert(\r\n        self,\r\n    ):\r\n\r\n        class_label = self.dataset[\"train\"].features[\"sentences\"][0][\"named_entities\"].feature\r\n        # label_set = list(set([\r\n        #     label.split(\"-\")[1] if label != \"O\" else label for label in class_label.names\r\n        # ]))\r\n\r\n        def prepare_chunk(token, entity):\r\n            assert len(token) == len(entity)\r\n            # Sequence length\r\n            length = len(token)\r\n            # Variable used\r\n            entity_chunk = defaultdict(list)\r\n            idx = flag = 0\r\n            # While loop\r\n            while idx < length:\r\n                if entity[idx] == \"O\":\r\n                    flag += 1\r\n                    idx += 1\r\n                else:\r\n                    iob_tp, lab_tp = entity[idx].split(\"-\")\r\n                    assert iob_tp == \"B\"\r\n                    idx += 1\r\n                    while idx < length and entity[idx].startswith(\"I-\"):\r\n                        idx += 1\r\n                    entity_chunk[lab_tp].append(token[flag: idx])\r\n                    flag = idx\r\n            entity_chunk = dict(entity_chunk)\r\n            # for label in label_set:\r\n            #     if label != \"O\" and label not in entity_chunk.keys():\r\n            #         entity_chunk[label] = None\r\n            return entity_chunk\r\n\r\n        def prepare_features(\r\n            batch: Dict[str, List],\r\n        ) -> Dict[str, List]:\r\n            sentence = [\r\n                sent for doc_sent in batch[\"sentences\"] for sent in doc_sent\r\n            ]\r\n            feature = {\r\n                \"sentence\": list(),\r\n            }\r\n            for sent in sentence:\r\n                token = sent[\"words\"]\r\n                entity = class_label.int2str(sent[\"named_entities\"])\r\n                entity_chunk = prepare_chunk(token, entity)\r\n                sent_feat = {\r\n                    \"token\": token,\r\n                    \"entity\": entity,\r\n                    \"entity_chunk\": entity_chunk,\r\n                }\r\n                feature[\"sentence\"].append(sent_feat)\r\n\r\n            return feature\r\n\r\n        column_names = self.dataset.column_names[\"train\"]\r\n        dataset = self.dataset.map(\r\n            function=prepare_features,\r\n            with_indices=False,\r\n            batched=True,\r\n            batch_size=3,\r\n            remove_columns=column_names,\r\n            num_proc=1,\r\n        )\r\n        dataset.save_to_disk(\r\n            dataset_dict_path=os.path.join(\"data\", self.path, self.name)\r\n        )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    converter = ConllConverter(\r\n        path=\"conll2012_ontonotesv5\",\r\n        name=\"english_v4\",\r\n        cache_dir=\"cache\",\r\n    )\r\n    converter.convert()\r\n\r\n```\r\n\r\n## Expected results\r\nI want to use the dataset to perform NER task and to change the label list into a {Entity Type: list of spans} format.\r\n\r\n## Actual results\r\n<details>\r\n<summary>Traceback</summary>\r\n\r\n```python\r\nTraceback (most recent call last):                                                                                                                                                                            | 0/81 [00:00<?, ?ba/s]\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/multiprocess/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 532, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 499, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/fingerprint.py\", line 458, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2751, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/arrow_writer.py\", line 503, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow/array.pxi\", line 230, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/arrow_writer.py\", line 198, in __arrow_array__\r\n    out = cast_array_to_feature(out, type, allow_number_to_str=not self.trying_type)\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/table.py\", line 1675, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/table.py\", line 1793, in cast_array_to_feature\r\n    arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/table.py\", line 1793, in <listcomp>\r\n    arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/table.py\", line 1675, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/table.py\", line 1844, in cast_array_to_feature\r\n    raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\nTypeError: Couldn't cast array of type\r\nstruct<CARDINAL: list<item: list<item: string>>, DATE: list<item: list<item: string>>, EVENT: list<item: list<item: string>>, FAC: list<item: list<item: string>>, GPE: list<item: list<item: string>>, LANGUAGE: list<item: list<item: string>>, LAW: list<item: list<item: string>>, LOC: list<item: list<item: string>>, MONEY: list<item: list<item: string>>, NORP: list<item: list<item: string>>, ORDINAL: list<item: list<item: string>>, ORG: list<item: list<item: string>>, PERCENT: list<item: list<item: string>>, PERSON: list<item: list<item: string>>, QUANTITY: list<item: list<item: string>>, TIME: list<item: list<item: string>>, WORK_OF_ART: list<item: list<item: string>>>\r\nto\r\n{'CARDINAL': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'DATE': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'EVENT': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'FAC': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'GPE': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'LAW': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'LOC': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'MONEY': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'NORP': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'ORDINAL': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'ORG': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'PERCENT': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'PERSON': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'PRODUCT': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'QUANTITY': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'TIME': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'WORK_OF_ART': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None)}\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home2/jiangwangyi/workspace/work/Entity/dataconverter.py\", line 110, in <module>\r\n    converter.convert()\r\n  File \"/home2/jiangwangyi/workspace/work/Entity/dataconverter.py\", line 91, in convert\r\n    dataset = self.dataset.map(\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/dataset_dict.py\", line 770, in map\r\n    {\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/dataset_dict.py\", line 771, in <dictcomp>\r\n    k: dataset.map(\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2459, in map\r\n    transformed_shards[index] = async_result.get()\r\n  File \"/home2/jiangwangyi/miniconda3/lib/python3.9/site-packages/multiprocess/pool.py\", line 771, in get\r\n    raise self._value\r\nTypeError: Couldn't cast array of type\r\nstruct<CARDINAL: list<item: list<item: string>>, DATE: list<item: list<item: string>>, EVENT: list<item: list<item: string>>, FAC: list<item: list<item: string>>, GPE: list<item: list<item: string>>, LANGUAGE: list<item: list<item: string>>, LAW: list<item: list<item: string>>, LOC: list<item: list<item: string>>, MONEY: list<item: list<item: string>>, NORP: list<item: list<item: string>>, ORDINAL: list<item: list<item: string>>, ORG: list<item: list<item: string>>, PERCENT: list<item: list<item: string>>, PERSON: list<item: list<item: string>>, QUANTITY: list<item: list<item: string>>, TIME: list<item: list<item: string>>, WORK_OF_ART: list<item: list<item: string>>>\r\nto\r\n{'CARDINAL': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'DATE': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'EVENT': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'FAC': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'GPE': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'LAW': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'LOC': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'MONEY': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'NORP': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'ORDINAL': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'ORG': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'PERCENT': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'PERSON': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'PRODUCT': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'QUANTITY': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'TIME': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None), 'WORK_OF_ART': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None)}\r\n```\r\n\r\n</details>\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.2\r\n- Platform: Ubuntu 18.04\r\n- Python version: 3.9.7\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4405/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4400", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4400/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4400/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4400/events", "html_url": "https://github.com/huggingface/datasets/issues/4400", "id": 1247404237, "node_id": "I_kwDODunzps5KWeDN", "number": 4400, "title": "load dataset wikitext-2-raw-v1 failed. Could not reach wikitext-2-raw-v1.py.", "user": {"login": "cailun01", "id": 20658907, "node_id": "MDQ6VXNlcjIwNjU4OTA3", "avatar_url": "https://avatars.githubusercontent.com/u/20658907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cailun01", "html_url": "https://github.com/cailun01", "followers_url": "https://api.github.com/users/cailun01/followers", "following_url": "https://api.github.com/users/cailun01/following{/other_user}", "gists_url": "https://api.github.com/users/cailun01/gists{/gist_id}", "starred_url": "https://api.github.com/users/cailun01/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cailun01/subscriptions", "organizations_url": "https://api.github.com/users/cailun01/orgs", "repos_url": "https://api.github.com/users/cailun01/repos", "events_url": "https://api.github.com/users/cailun01/events{/privacy}", "received_events_url": "https://api.github.com/users/cailun01/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-05-25T03:10:44Z", "updated_at": "2022-10-24T06:10:27Z", "closed_at": "2022-05-25T03:26:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nCould not reach wikitext-2-raw-v1.py\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"wikitext-2-raw-v1\")\r\n```\r\n\r\n## Expected results\r\nDownload `wikitext-2-raw-v1` dataset successfully.\r\n\r\n## Actual results\r\n```\r\n  File \"load_datasets.py\", line 13, in <module>\r\n    load_dataset(\"wikitext-2-raw-v1\")\r\n  File \"/root/miniconda3/lib/python3.6/site-packages/datasets/load.py\", line 1715, in load_dataset\r\n    **config_kwargs,\r\n  File \"/root/miniconda3/lib/python3.6/site-packages/datasets/load.py\", line 1536, in load_dataset_builder\r\n    data_files=data_files,\r\n  File \"/root/miniconda3/lib/python3.6/site-packages/datasets/load.py\", line 1282, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/root/miniconda3/lib/python3.6/site-packages/datasets/load.py\", line 1224, in dataset_module_factory\r\n    dynamic_modules_path=dynamic_modules_path,\r\n  File \"/root/miniconda3/lib/python3.6/site-packages/datasets/load.py\", line 559, in get_module\r\n    local_path = self.download_loading_script(revision)\r\n  File \"/root/miniconda3/lib/python3.6/site-packages/datasets/load.py\", line 539, in download_loading_script\r\n    return cached_path(file_path, download_config=download_config)\r\n  File \"/root/miniconda3/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 246, in cached_path\r\n    download_desc=download_config.download_desc,\r\n  File \"/root/miniconda3/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 582, in get_from_cache\r\n    raise ConnectionError(f\"Couldn't reach {url} ({repr(head_error)})\")\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/wikitext-2-raw-v1/wikitext-2-raw-v1.py (ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=100)\",),))\r\n```\r\nI tried to download wikitext-2-raw-v1.py by chrome and got:\r\n![image](https://user-images.githubusercontent.com/20658907/170171595-0ca9f1da-c05a-4b57-861e-9530bfa3bdb9.png)\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.2\r\n- Platform: CentOS 7\r\n- Python version: 3.6\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4400/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4400/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4399", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4399/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4399/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4399/events", "html_url": "https://github.com/huggingface/datasets/issues/4399", "id": 1246948299, "node_id": "I_kwDODunzps5KUuvL", "number": 4399, "title": "LocalDatasetModuleFactoryWithoutScript extracts invalid builder name", "user": {"login": "apohllo", "id": 40543, "node_id": "MDQ6VXNlcjQwNTQz", "avatar_url": "https://avatars.githubusercontent.com/u/40543?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apohllo", "html_url": "https://github.com/apohllo", "followers_url": "https://api.github.com/users/apohllo/followers", "following_url": "https://api.github.com/users/apohllo/following{/other_user}", "gists_url": "https://api.github.com/users/apohllo/gists{/gist_id}", "starred_url": "https://api.github.com/users/apohllo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apohllo/subscriptions", "organizations_url": "https://api.github.com/users/apohllo/orgs", "repos_url": "https://api.github.com/users/apohllo/repos", "events_url": "https://api.github.com/users/apohllo/events{/privacy}", "received_events_url": "https://api.github.com/users/apohllo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892877, "node_id": "MDU6TGFiZWwxOTM1ODkyODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-05-24T18:03:01Z", "updated_at": "2022-09-12T15:30:43Z", "closed_at": "2022-09-12T15:30:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nTrying to load a local dataset raises an error indicating that the config builder has to have a name.\r\nNo error should be reported, since the call is completly valid.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset(\"./data/some-dataset/\", name=\"some-name\")\r\n```\r\n\r\n## Expected results\r\nThe dataset should be loaded.\r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_lquad.py\", line 19, in <module>\r\n    load(tokenize_target_function, tokenize_target_function, {}, tokenizer)\r\n  File \"train_lquad.py\", line 14, in load\r\n    dataset = load_dataset(\"./data/lquad/\", name=\"lquad\")\r\n  File \"/net/pr2/scratch/people/plgapohl/python-3.8.6/lib/python3.8/site-packages/datasets/load.py\", line 1708, in load_dataset                                                                           \r\n    builder_instance = load_dataset_builder(\r\n  File \"/net/pr2/scratch/people/plgapohl/python-3.8.6/lib/python3.8/site-packages/datasets/load.py\", line 1560, in load_dataset_builder                                                                   \r\n    builder_instance: DatasetBuilder = builder_cls(\r\n  File \"/net/pr2/scratch/people/plgapohl/python-3.8.6/lib/python3.8/site-packages/datasets/builder.py\", line 269, in __init__                                                                             \r\n    self.config, self.config_id = self._create_builder_config(\r\n  File \"/net/pr2/scratch/people/plgapohl/python-3.8.6/lib/python3.8/site-packages/datasets/builder.py\", line 403, in _create_builder_config                                                               \r\n    raise ValueError(f\"BuilderConfig must have a name, got {builder_config.name}\")\r\nValueError: BuilderConfig must have a name, got\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 2.2.2\r\n- Platform: Linux-4.18.0-348.20.1.el8_5.x86_64-x86_64-with-glibc2.2.5\r\n- Python version: 3.8.6\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n\r\nThe error is probably in line 795 in load.py:\r\n\r\n```\r\n builder_kwargs = {                        \r\n     \"hash\": hash,\r\n     \"data_files\": data_files,\r\n     \"name\": os.path.basename(self.path),\r\n     \"base_path\": self.path,\r\n     **builder_kwargs,\r\n }\r\n```\r\n\r\n`os.path.basename` for a directory returns an empty string, rather than the name of the directory.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4399/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4399/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4398", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4398/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4398/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4398/events", "html_url": "https://github.com/huggingface/datasets/issues/4398", "id": 1246666749, "node_id": "I_kwDODunzps5KTp_9", "number": 4398, "title": "Calling `cast_column`/`remove_columns` and a sequence of `map` operations ends up making `faiss` fail with `ValueError`", "user": {"login": "alvarobartt", "id": 36760800, "node_id": "MDQ6VXNlcjM2NzYwODAw", "avatar_url": "https://avatars.githubusercontent.com/u/36760800?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alvarobartt", "html_url": "https://github.com/alvarobartt", "followers_url": "https://api.github.com/users/alvarobartt/followers", "following_url": "https://api.github.com/users/alvarobartt/following{/other_user}", "gists_url": "https://api.github.com/users/alvarobartt/gists{/gist_id}", "starred_url": "https://api.github.com/users/alvarobartt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alvarobartt/subscriptions", "organizations_url": "https://api.github.com/users/alvarobartt/orgs", "repos_url": "https://api.github.com/users/alvarobartt/repos", "events_url": "https://api.github.com/users/alvarobartt/events{/privacy}", "received_events_url": "https://api.github.com/users/alvarobartt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-05-24T14:41:34Z", "updated_at": "2022-06-14T16:01:56Z", "closed_at": "2022-06-14T16:01:56Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "First of all, sorry in advance for the unclear title, but this bug is weird to explain (at least for me), so I tried my best to summarize all the information in this issue.\r\n\r\n## Describe the bug\r\n\r\nCalling a certain combination of operations over a \ud83e\udd17 `Dataset` and then trying to calculate the `faiss` index with `.add_faiss_index` ends up throwing an exception while trying to set the format back of a previously removed column. But this just happens over certain conditions... I'll present some scenarios below!\r\n\r\n## Steps to reproduce the bug\r\n\r\nAssuming the following dataset named `sample.csv` with some IMDb data:\r\n\r\n```csv\r\nid,title,summary\r\n1877830,\"The Batman\",\"When a sadistic serial killer begins murdering key political figures in Gotham, Batman is forced to investigate the city's hidden corruption and question his family's involvement.\"\r\n9419884,\"Doctor Strange in the Multiverse of Madness\",\"Doctor Strange teams up with a mysterious teenage girl from his dreams who can travel across multiverses, to battle multiple threats, including other-universe versions of himself, which threaten to wipe out millions across the multiverse. They seek help from Wanda the Scarlet Witch, Wong and others.\"\r\n11138512,\"The Northman\",\"From visionary director Robert Eggers comes The Northman, an action-filled epic that follows a young Viking prince on his quest to avenge his father's murder.\" \r\n1745960,\"Top Gun: Maverick\",\"After more than thirty years of service as one of the Navy's top aviators, Pete Mitchell is where he belongs, pushing the envelope as a courageous test pilot and dodging the advancement in rank that would ground him.\"\r\n```\r\n\r\nWe'll be able to reproduce the bug using the following piece of code:\r\n\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\r\nimport torch\r\n\r\ntorch.set_grad_enabled(False)\r\nctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\r\nctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\r\n\r\nfrom datasets import load_dataset, Value\r\n\r\nds = load_dataset(\"csv\", data_files=[\"sample.csv\"], split=\"train\")\r\nds = ds.cast_column(\"id\", Value(\"int32\")) # from `int64` to `int32`\r\nds = ds.map(lambda x: {\"inputs\": f\"{ctx_tokenizer.sep_token}\".join([\"title\", \"summary\"])})\r\nds = ds.remove_columns([\"title\", \"summary\"])\r\n\r\ndef generate_embeddings(x):\r\n    return {\"embeddings\": ctx_encoder(**ctx_tokenizer(x[\"inputs\"], return_tensors=\"pt\"))[0][0].numpy()}\r\n\r\nds = ds.map(generate_embeddings)\r\nds = ds.remove_columns(\"inputs\")\r\nds.add_faiss_index(column=\"embeddings\") # It fails here!\r\n```\r\n\r\nThe code above is an adaptation of https://huggingface.co/docs/datasets/faiss_es, for the sake of presenting the bug with a simple example.\r\n\r\n## Expected results\r\n\r\nIdeally, the `faiss` index should be calculated over the \ud83e\udd17 `Dataset` and no exception should be triggered.\r\n\r\n## Actual results\r\n\r\nBut what happens instead is that a `ValueError: Columns ['inputs'] not in the dataset. Current columns in the dataset: ['id', 'embeddings']`, which makes no sense as that column has been previously dropped.\r\n\r\n## Environment info\r\n\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.2\r\n- Platform: Linux-5.4.0-1074-azure-x86_64-with-glibc2.31\r\n- Python version: 3.9.5\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4398/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4398/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4386", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4386/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4386/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4386/events", "html_url": "https://github.com/huggingface/datasets/issues/4386", "id": 1243965532, "node_id": "I_kwDODunzps5KJWhc", "number": 4386, "title": "Bug for wiki_auto_asset_turk from GEM", "user": {"login": "StevenTang1998", "id": 37647985, "node_id": "MDQ6VXNlcjM3NjQ3OTg1", "avatar_url": "https://avatars.githubusercontent.com/u/37647985?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StevenTang1998", "html_url": "https://github.com/StevenTang1998", "followers_url": "https://api.github.com/users/StevenTang1998/followers", "following_url": "https://api.github.com/users/StevenTang1998/following{/other_user}", "gists_url": "https://api.github.com/users/StevenTang1998/gists{/gist_id}", "starred_url": "https://api.github.com/users/StevenTang1998/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StevenTang1998/subscriptions", "organizations_url": "https://api.github.com/users/StevenTang1998/orgs", "repos_url": "https://api.github.com/users/StevenTang1998/repos", "events_url": "https://api.github.com/users/StevenTang1998/events{/privacy}", "received_events_url": "https://api.github.com/users/StevenTang1998/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2022-05-21T12:31:30Z", "updated_at": "2022-05-24T05:55:52Z", "closed_at": "2022-05-23T10:29:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe script of wiki_auto_asset_turk for GEM may be out of date.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\ndatasets.load_dataset('gem', 'wiki_auto_asset_turk')\r\n```\r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/load.py\", line 1731, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/builder.py\", line 640, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/builder.py\", line 1158, in _download_and_prepare\r\n    super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/builder.py\", line 707, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/tangtianyi/.cache/huggingface/modules/datasets_modules/datasets/gem/982a54473b12c6a6e40d4356e025fb7172a5bb2065e655e2c1af51f2b3cf4ca1/gem.py\", line 538, in _split_generators\r\n    dl_dir = dl_manager.download_and_extract(_URLs[self.config.name])\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 416, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 294, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 351, in map_nested\r\n    mapped = [\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 352, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True, None))\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 288, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 320, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 234, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/home/tangtianyi/miniconda3/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 579, in get_from_cache\r\n    raise FileNotFoundError(f\"Couldn't find file at {url}\")\r\nFileNotFoundError: Couldn't find file at https://github.com/facebookresearch/asset/raw/master/dataset/asset.test.orig\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4386/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4386/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4383", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4383/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4383/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4383/events", "html_url": "https://github.com/huggingface/datasets/issues/4383", "id": 1243856981, "node_id": "I_kwDODunzps5KI8BV", "number": 4383, "title": "L", "user": {"login": "AronCodes21", "id": 99847861, "node_id": "U_kgDOBfOOtQ", "avatar_url": "https://avatars.githubusercontent.com/u/99847861?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AronCodes21", "html_url": "https://github.com/AronCodes21", "followers_url": "https://api.github.com/users/AronCodes21/followers", "following_url": "https://api.github.com/users/AronCodes21/following{/other_user}", "gists_url": "https://api.github.com/users/AronCodes21/gists{/gist_id}", "starred_url": "https://api.github.com/users/AronCodes21/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AronCodes21/subscriptions", "organizations_url": "https://api.github.com/users/AronCodes21/orgs", "repos_url": "https://api.github.com/users/AronCodes21/repos", "events_url": "https://api.github.com/users/AronCodes21/events{/privacy}", "received_events_url": "https://api.github.com/users/AronCodes21/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-05-21T03:47:58Z", "updated_at": "2022-05-21T19:20:13Z", "closed_at": "2022-05-21T19:20:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the   L\nL\n## Expected   L\nA clear and concise lmll\nSpecify the actual results or traceback.\n\n## Environment info\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\n- `datasets` version:\n- Platform:\n- Python version:\n- PyArrow version:", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4383/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4383/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4381", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4381/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4381/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4381/events", "html_url": "https://github.com/huggingface/datasets/issues/4381", "id": 1243478863, "node_id": "I_kwDODunzps5KHftP", "number": 4381, "title": "Bug in caching 2 datasets both with the same builder class name", "user": {"login": "NouamaneTazi", "id": 29777165, "node_id": "MDQ6VXNlcjI5Nzc3MTY1", "avatar_url": "https://avatars.githubusercontent.com/u/29777165?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NouamaneTazi", "html_url": "https://github.com/NouamaneTazi", "followers_url": "https://api.github.com/users/NouamaneTazi/followers", "following_url": "https://api.github.com/users/NouamaneTazi/following{/other_user}", "gists_url": "https://api.github.com/users/NouamaneTazi/gists{/gist_id}", "starred_url": "https://api.github.com/users/NouamaneTazi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NouamaneTazi/subscriptions", "organizations_url": "https://api.github.com/users/NouamaneTazi/orgs", "repos_url": "https://api.github.com/users/NouamaneTazi/repos", "events_url": "https://api.github.com/users/NouamaneTazi/events{/privacy}", "received_events_url": "https://api.github.com/users/NouamaneTazi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-05-20T18:18:03Z", "updated_at": "2022-06-02T08:18:37Z", "closed_at": "2022-05-25T05:16:15Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nThe two datasets `mteb/mtop_intent` and `mteb/mtop_domain `use both the same cache folder `.cache/huggingface/datasets/mteb___mtop`. So if you first load `mteb/mtop_intent` then datasets will not load `mteb/mtop_domain`.\r\nIf you delete this cache folder and flip the order how you load the two datasets , you will get the opposite datasets loaded (difference is here in terms of the label and label_text).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\n\r\ndataset = datasets.load_dataset(\"mteb/mtop_intent\", \"en\")\r\nprint(dataset['train'][0])\r\ndataset = datasets.load_dataset(\"mteb/mtop_domain\", \"en\")\r\nprint(dataset['train'][0])\r\n```\r\n\r\n## Expected results\r\n```\r\nReusing dataset mtop (/home/nouamane/.cache/huggingface/datasets/mteb___mtop_intent/en/0.0.0/f930e32a294fed424f70263d8802390e350fff17862266e5fc156175c07d9c35)\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 920.14it/s]\r\n{'id': 3232343436343136, 'text': 'Has Angelika Kratzer video messaged me?', 'label': 1, 'label_text': 'GET_MESSAGE'}\r\nReusing dataset mtop (/home/nouamane/.cache/huggingface/datasets/mteb___mtop_domain/en/0.0.0/f930e32a294fed424f70263d8802390e350fff17862266e5fc156175c07d9c35)\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 1307.59it/s]\r\n{'id': 3232343436343136, 'text': 'Has Angelika Kratzer video messaged me?', 'label': 0, 'label_text': 'messaging'}\r\n```\r\n\r\n## Actual results\r\n```\r\nReusing dataset mtop (/home/nouamane/.cache/huggingface/datasets/mteb___mtop/en/0.0.0/f930e32a294fed424f70263d8802390e350fff17862266e5fc156175c07d9c35)\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 920.14it/s]\r\n{'id': 3232343436343136, 'text': 'Has Angelika Kratzer video messaged me?', 'label': 1, 'label_text': 'GET_MESSAGE'}\r\nReusing dataset mtop (/home/nouamane/.cache/huggingface/datasets/mteb___mtop/en/0.0.0/f930e32a294fed424f70263d8802390e350fff17862266e5fc156175c07d9c35)\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 1307.59it/s]\r\n{'id': 3232343436343136, 'text': 'Has Angelika Kratzer video messaged me?', 'label': 1, 'label_text': 'GET_MESSAGE'}\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.1\r\n- Platform: macOS-12.1-arm64-arm-64bit\r\n- Python version: 3.9.12\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4381/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4379", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4379/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4379/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4379/events", "html_url": "https://github.com/huggingface/datasets/issues/4379", "id": 1243175854, "node_id": "I_kwDODunzps5KGVuu", "number": 4379, "title": "Latest dill release raises exception", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2022-05-20T13:48:36Z", "updated_at": "2022-05-21T15:53:26Z", "closed_at": "2022-05-20T17:06:27Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAs reported by @sgugger, latest dill release is breaking things with Datasets.\r\n\r\n```\r\n______________ ExamplesTests.test_run_speech_recognition_seq2seq _______________\r\n\r\n\r\nself = <multiprocess.pool.ApplyResult object at 0x7fa5981a1cd0>, timeout = None\r\n\r\n    def get(self, timeout=None):\r\n        self.wait(timeout)\r\n        if not self.ready():\r\n            raise TimeoutError\r\n        if self._success:\r\n            return self._value\r\n        else:\r\n>           raise self._value\r\nE           TypeError: '>' not supported between instances of 'NoneType' and 'float'\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4379/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4379/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4361", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4361/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4361/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4361/events", "html_url": "https://github.com/huggingface/datasets/issues/4361", "id": 1238671931, "node_id": "I_kwDODunzps5J1KI7", "number": 4361, "title": "`udhr` doesn't load, dataset checksum mismatch", "user": {"login": "leondz", "id": 121934, "node_id": "MDQ6VXNlcjEyMTkzNA==", "avatar_url": "https://avatars.githubusercontent.com/u/121934?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leondz", "html_url": "https://github.com/leondz", "followers_url": "https://api.github.com/users/leondz/followers", "following_url": "https://api.github.com/users/leondz/following{/other_user}", "gists_url": "https://api.github.com/users/leondz/gists{/gist_id}", "starred_url": "https://api.github.com/users/leondz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leondz/subscriptions", "organizations_url": "https://api.github.com/users/leondz/orgs", "repos_url": "https://api.github.com/users/leondz/repos", "events_url": "https://api.github.com/users/leondz/events{/privacy}", "received_events_url": "https://api.github.com/users/leondz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-05-17T13:47:09Z", "updated_at": "2022-06-08T19:11:21Z", "closed_at": "2022-06-08T19:11:21Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading `udhr` fails due to a checksum mismatch for some source files. Looks like both of the source files on unicode.org have changed:\r\n\r\nsize + checksum in datasets repo:\r\n```\r\n(hfdev) leon@blade:~/datasets/datasets/udhr$ jq .default.download_checksums < dataset_infos.json \r\n{\r\n  \"https://unicode.org/udhr/assemblies/udhr_xml.zip\": {\r\n    \"num_bytes\": 2273633,\r\n    \"checksum\": \"0565fa62c2ff155b84123198bcc967edd8c5eb9679eadc01e6fb44a5cf730fee\"\r\n  },\r\n  \"https://unicode.org/udhr/assemblies/udhr_txt.zip\": {\r\n    \"num_bytes\": 2107471,\r\n    \"checksum\": \"087b474a070dd4096ae3028f9ee0b30dcdcb030cc85a1ca02e143be46327e5e5\"\r\n  }\r\n}\r\n```\r\n\r\nsize + checksum regenerated from current source files:\r\n```\r\n(hfdev) leon@blade:~/datasets/datasets/udhr$ rm dataset_infos.json\r\n(hfdev) leon@blade:~/datasets/datasets/udhr$ datasets-cli test --save_infos udhr.py\r\nUsing custom data configuration default\r\nTesting builder 'default' (1/1)\r\nDownloading and preparing dataset udhn/default (download: 4.18 MiB, generated: 6.15 MiB, post-processed: Unknown size, total: 10.33 MiB) to /home/leon/.cache/huggingface/datasets/udhn/default/0.0.0/ad74b91fa2b3c386e5751b0c52bdfda76d334f76731142fd432d4acc2e2fde66...\r\nDataset udhn downloaded and prepared to /home/leon/.cache/huggingface/datasets/udhn/default/0.0.0/ad74b91fa2b3c386e5751b0c52bdfda76d334f76731142fd432d4acc2e2fde66. Subsequent calls will reuse this data.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 686.69it/s]\r\nDataset Infos file saved at dataset_infos.json\r\nTest successful.\r\n(hfdev) leon@blade:~/datasets/datasets/udhr$ jq .default.download_checksums < dataset_infos.json \r\n{\r\n  \"https://unicode.org/udhr/assemblies/udhr_xml.zip\": {\r\n    \"num_bytes\": 2389690,\r\n    \"checksum\": \"a3350912790196c6e1b26bfd1c8a50e8575f5cf185922ecd9bd15713d7d21438\"\r\n  },\r\n  \"https://unicode.org/udhr/assemblies/udhr_txt.zip\": {\r\n    \"num_bytes\": 2215441,\r\n    \"checksum\": \"cb87ecb25b56f34e4fd6f22b323000524fd9c06ae2a29f122b048789cf17e9fe\"\r\n  }\r\n}\r\n(hfdev) leon@blade:~/datasets/datasets/udhr$ \r\n\r\n```\r\n\r\n\r\n--- is unicode.org a sustainable hosting solution for this dataset?\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nudhr = load_dataset(\"udhr\")\r\n```\r\n\r\n## Expected results\r\nThat a Dataset object containing the UDHR data will be returned.\r\n\r\n## Actual results\r\n```\r\n>>> d = load_dataset('udhr')\r\nUsing custom data configuration default\r\nDownloading and preparing dataset udhn/default (download: 4.18 MiB, generated: 6.15 MiB, post-processed: Unknown size, total: 10.33 MiB) to /home/leon/.cache/huggingface/datasets/udhn/default/0.0.0/ad74b91fa2b3c386e5751b0c52bdfda76d334f76731142fd432d4acc2e2fde66...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/leon/.local/lib/python3.9/site-packages/datasets/load.py\", line 1731, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/leon/.local/lib/python3.9/site-packages/datasets/builder.py\", line 613, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/leon/.local/lib/python3.9/site-packages/datasets/builder.py\", line 1117, in _download_and_prepare\r\n    super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n  File \"/home/leon/.local/lib/python3.9/site-packages/datasets/builder.py\", line 684, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/home/leon/.local/lib/python3.9/site-packages/datasets/utils/info_utils.py\", line 40, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://unicode.org/udhr/assemblies/udhr_xml.zip', 'https://unicode.org/udhr/assemblies/udhr_txt.zip']\r\n>>> \r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.1 commit/4110fb6034f79c5fb470cf1043ff52180e9c63b7\r\n- Platform: Linux Ubuntu 20.04\r\n- Python version: 3.9.12\r\n- PyArrow version: 8.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4361/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4354", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4354/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4354/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4354/events", "html_url": "https://github.com/huggingface/datasets/issues/4354", "id": 1236404383, "node_id": "I_kwDODunzps5Jsgif", "number": 4354, "title": "Problems with WMT dataset", "user": {"login": "eldarkurtic", "id": 8884008, "node_id": "MDQ6VXNlcjg4ODQwMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/8884008?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eldarkurtic", "html_url": "https://github.com/eldarkurtic", "followers_url": "https://api.github.com/users/eldarkurtic/followers", "following_url": "https://api.github.com/users/eldarkurtic/following{/other_user}", "gists_url": "https://api.github.com/users/eldarkurtic/gists{/gist_id}", "starred_url": "https://api.github.com/users/eldarkurtic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eldarkurtic/subscriptions", "organizations_url": "https://api.github.com/users/eldarkurtic/orgs", "repos_url": "https://api.github.com/users/eldarkurtic/repos", "events_url": "https://api.github.com/users/eldarkurtic/events{/privacy}", "received_events_url": "https://api.github.com/users/eldarkurtic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2067388877, "node_id": "MDU6TGFiZWwyMDY3Mzg4ODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset%20bug", "name": "dataset bug", "color": "2edb81", "default": false, "description": "A bug in a dataset script provided in the library"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2022-05-15T20:58:26Z", "updated_at": "2022-07-11T14:54:02Z", "closed_at": "2022-07-11T14:54:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to load WMT15 dataset and to define which data-sources to use for train/validation/test splits, but unfortunately it seems that the official documentation at [https://huggingface.co/datasets/wmt15#:~:text=Versions%20exists%20for,wmt_translate%22%2C%20config%3Dconfig)](https://huggingface.co/datasets/wmt15#:~:text=Versions%20exists%20for,wmt_translate%22%2C%20config%3Dconfig)) doesn't work anymore.\r\n\r\n## Steps to reproduce the bug\r\n```shell\r\n>>> import datasets\r\n>>> a = datasets.translate.wmt.WmtConfig()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'datasets' has no attribute 'translate'\r\n>>> a = datasets.wmt.WmtConfig()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'datasets' has no attribute 'wmt'\r\n```\r\n\r\n## Expected results\r\nTo load WMT15 with given data-sources.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-5.10.0-10-amd64-x86_64-with-glibc2.17\r\n- Python version: 3.8.12\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4354/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4349", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4349/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4349/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4349/events", "html_url": "https://github.com/huggingface/datasets/issues/4349", "id": 1235474765, "node_id": "I_kwDODunzps5Jo9lN", "number": 4349, "title": "Dataset.map()'s fails at any value of parameter writer_batch_size ", "user": {"login": "plamb-viso", "id": 99206017, "node_id": "U_kgDOBenDgQ", "avatar_url": "https://avatars.githubusercontent.com/u/99206017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/plamb-viso", "html_url": "https://github.com/plamb-viso", "followers_url": "https://api.github.com/users/plamb-viso/followers", "following_url": "https://api.github.com/users/plamb-viso/following{/other_user}", "gists_url": "https://api.github.com/users/plamb-viso/gists{/gist_id}", "starred_url": "https://api.github.com/users/plamb-viso/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/plamb-viso/subscriptions", "organizations_url": "https://api.github.com/users/plamb-viso/orgs", "repos_url": "https://api.github.com/users/plamb-viso/repos", "events_url": "https://api.github.com/users/plamb-viso/events{/privacy}", "received_events_url": "https://api.github.com/users/plamb-viso/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-05-13T16:55:12Z", "updated_at": "2022-06-02T12:51:11Z", "closed_at": "2022-05-14T15:08:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nIf the the value of `writer_batch_size` is less than the total number of instances in the dataset it will fail at that same number of instances. If it is greater than the total number of instances, it fails on the last instance.\r\n\r\nContext:\r\nI am attempting to fine-tune a pre-trained HuggingFace transformers model called LayoutLMv2. This model takes three inputs: document images, words and word bounding boxes. [The Processor for this model has two options](https://huggingface.co/docs/transformers/model_doc/layoutlmv2#usage-layoutlmv2processor), the default is passing a document to the Processor and allowing it to create images of the document and use PyTesseract to perform OCR and generate words/bounding boxes. The other option is to provide `revision=\"no_ocr\"` to the pre-trained model which allows you to use your own OCR results (in my case, Amazon Textract) so you have to provide the image, words and bounding boxes yourself. I am using this second option which might be good context for the bug.\r\n\r\nI am using the Dataset.map() paradigm to create these three inputs, encode them and save the dataset. Note that my documents (data instances) on average are fairly large and can range from 1 page up to 300 pages.\r\nCode I am using is provided below\r\n\r\n## Steps to reproduce the bug\r\nI do not have explicit sample code, but I will paste the code I'm using in case reading it helps. When `.map()` is called, the dataset has 2933 rows, many of which represent large pdf documents.\r\n```python\r\ndef get_encoded_data(data):\r\n    dataset = Dataset.from_pandas(data)\r\n    unique_labels = data['label'].unique()\r\n    features = Features({\r\n        'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\r\n        'input_ids': Sequence(feature=Value(dtype='int64')),\r\n        'attention_mask': Sequence(Value(dtype='int64')),\r\n        'token_type_ids': Sequence(Value(dtype='int64')),\r\n        'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\r\n        'label': ClassLabel(num_classes=len(unique_labels), names=unique_labels),\r\n    })\r\n\r\n    encoded_dataset = dataset.map(preprocess_data, features=features, remove_columns=dataset.column_names, writer_batch_size=dataset.num_rows+1)\r\n    encoded_dataset.save_to_disk(TRAINING_DATA_PATH + ENCODED_DATASET_NAME)\r\n    encoded_dataset.set_format(type=\"torch\")\r\n    return encoded_dataset\r\n```\r\n```python\r\nPROCESSOR = LayoutLMv2Processor.from_pretrained(MODEL_PATH, revision=\"no_ocr\", use_fast=False)\r\n\r\ndef preprocess_data(examples):\r\n    directory = os.path.join(FILES_PATH, examples['file_location'])\r\n    images_dir = os.path.join(directory, PDF_IMAGE_DIR)\r\n    textract_response_path = os.path.join(directory, 'textract.json')\r\n    doc_meta_path = os.path.join(directory, 'doc_meta.json')\r\n    textract_document = get_textract_document(textract_response_path, doc_meta_path)\r\n    images, words, bboxes = get_doc_training_data(images_dir, textract_document)\r\n    encoded_inputs = PROCESSOR(images, words, boxes=bboxes, padding=\"max_length\", truncation=True)\r\n    # https://github.com/NielsRogge/Transformers-Tutorials/issues/36\r\n    encoded_inputs[\"image\"] = np.array(encoded_inputs[\"image\"])\r\n    encoded_inputs[\"label\"] = examples['label_id']\r\n    return encoded_inputs\r\n```\r\n\r\n## Expected results\r\nMy expectation is that `writer_batch_size` allows one to simply trade off performance and memory requirements, not that it must be a specific number for `.map()` to function correctly.\r\n\r\n## Actual results\r\nIf writer_batch_size is set to a value less than the number of rows, I get either:\r\n\r\n```\r\nOverflowError: There was an overflow with type <class 'list'>. Try to reduce writer_batch_size to have batches smaller than 2GB.\r\n(offset overflow while concatenating arrays)\r\n```\r\nor simply\r\n\r\n```\r\nzsh: killed     python doc_classification.py\r\n\r\nUserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n```\r\n\r\nIf it is greater than the number of rows, i get the `zsh: killed` error above\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.1.0\r\n- Platform: macOS-12.2.1-arm64-arm-64bit\r\n- Python version: 3.9.12\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4349/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4349/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4348", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4348/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4348/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4348/events", "html_url": "https://github.com/huggingface/datasets/issues/4348", "id": 1235432976, "node_id": "I_kwDODunzps5JozYQ", "number": 4348, "title": "`inspect` functions can't fetch dataset script from the Hub", "user": {"login": "stevhliu", "id": 59462357, "node_id": "MDQ6VXNlcjU5NDYyMzU3", "avatar_url": "https://avatars.githubusercontent.com/u/59462357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stevhliu", "html_url": "https://github.com/stevhliu", "followers_url": "https://api.github.com/users/stevhliu/followers", "following_url": "https://api.github.com/users/stevhliu/following{/other_user}", "gists_url": "https://api.github.com/users/stevhliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/stevhliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stevhliu/subscriptions", "organizations_url": "https://api.github.com/users/stevhliu/orgs", "repos_url": "https://api.github.com/users/stevhliu/repos", "events_url": "https://api.github.com/users/stevhliu/events{/privacy}", "received_events_url": "https://api.github.com/users/stevhliu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-05-13T16:08:26Z", "updated_at": "2022-06-09T10:26:06Z", "closed_at": "2022-06-09T10:26:06Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "The `inspect_dataset` and `inspect_metric` functions are unable to retrieve a dataset or metric script from the Hub and store it locally at the specified `local_path`:\r\n\r\n```py\r\n>>> from datasets import inspect_dataset\r\n>>> inspect_dataset('rotten_tomatoes', local_path='path/to/my/local/folder')\r\n\r\nFileNotFoundError: Couldn't find a dataset script at /content/rotten_tomatoes/rotten_tomatoes.py or any data file in the same directory.\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4348/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4348/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4346", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4346/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4346/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4346/events", "html_url": "https://github.com/huggingface/datasets/issues/4346", "id": 1235067062, "node_id": "I_kwDODunzps5JnaC2", "number": 4346, "title": "GH Action to build documentation never ends", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-05-13T10:44:44Z", "updated_at": "2022-05-13T11:22:00Z", "closed_at": "2022-05-13T11:22:00Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nSee: https://github.com/huggingface/datasets/runs/6418035586?check_suite_focus=true\r\n\r\nI finally forced the cancel of the workflow.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4346/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4341", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4341/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4341/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4341/events", "html_url": "https://github.com/huggingface/datasets/issues/4341", "id": 1234739703, "node_id": "I_kwDODunzps5JmKH3", "number": 4341, "title": "Failing CI on Windows for sari and wiki_split metrics", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-05-13T04:55:17Z", "updated_at": "2022-05-13T05:47:41Z", "closed_at": "2022-05-13T05:47:41Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nOur CI is failing from yesterday on Windows for metrics: sari and wiki_split\r\n```\r\nFAILED tests/test_metric_common.py::LocalMetricTest::test_load_metric_sari - ...\r\nFAILED tests/test_metric_common.py::LocalMetricTest::test_load_metric_wiki_split\r\n```\r\n\r\nSee: https://app.circleci.com/pipelines/github/huggingface/datasets/11928/workflows/79daa5e7-65c9-4e85-829b-00d2bfbd076a/jobs/71594", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4341/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4341/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4327", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4327/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4327/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4327/events", "html_url": "https://github.com/huggingface/datasets/issues/4327", "id": 1233840020, "node_id": "I_kwDODunzps5JiueU", "number": 4327, "title": "`wikipedia` pre-processed datasets", "user": {"login": "vpj", "id": 81152, "node_id": "MDQ6VXNlcjgxMTUy", "avatar_url": "https://avatars.githubusercontent.com/u/81152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vpj", "html_url": "https://github.com/vpj", "followers_url": "https://api.github.com/users/vpj/followers", "following_url": "https://api.github.com/users/vpj/following{/other_user}", "gists_url": "https://api.github.com/users/vpj/gists{/gist_id}", "starred_url": "https://api.github.com/users/vpj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vpj/subscriptions", "organizations_url": "https://api.github.com/users/vpj/orgs", "repos_url": "https://api.github.com/users/vpj/repos", "events_url": "https://api.github.com/users/vpj/events{/privacy}", "received_events_url": "https://api.github.com/users/vpj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-05-12T11:25:42Z", "updated_at": "2022-08-31T08:26:57Z", "closed_at": "2022-08-31T08:26:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n[Wikipedia](https://huggingface.co/datasets/wikipedia) dataset readme says that certain subsets are preprocessed. However it seems like they are not available. When I try to load them it takes a really long time, and it seems like it's processing them.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"wikipedia\", \"20220301.en\")\r\n```\r\n\r\n## Expected results\r\nTo load the dataset\r\n\r\n## Actual results\r\nTakes a very long time to load (after downloading)\r\n\r\nAfter `Downloading data files: 100%`. It takes hours and gets killed.\r\nTried `wikipedia.simple` and it got processed after ~30mins.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4327/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4323", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4323/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4323/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4323/events", "html_url": "https://github.com/huggingface/datasets/issues/4323", "id": 1233634928, "node_id": "I_kwDODunzps5Jh8Zw", "number": 4323, "title": "Audio can not find value[\"bytes\"]", "user": {"login": "YooSungHyun", "id": 34292279, "node_id": "MDQ6VXNlcjM0MjkyMjc5", "avatar_url": "https://avatars.githubusercontent.com/u/34292279?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YooSungHyun", "html_url": "https://github.com/YooSungHyun", "followers_url": "https://api.github.com/users/YooSungHyun/followers", "following_url": "https://api.github.com/users/YooSungHyun/following{/other_user}", "gists_url": "https://api.github.com/users/YooSungHyun/gists{/gist_id}", "starred_url": "https://api.github.com/users/YooSungHyun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YooSungHyun/subscriptions", "organizations_url": "https://api.github.com/users/YooSungHyun/orgs", "repos_url": "https://api.github.com/users/YooSungHyun/repos", "events_url": "https://api.github.com/users/YooSungHyun/events{/privacy}", "received_events_url": "https://api.github.com/users/YooSungHyun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2022-05-12T08:31:58Z", "updated_at": "2022-07-07T13:16:08Z", "closed_at": "2022-07-07T13:16:08Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI wrote down _generate_examples like:\r\n![image](https://user-images.githubusercontent.com/34292279/168027186-2fe8b255-2cd8-4b9b-ab1e-8d5a7182979b.png)\r\n\r\nbut where is the bytes?\r\n![image](https://user-images.githubusercontent.com/34292279/168027330-f2496dd0-1d99-464c-b15c-bc57eee0415a.png)\r\n\r\n\r\n## Expected results\r\nvalue[\"bytes\"] is not None, so i can make datasets with bytes, not path\r\n\r\n## bytes looks like:\r\nblah blah~~\r\n\\xfe\\x03\\x00\\xfb\\x06\\x1c\\x0bo\\x074\\x03\\xaf\\x01\\x13\\x04\\xbc\\x06\\x8c\\x05y\\x05,\\t7\\x08\\xaf\\x03\\xc0\\xfe\\xe8\\xfc\\x94\\xfe\\xb7\\xfd\\xea\\xfa\\xd5\\xf9$\\xf9>\\xf9\\x1f\\xf8\\r\\xf5F\\xf49\\xf4\\xda\\xf5-\\xf8\\n\\xf8k\\xf8\\x07\\xfb\\x18\\xfd\\xd9\\xfdv\\xfd\"\\xfe\\xcc\\x01\\x1c\\x04\\x08\\x04@\\x04{\\x06^\\tf\\t\\x1e\\x07\\x8b\\x06\\x02\\x08\\x13\\t\\x07\\x08 \\x06g\\x06\"\\x06\\xa0\\x03\\xc6\\x002\\xff \\xff\\x1d\\xff\\x19\\xfd?\\xfb\\xdb\\xfa\\xfc\\xfa$\\xfb}\\xf9\\xe5\\xf7\\xf9\\xf7\\xce\\xf8.\\xf9b\\xf9\\xc5\\xf9\\xc0\\xfb\\xfa\\xfcP\\xfc\\xba\\xfbQ\\xfc1\\xfe\\x9f\\xff\\x12\\x00\\xa2\\x00\\x18\\x02Z\\x03\\x02\\x04\\xb1\\x03\\xc5\\x03W\\x04\\x82\\x04\\x8f\\x04U\\x04\\xb6\\x04\\x10\\x05{\\x04\\x83\\x02\\x17\\x01\\x1d\\x00\\xa0\\xff\\xec\\xfe\\x03\\xfe#\\xfe\\xc2\\xfe2\\xff\\xe6\\xfe\\x9a\\xfe~\\x01\\x91\\x08\\xb3\\tU\\x05\\x10\\x024\\x02\\xe4\\x05\\xa8\\x07\\xa7\\x053\\x07I\\n\\x91\\x07v\\x02\\x95\\xfd\\xbb\\xfd\\x96\\xff\\x01\\xfe\\x1e\\xfb\\xbb\\xf9S\\xf8!\\xf8\\xf4\\xf5\\xd6\\xf3\\xf7\\xf3l\\xf4d\\xf6l\\xf7d\\xf6b\\xf7\\xc1\\xfa(\\xfd\\xcf\\xfd*\\xfdq\\xfe\\xe9\\x01\\xa8\\x03t\\x03\\x17\\x04B\\x07\\xce\\t\\t\\t\\xeb\\x06\\x0c\\x07\\x95\\x08\\x92\\t\\xbc\\x07O\\x06\\xfb\\x06\\xd2\\x06U\\x04\\x00\\x02\\x92\\x00\\xdc\\x00\\x84\\x00 \\xfeT\\xfc\\xf1\\xfb\\x82\\xfc\\x97\\xfb}\\xf9\\x00\\xf8_\\xf8\\x0b\\xf9\\xe5\\xf8\\xe2\\xf7\\xaa\\xf8\\xb2\\xfa\\x10\\xfbl\\xfa\\xf5\\xf9Y\\xfb\\xc0\\xfd\\xe8\\xfe\\xec\\xfe1\\x00\\xad\\x01\\xec\\x02E\\x03\\x13\\x03\\x9b\\x03o\\x04\\xce\\x04\\xa8\\x04\\xb2\\x04\\x1b\\x05\\xc0\\x05\\xd2\\x04\\xe8\\x02z\\x01\\xbe\\x00\\xae\\x00\\x07\\x00$\\xff|\\xff\\x8e\\x00\\x13\\x00\\x10\\xff\\x98\\xff0\\x05{\\x0b\\x05\\t\\xaa\\x03\\x82\\x01n\\x03\r\nblah blah~~\r\n\r\nthat function not return None\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:2.2.1\r\n- Platform:ubuntu 18.04\r\n- Python version:3.6.9\r\n- PyArrow version:6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4323/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4320", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4320/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4320/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4320/events", "html_url": "https://github.com/huggingface/datasets/issues/4320", "id": 1233208864, "node_id": "I_kwDODunzps5JgUYg", "number": 4320, "title": "Multi-news dataset loader attempts to strip wrong character from beginning of summaries", "user": {"login": "JohnGiorgi", "id": 8917831, "node_id": "MDQ6VXNlcjg5MTc4MzE=", "avatar_url": "https://avatars.githubusercontent.com/u/8917831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JohnGiorgi", "html_url": "https://github.com/JohnGiorgi", "followers_url": "https://api.github.com/users/JohnGiorgi/followers", "following_url": "https://api.github.com/users/JohnGiorgi/following{/other_user}", "gists_url": "https://api.github.com/users/JohnGiorgi/gists{/gist_id}", "starred_url": "https://api.github.com/users/JohnGiorgi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JohnGiorgi/subscriptions", "organizations_url": "https://api.github.com/users/JohnGiorgi/orgs", "repos_url": "https://api.github.com/users/JohnGiorgi/repos", "events_url": "https://api.github.com/users/JohnGiorgi/events{/privacy}", "received_events_url": "https://api.github.com/users/JohnGiorgi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-05-11T21:36:41Z", "updated_at": "2022-05-16T13:52:10Z", "closed_at": "2022-05-16T13:52:10Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThe `multi_news.py` data loader has [a line which attempts to strip `\"- \"` from the beginning of summaries](https://github.com/huggingface/datasets/blob/aa743886221d76afb409d263e1b136e7a71fe2b4/datasets/multi_news/multi_news.py#L97). The actual character in the multi-news dataset, however, is `\"\u2013 \"`, which is different, e.g. `\"\u2013 \" != \"- \"`.\r\n\r\nI would have just opened a PR to fix the mistake, but I am wondering what the motivation for stripping this character is? AFAICT most approaches just leave it in, e.g. the current SOTA on this dataset, [PRIMERA](https://huggingface.co/allenai/PRIMERA-multinews) (you can see its in the generated summaries of the model in their [example notebook](https://github.com/allenai/PRIMER/blob/main/Evaluation_Example.ipynb)).\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.0\r\n- Platform: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4320/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4310", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4310/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4310/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4310/events", "html_url": "https://github.com/huggingface/datasets/issues/4310", "id": 1231319815, "node_id": "I_kwDODunzps5JZHMH", "number": 4310, "title": "Loading dataset with streaming: '_io.BufferedReader' object has no attribute 'loc'", "user": {"login": "milmin", "id": 72745467, "node_id": "MDQ6VXNlcjcyNzQ1NDY3", "avatar_url": "https://avatars.githubusercontent.com/u/72745467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/milmin", "html_url": "https://github.com/milmin", "followers_url": "https://api.github.com/users/milmin/followers", "following_url": "https://api.github.com/users/milmin/following{/other_user}", "gists_url": "https://api.github.com/users/milmin/gists{/gist_id}", "starred_url": "https://api.github.com/users/milmin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/milmin/subscriptions", "organizations_url": "https://api.github.com/users/milmin/orgs", "repos_url": "https://api.github.com/users/milmin/repos", "events_url": "https://api.github.com/users/milmin/events{/privacy}", "received_events_url": "https://api.github.com/users/milmin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-05-10T15:12:53Z", "updated_at": "2022-05-11T16:46:31Z", "closed_at": "2022-05-11T16:46:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading a datasets with `load_dataset` and `streaming=True` returns `AttributeError: '_io.BufferedReader' object has no attribute 'loc'`. Notice that loading with `streaming=False` works fine.\r\n\r\nIn the following steps we load parquet files but the same happens with pickle files. The problem seems to come from `fsspec` lib, I put in the environment info also `s3fs` and `fsspec` versions since I'm loading from an s3 bucket.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n# path is the path to parquet files\r\ndata_files = {\"train\": path + \"meta_train.parquet.gzip\", \"test\": path + \"meta_test.parquet.gzip\"}\r\ndataset = load_dataset(\"parquet\", data_files=data_files, streaming=True)\r\n```\r\n\r\n## Expected results\r\nA dataset object `datasets.dataset_dict.DatasetDict`\r\n\r\n## Actual results\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<command-562086> in <module>\r\n     11 \r\n     12 data_files = {\"train\": path + \"meta_train.parquet.gzip\", \"test\": path + \"meta_test.parquet.gzip\"}\r\n---> 13 dataset = load_dataset(\"parquet\", data_files=data_files, streaming=True)\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1679     if streaming:\r\n   1680         extend_dataset_builder_for_streaming(builder_instance, use_auth_token=use_auth_token)\r\n-> 1681         return builder_instance.as_streaming_dataset(\r\n   1682             split=split,\r\n   1683             use_auth_token=use_auth_token,\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/builder.py in as_streaming_dataset(self, split, base_path, use_auth_token)\r\n    904         )\r\n    905         self._check_manual_download(dl_manager)\r\n--> 906         splits_generators = {sg.name: sg for sg in self._split_generators(dl_manager)}\r\n    907         # By default, return all splits\r\n    908         if split is None:\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/packaged_modules/parquet/parquet.py in _split_generators(self, dl_manager)\r\n     30         if not self.config.data_files:\r\n     31             raise ValueError(f\"At least one data file must be specified, but got data_files={self.config.data_files}\")\r\n---> 32         data_files = dl_manager.download_and_extract(self.config.data_files)\r\n     33         if isinstance(data_files, (str, list, tuple)):\r\n     34             files = data_files\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/utils/streaming_download_manager.py in download_and_extract(self, url_or_urls)\r\n    798 \r\n    799     def download_and_extract(self, url_or_urls):\r\n--> 800         return self.extract(self.download(url_or_urls))\r\n    801 \r\n    802     def iter_archive(self, urlpath_or_buf: Union[str, io.BufferedReader]) -> Iterable[Tuple]:\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/utils/streaming_download_manager.py in extract(self, path_or_paths)\r\n    776 \r\n    777     def extract(self, path_or_paths):\r\n--> 778         urlpaths = map_nested(self._extract, path_or_paths, map_tuple=True)\r\n    779         return urlpaths\r\n    780 \r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm, desc)\r\n    312         num_proc = 1\r\n    313     if num_proc <= 1 or len(iterable) <= num_proc:\r\n--> 314         mapped = [\r\n    315             _single_map_nested((function, obj, types, None, True, None))\r\n    316             for obj in logging.tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/utils/py_utils.py in <listcomp>(.0)\r\n    313     if num_proc <= 1 or len(iterable) <= num_proc:\r\n    314         mapped = [\r\n--> 315             _single_map_nested((function, obj, types, None, True, None))\r\n    316             for obj in logging.tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n    317         ]\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/utils/py_utils.py in _single_map_nested(args)\r\n    267         return {k: _single_map_nested((function, v, types, None, True, None)) for k, v in pbar}\r\n    268     else:\r\n--> 269         mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n    270         if isinstance(data_struct, list):\r\n    271             return mapped\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/utils/py_utils.py in <listcomp>(.0)\r\n    267         return {k: _single_map_nested((function, v, types, None, True, None)) for k, v in pbar}\r\n    268     else:\r\n--> 269         mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n    270         if isinstance(data_struct, list):\r\n    271             return mapped\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/utils/py_utils.py in _single_map_nested(args)\r\n    249     # Singleton first to spare some computation\r\n    250     if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 251         return function(data_struct)\r\n    252 \r\n    253     # Reduce logging to keep things readable in multiprocessing with tqdm\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/utils/streaming_download_manager.py in _extract(self, urlpath)\r\n    781     def _extract(self, urlpath: str) -> str:\r\n    782         urlpath = str(urlpath)\r\n--> 783         protocol = _get_extraction_protocol(urlpath, use_auth_token=self.download_config.use_auth_token)\r\n    784         if protocol is None:\r\n    785             # no extraction\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/utils/streaming_download_manager.py in _get_extraction_protocol(urlpath, use_auth_token)\r\n    371         urlpath, kwargs = urlpath, {}\r\n    372     with fsspec.open(urlpath, **kwargs) as f:\r\n--> 373         return _get_extraction_protocol_with_magic_number(f)\r\n    374 \r\n    375 \r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/datasets/utils/streaming_download_manager.py in _get_extraction_protocol_with_magic_number(f)\r\n    335 def _get_extraction_protocol_with_magic_number(f) -> Optional[str]:\r\n    336     \"\"\"read the magic number from a file-like object and return the compression protocol\"\"\"\r\n--> 337     prev_loc = f.loc\r\n    338     magic_number = f.read(MAGIC_NUMBER_MAX_LENGTH)\r\n    339     f.seek(prev_loc)\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-a7e72260-221c-472b-85f4-bec801aee66d/lib/python3.8/site-packages/fsspec/implementations/local.py in __getattr__(self, item)\r\n    337 \r\n    338     def __getattr__(self, item):\r\n--> 339         return getattr(self.f, item)\r\n    340 \r\n    341     def __enter__(self):\r\n\r\nAttributeError: '_io.BufferedReader' object has no attribute 'loc'\r\n```\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.0-1071-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.2\r\n- `fsspec` version: 2021.08.1\r\n- `s3fs` version: 2021.08.1", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4310/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4310/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4306", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4306/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4306/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4306/events", "html_url": "https://github.com/huggingface/datasets/issues/4306", "id": 1231137204, "node_id": "I_kwDODunzps5JYam0", "number": 4306, "title": "`load_dataset` does not work with certain filename.", "user": {"login": "whatever60", "id": 57242693, "node_id": "MDQ6VXNlcjU3MjQyNjkz", "avatar_url": "https://avatars.githubusercontent.com/u/57242693?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whatever60", "html_url": "https://github.com/whatever60", "followers_url": "https://api.github.com/users/whatever60/followers", "following_url": "https://api.github.com/users/whatever60/following{/other_user}", "gists_url": "https://api.github.com/users/whatever60/gists{/gist_id}", "starred_url": "https://api.github.com/users/whatever60/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whatever60/subscriptions", "organizations_url": "https://api.github.com/users/whatever60/orgs", "repos_url": "https://api.github.com/users/whatever60/repos", "events_url": "https://api.github.com/users/whatever60/events{/privacy}", "received_events_url": "https://api.github.com/users/whatever60/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-05-10T13:14:04Z", "updated_at": "2022-05-10T18:58:36Z", "closed_at": "2022-05-10T18:58:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThis is a weird bug that took me some time to find out.\r\n\r\nI have a JSON dataset that I want to load with `load_dataset` like this:\r\n\r\n```\r\ndata_files = dict(train=\"train.json.zip\", val=\"val.json.zip\")\r\ndataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\r\n```\r\n\r\n## Expected results\r\nNo error.\r\n\r\n## Actual results\r\nThe val file is loaded as expected, but the train file throws JSON decoding error:\r\n\r\n```\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 <ipython-input-74-97947e92c100>:5 in <module>                                             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/site-packages/datasets/load.py:1687 in    \u2502\r\n\u2502 load_dataset                                                                              \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   1684 \u2502   try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES                       \u2502\r\n\u2502   1685 \u2502                                                                                  \u2502\r\n\u2502   1686 \u2502   # Download and prepare data                                                    \u2502\r\n\u2502 \u2771 1687 \u2502   builder_instance.download_and_prepare(                                         \u2502\r\n\u2502   1688 \u2502   \u2502   download_config=download_config,                                           \u2502\r\n\u2502   1689 \u2502   \u2502   download_mode=download_mode,                                               \u2502\r\n\u2502   1690 \u2502   \u2502   ignore_verifications=ignore_verifications,                                 \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/site-packages/datasets/builder.py:605 in  \u2502\r\n\u2502 download_and_prepare                                                                      \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502    602 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   except ConnectionError:                                    \u2502\r\n\u2502    603 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   logger.warning(\"HF google storage unreachable. Downloa \u2502\r\n\u2502    604 \u2502   \u2502   \u2502   \u2502   \u2502   if not downloaded_from_gcs:                                    \u2502\r\n\u2502 \u2771  605 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   self._download_and_prepare(                                \u2502\r\n\u2502    606 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   dl_manager=dl_manager, verify_infos=verify_infos, **do \u2502\r\n\u2502    607 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2502    608 \u2502   \u2502   \u2502   \u2502   \u2502   # Sync info                                                    \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/site-packages/datasets/builder.py:694 in  \u2502\r\n\u2502 _download_and_prepare                                                                     \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502    691 \u2502   \u2502   \u2502                                                                          \u2502\r\n\u2502    692 \u2502   \u2502   \u2502   try:                                                                   \u2502\r\n\u2502    693 \u2502   \u2502   \u2502   \u2502   # Prepare split will record examples associated to the split       \u2502\r\n\u2502 \u2771  694 \u2502   \u2502   \u2502   \u2502   self._prepare_split(split_generator, **prepare_split_kwargs)       \u2502\r\n\u2502    695 \u2502   \u2502   \u2502   except OSError as e:                                                   \u2502\r\n\u2502    696 \u2502   \u2502   \u2502   \u2502   raise OSError(                                                     \u2502\r\n\u2502    697 \u2502   \u2502   \u2502   \u2502   \u2502   \"Cannot find data file. \"                                      \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/site-packages/datasets/builder.py:1151 in \u2502\r\n\u2502 _prepare_split                                                                            \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   1148 \u2502   \u2502                                                                              \u2502\r\n\u2502   1149 \u2502   \u2502   generator = self._generate_tables(**split_generator.gen_kwargs)            \u2502\r\n\u2502   1150 \u2502   \u2502   with ArrowWriter(features=self.info.features, path=fpath) as writer:       \u2502\r\n\u2502 \u2771 1151 \u2502   \u2502   \u2502   for key, table in logging.tqdm(                                        \u2502\r\n\u2502   1152 \u2502   \u2502   \u2502   \u2502   generator, unit=\" tables\", leave=False, disable=True  # not loggin \u2502\r\n\u2502   1153 \u2502   \u2502   \u2502   ):                                                                     \u2502\r\n\u2502   1154 \u2502   \u2502   \u2502   \u2502   writer.write_table(table)                                          \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/site-packages/tqdm/notebook.py:257 in     \u2502\r\n\u2502 __iter__                                                                                  \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   254 \u2502                                                                                   \u2502\r\n\u2502   255 \u2502   def __iter__(self):                                                             \u2502\r\n\u2502   256 \u2502   \u2502   try:                                                                        \u2502\r\n\u2502 \u2771 257 \u2502   \u2502   \u2502   for obj in super(tqdm_notebook, self).__iter__():                       \u2502\r\n\u2502   258 \u2502   \u2502   \u2502   \u2502   # return super(tqdm...) will not catch exception                    \u2502\r\n\u2502   259 \u2502   \u2502   \u2502   \u2502   yield obj                                                           \u2502\r\n\u2502   260 \u2502   \u2502   # NB: except ... [ as ...] breaks IPython async KeyboardInterrupt           \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/site-packages/tqdm/std.py:1183 in         \u2502\r\n\u2502 __iter__                                                                                  \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   1180 \u2502   \u2502   # If the bar is disabled, then just walk the iterable                      \u2502\r\n\u2502   1181 \u2502   \u2502   # (note: keep this check outside the loop for performance)                 \u2502\r\n\u2502   1182 \u2502   \u2502   if self.disable:                                                           \u2502\r\n\u2502 \u2771 1183 \u2502   \u2502   \u2502   for obj in iterable:                                                   \u2502\r\n\u2502   1184 \u2502   \u2502   \u2502   \u2502   yield obj                                                          \u2502\r\n\u2502   1185 \u2502   \u2502   \u2502   return                                                                 \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/site-packages/datasets/packaged_modules/j \u2502\r\n\u2502 son/json.py:90 in _generate_tables                                                        \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502    87 \u2502   \u2502   \u2502   # If the file is one json object and if we need to look at the list of  \u2502\r\n\u2502    88 \u2502   \u2502   \u2502   if self.config.field is not None:                                       \u2502\r\n\u2502    89 \u2502   \u2502   \u2502   \u2502   with open(file, encoding=\"utf-8\") as f:                             \u2502\r\n\u2502 \u2771  90 \u2502   \u2502   \u2502   \u2502   \u2502   dataset = json.load(f)                                          \u2502\r\n\u2502    91 \u2502   \u2502   \u2502   \u2502                                                                       \u2502\r\n\u2502    92 \u2502   \u2502   \u2502   \u2502   # We keep only the field we are interested in                       \u2502\r\n\u2502    93 \u2502   \u2502   \u2502   \u2502   dataset = dataset[self.config.field]                                \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/json/__init__.py:293 in load              \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   290 \u2502   To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``           \u2502\r\n\u2502   291 \u2502   kwarg; otherwise ``JSONDecoder`` is used.                                       \u2502\r\n\u2502   292 \u2502   \"\"\"                                                                             \u2502\r\n\u2502 \u2771 293 \u2502   return loads(fp.read(),                                                         \u2502\r\n\u2502   294 \u2502   \u2502   cls=cls, object_hook=object_hook,                                           \u2502\r\n\u2502   295 \u2502   \u2502   parse_float=parse_float, parse_int=parse_int,                               \u2502\r\n\u2502   296 \u2502   \u2502   parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)   \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/json/__init__.py:357 in loads             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   354 \u2502   if (cls is None and object_hook is None and                                     \u2502\r\n\u2502   355 \u2502   \u2502   \u2502   parse_int is None and parse_float is None and                           \u2502\r\n\u2502   356 \u2502   \u2502   \u2502   parse_constant is None and object_pairs_hook is None and not kw):       \u2502\r\n\u2502 \u2771 357 \u2502   \u2502   return _default_decoder.decode(s)                                           \u2502\r\n\u2502   358 \u2502   if cls is None:                                                                 \u2502\r\n\u2502   359 \u2502   \u2502   cls = JSONDecoder                                                           \u2502\r\n\u2502   360 \u2502   if object_hook is not None:                                                     \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/json/decoder.py:337 in decode             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   334 \u2502   \u2502   containing a JSON document).                                                \u2502\r\n\u2502   335 \u2502   \u2502                                                                               \u2502\r\n\u2502   336 \u2502   \u2502   \"\"\"                                                                         \u2502\r\n\u2502 \u2771 337 \u2502   \u2502   obj, end = self.raw_decode(s, idx=_w(s, 0).end())                           \u2502\r\n\u2502   338 \u2502   \u2502   end = _w(s, end).end()                                                      \u2502\r\n\u2502   339 \u2502   \u2502   if end != len(s):                                                           \u2502\r\n\u2502   340 \u2502   \u2502   \u2502   raise JSONDecodeError(\"Extra data\", s, end)                             \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502 /home/tiankang/software/anaconda3/lib/python3.8/json/decoder.py:353 in raw_decode         \u2502\r\n\u2502                                                                                           \u2502\r\n\u2502   350 \u2502   \u2502                                                                               \u2502\r\n\u2502   351 \u2502   \u2502   \"\"\"                                                                         \u2502\r\n\u2502   352 \u2502   \u2502   try:                                                                        \u2502\r\n\u2502 \u2771 353 \u2502   \u2502   \u2502   obj, end = self.scan_once(s, idx)                                       \u2502\r\n\u2502   354 \u2502   \u2502   except StopIteration as err:                                                \u2502\r\n\u2502   355 \u2502   \u2502   \u2502   raise JSONDecodeError(\"Expecting value\", s, err.value) from None        \u2502\r\n\u2502   356 \u2502   \u2502   return obj, end                                                             \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nJSONDecodeError: Unterminated string starting at: line 85 column 20 (char 60051)\r\n```\r\n\r\nHowever, when I rename the `train.json.zip` to other names (like `training.json.zip`, or even to `train.json`), everything works fine; when I unzip the file to `train.json`, it works as well.\r\n\r\n## Environment info\r\n```\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-4.4.0-131-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4306/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4306/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4297", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4297/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4297/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4297/events", "html_url": "https://github.com/huggingface/datasets/issues/4297", "id": 1229735498, "node_id": "I_kwDODunzps5JTEZK", "number": 4297, "title": "Datasets YAML tagging space is down", "user": {"login": "leondz", "id": 121934, "node_id": "MDQ6VXNlcjEyMTkzNA==", "avatar_url": "https://avatars.githubusercontent.com/u/121934?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leondz", "html_url": "https://github.com/leondz", "followers_url": "https://api.github.com/users/leondz/followers", "following_url": "https://api.github.com/users/leondz/following{/other_user}", "gists_url": "https://api.github.com/users/leondz/gists{/gist_id}", "starred_url": "https://api.github.com/users/leondz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leondz/subscriptions", "organizations_url": "https://api.github.com/users/leondz/orgs", "repos_url": "https://api.github.com/users/leondz/repos", "events_url": "https://api.github.com/users/leondz/events{/privacy}", "received_events_url": "https://api.github.com/users/leondz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-05-09T13:45:05Z", "updated_at": "2022-05-09T14:44:25Z", "closed_at": "2022-05-09T14:44:25Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nThe neat hf spaces app for generating YAML tags for dataset `README.md`s is down\r\n\r\n## Steps to reproduce the bug\r\n1. Visit https://huggingface.co/spaces/huggingface/datasets-tagging\r\n\r\n## Expected results\r\nThere'll be a HF spaces web app for generating dataset metadata YAML\r\n\r\n## Actual results\r\nThere's an error message; here's the step where it breaks:\r\n\r\n```\r\nStep 18/29 : RUN pip install -r requirements.txt\r\n ---> Running in e88bfe7e7e0c\r\nDefaulting to user installation because normal site-packages is not writeable\r\nCollecting git+https://github.com/huggingface/datasets.git@update-task-list (from -r requirements.txt (line 4))\r\n  Cloning https://github.com/huggingface/datasets.git (to revision update-task-list) to /tmp/pip-req-build-bm8t0r0k\r\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/datasets.git /tmp/pip-req-build-bm8t0r0k\r\n  WARNING: Did not find branch or tag 'update-task-list', assuming revision or ref.\r\n  Running command git checkout -q update-task-list\r\n  error: pathspec 'update-task-list' did not match any file(s) known to git\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 git checkout -q update-task-list did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 git checkout -q update-task-list did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n```\r\n\r\n## Environment info\r\n\r\n- Platform: Linux / Brave\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4297/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4297/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4287", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4287/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4287/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4287/events", "html_url": "https://github.com/huggingface/datasets/issues/4287", "id": 1226806652, "node_id": "I_kwDODunzps5JH5V8", "number": 4287, "title": "\"NameError: name 'faiss' is not defined\" on `.add_faiss_index` when `device` is not None", "user": {"login": "alvarobartt", "id": 36760800, "node_id": "MDQ6VXNlcjM2NzYwODAw", "avatar_url": "https://avatars.githubusercontent.com/u/36760800?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alvarobartt", "html_url": "https://github.com/alvarobartt", "followers_url": "https://api.github.com/users/alvarobartt/followers", "following_url": "https://api.github.com/users/alvarobartt/following{/other_user}", "gists_url": "https://api.github.com/users/alvarobartt/gists{/gist_id}", "starred_url": "https://api.github.com/users/alvarobartt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alvarobartt/subscriptions", "organizations_url": "https://api.github.com/users/alvarobartt/orgs", "repos_url": "https://api.github.com/users/alvarobartt/repos", "events_url": "https://api.github.com/users/alvarobartt/events{/privacy}", "received_events_url": "https://api.github.com/users/alvarobartt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-05-05T15:09:45Z", "updated_at": "2022-05-10T13:53:19Z", "closed_at": "2022-05-10T13:53:19Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen using `datasets` to calculate the FAISS indices of a dataset, the exception `NameError: name 'faiss' is not defined` is triggered when trying to calculate those on a device (GPU), so `.add_faiss_index(..., device=0)` fails with that exception.\r\n\r\nAll that assuming that `datasets` is properly installed and `faiss-gpu` too, as well as all the CUDA drivers required.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom transformers import DPRContextEncoder, DPRContextEncoderTokenizer\r\nimport torch\r\ntorch.set_grad_enabled(False)\r\nctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\r\nctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\r\n\r\nfrom datasets import load_dataset\r\nds = load_dataset('crime_and_punish', split='train[:100]')\r\nds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example[\"line\"], return_tensors=\"pt\"))[0][0].numpy()})\r\n\r\nds_with_embeddings.add_faiss_index(column='embeddings', device=0) # default `device=None`\r\n```\r\n\r\n## Expected results\r\n\r\nA new column named `embeddings` in the dataset that we're adding the index to.\r\n\r\n## Actual results\r\n\r\nAn exception is triggered with the following message `NameError: name 'faiss' is not defined`.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.13.0-1022-azure-x86_64-with-glibc2.31\r\n- Python version: 3.9.12\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4287/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4271", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4271/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4271/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4271/events", "html_url": "https://github.com/huggingface/datasets/issues/4271", "id": 1224404403, "node_id": "I_kwDODunzps5I-u2z", "number": 4271, "title": "A typo in docs of datasets.disable_progress_bar", "user": {"login": "jiangwy99", "id": 39762734, "node_id": "MDQ6VXNlcjM5NzYyNzM0", "avatar_url": "https://avatars.githubusercontent.com/u/39762734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiangwy99", "html_url": "https://github.com/jiangwy99", "followers_url": "https://api.github.com/users/jiangwy99/followers", "following_url": "https://api.github.com/users/jiangwy99/following{/other_user}", "gists_url": "https://api.github.com/users/jiangwy99/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiangwy99/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiangwy99/subscriptions", "organizations_url": "https://api.github.com/users/jiangwy99/orgs", "repos_url": "https://api.github.com/users/jiangwy99/repos", "events_url": "https://api.github.com/users/jiangwy99/events{/privacy}", "received_events_url": "https://api.github.com/users/jiangwy99/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "stevhliu", "id": 59462357, "node_id": "MDQ6VXNlcjU5NDYyMzU3", "avatar_url": "https://avatars.githubusercontent.com/u/59462357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stevhliu", "html_url": "https://github.com/stevhliu", "followers_url": "https://api.github.com/users/stevhliu/followers", "following_url": "https://api.github.com/users/stevhliu/following{/other_user}", "gists_url": "https://api.github.com/users/stevhliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/stevhliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stevhliu/subscriptions", "organizations_url": "https://api.github.com/users/stevhliu/orgs", "repos_url": "https://api.github.com/users/stevhliu/repos", "events_url": "https://api.github.com/users/stevhliu/events{/privacy}", "received_events_url": "https://api.github.com/users/stevhliu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "stevhliu", "id": 59462357, "node_id": "MDQ6VXNlcjU5NDYyMzU3", "avatar_url": "https://avatars.githubusercontent.com/u/59462357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stevhliu", "html_url": "https://github.com/stevhliu", "followers_url": "https://api.github.com/users/stevhliu/followers", "following_url": "https://api.github.com/users/stevhliu/following{/other_user}", "gists_url": "https://api.github.com/users/stevhliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/stevhliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stevhliu/subscriptions", "organizations_url": "https://api.github.com/users/stevhliu/orgs", "repos_url": "https://api.github.com/users/stevhliu/repos", "events_url": "https://api.github.com/users/stevhliu/events{/privacy}", "received_events_url": "https://api.github.com/users/stevhliu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-05-03T17:44:56Z", "updated_at": "2022-05-04T06:58:35Z", "closed_at": "2022-05-04T06:58:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nin the docs of V2.1.0 datasets.disable_progress_bar, we should replace \"enable\" with \"disable\".", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4271/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4271/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4248", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4248/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4248/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4248/events", "html_url": "https://github.com/huggingface/datasets/issues/4248", "id": 1218460444, "node_id": "I_kwDODunzps5IoDsc", "number": 4248, "title": "conll2003 dataset loads original data.", "user": {"login": "sue991", "id": 26458611, "node_id": "MDQ6VXNlcjI2NDU4NjEx", "avatar_url": "https://avatars.githubusercontent.com/u/26458611?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sue991", "html_url": "https://github.com/sue991", "followers_url": "https://api.github.com/users/sue991/followers", "following_url": "https://api.github.com/users/sue991/following{/other_user}", "gists_url": "https://api.github.com/users/sue991/gists{/gist_id}", "starred_url": "https://api.github.com/users/sue991/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sue991/subscriptions", "organizations_url": "https://api.github.com/users/sue991/orgs", "repos_url": "https://api.github.com/users/sue991/repos", "events_url": "https://api.github.com/users/sue991/events{/privacy}", "received_events_url": "https://api.github.com/users/sue991/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-04-28T09:33:31Z", "updated_at": "2022-07-18T07:15:48Z", "closed_at": "2022-07-18T07:15:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI load `conll2003` dataset to use refined data like [this](https://huggingface.co/datasets/conll2003/viewer/conll2003/train)  preview, but it is original data that contains `'-DOCSTART- -X- -X- O'` text.\r\n\r\nIs this a bug or should I use another dataset_name like `lhoestq/conll2003` ?\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"conll2003\")\r\n```\r\n\r\n## Expected results\r\n{\r\n    \"chunk_tags\": [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0],\r\n    \"id\": \"0\",\r\n    \"ner_tags\": [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n    \"pos_tags\": [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7],\r\n    \"tokens\": [\"The\", \"European\", \"Commission\", \"said\", \"on\", \"Thursday\", \"it\", \"disagreed\", \"with\", \"German\", \"advice\", \"to\", \"consumers\", \"to\", \"shun\", \"British\", \"lamb\", \"until\", \"scientists\", \"determine\", \"whether\", \"mad\", \"cow\", \"disease\", \"can\", \"be\", \"transmitted\", \"to\", \"sheep\", \".\"]\r\n}\r\n\r\n## Actual results\r\n```python\r\nprint(dataset)\r\n\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text'],\r\n        num_rows: 219554\r\n    })\r\n    test: Dataset({\r\n        features: ['text'],\r\n        num_rows: 50350\r\n    })\r\n    validation: Dataset({\r\n        features: ['text'],\r\n        num_rows: 55044\r\n    })\r\n})\r\n```\r\n\r\n```python\r\nfor i in range(20):\r\n    print(dataset['train'][i])\r\n\r\n{'text': '-DOCSTART- -X- -X- O'}\r\n{'text': ''}\r\n{'text': 'EU NNP B-NP B-ORG'}\r\n{'text': 'rejects VBZ B-VP O'}\r\n{'text': 'German JJ B-NP B-MISC'}\r\n{'text': 'call NN I-NP O'}\r\n{'text': 'to TO B-VP O'}\r\n{'text': 'boycott VB I-VP O'}\r\n{'text': 'British JJ B-NP B-MISC'}\r\n{'text': 'lamb NN I-NP O'}\r\n{'text': '. . O O'}\r\n{'text': ''}\r\n{'text': 'Peter NNP B-NP B-PER'}\r\n{'text': 'Blackburn NNP I-NP I-PER'}\r\n{'text': ''}\r\n{'text': 'BRUSSELS NNP B-NP B-LOC'}\r\n{'text': '1996-08-22 CD I-NP O'}\r\n{'text': ''}\r\n{'text': 'The DT B-NP O'}\r\n{'text': 'European NNP I-NP B-ORG'}\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4248/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4241", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4241/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4241/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4241/events", "html_url": "https://github.com/huggingface/datasets/issues/4241", "id": 1217423686, "node_id": "I_kwDODunzps5IkGlG", "number": 4241, "title": "NonMatchingChecksumError when attempting to download GLUE", "user": {"login": "drussellmrichie", "id": 9650729, "node_id": "MDQ6VXNlcjk2NTA3Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/9650729?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drussellmrichie", "html_url": "https://github.com/drussellmrichie", "followers_url": "https://api.github.com/users/drussellmrichie/followers", "following_url": "https://api.github.com/users/drussellmrichie/following{/other_user}", "gists_url": "https://api.github.com/users/drussellmrichie/gists{/gist_id}", "starred_url": "https://api.github.com/users/drussellmrichie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drussellmrichie/subscriptions", "organizations_url": "https://api.github.com/users/drussellmrichie/orgs", "repos_url": "https://api.github.com/users/drussellmrichie/repos", "events_url": "https://api.github.com/users/drussellmrichie/events{/privacy}", "received_events_url": "https://api.github.com/users/drussellmrichie/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-04-27T14:14:21Z", "updated_at": "2022-04-28T07:45:27Z", "closed_at": "2022-04-28T07:45:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to download the GLUE dataset from the NLP module but get an error (see below).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport nlp\r\nnlp.__version__ # '0.2.0'\r\nnlp.load_dataset('glue', name=\"rte\", download_mode=\"force_redownload\")\r\n```\r\n\r\n## Expected results\r\nI expect the dataset to download without an error.\r\n\r\n## Actual results\r\n```\r\nINFO:nlp.load:Checking /home/richier/.cache/huggingface/datasets/5fe6ab0df8a32a3371b2e6a969d31d855a19563724fb0d0f163748c270c0ac60.2ea96febf19981fae5f13f0a43d4e2aa58bc619bc23acf06de66675f425a5538.py for additional imports.\r\nINFO:nlp.load:Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/glue/glue.py at /home/richier/anaconda3/envs/py36_bert_ee_torch1_11/lib/python3.6/site-packages/nlp/datasets/glue\r\nINFO:nlp.load:Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/glue/glue.py at /home/richier/anaconda3/envs/py36_bert_ee_torch1_11/lib/python3.6/site-packages/nlp/datasets/glue/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\r\nINFO:nlp.load:Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/glue/glue.py to /home/richier/anaconda3/envs/py36_bert_ee_torch1_11/lib/python3.6/site-packages/nlp/datasets/glue/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4/glue.py\r\nINFO:nlp.load:Found dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/glue/dataset_infos.json to /home/richier/anaconda3/envs/py36_bert_ee_torch1_11/lib/python3.6/site-packages/nlp/datasets/glue/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4/dataset_infos.json\r\nINFO:nlp.load:Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/glue/glue.py at /home/richier/anaconda3/envs/py36_bert_ee_torch1_11/lib/python3.6/site-packages/nlp/datasets/glue/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4/glue.json\r\nINFO:nlp.info:Loading Dataset Infos from /home/richier/anaconda3/envs/py36_bert_ee_torch1_11/lib/python3.6/site-packages/nlp/datasets/glue/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4\r\nINFO:nlp.builder:Generating dataset glue (/home/richier/.cache/huggingface/datasets/glue/rte/1.0.0)\r\nINFO:nlp.builder:Dataset not on Hf google storage. Downloading and preparing it from source\r\nINFO:nlp.utils.file_utils:Couldn't get ETag version for url https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\r\nINFO:nlp.utils.file_utils:https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb not found in cache or force_download set to True, downloading to /home/richier/.cache/huggingface/datasets/downloads/tmpldt3n805\r\nDownloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, total: 2.49 MiB) to /home/richier/.cache/huggingface/datasets/glue/rte/1.0.0...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 73.0/73.0 [00:00<00:00, 73.9kB/s]\r\nINFO:nlp.utils.file_utils:storing https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb in cache at /home/richier/.cache/huggingface/datasets/downloads/e8b62ee44e6f8b6aea761935928579ffe1aa55d161808c482e0725abbdcf9c64\r\nINFO:nlp.utils.file_utils:creating metadata file for /home/richier/.cache/huggingface/datasets/downloads/e8b62ee44e6f8b6aea761935928579ffe1aa55d161808c482e0725abbdcf9c64\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-7-669a8343dcc1> in <module>\r\n----> 1 nlp.load_dataset('glue', name=\"rte\", download_mode=\"force_redownload\")\r\n\r\n~/anaconda3/envs/py36_bert_ee_torch1_11/lib/python3.6/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    518         download_mode=download_mode,\r\n    519         ignore_verifications=ignore_verifications,\r\n--> 520         save_infos=save_infos,\r\n    521     )\r\n    522 \r\n\r\n~/anaconda3/envs/py36_bert_ee_torch1_11/lib/python3.6/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    418                 verify_infos = not save_infos and not ignore_verifications\r\n    419                 self._download_and_prepare(\r\n--> 420                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    421                 )\r\n    422                 # Sync info\r\n\r\n~/anaconda3/envs/py36_bert_ee_torch1_11/lib/python3.6/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    458         # Checksums verification\r\n    459         if verify_infos:\r\n--> 460             verify_checksums(self.info.download_checksums, dl_manager.get_recorded_sizes_checksums())\r\n    461         for split_generator in split_generators:\r\n    462             if str(split_generator.split_info.name).lower() == \"all\":\r\n\r\n~/anaconda3/envs/py36_bert_ee_torch1_11/lib/python3.6/site-packages/nlp/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums)\r\n     34     bad_urls = [url for url in expected_checksums if expected_checksums[url] != recorded_checksums[url]]\r\n     35     if len(bad_urls) > 0:\r\n---> 36         raise NonMatchingChecksumError(str(bad_urls))\r\n     37     logger.info(\"All the checksums matched successfully.\")\r\n     38 \r\n\r\nNonMatchingChecksumError: ['https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb']\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-4.18.0-348.20.1.el8_5.x86_64-x86_64-with-redhat-8.5-Ootpa\r\n- Python version: 3.6.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.1.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4241/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4241/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4238", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4238/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4238/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4238/events", "html_url": "https://github.com/huggingface/datasets/issues/4238", "id": 1217168123, "node_id": "I_kwDODunzps5IjIL7", "number": 4238, "title": "Dataset caching policy", "user": {"login": "loretoparisi", "id": 163333, "node_id": "MDQ6VXNlcjE2MzMzMw==", "avatar_url": "https://avatars.githubusercontent.com/u/163333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loretoparisi", "html_url": "https://github.com/loretoparisi", "followers_url": "https://api.github.com/users/loretoparisi/followers", "following_url": "https://api.github.com/users/loretoparisi/following{/other_user}", "gists_url": "https://api.github.com/users/loretoparisi/gists{/gist_id}", "starred_url": "https://api.github.com/users/loretoparisi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loretoparisi/subscriptions", "organizations_url": "https://api.github.com/users/loretoparisi/orgs", "repos_url": "https://api.github.com/users/loretoparisi/repos", "events_url": "https://api.github.com/users/loretoparisi/events{/privacy}", "received_events_url": "https://api.github.com/users/loretoparisi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-04-27T10:42:11Z", "updated_at": "2022-04-27T16:29:25Z", "closed_at": "2022-04-27T16:28:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI cannot clean cache of my datasets files, despite I have updated the `csv` files on the repository [here](https://huggingface.co/datasets/loretoparisi/tatoeba-sentences). The original file had a line with bad characters, causing the following error\r\n\r\n```\r\n[/usr/local/lib/python3.7/dist-packages/datasets/features/features.py](https://localhost:8080/#) in str2int(self, values)\r\n    852                 if value not in self._str2int:\r\n    853                     value = str(value).strip()\r\n--> 854                 output.append(self._str2int[str(value)])\r\n    855             else:\r\n    856                 # No names provided, try to integerize\r\n\r\nKeyError: '\\\\N'\r\n```\r\n\r\nThe file now is cleanup up, but I still get the error. This happens even if I inspect the local cached contents, and cleanup the files locally:\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\ndataset_builder = load_dataset_builder(\"loretoparisi/tatoeba-sentences\")\r\nprint(dataset_builder.cache_dir)\r\nprint(dataset_builder.info.features)\r\nprint(dataset_builder.info.splits)\r\n```\r\n\r\n```\r\nUsing custom data configuration loretoparisi--tatoeba-sentences-e59b8ad92f1bb8dd\r\n/root/.cache/huggingface/datasets/csv/loretoparisi--tatoeba-sentences-e59b8ad92f1bb8dd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\r\nNone\r\nNone\r\n```\r\n\r\nand removing files located at `/root/.cache/huggingface/datasets/csv/loretoparisi--tatoeba-sentences-*`.\r\n Is there any remote file caching policy in place? If so, is it possibile to programmatically disable it? \r\nCurrently it seems that the file `test.csv` on the repo [here](https://huggingface.co/datasets/loretoparisi/tatoeba-sentences/blob/main/test.csv) is cached remotely. In fact I download locally the file from raw link, the file is up-to-date; but If I use it within `datasets` as shown above, it gives to me always the first revision of the file, not the last.\r\n\r\nThank you.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\n\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\r\n     \"loretoparisi/tatoeba-sentences\",\r\n     data_files=data_files,\r\n     delimiter='\\t', \r\n     column_names=['label', 'text'],\r\n)\r\n# You can make this part faster with num_proc=<some int>\r\nsentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\nsentences = sentences.shuffle()\r\n```\r\n\r\n## Expected results\r\nProperly tokenize dataset file `test.csv` without issues.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n```\r\nDownloading data files: 100%\r\n2/2 [00:16<00:00, 7.34s/it]\r\nDownloading data: 100%\r\n391M/391M [00:12<00:00, 36.6MB/s]\r\nDownloading data: 100%\r\n92.4M/92.4M [00:02<00:00, 40.0MB/s]\r\nExtracting data files: 100%\r\n2/2 [00:00<00:00, 47.66it/s]\r\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/loretoparisi--tatoeba-sentences-efeff8965c730a2c/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\r\n100%\r\n2/2 [00:00<00:00, 25.94it/s]\r\n11%\r\n942339/8256449 [01:55<13:11, 9245.85ex/s]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n[<ipython-input-3-6a9867fad8d6>](https://localhost:8080/#) in <module>()\r\n     12 )\r\n     13 # You can make this part faster with num_proc=<some int>\r\n---> 14 sentences = sentences.map(lambda ex: {\"label\" : features[\"label\"].str2int(ex[\"label\"]) if ex[\"label\"] is not None else None}, features=features)\r\n     15 sentences = sentences.shuffle()\r\n\r\n10 frames\r\n[/usr/local/lib/python3.7/dist-packages/datasets/features/features.py](https://localhost:8080/#) in str2int(self, values)\r\n    852                 if value not in self._str2int:\r\n    853                     value = str(value).strip()\r\n--> 854                 output.append(self._str2int[str(value)])\r\n    855             else:\r\n    856                 # No names provided, try to integerize\r\n\r\nKeyError: '\\\\N'\r\n```\r\n\r\n## Environment info\r\n```\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n- ```\r\n\r\n\r\n```\r\n- `transformers` version: 4.18.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- Huggingface_hub version: 0.5.1\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.8.0 (True)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- ```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4238/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4238/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4211", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4211/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4211/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4211/events", "html_url": "https://github.com/huggingface/datasets/issues/4211", "id": 1214361837, "node_id": "I_kwDODunzps5IYbDt", "number": 4211, "title": "DatasetDict containing Datasets with different features when pushed to hub gets remapped features", "user": {"login": "pietrolesci", "id": 61748653, "node_id": "MDQ6VXNlcjYxNzQ4NjUz", "avatar_url": "https://avatars.githubusercontent.com/u/61748653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietrolesci", "html_url": "https://github.com/pietrolesci", "followers_url": "https://api.github.com/users/pietrolesci/followers", "following_url": "https://api.github.com/users/pietrolesci/following{/other_user}", "gists_url": "https://api.github.com/users/pietrolesci/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietrolesci/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietrolesci/subscriptions", "organizations_url": "https://api.github.com/users/pietrolesci/orgs", "repos_url": "https://api.github.com/users/pietrolesci/repos", "events_url": "https://api.github.com/users/pietrolesci/events{/privacy}", "received_events_url": "https://api.github.com/users/pietrolesci/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2022-04-25T11:22:54Z", "updated_at": "2023-04-06T19:25:50Z", "closed_at": "2022-05-20T15:15:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi there,\r\n\r\nI am trying to load a dataset to the Hub. This dataset is a `DatasetDict` composed of various splits. Some splits have a different `Feature` mapping. Locally, the DatasetDict preserves the individual features but if I `push_to_hub` and then `load_dataset`, the features are all the same.\r\n\r\nDataset and code to reproduce available [here](https://huggingface.co/datasets/pietrolesci/robust_nli).\r\n\r\nIn short:\r\n\r\nI have 3 feature mapping\r\n```python\r\nTri_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=3, names=[\"entailment\", \"neutral\", \"contradiction\"]),\r\n    }\r\n)\r\n\r\nEnt_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=2, names=[\"non-entailment\", \"entailment\"]),\r\n    }\r\n)\r\n\r\nCon_features = Features(\r\n    {\r\n        \"idx\": Value(dtype=\"int64\"),\r\n        \"premise\": Value(dtype=\"string\"),\r\n        \"hypothesis\": Value(dtype=\"string\"),\r\n        \"label\": ClassLabel(num_classes=2, names=[\"non-contradiction\", \"contradiction\"]),\r\n    }\r\n)\r\n```\r\n\r\nThen I create different datasets\r\n\r\n```python\r\ndataset_splits = {}\r\n\r\nfor split in df[\"split\"].unique():\r\n    print(split)\r\n    df_split = df.loc[df[\"split\"] == split].copy()\r\n    \r\n    if split in Tri_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2})\r\n        ds = Dataset.from_pandas(df_split, features=Tri_features)\r\n    \r\n    elif split in Ent_bin_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"non-entailment\": 0, \"entailment\": 1})\r\n        ds = Dataset.from_pandas(df_split, features=Ent_features)\r\n    \r\n    elif split in Con_bin_dataset:\r\n        df_split[\"label\"] = df_split[\"label\"].map({\"non-contradiction\": 0, \"contradiction\": 1})\r\n        ds = Dataset.from_pandas(df_split, features=Con_features)\r\n\r\n    else:\r\n        print(\"ERROR:\", split)\r\n    dataset_splits[split] = ds\r\ndatasets = DatasetDict(dataset_splits)\r\n```\r\n\r\nI then push to hub\r\n\r\n```python\r\ndatasets.push_to_hub(\"pietrolesci/robust_nli\", token=\"<token>\")\r\n```\r\n\r\nFinally, I load it from the hub\r\n\r\n```python\r\ndatasets_loaded_from_hub = load_dataset(\"pietrolesci/robust_nli\")\r\n```\r\n\r\nAnd I get that\r\n\r\n```python\r\ndatasets[\"LI_TS\"].features != datasets_loaded_from_hub[\"LI_TS\"].features\r\n```\r\n\r\nsince \r\n\r\n```python\r\n\"label\": ClassLabel(num_classes=2, names=[\"non-contradiction\", \"contradiction\"])\r\n```\r\n\r\ngets remapped to \r\n\r\n```python\r\n \"label\": ClassLabel(num_classes=3, names=[\"entailment\", \"neutral\", \"contradiction\"])\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4211/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4211/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4210", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4210/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4210/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4210/events", "html_url": "https://github.com/huggingface/datasets/issues/4210", "id": 1214089130, "node_id": "I_kwDODunzps5IXYeq", "number": 4210, "title": "TypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'", "user": {"login": "loretoparisi", "id": 163333, "node_id": "MDQ6VXNlcjE2MzMzMw==", "avatar_url": "https://avatars.githubusercontent.com/u/163333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loretoparisi", "html_url": "https://github.com/loretoparisi", "followers_url": "https://api.github.com/users/loretoparisi/followers", "following_url": "https://api.github.com/users/loretoparisi/following{/other_user}", "gists_url": "https://api.github.com/users/loretoparisi/gists{/gist_id}", "starred_url": "https://api.github.com/users/loretoparisi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loretoparisi/subscriptions", "organizations_url": "https://api.github.com/users/loretoparisi/orgs", "repos_url": "https://api.github.com/users/loretoparisi/repos", "events_url": "https://api.github.com/users/loretoparisi/events{/privacy}", "received_events_url": "https://api.github.com/users/loretoparisi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-04-25T07:28:42Z", "updated_at": "2022-05-31T12:16:31Z", "closed_at": "2022-05-31T12:16:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "### System Info\r\n\r\n```shell\r\n- `transformers` version: 4.18.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- Huggingface_hub version: 0.5.1\r\n- PyTorch version (GPU?): 1.10.0+cu111 (True)\r\n- Tensorflow version (GPU?): 2.8.0 (True)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\n@LysandreJik \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n```python\r\nfrom datasets import load_dataset,Features,Value,ClassLabel\r\n\r\nclass_names = [\"cmn\",\"deu\",\"rus\",\"fra\",\"eng\",\"jpn\",\"spa\",\"ita\",\"kor\",\"vie\",\"nld\",\"epo\",\"por\",\"tur\",\"heb\",\"hun\",\"ell\",\"ind\",\"ara\",\"arz\",\"fin\",\"bul\",\"yue\",\"swe\",\"ukr\",\"bel\",\"que\",\"ces\",\"swh\",\"nno\",\"wuu\",\"nob\",\"zsm\",\"est\",\"kat\",\"pol\",\"lat\",\"urd\",\"sqi\",\"isl\",\"fry\",\"afr\",\"ron\",\"fao\",\"san\",\"bre\",\"tat\",\"yid\",\"uig\",\"uzb\",\"srp\",\"qya\",\"dan\",\"pes\",\"slk\",\"eus\",\"cycl\",\"acm\",\"tgl\",\"lvs\",\"kaz\",\"hye\",\"hin\",\"lit\",\"ben\",\"cat\",\"bos\",\"hrv\",\"tha\",\"orv\",\"cha\",\"mon\",\"lzh\",\"scn\",\"gle\",\"mkd\",\"slv\",\"frm\",\"glg\",\"vol\",\"ain\",\"jbo\",\"tok\",\"ina\",\"nds\",\"mal\",\"tlh\",\"roh\",\"ltz\",\"oss\",\"ido\",\"gla\",\"mlt\",\"sco\",\"ast\",\"jav\",\"oci\",\"ile\",\"ota\",\"xal\",\"tel\",\"sjn\",\"nov\",\"khm\",\"tpi\",\"ang\",\"aze\",\"tgk\",\"tuk\",\"chv\",\"hsb\",\"dsb\",\"bod\",\"sme\",\"cym\",\"mri\",\"ksh\",\"kmr\",\"ewe\",\"kab\",\"ber\",\"tpw\",\"udm\",\"lld\",\"pms\",\"lad\",\"grn\",\"mlg\",\"xho\",\"pnb\",\"grc\",\"hat\",\"lao\",\"npi\",\"cor\",\"nah\",\"avk\",\"mar\",\"guj\",\"pan\",\"kir\",\"myv\",\"prg\",\"sux\",\"crs\",\"ckt\",\"bak\",\"zlm\",\"hil\",\"cbk\",\"chr\",\"nav\",\"lkt\",\"enm\",\"arq\",\"lin\",\"abk\",\"pcd\",\"rom\",\"gsw\",\"tam\",\"zul\",\"awa\",\"wln\",\"amh\",\"bar\",\"hbo\",\"mhr\",\"bho\",\"mrj\",\"ckb\",\"osx\",\"pfl\",\"mgm\",\"sna\",\"mah\",\"hau\",\"kan\",\"nog\",\"sin\",\"glv\",\"dng\",\"kal\",\"liv\",\"vro\",\"apc\",\"jdt\",\"fur\",\"che\",\"haw\",\"yor\",\"crh\",\"pdc\",\"ppl\",\"kin\",\"shs\",\"mnw\",\"tet\",\"sah\",\"kum\",\"ngt\",\"nya\",\"pus\",\"hif\",\"mya\",\"moh\",\"wol\",\"tir\",\"ton\",\"lzz\",\"oar\",\"lug\",\"brx\",\"non\",\"mww\",\"hak\",\"nlv\",\"ngu\",\"bua\",\"aym\",\"vec\",\"ibo\",\"tkl\",\"bam\",\"kha\",\"ceb\",\"lou\",\"fuc\",\"smo\",\"gag\",\"lfn\",\"arg\",\"umb\",\"tyv\",\"kjh\",\"oji\",\"cyo\",\"urh\",\"kzj\",\"pam\",\"srd\",\"lmo\",\"swg\",\"mdf\",\"gil\",\"snd\",\"tso\",\"sot\",\"zza\",\"tsn\",\"pau\",\"som\",\"egl\",\"ady\",\"asm\",\"ori\",\"dtp\",\"cho\",\"max\",\"kam\",\"niu\",\"sag\",\"ilo\",\"kaa\",\"fuv\",\"nch\",\"hoc\",\"iba\",\"gbm\",\"sun\",\"war\",\"mvv\",\"pap\",\"ary\",\"kxi\",\"csb\",\"pag\",\"cos\",\"rif\",\"kek\",\"krc\",\"aii\",\"ban\",\"ssw\",\"tvl\",\"mfe\",\"tah\",\"bvy\",\"bcl\",\"hnj\",\"nau\",\"nst\",\"afb\",\"quc\",\"min\",\"tmw\",\"mad\",\"bjn\",\"mai\",\"cjy\",\"got\",\"hsn\",\"gan\",\"tzl\",\"dws\",\"ldn\",\"afh\",\"sgs\",\"krl\",\"vep\",\"rue\",\"tly\",\"mic\",\"ext\",\"izh\",\"sma\",\"jam\",\"cmo\",\"mwl\",\"kpv\",\"koi\",\"bis\",\"ike\",\"run\",\"evn\",\"ryu\",\"mnc\",\"aoz\",\"otk\",\"kas\",\"aln\",\"akl\",\"yua\",\"shy\",\"fkv\",\"gos\",\"fij\",\"thv\",\"zgh\",\"gcf\",\"cay\",\"xmf\",\"tig\",\"div\",\"lij\",\"rap\",\"hrx\",\"cpi\",\"tts\",\"gaa\",\"tmr\",\"iii\",\"ltg\",\"bzt\",\"syc\",\"emx\",\"gom\",\"chg\",\"osp\",\"stq\",\"frr\",\"fro\",\"nys\",\"toi\",\"new\",\"phn\",\"jpa\",\"rel\",\"drt\",\"chn\",\"pli\",\"laa\",\"bal\",\"hdn\",\"hax\",\"mik\",\"ajp\",\"xqa\",\"pal\",\"crk\",\"mni\",\"lut\",\"ayl\",\"ood\",\"sdh\",\"ofs\",\"nus\",\"kiu\",\"diq\",\"qxq\",\"alt\",\"bfz\",\"klj\",\"mus\",\"srn\",\"guc\",\"lim\",\"zea\",\"shi\",\"mnr\",\"bom\",\"sat\",\"szl\"]\r\nfeatures = Features({ 'label': ClassLabel(names=class_names), 'text': Value('string')})\r\nnum_labels = features['label'].num_classes\r\ndata_files = { \"train\": \"train.csv\", \"test\": \"test.csv\" }\r\nsentences = load_dataset(\"loretoparisi/tatoeba-sentences\",\r\n                             data_files=data_files,\r\n                             delimiter='\\t', \r\n                             column_names=['label', 'text'],\r\n                             features = features\r\n```\r\n\r\nERROR:\r\n```\r\nClassLabel(num_classes=403, names=['cmn', 'deu', 'rus', 'fra', 'eng', 'jpn', 'spa', 'ita', 'kor', 'vie', 'nld', 'epo', 'por', 'tur', 'heb', 'hun', 'ell', 'ind', 'ara', 'arz', 'fin', 'bul', 'yue', 'swe', 'ukr', 'bel', 'que', 'ces', 'swh', 'nno', 'wuu', 'nob', 'zsm', 'est', 'kat', 'pol', 'lat', 'urd', 'sqi', 'isl', 'fry', 'afr', 'ron', 'fao', 'san', 'bre', 'tat', 'yid', 'uig', 'uzb', 'srp', 'qya', 'dan', 'pes', 'slk', 'eus', 'cycl', 'acm', 'tgl', 'lvs', 'kaz', 'hye', 'hin', 'lit', 'ben', 'cat', 'bos', 'hrv', 'tha', 'orv', 'cha', 'mon', 'lzh', 'scn', 'gle', 'mkd', 'slv', 'frm', 'glg', 'vol', 'ain', 'jbo', 'tok', 'ina', 'nds', 'mal', 'tlh', 'roh', 'ltz', 'oss', 'ido', 'gla', 'mlt', 'sco', 'ast', 'jav', 'oci', 'ile', 'ota', 'xal', 'tel', 'sjn', 'nov', 'khm', 'tpi', 'ang', 'aze', 'tgk', 'tuk', 'chv', 'hsb', 'dsb', 'bod', 'sme', 'cym', 'mri', 'ksh', 'kmr', 'ewe', 'kab', 'ber', 'tpw', 'udm', 'lld', 'pms', 'lad', 'grn', 'mlg', 'xho', 'pnb', 'grc', 'hat', 'lao', 'npi', 'cor', 'nah', 'avk', 'mar', 'guj', 'pan', 'kir', 'myv', 'prg', 'sux', 'crs', 'ckt', 'bak', 'zlm', 'hil', 'cbk', 'chr', 'nav', 'lkt', 'enm', 'arq', 'lin', 'abk', 'pcd', 'rom', 'gsw', 'tam', 'zul', 'awa', 'wln', 'amh', 'bar', 'hbo', 'mhr', 'bho', 'mrj', 'ckb', 'osx', 'pfl', 'mgm', 'sna', 'mah', 'hau', 'kan', 'nog', 'sin', 'glv', 'dng', 'kal', 'liv', 'vro', 'apc', 'jdt', 'fur', 'che', 'haw', 'yor', 'crh', 'pdc', 'ppl', 'kin', 'shs', 'mnw', 'tet', 'sah', 'kum', 'ngt', 'nya', 'pus', 'hif', 'mya', 'moh', 'wol', 'tir', 'ton', 'lzz', 'oar', 'lug', 'brx', 'non', 'mww', 'hak', 'nlv', 'ngu', 'bua', 'aym', 'vec', 'ibo', 'tkl', 'bam', 'kha', 'ceb', 'lou', 'fuc', 'smo', 'gag', 'lfn', 'arg', 'umb', 'tyv', 'kjh', 'oji', 'cyo', 'urh', 'kzj', 'pam', 'srd', 'lmo', 'swg', 'mdf', 'gil', 'snd', 'tso', 'sot', 'zza', 'tsn', 'pau', 'som', 'egl', 'ady', 'asm', 'ori', 'dtp', 'cho', 'max', 'kam', 'niu', 'sag', 'ilo', 'kaa', 'fuv', 'nch', 'hoc', 'iba', 'gbm', 'sun', 'war', 'mvv', 'pap', 'ary', 'kxi', 'csb', 'pag', 'cos', 'rif', 'kek', 'krc', 'aii', 'ban', 'ssw', 'tvl', 'mfe', 'tah', 'bvy', 'bcl', 'hnj', 'nau', 'nst', 'afb', 'quc', 'min', 'tmw', 'mad', 'bjn', 'mai', 'cjy', 'got', 'hsn', 'gan', 'tzl', 'dws', 'ldn', 'afh', 'sgs', 'krl', 'vep', 'rue', 'tly', 'mic', 'ext', 'izh', 'sma', 'jam', 'cmo', 'mwl', 'kpv', 'koi', 'bis', 'ike', 'run', 'evn', 'ryu', 'mnc', 'aoz', 'otk', 'kas', 'aln', 'akl', 'yua', 'shy', 'fkv', 'gos', 'fij', 'thv', 'zgh', 'gcf', 'cay', 'xmf', 'tig', 'div', 'lij', 'rap', 'hrx', 'cpi', 'tts', 'gaa', 'tmr', 'iii', 'ltg', 'bzt', 'syc', 'emx', 'gom', 'chg', 'osp', 'stq', 'frr', 'fro', 'nys', 'toi', 'new', 'phn', 'jpa', 'rel', 'drt', 'chn', 'pli', 'laa', 'bal', 'hdn', 'hax', 'mik', 'ajp', 'xqa', 'pal', 'crk', 'mni', 'lut', 'ayl', 'ood', 'sdh', 'ofs', 'nus', 'kiu', 'diq', 'qxq', 'alt', 'bfz', 'klj', 'mus', 'srn', 'guc', 'lim', 'zea', 'shi', 'mnr', 'bom', 'sat', 'szl'], id=None)\r\nValue(dtype='string', id=None)\r\nUsing custom data configuration loretoparisi--tatoeba-sentences-7b2c5e991f398f39\r\nDownloading and preparing dataset csv/loretoparisi--tatoeba-sentences to /root/.cache/huggingface/datasets/csv/loretoparisi--tatoeba-sentences-7b2c5e991f398f39/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\r\nDownloading data files: 100%\r\n2/2 [00:18<00:00, 8.06s/it]\r\nDownloading data: 100%\r\n391M/391M [00:13<00:00, 35.3MB/s]\r\nDownloading data: 100%\r\n92.4M/92.4M [00:02<00:00, 36.5MB/s]\r\nFailed to read file '/root/.cache/huggingface/datasets/downloads/933132df9905194ea9faeb30cabca8c49318795612f6495fcb941a290191dd5d' with error <class 'ValueError'>: invalid literal for int() with base 10: 'cmn'\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()\r\n\r\nTypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n15 frames\r\n/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()\r\n\r\nValueError: invalid literal for int() with base 10: 'cmn'\r\n```\r\n\r\nwhile loading without `features` it loads without errors\r\n\r\n```\r\nsentences = load_dataset(\"loretoparisi/tatoeba-sentences\",\r\n                             data_files=data_files,\r\n                             delimiter='\\t', \r\n                             column_names=['label', 'text']\r\n                         )\r\n```\r\n\r\nbut the `label` col seems to be wrong (without the `ClassLabel` object):\r\n\r\n```\r\nsentences['train'].features\r\n{'label': Value(dtype='string', id=None),\r\n 'text': Value(dtype='string', id=None)}\r\n```\r\n\r\nThe dataset was https://huggingface.co/datasets/loretoparisi/tatoeba-sentences\r\n\r\n\r\nDataset format is:\r\n\r\n```\r\nces\tNechci v\u011bd\u011bt, co je tam uvnit\u0159.\r\nces\tKdo o tom chce sly\u0161et?\r\ndeu\tTom sagte, er f\u00fchle sich nicht wohl.\r\nber\tMel-iyi-d anida-t tura ?\r\nhun\tGondom lesz r\u00e1 r\u00f6gt\u00f6n.\r\nber\tMel-iyi-d anida-tt tura ?\r\ndeu\tIch will dich nicht reden h\u00f6ren.\r\n```\r\n\r\n### Expected behavior\r\n\r\n```shell\r\ncorrectly load train and test files.\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4210/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4210/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4199", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4199/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4199/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4199/events", "html_url": "https://github.com/huggingface/datasets/issues/4199", "id": 1211953308, "node_id": "I_kwDODunzps5IPPCc", "number": 4199, "title": "Cache miss during reload for datasets using image fetch utilities through map ", "user": {"login": "apsdehal", "id": 3616806, "node_id": "MDQ6VXNlcjM2MTY4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3616806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apsdehal", "html_url": "https://github.com/apsdehal", "followers_url": "https://api.github.com/users/apsdehal/followers", "following_url": "https://api.github.com/users/apsdehal/following{/other_user}", "gists_url": "https://api.github.com/users/apsdehal/gists{/gist_id}", "starred_url": "https://api.github.com/users/apsdehal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apsdehal/subscriptions", "organizations_url": "https://api.github.com/users/apsdehal/orgs", "repos_url": "https://api.github.com/users/apsdehal/repos", "events_url": "https://api.github.com/users/apsdehal/events{/privacy}", "received_events_url": "https://api.github.com/users/apsdehal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-04-22T07:47:08Z", "updated_at": "2022-04-26T17:00:32Z", "closed_at": "2022-04-26T13:38:26Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nIt looks like that result of `.map` operation dataset are missing the cache when you reload the script and always run from scratch. In same interpretor session, they are able to find the cache and reload it. But, when you exit the interpretor and reload it, the downloading starts from scratch.\r\n\r\n## Steps to reproduce the bug\r\n\r\nUsing the example provided in `red_caps` dataset.\r\n```python\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom functools import partial\r\nimport io\r\nimport urllib\r\n\r\nimport PIL.Image\r\n\r\nimport datasets\r\nfrom datasets import load_dataset\r\nfrom datasets.utils.file_utils import get_datasets_user_agent\r\n\r\n\r\ndef fetch_single_image(image_url, timeout=None, retries=0):\r\n    for _ in range(retries + 1):\r\n        try:\r\n            request = urllib.request.Request(\r\n                image_url,\r\n                data=None,\r\n                headers={\"user-agent\": get_datasets_user_agent()},\r\n            )\r\n            with urllib.request.urlopen(request, timeout=timeout) as req:\r\n                image = PIL.Image.open(io.BytesIO(req.read()))\r\n            break\r\n        except Exception:\r\n            image = None\r\n    return image\r\n\r\n\r\ndef fetch_images(batch, num_threads, timeout=None, retries=0):\r\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\r\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\r\n        batch[\"image\"] = list(executor.map(lambda image_urls: [fetch_single_image_with_args(image_url) for image_url in image_urls], batch[\"image_url\"]))\r\n    return batch\r\n\r\n\r\ndef process_image_urls(batch):\r\n    processed_batch_image_urls = []\r\n    for image_url in batch[\"image_url\"]:\r\n        processed_example_image_urls = []\r\n        image_url_splits = re.findall(r\"http\\S+\", image_url)\r\n        for image_url_split in image_url_splits:\r\n            if \"imgur\" in image_url_split and \",\" in image_url_split:\r\n                for image_url_part in image_url_split.split(\",\"):\r\n                    if not image_url_part:\r\n                        continue\r\n                    image_url_part = image_url_part.strip()\r\n                    root, ext = os.path.splitext(image_url_part)\r\n                    if not root.startswith(\"http\"):\r\n                      root = \"http://i.imgur.com/\" + root\r\n                    root = root.split(\"#\")[0]\r\n                    if not ext:\r\n                      ext = \".jpg\"\r\n                    ext = re.split(r\"[?%]\", ext)[0]\r\n                    image_url_part = root + ext\r\n                    processed_example_image_urls.append(image_url_part)\r\n            else:\r\n                processed_example_image_urls.append(image_url_split)\r\n        processed_batch_image_urls.append(processed_example_image_urls)\r\n    batch[\"image_url\"] = processed_batch_image_urls\r\n    return batch\r\n\r\n\r\ndset = load_dataset(\"red_caps\", \"jellyfish\")\r\ndset = dset.map(process_image_urls, batched=True, num_proc=4)\r\nfeatures = dset[\"train\"].features.copy()\r\nfeatures[\"image\"] = datasets.Sequence(datasets.Image())\r\nnum_threads = 5\r\ndset = dset.map(fetch_images, batched=True, batch_size=50, features=features, fn_kwargs={\"num_threads\": num_threads})\r\n```\r\n\r\nRun this in an interpretor or as a script twice and see that the cache is missed the second time.\r\n\r\n## Expected results\r\nAt reload there should not be any cache miss\r\n\r\n## Actual results\r\nEvery time script is run, cache is missed and dataset is built from scratch.\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.1.dev0\r\n- Platform: Linux-4.19.0-20-cloud-amd64-x86_64-with-glibc2.10\r\n- Python version: 3.8.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4199/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4199/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4192", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4192/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4192/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4192/events", "html_url": "https://github.com/huggingface/datasets/issues/4192", "id": 1210692554, "node_id": "I_kwDODunzps5IKbPK", "number": 4192, "title": "load_dataset can't load local dataset,Unable to find ...", "user": {"login": "ahf876828330", "id": 33253979, "node_id": "MDQ6VXNlcjMzMjUzOTc5", "avatar_url": "https://avatars.githubusercontent.com/u/33253979?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahf876828330", "html_url": "https://github.com/ahf876828330", "followers_url": "https://api.github.com/users/ahf876828330/followers", "following_url": "https://api.github.com/users/ahf876828330/following{/other_user}", "gists_url": "https://api.github.com/users/ahf876828330/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahf876828330/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahf876828330/subscriptions", "organizations_url": "https://api.github.com/users/ahf876828330/orgs", "repos_url": "https://api.github.com/users/ahf876828330/repos", "events_url": "https://api.github.com/users/ahf876828330/events{/privacy}", "received_events_url": "https://api.github.com/users/ahf876828330/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-04-21T08:28:58Z", "updated_at": "2022-04-25T16:51:57Z", "closed_at": "2022-04-22T07:39:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\nTraceback (most recent call last):\r\n  File \"/home/gs603/ahf/pretrained/model.py\", line 48, in <module>\r\n    dataset = load_dataset(\"json\",data_files=\"dataset/dataset_infos.json\")\r\n  File \"/home/gs603/miniconda3/envs/coderepair/lib/python3.7/site-packages/datasets/load.py\", line 1675, in load_dataset\r\n    **config_kwargs,\r\n  File \"/home/gs603/miniconda3/envs/coderepair/lib/python3.7/site-packages/datasets/load.py\", line 1496, in load_dataset_builder\r\n    data_files=data_files,\r\n  File \"/home/gs603/miniconda3/envs/coderepair/lib/python3.7/site-packages/datasets/load.py\", line 1155, in dataset_module_factory\r\n    download_mode=download_mode,\r\n  File \"/home/gs603/miniconda3/envs/coderepair/lib/python3.7/site-packages/datasets/load.py\", line 800, in get_module\r\n    data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)\r\n  File \"/home/gs603/miniconda3/envs/coderepair/lib/python3.7/site-packages/datasets/data_files.py\", line 582, in from_local_or_remote\r\n    if not isinstance(patterns_for_key, DataFilesList)\r\n  File \"/home/gs603/miniconda3/envs/coderepair/lib/python3.7/site-packages/datasets/data_files.py\", line 544, in from_local_or_remote\r\n    data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)\r\n  File \"/home/gs603/miniconda3/envs/coderepair/lib/python3.7/site-packages/datasets/data_files.py\", line 194, in resolve_patterns_locally_or_by_urls\r\n    for path in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions):\r\n  File \"/home/gs603/miniconda3/envs/coderepair/lib/python3.7/site-packages/datasets/data_files.py\", line 144, in _resolve_single_pattern_locally\r\n    raise FileNotFoundError(error_msg)\r\nFileNotFoundError: Unable to find '/home/gs603/ahf/pretrained/dataset/dataset_infos.json' at /home/gs603/ahf/pretrained\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/33253979/164413285-84ea65ac-9126-408f-9cd2-ce4751a5dd73.png)\r\n![image](https://user-images.githubusercontent.com/33253979/164413338-4735142f-408b-41d9-ab87-8484de2be54f.png)\r\n\r\nthe code is in the model.py,why I can't use the load_dataset function to load my local dataset?", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4192/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4192/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4182", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4182/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4182/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4182/events", "html_url": "https://github.com/huggingface/datasets/issues/4182", "id": 1208285235, "node_id": "I_kwDODunzps5IBPgz", "number": 4182, "title": "Zenodo.org download is not responding", "user": {"login": "dkajtoch", "id": 32985207, "node_id": "MDQ6VXNlcjMyOTg1MjA3", "avatar_url": "https://avatars.githubusercontent.com/u/32985207?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dkajtoch", "html_url": "https://github.com/dkajtoch", "followers_url": "https://api.github.com/users/dkajtoch/followers", "following_url": "https://api.github.com/users/dkajtoch/following{/other_user}", "gists_url": "https://api.github.com/users/dkajtoch/gists{/gist_id}", "starred_url": "https://api.github.com/users/dkajtoch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dkajtoch/subscriptions", "organizations_url": "https://api.github.com/users/dkajtoch/orgs", "repos_url": "https://api.github.com/users/dkajtoch/repos", "events_url": "https://api.github.com/users/dkajtoch/events{/privacy}", "received_events_url": "https://api.github.com/users/dkajtoch/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-04-19T12:26:57Z", "updated_at": "2022-04-20T07:11:05Z", "closed_at": "2022-04-20T07:11:05Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nSource download_url from zenodo.org does not respond. \r\n`_DOWNLOAD_URL = \"https://zenodo.org/record/2787612/files/SICK.zip?download=1\"`\r\nOther datasets also use zenodo.org to store data and they cannot be downloaded as well.\r\n\r\nIt would be better to actually use more reliable way to store original data like s3 bucket.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset(\"sick\")\r\n```\r\n\r\n## Expected results\r\nDataset should be downloaded.\r\n\r\n## Actual results\r\nConnectionError: Couldn't reach https://zenodo.org/record/2787612/files/SICK.zip?download=1 (ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='zenodo.org', port=443): Read timed out. (read timeout=100)\")))\r\n\r\n## Environment info\r\n- `datasets` version: 2.1.0\r\n- Platform: Darwin-21.4.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.3.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4182/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4182/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4179", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4179/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4179/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4179/events", "html_url": "https://github.com/huggingface/datasets/issues/4179", "id": 1208001118, "node_id": "I_kwDODunzps5IAKJe", "number": 4179, "title": "Dataset librispeech_asr fails to load", "user": {"login": "albertz", "id": 59132, "node_id": "MDQ6VXNlcjU5MTMy", "avatar_url": "https://avatars.githubusercontent.com/u/59132?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertz", "html_url": "https://github.com/albertz", "followers_url": "https://api.github.com/users/albertz/followers", "following_url": "https://api.github.com/users/albertz/following{/other_user}", "gists_url": "https://api.github.com/users/albertz/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertz/subscriptions", "organizations_url": "https://api.github.com/users/albertz/orgs", "repos_url": "https://api.github.com/users/albertz/repos", "events_url": "https://api.github.com/users/albertz/events{/privacy}", "received_events_url": "https://api.github.com/users/albertz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2022-04-19T08:45:48Z", "updated_at": "2022-07-27T16:10:00Z", "closed_at": "2022-07-27T16:10:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe dataset librispeech_asr (standard Librispeech) fails to load.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndatasets.load_dataset(\"librispeech_asr\")\r\n```\r\n\r\n## Expected results\r\nIt should download and prepare the whole dataset (all subsets).\r\n\r\nIn [the doc](https://huggingface.co/datasets/librispeech_asr), it says it has two configurations (clean and other).\r\nHowever, the dataset doc says that not specifying `split` should just load the whole dataset, which is what I want.\r\n\r\nAlso, in case of this specific dataset, this is also the standard what the community uses. When you look at any publications with results on Librispeech, they always use the whole train dataset for training.\r\n\r\n## Actual results\r\n```\r\n...\r\n  File \"/home/az/.cache/huggingface/modules/datasets_modules/datasets/librispeech_asr/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/librispeech_asr.py\", line 119, in LibrispeechASR._split_generators\r\n    line: archive_path = dl_manager.download(_DL_URLS[self.config.name])\r\n    locals:\r\n      archive_path = <not found>\r\n      dl_manager = <local> <datasets.utils.download_manager.DownloadManager object at 0x7fc07b426160>\r\n      dl_manager.download = <local> <bound method DownloadManager.download of <datasets.utils.download_manager.DownloadManager object at 0x7fc07b426160>>\r\n      _DL_URLS = <global> {'clean': {'dev': 'http://www.openslr.org/resources/12/dev-clean.tar.gz', 'test': 'http://www.openslr.org/resources/12/test-clean.tar.gz', 'train.100': 'http://www.openslr.org/resources/12/train-clean-100.tar.gz', 'train.360': 'http://www.openslr.org/resources/12/train-clean-360.tar.gz'}, 'other'...\r\n      self = <local> <datasets_modules.datasets.librispeech_asr.1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c.librispeech_asr.LibrispeechASR object at 0x7fc12a633310>\r\n      self.config = <local> BuilderConfig(name='default', version=0.0.0, data_dir='/home/az/i6/setups/2022-03-20--sis/work/i6_core/datasets/huggingface/DownloadAndPrepareHuggingFaceDatasetJob.TV6Nwm6dFReF/output/data_dir', data_files=None, description=None)\r\n      self.config.name = <local> 'default', len = 7\r\nKeyError: 'default'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.4.0-107-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.9\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.4.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4179/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4176", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4176/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4176/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4176/events", "html_url": "https://github.com/huggingface/datasets/issues/4176", "id": 1206515563, "node_id": "I_kwDODunzps5H6fdr", "number": 4176, "title": "Very slow between two operations", "user": {"login": "yananchen1989", "id": 26405281, "node_id": "MDQ6VXNlcjI2NDA1Mjgx", "avatar_url": "https://avatars.githubusercontent.com/u/26405281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yananchen1989", "html_url": "https://github.com/yananchen1989", "followers_url": "https://api.github.com/users/yananchen1989/followers", "following_url": "https://api.github.com/users/yananchen1989/following{/other_user}", "gists_url": "https://api.github.com/users/yananchen1989/gists{/gist_id}", "starred_url": "https://api.github.com/users/yananchen1989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yananchen1989/subscriptions", "organizations_url": "https://api.github.com/users/yananchen1989/orgs", "repos_url": "https://api.github.com/users/yananchen1989/repos", "events_url": "https://api.github.com/users/yananchen1989/events{/privacy}", "received_events_url": "https://api.github.com/users/yananchen1989/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-04-17T23:52:29Z", "updated_at": "2022-04-18T00:03:00Z", "closed_at": "2022-04-18T00:03:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, in the processing stage, I use two operations. The first one : map + filter, is very fast and it uses the full cores, while the socond step is very slow and did not use full cores. \r\n\r\nAlso, there is a significant lag between them.  Am I missing something ?\r\n\r\n\r\n\r\n ```\r\nraw_datasets = raw_datasets.map(split_func, \r\n                batched=False,\r\n                num_proc=args.preprocessing_num_workers,\r\n                load_from_cache_file=not args.overwrite_cache, \r\n                desc = \"running split para ==>\")\\\r\n                .filter(lambda example: example['text1']!='' and example['text2']!='', \r\n                    num_proc=args.preprocessing_num_workers, desc=\"filtering ==>\")\r\n\r\n\r\n    processed_datasets = raw_datasets.map(\r\n        preprocess_function,\r\n        batched=True, \r\n        num_proc=args.preprocessing_num_workers,\r\n        remove_columns=column_names,\r\n        load_from_cache_file=not args.overwrite_cache,\r\n        desc=\"Running tokenizer on dataset===>\",\r\n    )\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4176/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4176/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4150", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4150/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4150/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4150/events", "html_url": "https://github.com/huggingface/datasets/issues/4150", "id": 1201689730, "node_id": "I_kwDODunzps5HoFSC", "number": 4150, "title": "Inconsistent splits generation for datasets without loading script (packaged dataset puts everything into a single split)", "user": {"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-04-12T11:15:55Z", "updated_at": "2022-04-28T21:02:44Z", "closed_at": "2022-04-28T21:02:44Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nSplits for dataset loaders without scripts are prepared inconsistently. I think it might be confusing for users.\r\n\r\n## Steps to reproduce the bug\r\n* If you load a packaged datasets from Hub, it infers splits from directory structure / filenames (check out the data [here](https://huggingface.co/datasets/nateraw/test-imagefolder-dataset)):\r\n```python\r\nds = load_dataset(\"nateraw/test-imagefolder-dataset\")\r\nprint(ds)\r\n### Output:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 6\r\n    })\r\n    test: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 4\r\n    })\r\n})\r\n```\r\n* If you do the same from locally stored data specifying only directory path you'll get the same:\r\n```python\r\nds = load_dataset(\"/path/to/local/data/test-imagefolder-dataset\")\r\nprint(ds)\r\n### Output:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 6\r\n    })\r\n    test: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 4\r\n    })\r\n})\r\n```\r\n* However, if you explicitely specify package name (like `imagefolder`, `csv`, `json`), all the data is put into a single split:\r\n```python\r\nds = load_dataset(\"imagefolder\", data_dir=\"/path/to/local/data/test-imagefolder-dataset\")\r\nprint(ds)\r\n### Output:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 10\r\n    })\r\n})\r\n```\r\n\r\n## Expected results\r\nFor `load_dataset(\"imagefolder\", data_dir=\"/path/to/local/data/test-imagefolder-dataset\")` I expect the same output as of the two first options.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4150/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4150/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4149", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4149/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4149/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4149/events", "html_url": "https://github.com/huggingface/datasets/issues/4149", "id": 1201389221, "node_id": "I_kwDODunzps5Hm76l", "number": 4149, "title": "load_dataset for winoground returning decoding error", "user": {"login": "odellus", "id": 4686956, "node_id": "MDQ6VXNlcjQ2ODY5NTY=", "avatar_url": "https://avatars.githubusercontent.com/u/4686956?v=4", "gravatar_id": "", "url": "https://api.github.com/users/odellus", "html_url": "https://github.com/odellus", "followers_url": "https://api.github.com/users/odellus/followers", "following_url": "https://api.github.com/users/odellus/following{/other_user}", "gists_url": "https://api.github.com/users/odellus/gists{/gist_id}", "starred_url": "https://api.github.com/users/odellus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/odellus/subscriptions", "organizations_url": "https://api.github.com/users/odellus/orgs", "repos_url": "https://api.github.com/users/odellus/repos", "events_url": "https://api.github.com/users/odellus/events{/privacy}", "received_events_url": "https://api.github.com/users/odellus/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2022-04-12T08:16:16Z", "updated_at": "2022-05-04T23:40:38Z", "closed_at": "2022-05-04T23:40:38Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to use datasets to load winoground and I'm getting a JSON decoding error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ntoken = 'hf_XXXXX' # my HF access token\r\ndatasets = load_dataset('facebook/winoground', use_auth_token=token)\r\n```\r\n\r\n## Expected results\r\nI downloaded images.zip and examples.jsonl manually. I was expecting to have some trouble decoding json so I didn't use jsonlines but instead was able to get a complete set of 400 examples by doing\r\n```python\r\nimport json\r\n\r\nwith open('examples.jsonl', 'r') as f:\r\n    examples = f.read().split('\\n')\r\n\r\n# Thinking this would error if the JSON is not utf-8 encoded\r\njson_data = [json.loads(x) for x in examples]\r\nprint(json_data[-1])\r\n```\r\nand I see\r\n```python\r\n{'caption_0': 'someone is overdoing it',\r\n 'caption_1': 'someone is doing it over',\r\n 'collapsed_tag': 'Relation',\r\n 'id': 399,\r\n 'image_0': 'ex_399_img_0',\r\n 'image_1': 'ex_399_img_1',\r\n 'num_main_preds': 1,\r\n 'secondary_tag': 'Morpheme-Level',\r\n 'tag': 'Scope, Preposition'}\r\n\r\n```\r\nso I'm not sure what's going on here honestly. The file `examples.jsonl` doesn't have non-UTF-8 encoded text.\r\n\r\n## Actual results\r\nDuring the split operation after downloading, datasets encounters an error in the JSON ([trace](https://gist.github.com/odellus/e55d390ca203386bf551f38e0c63a46b) abbreviated for brevity).\r\n```\r\ndatasets/packaged_modules/json/json.py:144 in Json._generate_tables(self, files)\r\n...\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.4\r\n- Platform: Linux-5.13.0-39-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4149/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4149/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4146", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4146/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4146/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4146/events", "html_url": "https://github.com/huggingface/datasets/issues/4146", "id": 1200215789, "node_id": "I_kwDODunzps5Hidbt", "number": 4146, "title": "SAMSum dataset viewer not working", "user": {"login": "aakashnegi10", "id": 39906333, "node_id": "MDQ6VXNlcjM5OTA2MzMz", "avatar_url": "https://avatars.githubusercontent.com/u/39906333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aakashnegi10", "html_url": "https://github.com/aakashnegi10", "followers_url": "https://api.github.com/users/aakashnegi10/followers", "following_url": "https://api.github.com/users/aakashnegi10/following{/other_user}", "gists_url": "https://api.github.com/users/aakashnegi10/gists{/gist_id}", "starred_url": "https://api.github.com/users/aakashnegi10/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aakashnegi10/subscriptions", "organizations_url": "https://api.github.com/users/aakashnegi10/orgs", "repos_url": "https://api.github.com/users/aakashnegi10/repos", "events_url": "https://api.github.com/users/aakashnegi10/events{/privacy}", "received_events_url": "https://api.github.com/users/aakashnegi10/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-04-11T16:22:57Z", "updated_at": "2022-04-29T16:26:09Z", "closed_at": "2022-04-29T16:26:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Dataset viewer issue for '*name of the dataset*'\r\n\r\n**Link:** *link to the dataset viewer page*\r\n\r\n*short description of the issue*\r\n\r\nAm I the one who added this dataset ? Yes-No\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4146/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4146/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4143", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4143/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4143/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4143/events", "html_url": "https://github.com/huggingface/datasets/issues/4143", "id": 1199937961, "node_id": "I_kwDODunzps5HhZmp", "number": 4143, "title": "Unable to download `Wikepedia` 20220301.en version", "user": {"login": "beyondguo", "id": 37113676, "node_id": "MDQ6VXNlcjM3MTEzNjc2", "avatar_url": "https://avatars.githubusercontent.com/u/37113676?v=4", "gravatar_id": "", "url": "https://api.github.com/users/beyondguo", "html_url": "https://github.com/beyondguo", "followers_url": "https://api.github.com/users/beyondguo/followers", "following_url": "https://api.github.com/users/beyondguo/following{/other_user}", "gists_url": "https://api.github.com/users/beyondguo/gists{/gist_id}", "starred_url": "https://api.github.com/users/beyondguo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/beyondguo/subscriptions", "organizations_url": "https://api.github.com/users/beyondguo/orgs", "repos_url": "https://api.github.com/users/beyondguo/repos", "events_url": "https://api.github.com/users/beyondguo/events{/privacy}", "received_events_url": "https://api.github.com/users/beyondguo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-04-11T13:00:14Z", "updated_at": "2022-08-17T00:37:55Z", "closed_at": "2022-04-21T17:04:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nUnable to download `Wikepedia` dataset, 20220301.en version\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n!pip install apache_beam mwparserfromhell\r\ndataset_wikipedia = load_dataset(\"wikipedia\", \"20220301.en\")\r\n```\r\n\r\n## Actual results\r\n```\r\nValueError: BuilderConfig 20220301.en not found. \r\nAvailable: ['20200501.aa', '20200501.ab', '20200501.ace', '20200501.ady', '20200501.af', '20200501.ak', '20200501.als', '20200501.am', '20200501.an', '20200501.ang', '20200501.ar', '20200501.arc', '20200501.arz', '20200501.as', '20200501.ast', '20200501.atj', '20200501.av', '20200501.ay', '20200501.az', '20200501.azb', '20200501.ba', '20200501.bar', '20200501.bat-smg', '20200501.bcl', '20200501.be', '20200501.be-x-old', '20200501.bg', '20200501.bh', '20200501.bi', '20200501.bjn', '20200501.bm', '20200501.bn', '20200501.bo', '20200501.bpy', '20200501.br', '20200501.bs', '20200501.bug', '20200501.bxr', '20200501.ca', '20200501.cbk-zam', '20200501.cdo', '20200501.ce', '20200501.ceb', '20200501.ch', '20200501.cho', '20200501.chr', '20200501.chy', '20200501.ckb', '20200501.co', '20200501.cr', '20200501.crh', '20200501.cs', '20200501.csb', '20200501.cu', '20200501.cv', '20200501.cy', '20200501.da', '20200501.de', '20200501.din', '20200501.diq', '20200501.dsb', '20200501.dty', '20200501.dv', '20200501.dz', '20200501.ee', '20200501.el', '20200501.eml', '20200501.en', '20200501.eo', '20200501.es', '20200501.et', '20200501.eu', '20200501.ext', '20200501.fa', '20200501.ff', '20200501.fi', '20200501.fiu-vro', '20200501.fj', '20200501.fo', '20200501.fr', '20200501.frp', '20200501.frr', '20200501.fur', '20200501.fy', '20200501.ga', '20200501.gag', '20200501.gan', '20200501.gd', '20200501.gl', '20200501.glk', '20200501.gn', '20200501.gom', '20200501.gor', '20200501.got', '20200501.gu', '20200501.gv', '20200501.ha', '20200501.hak', '20200501.haw', '20200501.he', '20200501.hi', '20200501.hif', '20200501.ho', '20200501.hr', '20200501.hsb', '20200501.ht', '20200501.hu', '20200501.hy', '20200501.ia', '20200501.id', '20200501.ie', '20200501.ig', '20200501.ii', '20200501.ik', '20200501.ilo', '20200501.inh', '20200501.io', '20200501.is', '20200501.it', '20200501.iu', '20200501.ja', '20200501.jam', '20200501.jbo', '20200501.jv', '20200501.ka', '20200501.kaa', '20200501.kab', '20200501.kbd', '20200501.kbp', '20200501.kg', '20200501.ki', '20200501.kj', '20200501.kk', '20200501.kl', '20200501.km', '20200501.kn', '20200501.ko', '20200501.koi', '20200501.krc', '20200501.ks', '20200501.ksh', '20200501.ku', '20200501.kv', '20200501.kw', '20200501.ky', '20200501.la', '20200501.lad', '20200501.lb', '20200501.lbe', '20200501.lez', '20200501.lfn', '20200501.lg', '20200501.li', '20200501.lij', '20200501.lmo', '20200501.ln', '20200501.lo', '20200501.lrc', '20200501.lt', '20200501.ltg', '20200501.lv', '20200501.mai', '20200501.map-bms', '20200501.mdf', '20200501.mg', '20200501.mh', '20200501.mhr', '20200501.mi', '20200501.min', '20200501.mk', '20200501.ml', '20200501.mn', '20200501.mr', '20200501.mrj', '20200501.ms', '20200501.mt', '20200501.mus', '20200501.mwl', '20200501.my', '20200501.myv', '20200501.mzn', '20200501.na', '20200501.nah', '20200501.nap', '20200501.nds', '20200501.nds-nl', '20200501.ne', '20200501.new', '20200501.ng', '20200501.nl', '20200501.nn', '20200501.no', '20200501.nov', '20200501.nrm', '20200501.nso', '20200501.nv', '20200501.ny', '20200501.oc', '20200501.olo', '20200501.om', '20200501.or', '20200501.os', '20200501.pa', '20200501.pag', '20200501.pam', '20200501.pap', '20200501.pcd', '20200501.pdc', '20200501.pfl', '20200501.pi', '20200501.pih', '20200501.pl', '20200501.pms', '20200501.pnb', '20200501.pnt', '20200501.ps', '20200501.pt', '20200501.qu', '20200501.rm', '20200501.rmy', '20200501.rn', '20200501.ro', '20200501.roa-rup', '20200501.roa-tara', '20200501.ru', '20200501.rue', '20200501.rw', '20200501.sa', '20200501.sah', '20200501.sat', '20200501.sc', '20200501.scn', '20200501.sco', '20200501.sd', '20200501.se', '20200501.sg', '20200501.sh', '20200501.si', '20200501.simple', '20200501.sk', '20200501.sl', '20200501.sm', '20200501.sn', '20200501.so', '20200501.sq', '20200501.sr', '20200501.srn', '20200501.ss', '20200501.st', '20200501.stq', '20200501.su', '20200501.sv', '20200501.sw', '20200501.szl', '20200501.ta', '20200501.tcy', '20200501.te', '20200501.tet', '20200501.tg', '20200501.th', '20200501.ti', '20200501.tk', '20200501.tl', '20200501.tn', '20200501.to', '20200501.tpi', '20200501.tr', '20200501.ts', '20200501.tt', '20200501.tum', '20200501.tw', '20200501.ty', '20200501.tyv', '20200501.udm', '20200501.ug', '20200501.uk', '20200501.ur', '20200501.uz', '20200501.ve', '20200501.vec', '20200501.vep', '20200501.vi', '20200501.vls', '20200501.vo', '20200501.wa', '20200501.war', '20200501.wo', '20200501.wuu', '20200501.xal', '20200501.xh', '20200501.xmf', '20200501.yi', '20200501.yo', '20200501.za', '20200501.zea', '20200501.zh', '20200501.zh-classical', '20200501.zh-min-nan', '20200501.zh-yue', '20200501.zu']\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Ubuntu\r\n- Python version: 3.6\r\n- PyArrow version: 6.0.1", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4143/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4140", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4140/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4140/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4140/events", "html_url": "https://github.com/huggingface/datasets/issues/4140", "id": 1199492356, "node_id": "I_kwDODunzps5Hfs0E", "number": 4140, "title": "Error loading arxiv data set", "user": {"login": "yjqiu", "id": 5383918, "node_id": "MDQ6VXNlcjUzODM5MTg=", "avatar_url": "https://avatars.githubusercontent.com/u/5383918?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yjqiu", "html_url": "https://github.com/yjqiu", "followers_url": "https://api.github.com/users/yjqiu/followers", "following_url": "https://api.github.com/users/yjqiu/following{/other_user}", "gists_url": "https://api.github.com/users/yjqiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yjqiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yjqiu/subscriptions", "organizations_url": "https://api.github.com/users/yjqiu/orgs", "repos_url": "https://api.github.com/users/yjqiu/repos", "events_url": "https://api.github.com/users/yjqiu/events{/privacy}", "received_events_url": "https://api.github.com/users/yjqiu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-04-11T07:06:34Z", "updated_at": "2022-04-12T16:24:08Z", "closed_at": "2022-04-12T16:24:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\nI met the error below when loading arxiv dataset via `nlp.load_dataset('scientific_papers', 'arxiv',)`. \r\n```\r\nTraceback (most recent call last):\r\n  File \"scripts/summarization.py\", line 354, in <module>\r\n    main(args)\r\n  File \"scripts/summarization.py\", line 306, in main\r\n    model.hf_datasets = nlp.load_dataset('scientific_papers', 'arxiv')\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/nlp/load.py\", line 549, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/nlp/builder.py\", line 463, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/nlp/builder.py\", line 522, in _download_and_prepare\r\n    self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/nlp/utils/info_utils.py\", line 38, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\nnlp.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?id=1b3rmCSIoh6VhD4HKWjI4HOW-cSwcwbeC&export=download', 'https://drive.google.com/uc?id=1lvsqvsFi3W-pE1SqNZI0s8NR9rC1tsja&export=download']\r\n```\r\n\r\nI then tried to ignore verification steps by `ignore_verifications=True` and there is another error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/nlp/builder.py\", line 537, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/nlp/builder.py\", line 810, in _prepare_split\r\n    for key, record in utils.tqdm(generator, unit=\" examples\", total=split_info.num_examples, leave=False):\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/tqdm/std.py\", line 1195, in __iter__\r\n    for obj in iterable:\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/nlp/datasets/scientific_papers/9e4f2cfe3d8494e9f34a84ce49c3214605b4b52a3d8eb199104430d04c52cc12/scientific_papers.py\", line 108, in _generate_examples\r\n    with open(path, encoding=\"utf-8\") as f:\r\nNotADirectoryError: [Errno 20] Not a directory: '/home/username/.cache/huggingface/datasets/downloads/c0deae7af7d9c87f25dfadf621f7126f708d7dcac6d353c7564883084a000076/arxiv-dataset/train.txt'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"scripts/summarization.py\", line 354, in <module>\r\n    main(args)\r\n  File \"scripts/summarization.py\", line 306, in main\r\n    model.hf_datasets = nlp.load_dataset('scientific_papers', 'arxiv', ignore_verifications=True)\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/nlp/load.py\", line 549, in load_dataset\r\n    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/nlp/builder.py\", line 463, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/opt/conda/envs/longformer/lib/python3.7/site-packages/nlp/builder.py\", line 539, in _download_and_prepare\r\n    raise OSError(\"Cannot find data file. \" + (self.manual_download_instructions or \"\"))\r\nOSError: Cannot find data file.\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4140/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4140/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4124", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4124/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4124/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4124/events", "html_url": "https://github.com/huggingface/datasets/issues/4124", "id": 1196469842, "node_id": "I_kwDODunzps5HUK5S", "number": 4124, "title": "Image decoding often fails when  transforming Image datasets", "user": {"login": "RafayAK", "id": 17025191, "node_id": "MDQ6VXNlcjE3MDI1MTkx", "avatar_url": "https://avatars.githubusercontent.com/u/17025191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RafayAK", "html_url": "https://github.com/RafayAK", "followers_url": "https://api.github.com/users/RafayAK/followers", "following_url": "https://api.github.com/users/RafayAK/following{/other_user}", "gists_url": "https://api.github.com/users/RafayAK/gists{/gist_id}", "starred_url": "https://api.github.com/users/RafayAK/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RafayAK/subscriptions", "organizations_url": "https://api.github.com/users/RafayAK/orgs", "repos_url": "https://api.github.com/users/RafayAK/repos", "events_url": "https://api.github.com/users/RafayAK/events{/privacy}", "received_events_url": "https://api.github.com/users/RafayAK/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2022-04-07T19:17:25Z", "updated_at": "2022-04-13T14:01:16Z", "closed_at": "2022-04-13T14:01:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen transforming/modifying images in an image dataset using the `map` function the PIL images often fail to decode in time for the image transforms, causing errors.\r\n\r\nUsing a debugger it is easy to see what the problem is, the Image decode invocation does not take place and the resulting image passed around is still raw bytes:\r\n```\r\n[{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00 \\x00\\x00\\x00 \\x08\\x02\\x00\\x00\\x00\\xfc\\x18\\xed\\xa3\\x00\\x00\\x08\\x02IDATx\\x9cEVIs[\\xc7\\x11\\xeemf\\xde\\x82\\x8d\\x80\\x08\\x89\"\\xb5V\\\\\\xb6\\x94(\\xe5\\x9f\\x90\\xca5\\x7f$\\xa7T\\xe5\\x9f&9\\xd9\\x8a\\\\.\\xdb\\xa4$J\\xa4\\x00\\x02x\\xc0{\\xb3t\\xe7\\x00\\xca\\x99\\xd3\\\\f\\xba\\xba\\xbf\\xa5?|\\xfa\\xf4\\xa2\\xeb\\xba\\xedv\\xa3f^\\xf8\\xd5\\x0bY\\xb6\\x10\\xb3\\xaaDq\\xcd\\x83\\x87\\xdf5\\xf3gZ\\x1a\\x04\\x0f\\xa0fp\\xfa\\xe0\\xd4\\x07?\\x9dN\\xc4\\xb1\\x99\\xfd\\xf2\\xcb/\\x97\\x97\\x97H\\xa2\\xaaf\\x16\\x82\\xaf\\xeb\\xca{\\xbf\\xd9l.\\xdf\\x7f\\xfa\\xcb_\\xff&\\x88\\x08\\x00\\x80H\\xc0\\x80@.;\\x0f\\x8c@#v\\xe3\\xe5\\xfc\\xd1\\x9f\\xee6q\\xbf\\xdf\\xa6\\x14\\'\\x93\\xf1\\xc3\\xe5\\xe3\\xd1x\\x14c\\x8c1\\xa5\\x1c\\x9dsM\\xd3\\xb4\\xed\\x08\\x89SJ)\\xa5\\xedv\\xbb^\\xafNO\\x97D\\x84Hf .... \r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, Dataset\r\nimport numpy as np\r\n# seeded NumPy random number generator for reprodducinble results.\r\nrng = np.random.default_rng(seed=0)\r\n\r\ntest_dataset = load_dataset('cifar100', split=\"test\")\r\n\r\ndef preprocess_data(dataset):\r\n    \"\"\"\r\n    Helper function to pre-process HuggingFace Cifar-100 Dataset to remove fine_label and coarse_label columns and\r\n    add is_flipped column\r\n    Args:\r\n        dataset: HuggingFace CIFAR-100 Dataset Object\r\n\r\n    Returns:\r\n        new_dataset: A Dataset object with \"img\" and \"is_flipped\" columns only\r\n\r\n    \"\"\"\r\n    # remove fine_label and coarse_label columns\r\n    new_dataset = dataset.remove_columns(['fine_label', 'coarse_label'])\r\n    # add the column for is_flipped\r\n    new_dataset = new_dataset.add_column(name=\"is_flipped\", column=np.zeros((len(new_dataset)), dtype=np.uint8))\r\n\r\n    return new_dataset\r\n\r\n\r\ndef generate_flipped_data(example, p=0.5):\r\n    \"\"\"\r\n    A Dataset mapping function that transforms some of the images up-side-down.\r\n    If the probability value (p) is 0.5 approximately half the images will be flipped upside-down\r\n    Args:\r\n        example: An example from the dataset containing a Python dictionary with \"img\" and \"is_flipped\" key-value pair\r\n        p: the probability of flipping the image up-side-down, Default 0.5\r\n\r\n    Returns:\r\n        example: A Dataset object\r\n\r\n    \"\"\"\r\n    # example['img'] = example['img']\r\n    if rng.random() > p:  # the flip the image and set is_flipped column to 1\r\n        example['img'] = example['img'].transpose(\r\n            1)  # ImageOps.flip(example['img'])  #example['img'].transpose(Image.FLIP_TOP_BOTTOM)\r\n        example['is_flipped'] = 1\r\n\r\n    return example\r\n\r\nmy_test = preprocess_data(test_dataset)\r\nmy_test = my_test.map(generate_flipped_data)\r\n\r\n```\r\n\r\n## Expected results\r\nThe dataset should be transformed without problems.\r\n\r\n## Actual results\r\n```\r\n/home/rafay/anaconda3/envs/pytorch_new/bin/python /home/rafay/Documents/you_only_live_once/upside_down_detector/create_dataset.py\r\nReusing dataset cifar100 (/home/rafay/.cache/huggingface/datasets/cifar100/cifar100/1.0.0/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142)\r\nReusing dataset cifar100 (/home/rafay/.cache/huggingface/datasets/cifar100/cifar100/1.0.0/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142)\r\n 20%|\u2588\u2589        | 1999/10000 [00:00<00:01, 5560.44ex/s]\r\nTraceback (most recent call last):\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2326, in _map_single\r\n    writer.write(example)\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 441, in write\r\n    self.write_examples_on_file()\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 399, in write_examples_on_file\r\n    self.write_batch(batch_examples=batch_examples)\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 492, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow/array.pxi\", line 230, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 185, in __arrow_array__\r\n    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\r\n  File \"pyarrow/array.pxi\", line 316, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 39, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Could not convert <PIL.Image.Image image mode=RGB size=32x32 at 0x7F56AEE61DE0> with type Image: did not recognize Python value type when inferring an Arrow data type\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/rafay/Documents/you_only_live_once/upside_down_detector/create_dataset.py\", line 55, in <module>\r\n    my_test = my_test.map(generate_flipped_data)\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 1953, in map\r\n    return self._map_single(\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 519, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 486, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/fingerprint.py\", line 458, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2360, in _map_single\r\n    writer.finalize()\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 522, in finalize\r\n    self.write_examples_on_file()\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 399, in write_examples_on_file\r\n    self.write_batch(batch_examples=batch_examples)\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 492, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow/array.pxi\", line 230, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/home/rafay/anaconda3/envs/pytorch_new/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 185, in __arrow_array__\r\n    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\r\n  File \"pyarrow/array.pxi\", line 316, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 39, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Could not convert <PIL.Image.Image image mode=RGB size=32x32 at 0x7F56AEE61DE0> with type Image: did not recognize Python value type when inferring an Arrow data type\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux(Fedora 35)\r\n- Python version: 3.10\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4124/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4122", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4122/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4122/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4122/events", "html_url": "https://github.com/huggingface/datasets/issues/4122", "id": 1196095072, "node_id": "I_kwDODunzps5HSvZg", "number": 4122, "title": "medical_dialog zh has very slow _generate_examples", "user": {"login": "nbroad1881", "id": 24982805, "node_id": "MDQ6VXNlcjI0OTgyODA1", "avatar_url": "https://avatars.githubusercontent.com/u/24982805?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nbroad1881", "html_url": "https://github.com/nbroad1881", "followers_url": "https://api.github.com/users/nbroad1881/followers", "following_url": "https://api.github.com/users/nbroad1881/following{/other_user}", "gists_url": "https://api.github.com/users/nbroad1881/gists{/gist_id}", "starred_url": "https://api.github.com/users/nbroad1881/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nbroad1881/subscriptions", "organizations_url": "https://api.github.com/users/nbroad1881/orgs", "repos_url": "https://api.github.com/users/nbroad1881/repos", "events_url": "https://api.github.com/users/nbroad1881/events{/privacy}", "received_events_url": "https://api.github.com/users/nbroad1881/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-04-07T14:00:51Z", "updated_at": "2022-04-08T16:20:51Z", "closed_at": "2022-04-08T16:20:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nAfter downloading the files from Google Drive, `load_dataset(\"medical_dialog\", \"zh\", data_dir=\"./\")` takes an unreasonable amount of time. Generating the train/test split for 33% of the dataset takes over 4.5 hours.\r\n\r\n## Steps to reproduce the bug\r\nThe easiest way I've found to download files from Google Drive is to use `gdown` and use Google Colab because the download speeds will be very high due to the fact that they are both in Google Cloud.\r\n\r\n```python\r\nfile_ids = [\r\n        \"1AnKxGEuzjeQsDHHqL3NqI_aplq2hVL_E\",\r\n        \"1tt7weAT1SZknzRFyLXOT2fizceUUVRXX\",\r\n        \"1A64VBbsQ_z8wZ2LDox586JIyyO6mIwWc\",\r\n        \"1AKntx-ECnrxjB07B6BlVZcFRS4YPTB-J\",\r\n        \"1xUk8AAua_x27bHUr-vNoAuhEAjTxOvsu\",\r\n        \"1ezKTfe7BgqVN5o-8Vdtr9iAF0IueCSjP\",\r\n        \"1tA7bSOxR1RRNqZst8cShzhuNHnayUf7c\",\r\n        \"1pA3bCFA5nZDhsQutqsJcH3d712giFb0S\",\r\n        \"1pTLFMdN1A3ro-KYghk4w4sMz6aGaMOdU\",\r\n        \"1dUSnG0nUPq9TEQyHd6ZWvaxO0OpxVjXD\",\r\n        \"1UfCH05nuWiIPbDZxQzHHGAHyMh8dmPQH\",\r\n]\r\nfor i in file_ids:\r\n    url = f\"https://drive.google.com/uc?id={i}\"\r\n    !gdown $url\r\n\r\n\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"medical_dialog\", \"zh\", data_dir=\"./\")\r\n```\r\n\r\n## Expected results\r\nFaster load time\r\n\r\n## Actual results\r\n`Generating train split: 33%: 625519/1921127 [4:31:03<31:39:20, 11.37 examples/s]`\r\n\r\n## Environment info\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5\r\n\r\n@vrindaprabhu , could you take a look at this since you implemented it? I think the `_generate_examples` function might need to be rewritten", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4122/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4121", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4121/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4121/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4121/events", "html_url": "https://github.com/huggingface/datasets/issues/4121", "id": 1196000018, "node_id": "I_kwDODunzps5HSYMS", "number": 4121, "title": "datasets.load_metric can not load a local metirc", "user": {"login": "Gare-Ng", "id": 51749469, "node_id": "MDQ6VXNlcjUxNzQ5NDY5", "avatar_url": "https://avatars.githubusercontent.com/u/51749469?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gare-Ng", "html_url": "https://github.com/Gare-Ng", "followers_url": "https://api.github.com/users/Gare-Ng/followers", "following_url": "https://api.github.com/users/Gare-Ng/following{/other_user}", "gists_url": "https://api.github.com/users/Gare-Ng/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gare-Ng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gare-Ng/subscriptions", "organizations_url": "https://api.github.com/users/Gare-Ng/orgs", "repos_url": "https://api.github.com/users/Gare-Ng/repos", "events_url": "https://api.github.com/users/Gare-Ng/events{/privacy}", "received_events_url": "https://api.github.com/users/Gare-Ng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-04-07T12:48:56Z", "updated_at": "2023-01-18T14:30:46Z", "closed_at": "2022-04-07T13:53:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nNo matter how I hard try to tell load_metric that I want to load a local metric file, it still continues to fetch things on the Internet. And unfortunately it says 'ConnectionError: Couldn't reach'. However I can download this file without connectionerror and tell load_metric its local directory. And it comes back where it begins...\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nmetric = load_metric(path=r'C:\\Users\\Gare\\PycharmProjects\\Gare\\blue\\bleu.py')\r\n    ConnectionError: Couldn't reach https://github.com/tensorflow/nmt/raw/master/nmt/scripts/bleu.py\r\n\r\nmetric = load_metric(path='bleu')\r\n    ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/bleu/bleu.py\r\n\r\nmetric = load_metric(path='./blue/bleu.py')\r\n    ConnectionError: Couldn't reach https://github.com/tensorflow/nmt/raw/master/nmt/scripts/bleu.py\r\n```\r\n\r\n## Expected results\r\nI do read the docs [here](https://huggingface.co/docs/datasets/package_reference/loading_methods#datasets.load_metric). There are no other parameters that help function to distinguish from local and online file but path. As what I code above, it should load from local.\r\n\r\n## Actual results\r\n\r\n> metric = load_metric(path=r'C:\\Users\\Gare\\PycharmProjects\\Gare\\blue\\bleu.py')\r\n\r\n> ~\\AppData\\Local\\Temp\\ipykernel_19636\\1855752034.py in <module>\r\n----> 1 metric = load_metric(path=r'C:\\Users\\Gare\\PycharmProjects\\Gare\\blue\\bleu.py')\r\nD:\\Program Files\\Anaconda\\envs\\Gare\\lib\\site-packages\\datasets\\load.py in load_metric(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, script_version, **metric_init_kwargs)\r\n    817         if data_files is None and data_dir is not None:\r\n    818             data_files = os.path.join(data_dir, \"**\")\r\n--> 819 \r\n    820         self.name = name\r\n    821         self.revision = revision\r\nD:\\Program Files\\Anaconda\\envs\\Gare\\lib\\site-packages\\datasets\\load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, return_associated_base_path, data_files, **download_kwargs)\r\n    639         self,\r\n    640         path: str,\r\n--> 641         download_config: Optional[DownloadConfig] = None,\r\n    642         download_mode: Optional[DownloadMode] = None,\r\n    643         dynamic_modules_path: Optional[str] = None,\r\nD:\\Program Files\\Anaconda\\envs\\Gare\\lib\\site-packages\\datasets\\utils\\file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    297             token = hf_api.HfFolder.get_token()\r\n    298         if token:\r\n--> 299             headers[\"authorization\"] = f\"Bearer {token}\"\r\n    300     return headers\r\n    301 \r\nD:\\Program Files\\Anaconda\\envs\\Gare\\lib\\site-packages\\datasets\\utils\\file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\r\n    604             def _resumable_file_manager():\r\n    605                 with open(incomplete_path, \"a+b\") as f:\r\n--> 606                     yield f\r\n    607 \r\n    608             temp_file_manager = _resumable_file_manager\r\nConnectionError: Couldn't reach https://github.com/tensorflow/nmt/raw/master/nmt/scripts/bleu.py\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Windows-10-10.0.22000-SP0\r\n- Python version: 3.7.13\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.3.4\r\n\r\nAny advice would be appreciated.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4121/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4118", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4118/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4118/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4118/events", "html_url": "https://github.com/huggingface/datasets/issues/4118", "id": 1195638944, "node_id": "I_kwDODunzps5HRACg", "number": 4118, "title": "Failing CI tests on Windows", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-04-07T07:36:25Z", "updated_at": "2022-04-07T07:57:13Z", "closed_at": "2022-04-07T07:57:13Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nOur CI Windows tests are failing from yesterday: https://app.circleci.com/pipelines/github/huggingface/datasets/11092/workflows/9cfdb1dd-0fec-4fe0-8122-5f533192ebdc/jobs/67414\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4118/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4117", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4117/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4117/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4117/events", "html_url": "https://github.com/huggingface/datasets/issues/4117", "id": 1195552406, "node_id": "I_kwDODunzps5HQq6W", "number": 4117, "title": "AttributeError: module 'huggingface_hub' has no attribute 'hf_api'", "user": {"login": "arymbe", "id": 4567991, "node_id": "MDQ6VXNlcjQ1Njc5OTE=", "avatar_url": "https://avatars.githubusercontent.com/u/4567991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arymbe", "html_url": "https://github.com/arymbe", "followers_url": "https://api.github.com/users/arymbe/followers", "following_url": "https://api.github.com/users/arymbe/following{/other_user}", "gists_url": "https://api.github.com/users/arymbe/gists{/gist_id}", "starred_url": "https://api.github.com/users/arymbe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arymbe/subscriptions", "organizations_url": "https://api.github.com/users/arymbe/orgs", "repos_url": "https://api.github.com/users/arymbe/repos", "events_url": "https://api.github.com/users/arymbe/events{/privacy}", "received_events_url": "https://api.github.com/users/arymbe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2022-04-07T05:52:36Z", "updated_at": "2022-07-28T16:44:04Z", "closed_at": "2022-04-19T15:36:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nCould you help me please. I got this following error.\r\n\r\nAttributeError: module 'huggingface_hub' has no attribute 'hf_api'\r\n\r\n## Steps to reproduce the bug\r\nwhen I imported the datasets\r\n\r\n# Sample code to reproduce the bug\r\nfrom datasets import list_datasets, load_dataset, list_metrics, load_metric\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: macOS-12.3-x86_64-i386-64bit\r\n- Python version: 3.8.9\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.3.5\r\n- Huggingface-hub: 0.5.0\r\n- Transformers: 4.18.0\r\n\r\nThank you in advance.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4117/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4117/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4113", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4113/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4113/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4113/events", "html_url": "https://github.com/huggingface/datasets/issues/4113", "id": 1194843532, "node_id": "I_kwDODunzps5HN92M", "number": 4113, "title": "Multiprocessing with FileLock fails in python 3.9", "user": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-04-06T16:27:09Z", "updated_at": "2022-11-28T11:49:14Z", "closed_at": "2022-11-28T11:49:14Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "On python 3.9, this code hangs:\r\n```python\r\nfrom multiprocessing import Pool\r\nfrom filelock import FileLock\r\n\r\n\r\ndef run(i):\r\n    print(f\"got the lock in multi process [{i}]\")\r\n\r\n\r\nwith FileLock(\"tmp.lock\"):\r\n    with Pool(2) as pool:\r\n        pool.map(run, range(2))\r\n\r\n```\r\n\r\nThis is because the subprocesses try to acquire the lock from the main process for some reason. This is not the case in older versions of python.\r\n\r\nThis can cause many issues in python 3.9. In particular, we use multiprocessing to fetch data files when you load a dataset (as long as there are >16 data files). Therefore `imagefolder` hangs, and I expect any dataset that needs to download >16 files to hang as well.\r\n\r\nLet's see if we can fix this and have a CI that runs on 3.9.\r\n\r\ncc @mariosasko @julien-c ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4113/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4113/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4107", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4107/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4107/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4107/events", "html_url": "https://github.com/huggingface/datasets/issues/4107", "id": 1194484885, "node_id": "I_kwDODunzps5HMmSV", "number": 4107, "title": "Unable to view the dataset and loading the same dataset throws the error - ArrowInvalid: Exceeded maximum rows", "user": {"login": "Pavithree", "id": 23344465, "node_id": "MDQ6VXNlcjIzMzQ0NDY1", "avatar_url": "https://avatars.githubusercontent.com/u/23344465?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Pavithree", "html_url": "https://github.com/Pavithree", "followers_url": "https://api.github.com/users/Pavithree/followers", "following_url": "https://api.github.com/users/Pavithree/following{/other_user}", "gists_url": "https://api.github.com/users/Pavithree/gists{/gist_id}", "starred_url": "https://api.github.com/users/Pavithree/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Pavithree/subscriptions", "organizations_url": "https://api.github.com/users/Pavithree/orgs", "repos_url": "https://api.github.com/users/Pavithree/repos", "events_url": "https://api.github.com/users/Pavithree/events{/privacy}", "received_events_url": "https://api.github.com/users/Pavithree/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-04-06T11:37:15Z", "updated_at": "2022-04-08T07:13:07Z", "closed_at": "2022-04-06T14:39:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Dataset viewer issue - -ArrowInvalid: Exceeded maximum rows\r\n\r\n**Link:** *https://huggingface.co/datasets/Pavithree/explainLikeImFive*\r\n\r\n*This is the subset of original eli5 dataset  https://huggingface.co/datasets/vblagoje/lfqa. I just filtered the data samples which belongs to one particular subreddit thread. However, the dataset preview for train split  returns  the below mentioned error:\r\nStatus code:   400\r\nException:     ArrowInvalid\r\nMessage:       Exceeded maximum rows\r\nWhen I try to load the same dataset it returns ArrowInvalid: Exceeded maximum rows error*\r\n\r\nAm I the one who added this dataset ? Yes \r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4107/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4105", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4105/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4105/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4105/events", "html_url": "https://github.com/huggingface/datasets/issues/4105", "id": 1194297119, "node_id": "I_kwDODunzps5HL4cf", "number": 4105, "title": "push to hub fails with huggingface-hub 0.5.0", "user": {"login": "frascuchon", "id": 2518789, "node_id": "MDQ6VXNlcjI1MTg3ODk=", "avatar_url": "https://avatars.githubusercontent.com/u/2518789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frascuchon", "html_url": "https://github.com/frascuchon", "followers_url": "https://api.github.com/users/frascuchon/followers", "following_url": "https://api.github.com/users/frascuchon/following{/other_user}", "gists_url": "https://api.github.com/users/frascuchon/gists{/gist_id}", "starred_url": "https://api.github.com/users/frascuchon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frascuchon/subscriptions", "organizations_url": "https://api.github.com/users/frascuchon/orgs", "repos_url": "https://api.github.com/users/frascuchon/repos", "events_url": "https://api.github.com/users/frascuchon/events{/privacy}", "received_events_url": "https://api.github.com/users/frascuchon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-04-06T08:59:57Z", "updated_at": "2022-04-13T14:30:47Z", "closed_at": "2022-04-13T14:30:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n`ds.push_to_hub` is failing when updating a dataset in the form \"org_id/repo_id\"\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"rubrix/news_test\")\r\nds.push_to_hub(\"<your-user>/news_test\", token=\"<your-token>\")\r\n```\r\n\r\n## Expected results\r\nThe dataset is successfully uploaded\r\n\r\n## Actual results\r\nAn error validation is raised:\r\n\r\n```bash\r\nif repo_id and (name or organization):\r\n>           raise ValueError(\r\n                \"Only pass `repo_id` and leave deprecated `name` and \"\r\n                \"`organization` to be None.\"\r\nE               ValueError: Only pass `repo_id` and leave deprecated `name` and `organization` to be None.\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.1\r\n- `huggingface-hub`: 0.5\r\n- Platform: macOS\r\n- Python version: 3.8.12\r\n- PyArrow version: 6.0.0\r\n\r\ncc @adrinjalali \r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4105/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4105/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4099", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4099/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4099/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4099/events", "html_url": "https://github.com/huggingface/datasets/issues/4099", "id": 1193253768, "node_id": "I_kwDODunzps5HH5uI", "number": 4099, "title": "UnicodeDecodeError: 'ascii' codec can't decode byte 0xe5 in position 213: ordinal not in range(128)", "user": {"login": "andreybond", "id": 20210017, "node_id": "MDQ6VXNlcjIwMjEwMDE3", "avatar_url": "https://avatars.githubusercontent.com/u/20210017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andreybond", "html_url": "https://github.com/andreybond", "followers_url": "https://api.github.com/users/andreybond/followers", "following_url": "https://api.github.com/users/andreybond/following{/other_user}", "gists_url": "https://api.github.com/users/andreybond/gists{/gist_id}", "starred_url": "https://api.github.com/users/andreybond/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andreybond/subscriptions", "organizations_url": "https://api.github.com/users/andreybond/orgs", "repos_url": "https://api.github.com/users/andreybond/repos", "events_url": "https://api.github.com/users/andreybond/events{/privacy}", "received_events_url": "https://api.github.com/users/andreybond/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-04-05T14:42:38Z", "updated_at": "2022-04-06T06:37:44Z", "closed_at": "2022-04-06T06:35:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nError \"UnicodeDecodeError: 'ascii' codec can't decode byte 0xe5 in position 213: ordinal not in range(128)\" is thrown when downloading dataset.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset \r\ndatasets = load_dataset(\"nielsr/XFUN\", \"xfun.ja\")\r\n```\r\n\r\n## Expected results\r\nDataset should be downloaded without exceptions\r\n\r\n## Actual results\r\nStack trace (for the second-time execution):\r\nDownloading and preparing dataset xfun/xfun.ja to /root/.cache/huggingface/datasets/nielsr___xfun/xfun.ja/0.0.0/e06e948b673d1be9a390a83c05c10e49438bf03dd85ae9a4fe06f8747a724477...\r\nDownloading data files: 100%\r\n2/2 [00:00<00:00, 88.48it/s]\r\nExtracting data files: 100%\r\n2/2 [00:00<00:00, 79.60it/s]\r\n\r\nUnicodeDecodeErrorTraceback (most recent call last)\r\n<ipython-input-31-79c26bd1109c> in <module>\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 datasets = load_dataset(\"nielsr/XFUN\", \"xfun.ja\")\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    604         )\r\n    605 \r\n--> 606         # By default, return all splits\r\n    607         if split is None:\r\n    608             split = {s: s for s in self.info.splits}\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos)\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    692         Args:\r\n    693             split: `datasets.Split` which subset of the data to read.\r\n--> 694 \r\n    695         Returns:\r\n    696             `Dataset`\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/builder.py in _prepare_split(self, split_generator, check_duplicate_keys)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tqdm/notebook.py in __iter__(self)\r\n    252         if not self.disable:\r\n    253             self.display(check_delay=False)\r\n--> 254 \r\n    255     def __iter__(self):\r\n    256         try:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tqdm/std.py in __iter__(self)\r\n   1183             for obj in iterable:\r\n   1184                 yield obj\r\n-> 1185             return\r\n   1186 \r\n   1187         mininterval = self.mininterval\r\n\r\n~/.cache/huggingface/modules/datasets_modules/datasets/nielsr--XFUN/e06e948b673d1be9a390a83c05c10e49438bf03dd85ae9a4fe06f8747a724477/XFUN.py in _generate_examples(self, filepaths)\r\n    140             logger.info(\"Generating examples from = %s\", filepath)\r\n    141             with open(filepath[0], \"r\") as f:\r\n--> 142                 data = json.load(f)\r\n    143 \r\n    144             for doc in data[\"documents\"]:\r\n\r\n/usr/lib/python3.6/json/__init__.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\r\n    294 \r\n    295     \"\"\"\r\n--> 296     return loads(fp.read(),\r\n    297         cls=cls, object_hook=object_hook,\r\n    298         parse_float=parse_float, parse_int=parse_int,\r\n\r\n/usr/lib/python3.6/encodings/ascii.py in decode(self, input, final)\r\n     24 class IncrementalDecoder(codecs.IncrementalDecoder):\r\n     25     def decode(self, input, final=False):\r\n---> 26         return codecs.ascii_decode(input, self.errors)[0]\r\n     27 \r\n     28 class StreamWriter(Codec,codecs.StreamWriter):\r\n\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe5 in position 213: ordinal not in range(128)\r\n\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0 (but reproduced with many previous versions)\r\n- Platform: Docker: Linux da5b74136d6b 5.3.0-1031-azure #32~18.04.1-Ubuntu SMP Mon Jun 22 15:27:23 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux ; Base docker image is : huggingface/transformers-pytorch-cpu\r\n- Python version: 3.6.9\r\n- PyArrow version:   6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4099/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4085", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4085/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4085/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4085/events", "html_url": "https://github.com/huggingface/datasets/issues/4085", "id": 1190621345, "node_id": "I_kwDODunzps5G93Ch", "number": 4085, "title": "datasets.set_progress_bar_enabled(False) not working in datasets v2", "user": {"login": "virilo", "id": 3381112, "node_id": "MDQ6VXNlcjMzODExMTI=", "avatar_url": "https://avatars.githubusercontent.com/u/3381112?v=4", "gravatar_id": "", "url": "https://api.github.com/users/virilo", "html_url": "https://github.com/virilo", "followers_url": "https://api.github.com/users/virilo/followers", "following_url": "https://api.github.com/users/virilo/following{/other_user}", "gists_url": "https://api.github.com/users/virilo/gists{/gist_id}", "starred_url": "https://api.github.com/users/virilo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/virilo/subscriptions", "organizations_url": "https://api.github.com/users/virilo/orgs", "repos_url": "https://api.github.com/users/virilo/repos", "events_url": "https://api.github.com/users/virilo/events{/privacy}", "received_events_url": "https://api.github.com/users/virilo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-04-02T12:40:10Z", "updated_at": "2022-09-17T02:18:03Z", "closed_at": "2022-04-04T06:44:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\ndatasets.set_progress_bar_enabled(False) not working in datasets v2\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndatasets.set_progress_bar_enabled(False)\r\n```\r\n\r\n## Expected results\r\ndatasets not using any progress bar\r\n\r\n## Actual results\r\n\r\nAttributeError: module 'datasets' has no attribute 'set_progress_bar_enabled\r\n\r\n## Environment info\r\n\r\ndatasets version 2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4085/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4084", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4084/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4084/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4084/events", "html_url": "https://github.com/huggingface/datasets/issues/4084", "id": 1190060415, "node_id": "I_kwDODunzps5G7uF_", "number": 4084, "title": "Errors in `Train with Datasets` Tensorflow code section on Huggingface.co", "user": {"login": "blackhat-coder", "id": 57095771, "node_id": "MDQ6VXNlcjU3MDk1Nzcx", "avatar_url": "https://avatars.githubusercontent.com/u/57095771?v=4", "gravatar_id": "", "url": "https://api.github.com/users/blackhat-coder", "html_url": "https://github.com/blackhat-coder", "followers_url": "https://api.github.com/users/blackhat-coder/followers", "following_url": "https://api.github.com/users/blackhat-coder/following{/other_user}", "gists_url": "https://api.github.com/users/blackhat-coder/gists{/gist_id}", "starred_url": "https://api.github.com/users/blackhat-coder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/blackhat-coder/subscriptions", "organizations_url": "https://api.github.com/users/blackhat-coder/orgs", "repos_url": "https://api.github.com/users/blackhat-coder/repos", "events_url": "https://api.github.com/users/blackhat-coder/events{/privacy}", "received_events_url": "https://api.github.com/users/blackhat-coder/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-04-01T17:02:47Z", "updated_at": "2022-04-04T07:24:37Z", "closed_at": "2022-04-04T07:21:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nHi\r\n### Error 1\r\nRunning the Tensforlow code on [Huggingface](https://huggingface.co/docs/datasets/use_dataset) gives a TypeError: __init__() got an unexpected keyword argument 'return_tensors' \r\n### Error 2\r\n`DataCollatorWithPadding` isn't imported\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport tensorflow as tf\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\r\ndataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\r\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\r\ntrain_dataset = dataset[\"train\"].to_tf_dataset(\r\n  columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\r\n  shuffle=True,\r\n  batch_size=16,\r\n  collate_fn=data_collator,\r\n)\r\n```\r\nThis is the same code on Huggingface.co\r\n\r\n## Actual results\r\nTypeError: __init__() got an unexpected keyword argument 'return_tensors'\r\n\r\n## Environment info\r\n- `datasets` version: 2.0.0\r\n- Platform: Windows-10-10.0.19044-SP0\r\n- Python version: 3.9.7\r\n- PyArrow version: 6.0.0\r\n- Pandas version: 1.4.1\r\n> ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4084/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4077", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4077/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4077/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4077/events", "html_url": "https://github.com/huggingface/datasets/issues/4077", "id": 1189467585, "node_id": "I_kwDODunzps5G5dXB", "number": 4077, "title": "ArrowInvalid: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.", "user": {"login": "NielsRogge", "id": 48327001, "node_id": "MDQ6VXNlcjQ4MzI3MDAx", "avatar_url": "https://avatars.githubusercontent.com/u/48327001?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NielsRogge", "html_url": "https://github.com/NielsRogge", "followers_url": "https://api.github.com/users/NielsRogge/followers", "following_url": "https://api.github.com/users/NielsRogge/following{/other_user}", "gists_url": "https://api.github.com/users/NielsRogge/gists{/gist_id}", "starred_url": "https://api.github.com/users/NielsRogge/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NielsRogge/subscriptions", "organizations_url": "https://api.github.com/users/NielsRogge/orgs", "repos_url": "https://api.github.com/users/NielsRogge/repos", "events_url": "https://api.github.com/users/NielsRogge/events{/privacy}", "received_events_url": "https://api.github.com/users/NielsRogge/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-04-01T08:49:13Z", "updated_at": "2022-04-01T16:16:19Z", "closed_at": "2022-04-01T16:16:19Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen uploading a relatively large image dataset of > 1GB, reloading doesn't work for me, even though pushing to the hub went just fine.\r\n\r\nBasically, I do:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"imagefolder\", data_files=\"path_to_my_files\")\r\n\r\ndataset.push_to_hub(\"dataset_name\") # works fine, no errors\r\n\r\nreloaded_dataset = load_dataset(\"dataset_name\")\r\n```\r\n\r\nand it returns:\r\n\r\n```\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\r\n```\r\n\r\nI created a Colab notebook to reproduce my error: https://colab.research.google.com/drive/141LJCcM2XyqprPY83nIQ-Zk3BbxWeahq?usp=sharing\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4077/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4077/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4061", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4061/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4061/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4061/events", "html_url": "https://github.com/huggingface/datasets/issues/4061", "id": 1186317071, "node_id": "I_kwDODunzps5GtcMP", "number": 4061, "title": "Loading cnn_dailymail dataset failed", "user": {"login": "Arij-Aladel", "id": 68355048, "node_id": "MDQ6VXNlcjY4MzU1MDQ4", "avatar_url": "https://avatars.githubusercontent.com/u/68355048?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Arij-Aladel", "html_url": "https://github.com/Arij-Aladel", "followers_url": "https://api.github.com/users/Arij-Aladel/followers", "following_url": "https://api.github.com/users/Arij-Aladel/following{/other_user}", "gists_url": "https://api.github.com/users/Arij-Aladel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Arij-Aladel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Arij-Aladel/subscriptions", "organizations_url": "https://api.github.com/users/Arij-Aladel/orgs", "repos_url": "https://api.github.com/users/Arij-Aladel/repos", "events_url": "https://api.github.com/users/Arij-Aladel/events{/privacy}", "received_events_url": "https://api.github.com/users/Arij-Aladel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892865, "node_id": "MDU6TGFiZWwxOTM1ODkyODY1", "url": "https://api.github.com/repos/huggingface/datasets/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-30T11:29:02Z", "updated_at": "2022-03-30T13:36:14Z", "closed_at": "2022-03-30T13:36:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI wanted to load cnn_dailymail dataset from huggingface datasets on jupyter lab, but I am getting an error ` NotADirectoryError:[Errno20] Not a directory ` while loading it.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('cnn_dailymail', '3.0.0')\r\n```\r\n\r\n## Expected results\r\nload `cnn_dailymail` dataset succesfully\r\n\r\n## Actual results\r\nfailed to load and get error \r\n\r\n> NotADirectoryError: [Errno 20] Not a directory\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` 1.8.0:\r\n- Platform: Ubuntu-20.04\r\n- Python version: 3.9.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4061/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4057", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4057/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4057/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4057/events", "html_url": "https://github.com/huggingface/datasets/issues/4057", "id": 1185442001, "node_id": "I_kwDODunzps5GqGjR", "number": 4057, "title": "`load_dataset` consumes too much memory for audio + tar archives", "user": {"login": "JFCeron", "id": 50839826, "node_id": "MDQ6VXNlcjUwODM5ODI2", "avatar_url": "https://avatars.githubusercontent.com/u/50839826?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JFCeron", "html_url": "https://github.com/JFCeron", "followers_url": "https://api.github.com/users/JFCeron/followers", "following_url": "https://api.github.com/users/JFCeron/following{/other_user}", "gists_url": "https://api.github.com/users/JFCeron/gists{/gist_id}", "starred_url": "https://api.github.com/users/JFCeron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JFCeron/subscriptions", "organizations_url": "https://api.github.com/users/JFCeron/orgs", "repos_url": "https://api.github.com/users/JFCeron/repos", "events_url": "https://api.github.com/users/JFCeron/events{/privacy}", "received_events_url": "https://api.github.com/users/JFCeron/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 18, "created_at": "2022-03-29T21:38:55Z", "updated_at": "2022-08-16T10:22:55Z", "closed_at": "2022-08-16T10:22:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n\r\n## Description\r\n`load_dataset` consumes more and more memory until it's killed, even though it's made with a generator. I'm adding a loading script for a new dataset, made up of ~15s audio coming from a tar file. Tried setting `DEFAULT_WRITER_BATCH_SIZE = 1` as per the discussion in #741 but the problem persists.\r\n\r\n## Steps to reproduce the bug\r\nHere's my implementation of `_generate_examples`:\r\n\r\n```python\r\nclass MyDatasetBuilder(datasets.GeneratorBasedBuilder):\r\n    DEFAULT_WRITER_BATCH_SIZE = 1\r\n    ...\r\n\r\n    def _split_generators(self, dl_manager):\r\n        archive_path = dl_manager.download(_DL_URLS[self.config.name])\r\n            return [\r\n                datasets.SplitGenerator(\r\n                    name=datasets.Split.TRAIN,\r\n                    gen_kwargs={\r\n                        \"audio_tarfile_path\": archive_path[\"audio_tarfile\"]\r\n                    },\r\n                ),\r\n            ]\r\n    \r\n    def _generate_examples(self, audio_tarfile_path):\r\n        key = 0\r\n        with tarfile.open(audio_tarfile_path, mode=\"r|\") as audio_tarfile:\r\n            for audio_tarinfo in audio_tarfile:\r\n                audio_name = audio_tarinfo.name\r\n                audio_file_obj = audio_tarfile.extractfile(audio_tarinfo)\r\n                yield key, {\"audio\": {\"path\": audio_name, \"bytes\": audio_file_obj.read()}}\r\n                key += 1\r\n```\r\nI then try to load via `ds = load_dataset('./datasets/my_new_dataset', writer_batch_size=1)`, and memory usage grows until all 8GB of my machine are taken and process is killed (`Killed`). Also tried an untarred version of this using `os.walk` but the same happened.\r\n\r\nI created a script to confirm that one can safely go through such a generator, which runs just fine with memory <500MB at all times.\r\n\r\n```python\r\nimport tarfile\r\n\r\ndef generate_examples():\r\n    audio_tarfile = tarfile.open(\"audios.tar\", mode=\"r|\")\r\n    key = 0\r\n    for audio_tarinfo in audio_tarfile:\r\n        audio_name = audio_tarinfo.name\r\n        audio_file_obj = audio_tarfile.extractfile(audio_tarinfo)\r\n        yield key, {\"audio\": {\"path\": audio_name, \"bytes\": audio_file_obj.read()}}\r\n        key += 1\r\n\r\nif __name__ == \"__main__\":\r\n    examples = generate_examples()\r\n    for example in examples:\r\n        pass\r\n```\r\n## Expected results\r\nMemory consumption should be similar to the non-huggingface script.\r\n\r\n## Actual results\r\nProcess is killed after consuming too much memory.\r\n\r\n## Environment info\r\n- `datasets` version: 2.0.1.dev0\r\n- Platform: Linux-4.19.0-20-cloud-amd64-x86_64-with-debian-10.12\r\n- Python version: 3.7.12\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.3.5", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4057/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4057/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4048", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4048/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4048/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4048/events", "html_url": "https://github.com/huggingface/datasets/issues/4048", "id": 1183804576, "node_id": "I_kwDODunzps5Gj2yg", "number": 4048, "title": "Split size error on `amazon_us_reviews` / `PC_v1_00` dataset", "user": {"login": "trentonstrong", "id": 191985, "node_id": "MDQ6VXNlcjE5MTk4NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/191985?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trentonstrong", "html_url": "https://github.com/trentonstrong", "followers_url": "https://api.github.com/users/trentonstrong/followers", "following_url": "https://api.github.com/users/trentonstrong/following{/other_user}", "gists_url": "https://api.github.com/users/trentonstrong/gists{/gist_id}", "starred_url": "https://api.github.com/users/trentonstrong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trentonstrong/subscriptions", "organizations_url": "https://api.github.com/users/trentonstrong/orgs", "repos_url": "https://api.github.com/users/trentonstrong/repos", "events_url": "https://api.github.com/users/trentonstrong/events{/privacy}", "received_events_url": "https://api.github.com/users/trentonstrong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892877, "node_id": "MDU6TGFiZWwxOTM1ODkyODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": {"login": "trentonstrong", "id": 191985, "node_id": "MDQ6VXNlcjE5MTk4NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/191985?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trentonstrong", "html_url": "https://github.com/trentonstrong", "followers_url": "https://api.github.com/users/trentonstrong/followers", "following_url": "https://api.github.com/users/trentonstrong/following{/other_user}", "gists_url": "https://api.github.com/users/trentonstrong/gists{/gist_id}", "starred_url": "https://api.github.com/users/trentonstrong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trentonstrong/subscriptions", "organizations_url": "https://api.github.com/users/trentonstrong/orgs", "repos_url": "https://api.github.com/users/trentonstrong/repos", "events_url": "https://api.github.com/users/trentonstrong/events{/privacy}", "received_events_url": "https://api.github.com/users/trentonstrong/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "trentonstrong", "id": 191985, "node_id": "MDQ6VXNlcjE5MTk4NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/191985?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trentonstrong", "html_url": "https://github.com/trentonstrong", "followers_url": "https://api.github.com/users/trentonstrong/followers", "following_url": "https://api.github.com/users/trentonstrong/following{/other_user}", "gists_url": "https://api.github.com/users/trentonstrong/gists{/gist_id}", "starred_url": "https://api.github.com/users/trentonstrong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trentonstrong/subscriptions", "organizations_url": "https://api.github.com/users/trentonstrong/orgs", "repos_url": "https://api.github.com/users/trentonstrong/repos", "events_url": "https://api.github.com/users/trentonstrong/events{/privacy}", "received_events_url": "https://api.github.com/users/trentonstrong/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-03-28T18:12:04Z", "updated_at": "2022-04-08T12:29:30Z", "closed_at": "2022-04-08T12:29:30Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen downloading this subset as of 3-28-2022 you will encounter a split size error after the dataset is extracted. The extracted dataset has roughly ~6m rows while the split expects <1m. \r\n\r\nUpon digging a little deeper, I downloaded the raw files from `https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_PC_v1_00.tsv.gz` and extracted them. A line count via `wc -l` confirms the ~6m number that we see and the data looks valid at a glance (I did not check for duplicate rows). My guess is this file has either been updated in place or there is a bug in the dataset metadata.\r\n\r\nHappy to submit a PR and fix this up if turns out to be a metadata issue but wanted to get some other :eyes: on it first.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset('amazon_us_reviews', 'PC_v1_00')\r\n```\r\n\r\n## Expected results\r\nDataset is downloaded and extracted successfully.\r\n\r\n## Actual results\r\nAn split size exception is thrown.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4048/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4047", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4047/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4047/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4047/events", "html_url": "https://github.com/huggingface/datasets/issues/4047", "id": 1183789237, "node_id": "I_kwDODunzps5GjzC1", "number": 4047, "title": "Dataset.unique(column: str) -> ArrowNotImplementedError", "user": {"login": "orkenstein", "id": 1461936, "node_id": "MDQ6VXNlcjE0NjE5MzY=", "avatar_url": "https://avatars.githubusercontent.com/u/1461936?v=4", "gravatar_id": "", "url": "https://api.github.com/users/orkenstein", "html_url": "https://github.com/orkenstein", "followers_url": "https://api.github.com/users/orkenstein/followers", "following_url": "https://api.github.com/users/orkenstein/following{/other_user}", "gists_url": "https://api.github.com/users/orkenstein/gists{/gist_id}", "starred_url": "https://api.github.com/users/orkenstein/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/orkenstein/subscriptions", "organizations_url": "https://api.github.com/users/orkenstein/orgs", "repos_url": "https://api.github.com/users/orkenstein/repos", "events_url": "https://api.github.com/users/orkenstein/events{/privacy}", "received_events_url": "https://api.github.com/users/orkenstein/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-03-28T17:59:32Z", "updated_at": "2022-04-01T18:24:57Z", "closed_at": "2022-04-01T18:24:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI'm trying to use `unique()` function, but it fails\r\n## Steps to reproduce the bug\r\n1. Get dataset\r\n2. Call `unique`\r\n3. Error\r\n\r\n# Sample code to reproduce the bug\r\n```python\r\n!pip show datasets\r\n\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('wikiann', 'en')\r\n\r\ndataset['train'].column_names\r\ndataset['train'].unique(dataset['train'].column_names[0])\r\n```\r\n\r\n## Expected results\r\nIt would be nice to actually see unique items\r\n\r\n## Actual results\r\nError:\r\n```python\r\n---------------------------------------------------------------------------\r\nArrowNotImplementedError                  Traceback (most recent call last)\r\n[<ipython-input-10-5e0de07ed42c>](https://s0qyv2vjaji-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab-20220324-060046-RC00_436956229#) in <module>()\r\n      6 \r\n      7 dataset['train'].column_names\r\n----> 8 dataset['train'].unique(dataset['train'].column_names[0])\r\n\r\n5 frames\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowNotImplementedError: Function unique has no kernel matching input types (array[list<item: string>])\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Google Collab\r\n- Python version: 3.7.13\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4047/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4044", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4044/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4044/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4044/events", "html_url": "https://github.com/huggingface/datasets/issues/4044", "id": 1183658942, "node_id": "I_kwDODunzps5GjTO-", "number": 4044, "title": "CLI dummy data generation is broken", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-03-28T16:07:37Z", "updated_at": "2022-03-31T14:59:06Z", "closed_at": "2022-03-31T14:59:06Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nWe get a TypeError when running CLI dummy data generation:\r\n```shell\r\ndatasets-cli dummy_data datasets/<your-dataset-folder> --auto_generate\r\n```\r\ngives:\r\n```\r\n  File \".../huggingface/datasets/src/datasets/commands/dummy_data.py\", line 361, in _autogenerate_dummy_data\r\n    dataset_builder._prepare_split(split_generator)\r\nTypeError: _prepare_split() missing 1 required positional argument: 'check_duplicate_keys'\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4044/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4044/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4037", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4037/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4037/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4037/events", "html_url": "https://github.com/huggingface/datasets/issues/4037", "id": 1183144486, "node_id": "I_kwDODunzps5GhVom", "number": 4037, "title": "Error while building documentation", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-03-28T09:22:44Z", "updated_at": "2022-03-28T10:01:52Z", "closed_at": "2022-03-28T10:00:48Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nDocumentation building is failing:\r\n- https://github.com/huggingface/datasets/runs/5716300989?check_suite_focus=true\r\n\r\n```\r\nValueError: There was an error when converting ../datasets/docs/source/package_reference/main_classes.mdx to the MDX format.\r\nUnable to find datasets.filesystems.S3FileSystem in datasets. Make sure the path to that object is correct.\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4037/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4032", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4032/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4032/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4032/events", "html_url": "https://github.com/huggingface/datasets/issues/4032", "id": 1182595697, "node_id": "I_kwDODunzps5GfPpx", "number": 4032, "title": "can't download cats_vs_dogs dataset", "user": {"login": "RRaphaell", "id": 74569835, "node_id": "MDQ6VXNlcjc0NTY5ODM1", "avatar_url": "https://avatars.githubusercontent.com/u/74569835?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RRaphaell", "html_url": "https://github.com/RRaphaell", "followers_url": "https://api.github.com/users/RRaphaell/followers", "following_url": "https://api.github.com/users/RRaphaell/following{/other_user}", "gists_url": "https://api.github.com/users/RRaphaell/gists{/gist_id}", "starred_url": "https://api.github.com/users/RRaphaell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RRaphaell/subscriptions", "organizations_url": "https://api.github.com/users/RRaphaell/orgs", "repos_url": "https://api.github.com/users/RRaphaell/repos", "events_url": "https://api.github.com/users/RRaphaell/events{/privacy}", "received_events_url": "https://api.github.com/users/RRaphaell/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-27T17:05:39Z", "updated_at": "2022-03-28T07:44:24Z", "closed_at": "2022-03-28T07:44:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\ncan't download cats_vs_dogs dataset. error: Checksums didn't match for dataset source files\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"cats_vs_dogs\")\r\n```\r\n\r\n## Expected results\r\nloaded successfully.\r\n\r\n## Actual results\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip']\r\n\r\n## Environment info\r\nfresh google colab notebook\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4032/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4032/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4031", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4031/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4031/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4031/events", "html_url": "https://github.com/huggingface/datasets/issues/4031", "id": 1182415124, "node_id": "I_kwDODunzps5GejkU", "number": 4031, "title": "Cannot load the dataset conll2012_ontonotesv5", "user": {"login": "cathyxl", "id": 8326473, "node_id": "MDQ6VXNlcjgzMjY0NzM=", "avatar_url": "https://avatars.githubusercontent.com/u/8326473?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cathyxl", "html_url": "https://github.com/cathyxl", "followers_url": "https://api.github.com/users/cathyxl/followers", "following_url": "https://api.github.com/users/cathyxl/following{/other_user}", "gists_url": "https://api.github.com/users/cathyxl/gists{/gist_id}", "starred_url": "https://api.github.com/users/cathyxl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cathyxl/subscriptions", "organizations_url": "https://api.github.com/users/cathyxl/orgs", "repos_url": "https://api.github.com/users/cathyxl/repos", "events_url": "https://api.github.com/users/cathyxl/events{/privacy}", "received_events_url": "https://api.github.com/users/cathyxl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-27T07:38:23Z", "updated_at": "2022-03-28T06:58:31Z", "closed_at": "2022-03-28T06:31:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nCannot load the dataset conll2012_ontonotesv5\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('conll2012_ontonotesv5', 'english_v4', split=\"test\")\r\nprint(dataset)\r\n\r\n```\r\n\r\n## Expected results\r\nThe datasets should be downloaded successfully\r\n\r\n## Actual results\r\nraise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/zmycy7t9h9-1.zip']\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-5.4.0-88-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 7.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4031/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4031/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4027", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4027/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4027/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4027/events", "html_url": "https://github.com/huggingface/datasets/issues/4027", "id": 1180991344, "node_id": "I_kwDODunzps5GZH9w", "number": 4027, "title": "ElasticSearch Indexing example: TypeError: __init__() missing 1 required positional argument: 'scheme'", "user": {"login": "MoritzLaurer", "id": 41862082, "node_id": "MDQ6VXNlcjQxODYyMDgy", "avatar_url": "https://avatars.githubusercontent.com/u/41862082?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MoritzLaurer", "html_url": "https://github.com/MoritzLaurer", "followers_url": "https://api.github.com/users/MoritzLaurer/followers", "following_url": "https://api.github.com/users/MoritzLaurer/following{/other_user}", "gists_url": "https://api.github.com/users/MoritzLaurer/gists{/gist_id}", "starred_url": "https://api.github.com/users/MoritzLaurer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MoritzLaurer/subscriptions", "organizations_url": "https://api.github.com/users/MoritzLaurer/orgs", "repos_url": "https://api.github.com/users/MoritzLaurer/repos", "events_url": "https://api.github.com/users/MoritzLaurer/events{/privacy}", "received_events_url": "https://api.github.com/users/MoritzLaurer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892865, "node_id": "MDU6TGFiZWwxOTM1ODkyODY1", "url": "https://api.github.com/repos/huggingface/datasets/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-03-25T16:22:28Z", "updated_at": "2022-04-07T10:29:52Z", "closed_at": "2022-03-28T07:58:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am following the example in the documentation for elastic search step by step (on google colab): https://huggingface.co/docs/datasets/faiss_es#elasticsearch\r\n\r\n```\r\nfrom datasets import load_dataset\r\nsquad = load_dataset('crime_and_punish', split='train[:1000]')\r\n```\r\n\r\nWhen I run the line: \r\n`squad.add_elasticsearch_index(\"context\", host=\"localhost\", port=\"9200\")`\r\n\r\nI get the error: \r\n`TypeError: __init__() missing 1 required positional argument: 'scheme'`\r\n\r\n\r\n## Expected results\r\nNo error message\r\n\r\n## Actual results\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n\r\n[<ipython-input-23-9205593edef3>](https://localhost:8080/#) in <module>()\r\n      1 import elasticsearch\r\n----> 2 squad.add_elasticsearch_index(\"text\", host=\"localhost\", port=\"9200\")\r\n\r\n\r\n6 frames\r\n\r\n[/usr/local/lib/python3.7/dist-packages/elasticsearch/_sync/client/utils.py](https://localhost:8080/#) in host_mapping_to_node_config(host)\r\n    209         options[\"path_prefix\"] = options.pop(\"url_prefix\")\r\n    210 \r\n--> 211     return NodeConfig(**options)  # type: ignore\r\n    212 \r\n    213 \r\n\r\nTypeError: __init__() missing 1 required positional argument: 'scheme'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.2.0\r\n- Platform: Linux, Google Colab\r\n- Python version: Google Colab (probably 3.7)\r\n- PyArrow version: ?\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4027/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4027/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4015", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4015/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4015/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4015/events", "html_url": "https://github.com/huggingface/datasets/issues/4015", "id": 1180510856, "node_id": "I_kwDODunzps5GXSqI", "number": 4015, "title": "Can not correctly parse the classes with imagefolder", "user": {"login": "YiSyuanChen", "id": 21264909, "node_id": "MDQ6VXNlcjIxMjY0OTA5", "avatar_url": "https://avatars.githubusercontent.com/u/21264909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YiSyuanChen", "html_url": "https://github.com/YiSyuanChen", "followers_url": "https://api.github.com/users/YiSyuanChen/followers", "following_url": "https://api.github.com/users/YiSyuanChen/following{/other_user}", "gists_url": "https://api.github.com/users/YiSyuanChen/gists{/gist_id}", "starred_url": "https://api.github.com/users/YiSyuanChen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YiSyuanChen/subscriptions", "organizations_url": "https://api.github.com/users/YiSyuanChen/orgs", "repos_url": "https://api.github.com/users/YiSyuanChen/repos", "events_url": "https://api.github.com/users/YiSyuanChen/events{/privacy}", "received_events_url": "https://api.github.com/users/YiSyuanChen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-03-25T08:51:17Z", "updated_at": "2022-03-28T01:02:03Z", "closed_at": "2022-03-25T09:27:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI try to load my own image dataset with imagefolder, but the parsing of classes is incorrect.\r\n\r\n## Steps to reproduce the bug\r\nI organized my dataset (ImageNet) in the following structure:\r\n```\r\n  - imagenet/\r\n      - train/\r\n          - n01440764/\r\n              - ILSVRC2012_val_00000293.jpg\r\n              - ...... \r\n          - n01695060/\r\n          - ......\r\n      - val/ \r\n          - n01440764/\r\n          - n01695060/\r\n          - ......\r\n```\r\n\r\nAt first, I followed the instructions from the Huggingface [example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification#using-your-own-data) to load my data as:\r\n```\r\nfrom datasets import load_dataset\r\ndata_files = {'train': 'imagenet/train', 'val': 'imagenet/val'}\r\nds = load_dataset(\"nateraw/image-folder\", data_files=data_files, task=\"image-classification\")\r\n```\r\nbut it resulted following error (I mask my personal path as <PERSONAL_PATH>):\r\n```\r\nFileNotFoundError: Unable to find 'https://huggingface.co/datasets/nateraw/image-folder/resolve/main/imagenet/train' at <PERSONAL_PATH>/ImageNet/https:/huggingface.co/datasets/nateraw/image-folder/resolve/main\r\n```\r\n\r\nNext, I followed a recent issue #3960 to load data as:\r\n```\r\nfrom datasets import load_dataset\r\ndata_files = {'train': ['imagenet/train/**'], 'val': ['imagenet/val/**']}\r\nds = load_dataset(\"imagefolder\", data_files=data_files, task=\"image-classification\")\r\n```\r\nand the data can be loaded without error as: (I copy val folder to train folder for illustration)\r\n```\r\n>>> ds\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'labels'],\r\n        num_rows: 50000\r\n    })\r\n    val: Dataset({\r\n        features: ['image', 'labels'],\r\n        num_rows: 50000\r\n    })\r\n})\r\n```\r\nHowever, the parsed classes is wrong (should be 1000 classes):\r\n```\r\n>>> ds[\"train\"].features\r\n{'image': Image(decode=True, id=None), 'labels': ClassLabel(num_classes=1, names=['val'], id=None)}\r\n```\r\n\r\n## Expected results\r\nI expect that the \"labels\" in ds[\"train\"].features should contain 1000 classes.\r\n\r\n## Actual results\r\nThe \"labels\" in ds[\"train\"].features contains only 1 wrong class.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Ubuntu 18.04\r\n- Python version: Python 3.7.12\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4015/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4015/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4009", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4009/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4009/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4009/events", "html_url": "https://github.com/huggingface/datasets/issues/4009", "id": 1179658611, "node_id": "I_kwDODunzps5GUClz", "number": 4009, "title": "AMI load_dataset error: sndfile library not found", "user": {"login": "i-am-neo", "id": 102043285, "node_id": "U_kgDOBhUOlQ", "avatar_url": "https://avatars.githubusercontent.com/u/102043285?v=4", "gravatar_id": "", "url": "https://api.github.com/users/i-am-neo", "html_url": "https://github.com/i-am-neo", "followers_url": "https://api.github.com/users/i-am-neo/followers", "following_url": "https://api.github.com/users/i-am-neo/following{/other_user}", "gists_url": "https://api.github.com/users/i-am-neo/gists{/gist_id}", "starred_url": "https://api.github.com/users/i-am-neo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/i-am-neo/subscriptions", "organizations_url": "https://api.github.com/users/i-am-neo/orgs", "repos_url": "https://api.github.com/users/i-am-neo/repos", "events_url": "https://api.github.com/users/i-am-neo/events{/privacy}", "received_events_url": "https://api.github.com/users/i-am-neo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-03-24T15:13:38Z", "updated_at": "2022-03-24T15:46:38Z", "closed_at": "2022-03-24T15:17:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nGetting error message when loading AMI dataset.\r\n\r\n## Steps to reproduce the bug\r\n`python3 -c \"from datasets import load_dataset; print(load_dataset('ami', 'headset-single', split='validation')[0])\"\r\n\r\n`\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/neo/.virtualenvs/hubert/lib/python3.7/site-packages/datasets/load.py\", line 1707, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/neo/.virtualenvs/hubert/lib/python3.7/site-packages/datasets/builder.py\", line 595, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/neo/.virtualenvs/hubert/lib/python3.7/site-packages/datasets/builder.py\", line 690, in _download_and_prepare\r\n    ) from None\r\nOSError: Cannot find data file. \r\nOriginal error:\r\nsndfile library not found\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-4.19.0-18-cloud-amd64-x86_64-with-debian-10.11\r\n- Python version: 3.7.3\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4009/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4009/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4007", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4007/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4007/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4007/events", "html_url": "https://github.com/huggingface/datasets/issues/4007", "id": 1179381021, "node_id": "I_kwDODunzps5GS-0d", "number": 4007, "title": "set_format does not work with multi dimension tensor", "user": {"login": "phihung", "id": 5902432, "node_id": "MDQ6VXNlcjU5MDI0MzI=", "avatar_url": "https://avatars.githubusercontent.com/u/5902432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phihung", "html_url": "https://github.com/phihung", "followers_url": "https://api.github.com/users/phihung/followers", "following_url": "https://api.github.com/users/phihung/following{/other_user}", "gists_url": "https://api.github.com/users/phihung/gists{/gist_id}", "starred_url": "https://api.github.com/users/phihung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phihung/subscriptions", "organizations_url": "https://api.github.com/users/phihung/orgs", "repos_url": "https://api.github.com/users/phihung/repos", "events_url": "https://api.github.com/users/phihung/events{/privacy}", "received_events_url": "https://api.github.com/users/phihung/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-03-24T11:27:43Z", "updated_at": "2022-03-30T07:28:57Z", "closed_at": "2022-03-24T14:39:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nset_format only transforms the last dimension of a multi-dimension list to tensor\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport torch\r\nfrom datasets import Dataset\r\n\r\nds = Dataset.from_dict({\"A\": [torch.rand((2, 2))]})\r\n# ds = Dataset.from_dict({\"A\": [np.random.rand(2, 2)]})  # => same result\r\nds = ds.with_format(\"torch\")\r\nprint(ds[0])\r\n```\r\n\r\n## Expected results\r\n```\r\n{'A': [tensor([[0.6689, 0.1516], [0.1403, 0.5567]])]}\r\n```\r\n\r\n## Actual results\r\n```\r\n{'A': [tensor([0.6689, 0.1516]), tensor([0.1403, 0.5567])]}\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- datasets version: 2.0.0\r\n- Platform: Mac OSX\r\n- Python version: 3.8.12\r\n- PyArrow version: 7.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4007/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4007/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4003", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4003/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4003/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4003/events", "html_url": "https://github.com/huggingface/datasets/issues/4003", "id": 1179286877, "node_id": "I_kwDODunzps5GSn1d", "number": 4003, "title": "ASSIN2 dataset checksum bug", "user": {"login": "ruanchaves", "id": 14352388, "node_id": "MDQ6VXNlcjE0MzUyMzg4", "avatar_url": "https://avatars.githubusercontent.com/u/14352388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ruanchaves", "html_url": "https://github.com/ruanchaves", "followers_url": "https://api.github.com/users/ruanchaves/followers", "following_url": "https://api.github.com/users/ruanchaves/following{/other_user}", "gists_url": "https://api.github.com/users/ruanchaves/gists{/gist_id}", "starred_url": "https://api.github.com/users/ruanchaves/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ruanchaves/subscriptions", "organizations_url": "https://api.github.com/users/ruanchaves/orgs", "repos_url": "https://api.github.com/users/ruanchaves/repos", "events_url": "https://api.github.com/users/ruanchaves/events{/privacy}", "received_events_url": "https://api.github.com/users/ruanchaves/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-03-24T10:08:50Z", "updated_at": "2022-04-27T14:14:45Z", "closed_at": "2022-03-28T13:56:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nChecksum error after trying to load the [ASSIN 2 dataset](https://huggingface.co/datasets/assin2).\r\n\r\n`NonMatchingChecksumError` triggered by calling `load_dataset(\"assin2\")`.\r\n\r\nSimilar to #3952 , #3942 , #3941 , etc.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n[<ipython-input-13-c664a92ad5e7>](https://localhost:8080/#) in <module>()\r\n----> 1 load_dataset('assin2')\r\n\r\n4 frames\r\n[/usr/local/lib/python3.7/dist-packages/datasets/utils/info_utils.py](https://localhost:8080/#) in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/u/0/uc?id=1Q9j1a83CuKzsHCGaNulSkNxBm7Dkn7Ln&export=download']\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"assin2\")\r\n```\r\n\r\n## Expected results\r\nLoad the dataset.\r\n\r\n## Actual results\r\nThe dataset won't load. \r\n\r\n## Environment info\r\n- `datasets` version: 2.0.1.dev0\r\n- Platform: Google Colab\r\n- Python version:  3.7.12\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4003/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/4000", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/4000/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/4000/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/4000/events", "html_url": "https://github.com/huggingface/datasets/issues/4000", "id": 1178844616, "node_id": "I_kwDODunzps5GQ73I", "number": 4000, "title": "load_dataset error: sndfile library not found", "user": {"login": "i-am-neo", "id": 102043285, "node_id": "U_kgDOBhUOlQ", "avatar_url": "https://avatars.githubusercontent.com/u/102043285?v=4", "gravatar_id": "", "url": "https://api.github.com/users/i-am-neo", "html_url": "https://github.com/i-am-neo", "followers_url": "https://api.github.com/users/i-am-neo/followers", "following_url": "https://api.github.com/users/i-am-neo/following{/other_user}", "gists_url": "https://api.github.com/users/i-am-neo/gists{/gist_id}", "starred_url": "https://api.github.com/users/i-am-neo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/i-am-neo/subscriptions", "organizations_url": "https://api.github.com/users/i-am-neo/orgs", "repos_url": "https://api.github.com/users/i-am-neo/repos", "events_url": "https://api.github.com/users/i-am-neo/events{/privacy}", "received_events_url": "https://api.github.com/users/i-am-neo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-03-24T01:52:32Z", "updated_at": "2022-03-25T17:53:33Z", "closed_at": "2022-03-25T17:53:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nCan't load ami dataset\r\n\r\n## Steps to reproduce the bug\r\n```\r\npython3 -c \"from datasets import load_dataset; print(load_dataset('ami', 'headset-single', split='validation')[0])\"\r\n```\r\n\r\n## Expected results\r\n\r\n\r\n## Actual results\r\nDownloading and preparing dataset ami/headset-single (download: 10.71 GiB, generated: 49.99 MiB, post-processed: Unknown size, total: 10.76 GiB) to /home/neo/.cache/huggingface/datasets/ami/headset-single/1.6.2/2accdf810f7c0585f78f4bcfa47684fbb980e35d29ecf126e6906dbecb872d9e...\r\nAMI corpus cannot be downloaded using multi-processing. Setting number of downloaded processes `num_proc` to 1. \r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 136/136 [00:00<00:00, 36004.88it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 136/136 [00:01<00:00, 79.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00<00:00, 25343.23it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00<00:00, 2874.78it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00<00:00, 27950.38it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00<00:00, 2892.25it/s]\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/neo/.virtualenvs/hubert/lib/python3.7/site-packages/datasets/load.py\", line 1707, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/neo/.virtualenvs/hubert/lib/python3.7/site-packages/datasets/builder.py\", line 595, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/neo/.virtualenvs/hubert/lib/python3.7/site-packages/datasets/builder.py\", line 690, in _download_and_prepare\r\n    ) from None\r\nOSError: Cannot find data file. \r\nOriginal error:\r\nsndfile library not found\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-4.19.0-18-cloud-amd64-x86_64-with-debian-10.11\r\n- Python version: 3.7.3\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/4000/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/4000/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3996", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3996/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3996/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3996/events", "html_url": "https://github.com/huggingface/datasets/issues/3996", "id": 1178415905, "node_id": "I_kwDODunzps5GPTMh", "number": 3996, "title": "Audio.encode_example() throws an error when writing example from array", "user": {"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-03-23T17:11:47Z", "updated_at": "2022-03-29T14:16:13Z", "closed_at": "2022-03-29T14:16:13Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen trying to do `Audio().encode_example()` with preexisting array (see [this line](https://github.com/huggingface/datasets/blob/master/src/datasets/features/audio.py#L73)), `sf.write()` throws you an error:\r\n`TypeError: No format specified and unable to get format from file extension: <_io.BytesIO object at 0x7f4218c0db30>`\r\n\r\n## Steps to reproduce the bug\r\n### Sample code to reproduce the bug\r\n```python\r\n# download sample file\r\n!wget https://huggingface.co/datasets/polinaeterna/test_encode_example/resolve/main/common_voice_vi_21824030.mp3\r\n\r\narr, sr = librosa.load(\"common_voice_vi_21824030.mp3\")\r\n\r\nAudio().encode_example({\r\n    \"path\": \"common_voice_vi_21824030.mp3\",\r\n    \"array\": arr,\r\n    \"sampling_rate\":sr\r\n})\r\n```\r\n\r\n## Expected results\r\nAn encoded example (`{\"bytes\": b'....', \"path\": 'path'}`)\r\n\r\n## Actual results\r\n```python\r\nTypeError                                 Traceback (most recent call last)\r\nInput In [3], in <module>\r\n      1 arr, sr = librosa.load(\"common_voice_vi_21824030.mp3\")\r\n----> 3 Audio().encode_example({\r\n      4     \"path\": \"common_voice_vi_21824030.mp3\",\r\n      5     \"array\": arr,\r\n      6     \"sampling_rate\":sr\r\n      7 })\r\n\r\nFile ~/workspace/datasets/src/datasets/features/audio.py:75, in Audio.encode_example(self, value)\r\n     73 elif isinstance(value, dict) and \"array\" in value:\r\n     74     buffer = BytesIO()\r\n---> 75     sf.write(buffer, value[\"array\"], value[\"sampling_rate\"])\r\n     76     return {\"bytes\": buffer.getvalue(), \"path\": value.get(\"path\")}\r\n     77 elif value.get(\"bytes\") is not None or value.get(\"path\") is not None:\r\n\r\nFile ~/miniconda3/envs/datasets/lib/python3.8/site-packages/soundfile.py:314, in write(file, data, samplerate, subtype, endian, format, closefd)\r\n    312 else:\r\n    313     channels = data.shape[1]\r\n--> 314 with SoundFile(file, 'w', samplerate, channels,\r\n    315                subtype, endian, format, closefd) as f:\r\n    316     f.write(data)\r\n\r\nFile ~/miniconda3/envs/datasets/lib/python3.8/site-packages/soundfile.py:627, in SoundFile.__init__(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\r\n    625 mode_int = _check_mode(mode)\r\n    626 self._mode = mode\r\n--> 627 self._info = _create_info_struct(file, mode, samplerate, channels,\r\n    628                                  format, subtype, endian)\r\n    629 self._file = self._open(file, mode_int, closefd)\r\n    630 if set(mode).issuperset('r+') and self.seekable():\r\n    631     # Move write position to 0 (like in Python file objects)\r\n\r\nFile ~/miniconda3/envs/datasets/lib/python3.8/site-packages/soundfile.py:1416, in _create_info_struct(file, mode, samplerate, channels, format, subtype, endian)\r\n   1414 original_format = format\r\n   1415 if format is None:\r\n-> 1416     format = _get_format_from_filename(file, mode)\r\n   1417     assert isinstance(format, (_unicode, str))\r\n   1418 else:\r\n\r\nFile ~/miniconda3/envs/datasets/lib/python3.8/site-packages/soundfile.py:1457, in _get_format_from_filename(file, mode)\r\n   1455     pass\r\n   1456 if format.upper() not in _formats and 'r' not in mode:\r\n-> 1457     raise TypeError(\"No format specified and unable to get format from \"\r\n   1458                     \"file extension: {0!r}\".format(file))\r\n   1459 return format\r\n\r\nTypeError: No format specified and unable to get format from file extension: <_io.BytesIO object at 0x7fd8daf88180>\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: datasets master\r\n- Platform: Ubuntu 20.04\r\n- Python version: python 3.8.12\r\n- PyArrow version: 6.0.1\r\n\r\n## Solution\r\n\r\nI guess we just need to add `format` arg in [this line](https://github.com/huggingface/datasets/blob/master/src/datasets/features/audio.py#L75) like this:\r\n```python\r\nsf.write(buffer, value[\"array\"], value[\"sampling_rate\"], format=\"wav\")\r\n```\r\n\r\n\r\nBTW discovered this when trying to decode audio in mp3 format without torchaudio (would be useful for TensorFlow users), like this:\r\n```python\r\nfrom datasets import load_dataset, Features, Audio\r\n\r\nds = load_dataset(\"common_voice\", \"vi\", split=\"test\")\r\nds = ds.remove_columns(\"audio\")\r\nds.select(range(3))  # 3 samples just for testing\r\n\r\ndef load_mp3_with_librosa(example):\r\n    arr, sr = librosa.load(example[\"path\"])\r\n    example[\"audio\"] = {\r\n        \"path\": example[\"path\"],\r\n        \"array\": arr,\r\n        \"sampling_rate\": sr\r\n    }\r\n    return example\r\n\r\nupdated_dataset = ds.map(lambda example: load_mp3_with_librosa(example),\r\n                         features=Features(\r\n                             {\"audio\": Audio(decode=False)}\r\n                         ))\r\n```\r\n\r\n@lhoestq @mariosasko @albertvillanova am I right in my logic? do we agree that we can set wav as the format? \ud83e\udd17", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3996/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3996/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3992", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3992/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3992/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3992/events", "html_url": "https://github.com/huggingface/datasets/issues/3992", "id": 1177946153, "node_id": "I_kwDODunzps5GNggp", "number": 3992, "title": "Image column is not decoded in map when using with with_transform", "user": {"login": "phihung", "id": 5902432, "node_id": "MDQ6VXNlcjU5MDI0MzI=", "avatar_url": "https://avatars.githubusercontent.com/u/5902432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phihung", "html_url": "https://github.com/phihung", "followers_url": "https://api.github.com/users/phihung/followers", "following_url": "https://api.github.com/users/phihung/following{/other_user}", "gists_url": "https://api.github.com/users/phihung/gists{/gist_id}", "starred_url": "https://api.github.com/users/phihung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phihung/subscriptions", "organizations_url": "https://api.github.com/users/phihung/orgs", "repos_url": "https://api.github.com/users/phihung/repos", "events_url": "https://api.github.com/users/phihung/events{/privacy}", "received_events_url": "https://api.github.com/users/phihung/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-23T10:51:13Z", "updated_at": "2022-12-13T16:59:06Z", "closed_at": "2022-12-13T16:59:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nImage column is not _decoded_ in **map** when using with `with_transform`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import Image, Dataset\r\n\r\ndef add_C(batch):\r\n    batch[\"C\"] = batch[\"A\"]\r\n    return batch\r\n\r\nds = Dataset.from_dict({\"A\": [\"image.png\"]}).cast_column(\"A\",  Image())\r\nds = ds.with_transform(lambda x: x)  # <= This line causes the problem\r\nds = ds.map(add_C, batched=True)\r\nprint(ds[0])\r\n```\r\n\r\n## Expected results\r\n```\r\n{'C': <PIL.PngImagePlugin.PngImageFile>,  ...}\r\n```\r\n\r\n## Actual results\r\n```\r\n{'C': {'bytes': None, 'path': 'image.png'}, ...}\r\n```\r\n\r\nIf we remove the `with_transform` line, we get the expected result.\r\n\r\n## Environment info\r\n- `datasets` version: 2.0.0\r\n- Platform: Mac OSX\r\n- Python version: 3.8.12\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3992/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3985", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3985/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3985/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3985/events", "html_url": "https://github.com/huggingface/datasets/issues/3985", "id": 1175982937, "node_id": "I_kwDODunzps5GGBNZ", "number": 3985, "title": "[image feature] Too many files open error when image feature is returned as a path", "user": {"login": "apsdehal", "id": 3616806, "node_id": "MDQ6VXNlcjM2MTY4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3616806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apsdehal", "html_url": "https://github.com/apsdehal", "followers_url": "https://api.github.com/users/apsdehal/followers", "following_url": "https://api.github.com/users/apsdehal/following{/other_user}", "gists_url": "https://api.github.com/users/apsdehal/gists{/gist_id}", "starred_url": "https://api.github.com/users/apsdehal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apsdehal/subscriptions", "organizations_url": "https://api.github.com/users/apsdehal/orgs", "repos_url": "https://api.github.com/users/apsdehal/repos", "events_url": "https://api.github.com/users/apsdehal/events{/privacy}", "received_events_url": "https://api.github.com/users/apsdehal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-03-21T21:54:05Z", "updated_at": "2022-03-23T18:19:27Z", "closed_at": "2022-03-23T18:19:27Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nPR in context: #3967. If I load the dataset in this PR (TextVQA), and do a simple list comprehension on the dataset, I get `Too many open files error`. This is happening due to the way we are loading the image feature when a str path is returned from the `_generate_examples`. Specifically at https://github.com/huggingface/datasets/blob/508eb4ab5d52f590baa677b4f64b1cc069139f7b/src/datasets/features/image.py#L110, we are open the file handle to the image but never closing it. This in my understanding is causing the issue.\r\n\r\n## Steps to reproduce the bug\r\nPull the PR locally and run the following code\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"./datasets/textvqa\")[\"train\"]\r\ndata = [item for item in dataset]\r\n\r\n# Error happens\r\n```\r\n\r\n## Expected results\r\n\r\nList comprehension should work smoothly\r\n\r\n## Actual results\r\n\r\n`Too many open files error`\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.1.dev0\r\n- Platform: macOS-12.2-arm64-arm-64bit\r\n- Python version: 3.10.0\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3985/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3973", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3973/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3973/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3973/events", "html_url": "https://github.com/huggingface/datasets/issues/3973", "id": 1174455431, "node_id": "I_kwDODunzps5GAMSH", "number": 3973, "title": "ConnectionError  and SSLError", "user": {"login": "yanyu2015", "id": 11142054, "node_id": "MDQ6VXNlcjExMTQyMDU0", "avatar_url": "https://avatars.githubusercontent.com/u/11142054?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yanyu2015", "html_url": "https://github.com/yanyu2015", "followers_url": "https://api.github.com/users/yanyu2015/followers", "following_url": "https://api.github.com/users/yanyu2015/following{/other_user}", "gists_url": "https://api.github.com/users/yanyu2015/gists{/gist_id}", "starred_url": "https://api.github.com/users/yanyu2015/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yanyu2015/subscriptions", "organizations_url": "https://api.github.com/users/yanyu2015/orgs", "repos_url": "https://api.github.com/users/yanyu2015/repos", "events_url": "https://api.github.com/users/yanyu2015/events{/privacy}", "received_events_url": "https://api.github.com/users/yanyu2015/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-03-20T06:45:37Z", "updated_at": "2022-03-30T08:13:32Z", "closed_at": "2022-03-30T08:13:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "code\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('oscar', 'unshuffled_deduplicated_it')\r\n```\r\nbug report\r\n```\r\n---------------------------------------------------------------------------\r\nConnectionError                           Traceback (most recent call last)\r\n~\\AppData\\Local\\Temp/ipykernel_29788/2615425180.py in <module>\r\n----> 1 dataset = load_dataset('oscar', 'unshuffled_deduplicated_it')\r\n\r\nD:\\DataScience\\PythonSet\\IDES\\anaconda\\lib\\site-packages\\datasets\\load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1658 \r\n   1659     # Create a dataset builder\r\n-> 1660     builder_instance = load_dataset_builder(\r\n   1661         path=path,\r\n   1662         name=name,\r\n\r\nD:\\DataScience\\PythonSet\\IDES\\anaconda\\lib\\site-packages\\datasets\\load.py in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\r\n   1484         download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1485         download_config.use_auth_token = use_auth_token\r\n-> 1486     dataset_module = dataset_module_factory(\r\n   1487         path,\r\n   1488         revision=revision,\r\n\r\nD:\\DataScience\\PythonSet\\IDES\\anaconda\\lib\\site-packages\\datasets\\load.py in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1236                         f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n   1237                     ) from None\r\n-> 1238                 raise e1 from None\r\n   1239     else:\r\n   1240         raise FileNotFoundError(\r\n\r\nD:\\DataScience\\PythonSet\\IDES\\anaconda\\lib\\site-packages\\datasets\\load.py in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1173             if path.count(\"/\") == 0:  # even though the dataset is on the Hub, we get it from GitHub for now\r\n   1174                 # TODO(QL): use a Hub dataset module factory instead of GitHub\r\n-> 1175                 return GithubDatasetModuleFactory(\r\n   1176                     path,\r\n   1177                     revision=revision,\r\n\r\nD:\\DataScience\\PythonSet\\IDES\\anaconda\\lib\\site-packages\\datasets\\load.py in get_module(self)\r\n    531         revision = self.revision\r\n    532         try:\r\n--> 533             local_path = self.download_loading_script(revision)\r\n    534         except FileNotFoundError:\r\n    535             if revision is not None or os.getenv(\"HF_SCRIPTS_VERSION\", None) is not None:\r\n\r\nD:\\DataScience\\PythonSet\\IDES\\anaconda\\lib\\site-packages\\datasets\\load.py in download_loading_script(self, revision)\r\n    511         if download_config.download_desc is None:\r\n    512             download_config.download_desc = \"Downloading builder script\"\r\n--> 513         return cached_path(file_path, download_config=download_config)\r\n    514 \r\n    515     def download_dataset_infos_file(self, revision: Optional[str]) -> str:\r\n\r\nD:\\DataScience\\PythonSet\\IDES\\anaconda\\lib\\site-packages\\datasets\\utils\\file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    232     if is_remote_url(url_or_filename):\r\n    233         # URL, so get it from the cache (downloading if necessary)\r\n--> 234         output_path = get_from_cache(\r\n    235             url_or_filename,\r\n    236             cache_dir=cache_dir,\r\n\r\nD:\\DataScience\\PythonSet\\IDES\\anaconda\\lib\\site-packages\\datasets\\utils\\file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, download_desc)\r\n    580         _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\r\n    581         if head_error is not None:\r\n--> 582             raise ConnectionError(f\"Couldn't reach {url} ({repr(head_error)})\")\r\n    583         elif response is not None:\r\n    584             raise ConnectionError(f\"Couldn't reach {url} (error {response.status_code})\")\r\n\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/oscar/oscar.py (SSLError(MaxRetryError(\"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /huggingface/datasets/2.0.0/datasets/oscar/oscar.py (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\")))\r\n```\r\nIt may be caused by Caused by SSLError(in China?) because it works well on google colab.\r\nSo how can I download this dataset manually?\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3973/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3965", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3965/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3965/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3965/events", "html_url": "https://github.com/huggingface/datasets/issues/3965", "id": 1173708739, "node_id": "I_kwDODunzps5F9V_D", "number": 3965, "title": "TypeError: Couldn't cast array of type for JSONLines dataset", "user": {"login": "lewtun", "id": 26859204, "node_id": "MDQ6VXNlcjI2ODU5MjA0", "avatar_url": "https://avatars.githubusercontent.com/u/26859204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lewtun", "html_url": "https://github.com/lewtun", "followers_url": "https://api.github.com/users/lewtun/followers", "following_url": "https://api.github.com/users/lewtun/following{/other_user}", "gists_url": "https://api.github.com/users/lewtun/gists{/gist_id}", "starred_url": "https://api.github.com/users/lewtun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lewtun/subscriptions", "organizations_url": "https://api.github.com/users/lewtun/orgs", "repos_url": "https://api.github.com/users/lewtun/repos", "events_url": "https://api.github.com/users/lewtun/events{/privacy}", "received_events_url": "https://api.github.com/users/lewtun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-03-18T15:17:53Z", "updated_at": "2022-05-06T16:13:51Z", "closed_at": "2022-05-06T16:13:51Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nOne of the [course participants](https://discuss.huggingface.co/t/chapter-5-questions/11744/20?u=lewtun) is having trouble loading a JSONLines dataset that's composed of the GitHub issues from `spacy` (see stack trace below). \r\n\r\nThis reminds me a bit of #2799 where one can load the dataset in `pandas` but not in `datasets` and perhaps increasing the `block_size` is needed again.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfrom huggingface_hub import hf_hub_url\r\nimport pandas as pd\r\n\r\n# returns 'https://huggingface.co/datasets/Evan/spaCy-github-issues/resolve/main/spacy-issues.jsonl'\r\ndata_files = hf_hub_url(repo_id=\"Evan/spaCy-github-issues\", filename=\"spacy-issues.jsonl\", repo_type=\"dataset\")\r\n# throws TypeError: Couldn't cast array of type\r\ndset = load_dataset(\"json\", data_files=data_files, split=\"test\")\r\n# no problem with pandas - note this take a while as the file is >2GB\r\ndf = pd.read_json(data_files, orient=\"records\", lines=True)\r\ndf.head()\r\n```\r\n\r\n## Expected results\r\nI can load any line-separated JSON file, similar to pandas.\r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/load.py\", line 1702, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/builder.py\", line 594, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/builder.py\", line 683, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/builder.py\", line 1136, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/arrow_writer.py\", line 511, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/table.py\", line 1121, in table_cast\r\n    return cast_table_to_features(table, Features.from_arrow_schema(schema))\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/table.py\", line 1102, in cast_table_to_features\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/table.py\", line 1102, in <listcomp>\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/table.py\", line 944, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/table.py\", line 918, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/table.py\", line 918, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/table.py\", line 1086, in cast_array_to_feature\r\n    return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/table.py\", line 944, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/table.py\", line 920, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/Users/lewtun/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/table.py\", line 1019, in array_cast\r\n    raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{pa_type}\")\r\nTypeError: Couldn't cast array of type\r\nstruct<url: string, html_url: string, labels_url: string, id: int64, node_id: string, number: int64, title: string, description: string, creator: struct<login: string, id: int64, node_id: string, avatar_url: string, gravatar_id: string, url: string, html_url: string, followers_url: string, following_url: string, gists_url: string, starred_url: string, subscriptions_url: string, organizations_url: string, repos_url: string, events_url: string, received_events_url: string, type: string, site_admin: bool>, open_issues: int64, closed_issues: int64, state: string, created_at: timestamp[s], updated_at: timestamp[s], due_on: null, closed_at: timestamp[s]>\r\nto\r\nnull\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.9.7\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3965/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3965/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3961", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3961/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3961/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3961/events", "html_url": "https://github.com/huggingface/datasets/issues/3961", "id": 1173223086, "node_id": "I_kwDODunzps5F7fau", "number": 3961, "title": "Scores from Index at extra positions are not filtered out", "user": {"login": "vishalsrao", "id": 36671559, "node_id": "MDQ6VXNlcjM2NjcxNTU5", "avatar_url": "https://avatars.githubusercontent.com/u/36671559?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishalsrao", "html_url": "https://github.com/vishalsrao", "followers_url": "https://api.github.com/users/vishalsrao/followers", "following_url": "https://api.github.com/users/vishalsrao/following{/other_user}", "gists_url": "https://api.github.com/users/vishalsrao/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishalsrao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishalsrao/subscriptions", "organizations_url": "https://api.github.com/users/vishalsrao/orgs", "repos_url": "https://api.github.com/users/vishalsrao/repos", "events_url": "https://api.github.com/users/vishalsrao/events{/privacy}", "received_events_url": "https://api.github.com/users/vishalsrao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-03-18T06:13:23Z", "updated_at": "2022-04-12T14:41:58Z", "closed_at": "2022-04-12T14:41:58Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "If a FAISS index has fewer records than the requested number of top results (k), then it returns -1 in indices for the additional positions. The get_nearest_examples method only filters out the extra results from the dataset samples. It would be better to filter out extra scores too.\r\n\r\nReference: https://github.com/huggingface/datasets/blob/2.0.0/src/datasets/search.py#L693\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3961/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3961/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3959", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3959/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3959/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3959/events", "html_url": "https://github.com/huggingface/datasets/issues/3959", "id": 1172872695, "node_id": "I_kwDODunzps5F6J33", "number": 3959, "title": "Medium-sized dataset conversion from pandas causes a crash ", "user": {"login": "Antymon", "id": 641005, "node_id": "MDQ6VXNlcjY0MTAwNQ==", "avatar_url": "https://avatars.githubusercontent.com/u/641005?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Antymon", "html_url": "https://github.com/Antymon", "followers_url": "https://api.github.com/users/Antymon/followers", "following_url": "https://api.github.com/users/Antymon/following{/other_user}", "gists_url": "https://api.github.com/users/Antymon/gists{/gist_id}", "starred_url": "https://api.github.com/users/Antymon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Antymon/subscriptions", "organizations_url": "https://api.github.com/users/Antymon/orgs", "repos_url": "https://api.github.com/users/Antymon/repos", "events_url": "https://api.github.com/users/Antymon/events{/privacy}", "received_events_url": "https://api.github.com/users/Antymon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-03-17T20:20:35Z", "updated_at": "2022-12-12T17:14:06Z", "closed_at": "2022-04-20T12:35:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I am suffering from the following issue:\r\n\r\n## Describe the bug\r\nConversion to arrow dataset from pandas dataframe of a certain size deterministically causes the following crash:\r\n\r\n```\r\n  File \"/home/datasets_crash.py\", line 7, in <module>\r\n    arrow=datasets.Dataset.from_pandas(d)\r\n  File \"/home/.conda/envs/tools/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 783, in from_pandas\r\n    table = InMemoryTable.from_pandas(\r\n  File \"/home/.conda/envs/tools/lib/python3.9/site-packages/datasets/table.py\", line 379, in from_pandas\r\n    return cls(pa.Table.from_pandas(*args, **kwargs))\r\n  File \"pyarrow/table.pxi\", line 1487, in pyarrow.lib.Table.from_pandas\r\n  File \"pyarrow/table.pxi\", line 1532, in pyarrow.lib.Table.from_arrays\r\n  File \"pyarrow/table.pxi\", line 1181, in pyarrow.lib.Table.validate\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Column 1: In chunk 0: Invalid: List child array invalid: Invalid: Struct child array #1 has length smaller than expected for struct array (1192457 < 1192458)\r\n```\r\n\r\n## Steps to reproduce the bug\r\nI have a dataset made from replicated single example mocking a dict representation of a publication.\r\nI copy over this example 140k times and create a pandas frame.\r\nI use 'Dataset.from_pandas' and boom\r\n\r\n```python\r\n# Sample code to reproduce the bug\r\nimport copy\r\nimport datasets\r\nimport pandas\r\n# serialized dict is quite long to be realistic representation of a publication content\r\npaper_as_dict=eval(\"{'article_id': '2020-11-05T14:25:05.321Z02bc3286-91b7-486a-9c74-4f457fbc586a', 'sections': [{'section_id': 'body.0', 'paragraphs': [{'sentences': ['11010111001000000011010011110011101110111011000100001010011100101001111010110111101011101111101010101110001111011110111010111', '1101100110110010010101010100110011000111001100100000011100010111010000011100001101111000000011010111001111001010101111110011010010111011000110100110010', '101011011000010100000010011001011011000000110011011110000101001110110000010001100110111100011100110101010010110000101', '1101101110101010101000000010101011111001111000101000110001110100111000100000011001110100110000110100111011001010110011101001001110']}]}, {'section_id': 'body.1', 'paragraphs': [{'sentences': ['11111100100100111000101001011110100110011001011011001001100110100111011010000110011000010001010100101110001001101011110111110101111100001001001000011110110010110011100110110111110011100011111000101010111010101011001110000100000001001010010010011101111100011010', '10101000110000110111110011101111000101010010001001010000001111001100000010001000001110111110010011101000000111011', '111010011111101111110011111110110001000111100101001000100110101111110000111000111111110000101001101000110011010111011101001010110110001000100000001110001111100110110001110001001100011010100110100010100111000110110100010010100101011110000110000101010010001110101100000']}, {'sentences': ['111110011110110110001111001101011110010110100011101010110101011001101110110111100000111101010110011110111101001111000101110001001010010101100111111001001000011101000100110000101', '011101101101111101001100101010000010111101100101110100101000001100010100110011010010100001101001110111100011010011011111000111111101110001010111010011010110001000010101100110000100010110101110110011001010011001100111101100001001', '1110001011011010101001100001110001110001000111111111101110100001011101101001110100000110000011010001101010101110101110101101001010100100010000000010110010010010', '11101111000111111100111110010000111101110010010101001111011001111110011000011100110001010010000100101010', '111000110110110010101100010010100001100100110010101000001000011101000100101011011010000011001011011111001101100001110010100001111110111001001010101100100110001011011100000101010010000000001100010000101100110110111101110010100010011101110110111010011011000011001010111011100000000010101001011000100000011010100011101001011001010010011110100100']}, {'sentences': ['001101111100001101001001001110000110010101011101001001111111011000111001111011101011110111000000100001110110101110001010001111110100010', '0000110010110101001100011011000011001101001110001000000110010101000011101011110110000000100111000001010000101011111011110001001100001110101010101110101011111000000011001111011110001010010111010000100100000001111001011100101111010101111001001101100101001101111000111011010110010001010010010111010000001101101111100101000111101011001000101', '00000101100101100111101010000101011100101100001100011001100100001100001010001010010011001001111001000010100010000110100111110000001000101000111100010111110011000100000111100010000100010111100010101', '111100110010100110000010010101010101110011110100000101110000000111010101111001011110010101001110000001001000010110010010011110111110010110100101110011001101110111001111100011100100011110010010100101011111111']}, {'sentences': ['1100001110101111000001011001100110001011100011110110010011001000101000011110010101010011011000111010000101010011010000000111011001000010100101000011111101000000000101111000', '1110101000100110001111000011000101110111001100101010011001100011010011111111111010101011010101010011000101001100100000110010100110110110110001101100', '00010001100100101100100111111110111111101000100110101111101111110101110001010001011100000000000011010101101001111010001110101101110011001011111101110100010000111101', '011100011101011001000110010110100100000010100010010110011000000010101110011111111101010010010001100110101010010001100010110011110001011011101010111111100100110110010111101001100101010111001', '10111000011010101111110110011010101011111001000001010010111111010010111111100100010100110100101101110100110011001000110100000111000100110000001000111010', '0010011111111011100111010001111001011101001010000010110000010111000101001101000011101110100100000000100100010010101010100011100101001000100110110000010111111110000011011101111000111010']}]}, {'section_id': 'body.2.0', 'paragraphs': [{'sentences': ['110010010011001110100100011001111100010011110111101011011011001010010010010011101011', '000110101110011011101011000000100011111000001100011011110101101011000110011010001010001101101100000111100101001011111001001101111', '1000011100100000100100100010010000111011000100110010000011110111100110110001101001010100011111010100101000111', '11110111111000110010000000000100010010110001100010001010000111011000101100011010010101110110011010110101001101110011101011101100000001000100101011010110110100101011101010010101101000011110000010101011001011000001000000001010110000100010000100011110101001111100001000100000111000001010011111111110101010100011011000010000111000110', '1001000111011000111110001111111001100001000000101000111011101101100101010110001101000000001111010111100011111000000100001001110', '100110010111010101111010100000010001110101111001010010001100001110100100100101110011010101001000100101000100100011001110001100111000010010011011000010011010010000110001000000100011110010110110011010001100111010111110011']}, {'sentences': ['10010101011100010111011111001001001010100011001001111101101001000000001111101110000111101011000001001011101110101001100010010001101111001110000100010010001001101111011111110010011011110011', '110001110010110000101111000000110010010010100000010100001111101101000101100000000110000000011111011001111000010110110001011010011011101100100110011000100110101010111010111111000111001111010110010001001110100001011011000110000000111101110000001111011011101110100000100010000110001000000110100000', '101010000000010000110110111000110000100111000001110100101101101010001010010010101010100111010110001001000101011110010011001001001110111001101101100100011110011011110101100010110111001010000001000110100000001010011111111110111010011110001001110100011011000101011000110110011011010110100100011111111011100111110110000110011011110110110011101010101111001101010110101000000001100101111010000101110', '1010100110111111111000110110111110010100000100001110101110111001011000010001110110001111111110000101001001110010001110000111010101111010111111011100100011100111111101101111000010001100101000010001100110110100110111111100100011001011000001111110010100110111000010011110111011001101100000101011111110101000011000010', '00000001110000101001110101110011101001110011000111111101111101111000010011100000101000001011001110', '101000111010010000011010011010011010010010100010110100011100100111011101010100101110100111010001000000', '01101000110001101011001101100010100011011010000000001010101000010101000110100010000000110001110001010010000000101101000011000100000110011101100001010100011111101010010110001101110101010111101100001110000011001101', '0010010111000011110010011110001010100000111100001011010100100010101010010011101101100110001001111001000110000111011110010000110101010110111111010110100000011010001001010001000110001101101000101110001011110000101101110000110010110010111001100010011011100011', '00110111110000000100110111101011000100100110001000001001101011001000010100100001100111100110000110110101111010000010101000000101000011001011101001', '0100100001000111001110110110000001000100111001101101110100100111010111110001110010110111100110011111001001000011101110100101111011000110100000111010011101']}, {'sentences': ['100001001011101111111100110111011110001101111101100001000110110000100101011000000100000', '10101001001111110101001010100110011110101101001']}]}, {'section_id': 'body.2.0.0', 'paragraphs': [{'sentences': ['1110101100001100011000101000010000100010101101010110101011100101110110110111010101001100100000000111011001000100011110101011111010100101001010000010001001101010100011110010101110011001100010000100110011000011101010001000111001000001100', '101000000011001001110101000100101010000111000111100010010001111111100110001100000100011010011010010101101111010101010000110011101001111001111011111001110001010000110101101011101111010000001100', '01100001011110010100000101001101111101010011100010011001011110110010010011100101000', '0011100111000101111000010001111100000111000101110001111010001100001000111010000101100001110101100111111', '00001100000011110001011010010110000000111110110001111000110000011011001110000000100011001010110000010000010001101010101100000010011011000101011111100010010', '1011101011101111000001100100111000011000010010011110011000110111010010111100111101100110011010000110000111000110111110101111000001000010011101111000110000100011110101101101001101000110010000001000010011011010101100', '1000010011100011100000010011011111111110101101111011101010010111000000101011000000110101111000010011', '01100000110011001110101111101101011001011101000010001100101010100011010101010100111011011110100010100111', '011011010100011011110010101000110001111110110']}]}, {'section_id': 'body.2.0.1', 'paragraphs': [{'sentences': ['00111011011101000100100111000001101001011000111100100010101001010011001011000010011111001100000100010001100101110011001000110001101011010111011111011000010011010010111010011111101000110111011100010011100111111110110111011', '011011010101101101010000001011010110011111011110100111010101010110001101000010011111000011100', '110001000110010000000111101110111110101110111000101000010001110101000101001000111000010001011101010000110001010001101001001110111110111010111010011101000101101010000', '001000111110100110000001111100000111001110111001110111001000111010001001100111001101000001001001010111000111011100001111011001111110001011000111110011111101011101000100101001111011100001000110101010101111111110011111111011000101110001000000000100111011111011001100111', '11010101100010010100010010010101001011001011000001100010101111111101001101110011001010010100000111010101', '01110000110011111000110010011010000011100000010010001111100010010100100001011011111110001100', '011101111100011101100111110101111001101010010001001110101100001101000000111000']}]}, {'section_id': 'body.2.0.2', 'paragraphs': [{'sentences': ['0111011000110100110000001011001110111000011110100111011000000001000010001111111001101111011100101110101101000111000101000010000111011010110000011101111110111110100111000111000011', '00100110111000110101100111000110100010011010010101001010011000000101000110100110011010011111000100000011000000010001010000100111101011111111101010001111010000001011100001110100000101001101101010011011101000', '000001110001010010100101010100010101001100011001001101101101110111011111101010010111010110110111011110101100001000011110111011001', '0001110010111110100110110011000001111100100100110101011010010101010100101000010101000100101000011011', '1000010010010101001100101110010111010100000110101110000000111001111111001011111010000011110001011001001001000101', '0001111100111010010100010111010110011011000000001111010010110001000011010001100111101110001110000011010101111100001000011010110100000100100001111011110110000000101000010001111001010010110101110111101101110111000100', '1000101100001000100001101110111110000100000001000010101111010011010010010111011010100011001000100100001010001100110']}]}, {'section_id': 'body.2.0.3', 'paragraphs': [{'sentences': ['1010100111100011110110101011100001011010011010100100010011000110111000001010010110111001001101111000010100100110101001010001010001000110010000001', '100010101010100111000011111101010100101110011000100011100100100111000010000011001010010111011010000101010011011110111001010110', '0110000110110110110011011000011010010000001010011000010001011110110010000100011111010100110111111010010111000101111', '10100100000011100010110110011111011011101101111000001001010100001001011010000011001010101100000', '1011111111100001001100000010000100110010101000010100111111110010110011101110000101101011101', '10001111110000011100100000101100000000010000100000011100110000011110111010011101010111101001111000100000000110000011010010001100110111100001001011101011001111110010100111001001010001010011010010010111001101110101110000101011', '101101111111101101010010000110111110000110000111001001010011111101011001011010101100010100110101101011100111100100110010001011110001110010000011101100100100001001110010000010011111100110101']}]}, {'section_id': 'body.2.1', 'paragraphs': [{'sentences': ['1010010011010011001111111001000110010001101111101011001011011000101001010101010001000110100011110101110001110110111010010010100100111000101100100101111110100000011111001101010111101010100101011011110111111110', '000010101101111100000110010110011001111100001101011101000100010001001001000000101101000001110000011010111100000010010000010101110101100010011000101110110111111001000101000111000110100001001100001010101010100011', '0000000011101110111100100010111100101010110001111101110110010000100100010000101001101111001111001001100110010011010000101001110010000000100101011101001010100100011101101001011000010111110100101010110110011001110000110010010111110110101100001011101001100111010001000010111010001010000100010010011110111100110011100011111101101000011100111110101010100110001100100000100011011010111000111110010110100010111101001001101000001100100010000111110000011101111100111101000000000']}, {'sentences': ['01011000010110011000000101101000110101011010100111011001001001100001101101111101111001101111100101111001101011011001011110110110110100001100111111010100101110111111101000101100101010110011111011100101101010100110111001111100100011001110011101000110100000001100001100110001110101001000011010000110101011010000001111100100000100101110011000001001010011011101100011000001100000011', '1001100000101000000011110100110001100001101001100011010000111111010110101111001000100111000011010100100000110110001', '10010011000110110111010110000010010000000111101000100101100111101101001100111110101001001111100001110011110000010101000001000000010100011011110011000100110101001100110111111001101000011010100110000000011110001000101010101000110010010']}]}, {'section_id': 'body.2.2', 'paragraphs': [{'sentences': ['000011000000010011000001101111000101000111111111111010001011110000011001010111010101010110001111110000010', '10101001101011101010001111011000110100000100011110010001100111111101101100010010111110110101101011000011000001101110010111011111100111110000000101110010111', '100001011110010111010110001101101001100000000001000010110101011001111100101101101111010010111111000000111001111010011111000100010001111011110001010000110010101010111110100101011011100001010101000001011011111111101', '1000110111111011101000110101001111111111000100011001000011010100001010011110001111010011011111000111011100101001011111001000010101110110101000111011111111010010001101001010110111000011110101011000010000110', '1011100000100000010101101111001001100110111000010001011010111111000000001010101001111011101011010101101001111101101100101001011101000011011010001001101100100111101111111100010011010101111011100001100001000100101100100110101000010000011000000011001100000110000001', '0001001101111001111111010000001101010110110110100110110100000100110101101010010101011000010010111011000010111110000001110101110111000010011000100110111001000111011000100101110111111', '0110010010011000011010001111001100101001100001001000010100101100010110000000101010110001001010001100111101010001110010010000111011100101101010111111101001100010001011100110010100110111010101000100001110000101110011111011111000010101010110101100010010111100100010010100111110111100101010100011101001110110010000011110001010101010000100010000100111001111011101', '000001010000010001100000101011000000110101000100010111111100101111111000110111001001110110101111110011100001001000011001010000011011', '0101101001010101001101010100011000111011001000100001110100110011100000001001010110001101010110011100111111100101101111101111011001111111110010111010011011011111011011110000101011010', '11000001110111000001100100001110000111001010000101011011101010111001011100010010010111111111000011111110010111100011100110001001100011111010100111110111001110010', '0100010110100001010101110111100011100100010111111011101001100101111110101011010010101111001000101001111000001110001100011001110010100110101100110100100000001010101101011110011001000101100111001001001110100', '100000100010011111001101010000100110011110001100000010010110110100000111111011010100101111010111001110101000100001111101001110000011010110000010100', '00100110000011100101000110110001000011101000011010101000010001111011100001111111001011100111101000001000000110110001000101111010010010001100111', '0110110100011001110011001111100010101001011111011001011001101101010010101101110101010100001000100100000111101110001001110111000110011101101010100000101', '0011111010010011011101010110100110000011000011100100101011011001110110001110001111000011010111011000110100111111011101110111000010010000011011010011011100000011101100110110100100000010110101110100110101001100111011101001010111011011110100110101110010011011010001010111110011001000010100010101010010110010010110000100110001000011010011000100101011010100100111010']}]}]}\")\r\nd=pandas.DataFrame.from_records(copy.deepcopy(paper_as_dict) for _ in range(140_100))\r\narrow=datasets.Dataset.from_pandas(d)\r\n```\r\n\r\n## Expected results\r\nThe dataset should be converted without error.\r\n\r\n## Actual results\r\nError `pyarrow.lib.ArrowInvalid: Column 1: In chunk 0: Invalid: List child array invalid: Invalid: Struct child array #1 has length smaller than expected for struct array (1192457 < 1192458)`\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: datasets==1.18.4 pandas==1.3.5\r\n- Platform: macOS 11.6 or CentOS Linux 7 (Core)\r\n- Python version: Python 3.9.7\r\n- PyArrow version: pyarrow==3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3959/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3956", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3956/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3956/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3956/events", "html_url": "https://github.com/huggingface/datasets/issues/3956", "id": 1172272327, "node_id": "I_kwDODunzps5F33TH", "number": 3956, "title": "TypeError: __init__() missing 1 required positional argument: 'scheme'", "user": {"login": "amirj", "id": 1645137, "node_id": "MDQ6VXNlcjE2NDUxMzc=", "avatar_url": "https://avatars.githubusercontent.com/u/1645137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amirj", "html_url": "https://github.com/amirj", "followers_url": "https://api.github.com/users/amirj/followers", "following_url": "https://api.github.com/users/amirj/following{/other_user}", "gists_url": "https://api.github.com/users/amirj/gists{/gist_id}", "starred_url": "https://api.github.com/users/amirj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amirj/subscriptions", "organizations_url": "https://api.github.com/users/amirj/orgs", "repos_url": "https://api.github.com/users/amirj/repos", "events_url": "https://api.github.com/users/amirj/events{/privacy}", "received_events_url": "https://api.github.com/users/amirj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2022-03-17T11:43:13Z", "updated_at": "2022-05-04T16:37:10Z", "closed_at": "2022-03-28T08:00:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nBased on [this tutorial](https://huggingface.co/docs/datasets/faiss_es#elasticsearch) the provided code should add Elasticsearch index but raised the following error, probably the new Elasticsearch version is not compatible though the tutorial doesn't provide any information about the supporting Elasticsearch version.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import load_dataset\r\nsquad = load_dataset('squad', split='validation')\r\nsquad.add_elasticsearch_index(\"context\", host=\"localhost\", port=\"9200\")\r\n```\r\n\r\n## Expected results\r\n[Creating an elastic index based on the provided tutorial](https://huggingface.co/docs/datasets/faiss_es#elasticsearch)\r\n\r\n## Actual results\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-6-8fb51aa33961> in <module>\r\n      1 from datasets import load_dataset\r\n      2 squad = load_dataset('squad', split='validation')\r\n----> 3 squad.add_elasticsearch_index(\"context\", host=\"localhost\", port=\"9200\")\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py in add_elasticsearch_index(self, column, index_name, host, port, es_client, es_index_name, es_index_config)\r\n   3777         \"\"\"\r\n   3778         with self.formatted_as(type=None, columns=[column]):\r\n-> 3779             super().add_elasticsearch_index(\r\n   3780                 column=column,\r\n   3781                 index_name=index_name,\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/datasets/search.py in add_elasticsearch_index(self, column, index_name, host, port, es_client, es_index_name, es_index_config)\r\n    587         \"\"\"\r\n    588         index_name = index_name if index_name is not None else column\r\n--> 589         es_index = ElasticSearchIndex(\r\n    590             host=host, port=port, es_client=es_client, es_index_name=es_index_name, es_index_config=es_index_config\r\n    591         )\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/datasets/search.py in __init__(self, host, port, es_client, es_index_name, es_index_config)\r\n    123         from elasticsearch import Elasticsearch  # noqa: F811\r\n    124 \r\n--> 125         self.es_client = es_client if es_client is not None else Elasticsearch([{\"host\": host, \"port\": str(port)}])\r\n    126         self.es_index_name = (\r\n    127             es_index_name\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/elasticsearch/_sync/client/__init__.py in __init__(self, hosts, cloud_id, api_key, basic_auth, bearer_auth, opaque_id, headers, connections_per_node, http_compress, verify_certs, ca_certs, client_cert, client_key, ssl_assert_hostname, ssl_assert_fingerprint, ssl_version, ssl_context, ssl_show_warn, transport_class, request_timeout, node_class, node_pool_class, randomize_nodes_in_pool, node_selector_class, dead_node_backoff_factor, max_dead_node_backoff, serializer, serializers, default_mimetype, max_retries, retry_on_status, retry_on_timeout, sniff_on_start, sniff_before_requests, sniff_on_node_failure, sniff_timeout, min_delay_between_sniffing, sniffed_node_callback, meta_header, timeout, randomize_hosts, host_info_callback, sniffer_timeout, sniff_on_connection_fail, http_auth, maxsize, _transport)\r\n    310 \r\n    311         if _transport is None:\r\n--> 312             node_configs = client_node_configs(\r\n    313                 hosts,\r\n    314                 cloud_id=cloud_id,\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/elasticsearch/_sync/client/utils.py in client_node_configs(hosts, cloud_id, **kwargs)\r\n     99     else:\r\n    100         assert hosts is not None\r\n--> 101         node_configs = hosts_to_node_configs(hosts)\r\n    102 \r\n    103     # Remove all values which are 'DEFAULT' to avoid overwriting actual defaults.\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/elasticsearch/_sync/client/utils.py in hosts_to_node_configs(hosts)\r\n    142 \r\n    143         elif isinstance(host, Mapping):\r\n--> 144             node_configs.append(host_mapping_to_node_config(host))\r\n    145         else:\r\n    146             raise ValueError(\r\n\r\n~/opt/anaconda3/lib/python3.8/site-packages/elasticsearch/_sync/client/utils.py in host_mapping_to_node_config(host)\r\n    209         options[\"path_prefix\"] = options.pop(\"url_prefix\")\r\n    210 \r\n--> 211     return NodeConfig(**options)  # type: ignore\r\n    212 \r\n    213 \r\n\r\nTypeError: __init__() missing 1 required positional argument: 'scheme'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: Mac\r\n- Python version: 3.8.0\r\n- PyArrow version: 7.0.0\r\n- ElaticSearch Info: \r\n{\r\n  \"name\" : \"byname\",\r\n  \"cluster_name\" : \"elasticsearch_brew\",\r\n  \"cluster_uuid\" : \"9xkjrltiQIG0J95ciWhqRA\",\r\n  \"version\" : {\r\n    \"number\" : \"7.10.2-SNAPSHOT\",\r\n    \"build_flavor\" : \"oss\",\r\n    \"build_type\" : \"tar\",\r\n    \"build_hash\" : \"unknown\",\r\n    \"build_date\" : \"2021-01-16T01:41:27.115673Z\",\r\n    \"build_snapshot\" : true,\r\n    \"lucene_version\" : \"8.7.0\",\r\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\r\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\r\n  },\r\n  \"tagline\" : \"You Know, for Search\"\r\n}\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3956/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3956/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3952", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3952/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3952/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3952/events", "html_url": "https://github.com/huggingface/datasets/issues/3952", "id": 1171895531, "node_id": "I_kwDODunzps5F2bTr", "number": 3952, "title": "Checksum error for glue sst2, stsb, rte etc datasets", "user": {"login": "ravindra-ut", "id": 22090962, "node_id": "MDQ6VXNlcjIyMDkwOTYy", "avatar_url": "https://avatars.githubusercontent.com/u/22090962?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ravindra-ut", "html_url": "https://github.com/ravindra-ut", "followers_url": "https://api.github.com/users/ravindra-ut/followers", "following_url": "https://api.github.com/users/ravindra-ut/following{/other_user}", "gists_url": "https://api.github.com/users/ravindra-ut/gists{/gist_id}", "starred_url": "https://api.github.com/users/ravindra-ut/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ravindra-ut/subscriptions", "organizations_url": "https://api.github.com/users/ravindra-ut/orgs", "repos_url": "https://api.github.com/users/ravindra-ut/repos", "events_url": "https://api.github.com/users/ravindra-ut/events{/privacy}", "received_events_url": "https://api.github.com/users/ravindra-ut/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-03-17T03:45:47Z", "updated_at": "2022-03-17T07:10:15Z", "closed_at": "2022-03-17T07:10:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nChecksum error for glue sst2, stsb, rte etc datasets\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> nlp.load_dataset('glue', 'sst2')\r\nDownloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown sizetotal: 11.90 MiB) to \r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 73.0/73.0 [00:00<00:00, 18.2kB/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Python/3.8/lib/python/site-packages/nlp/load.py\", line 548, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Library/Python/3.8/lib/python/site-packages/nlp/builder.py\", line 462, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Library/Python/3.8/lib/python/site-packages/nlp/builder.py\", line 521, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/Library/Python/3.8/lib/python/site-packages/nlp/utils/info_utils.py\", line 38, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\nnlp.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8']\r\n```\r\n\r\n## Expected results\r\ndataset load should succeed without checksum error.\r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Python/3.8/lib/python/site-packages/nlp/load.py\", line 548, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Library/Python/3.8/lib/python/site-packages/nlp/builder.py\", line 462, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Library/Python/3.8/lib/python/site-packages/nlp/builder.py\", line 521, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/Library/Python/3.8/lib/python/site-packages/nlp/utils/info_utils.py\", line 38, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\nnlp.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8']\r\n```\r\n## Environment info\r\n- `datasets` version: '1.18.3'\r\n- Platform: Mac OS\r\n- Python version: Python 3.8.9\r\n- PyArrow version: '7.0.0'\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3952/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3951", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3951/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3951/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3951/events", "html_url": "https://github.com/huggingface/datasets/issues/3951", "id": 1171568814, "node_id": "I_kwDODunzps5F1Liu", "number": 3951, "title": "Forked streaming datasets try to `open` data urls rather than use network", "user": {"login": "dlwh", "id": 9633, "node_id": "MDQ6VXNlcjk2MzM=", "avatar_url": "https://avatars.githubusercontent.com/u/9633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dlwh", "html_url": "https://github.com/dlwh", "followers_url": "https://api.github.com/users/dlwh/followers", "following_url": "https://api.github.com/users/dlwh/following{/other_user}", "gists_url": "https://api.github.com/users/dlwh/gists{/gist_id}", "starred_url": "https://api.github.com/users/dlwh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dlwh/subscriptions", "organizations_url": "https://api.github.com/users/dlwh/orgs", "repos_url": "https://api.github.com/users/dlwh/repos", "events_url": "https://api.github.com/users/dlwh/events{/privacy}", "received_events_url": "https://api.github.com/users/dlwh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-16T21:21:02Z", "updated_at": "2022-06-10T20:47:26Z", "closed_at": "2022-06-10T20:47:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nBuilding on #3950, if you bypass the pickling problem you still can't use the dataset. Somehow something gets confused and the forked processes try to `open` urls rather than anything else.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom multiprocessing import freeze_support\r\n\r\nimport transformers\r\nfrom transformers import Trainer, AutoModelForCausalLM, TrainingArguments\r\nimport datasets\r\nimport torch.utils.data\r\n\r\n# work around #3950\r\nclass TorchIterableDataset(datasets.IterableDataset, torch.utils.data.IterableDataset):\r\n    pass\r\n\r\ndef _ensure_format(v: datasets.IterableDataset) -> datasets.IterableDataset:\r\n    return TorchIterableDataset(v._ex_iterable, v.info, v.split, \"torch\", v._shuffling)\r\n\r\nif __name__ == '__main__':\r\n    freeze_support()\r\n\r\n    ds = datasets.load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\r\n    ds = _ensure_format(ds)\r\n\r\n    model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\r\n    Trainer(model, train_dataset=ds, args=TrainingArguments(\"out\", max_steps=1000, dataloader_num_workers=4)).train()\r\n```\r\n\r\n## Expected results\r\n\r\nI'd expect the dataset to load the url correctly and produce examples.\r\n\r\n## Actual results\r\n```\r\n  warnings.warn(\r\n***** Running training *****\r\n  Num examples = 8000\r\n  Num Epochs = 9223372036854775807\r\n  Instantaneous batch size per device = 8\r\n  Total train batch size (w. parallel, distributed & accumulation) = 8\r\n  Gradient Accumulation steps = 1\r\n  Total optimization steps = 1000\r\n  0%|          | 0/1000 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/Users/dlwh/src/mistral/src/stream_fork_crash.py\", line 22, in <module>\r\n    Trainer(model, train_dataset=ds, args=TrainingArguments(\"out\", max_steps=1000, dataloader_num_workers=4)).train()\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/transformers/trainer.py\", line 1339, in train\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\r\n    data = self._next_data()\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1203, in _next_data\r\n    return self._process_data(data)\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1229, in _process_data\r\n    data.reraise()\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/torch/_utils.py\", line 434, in reraise\r\n    raise exception\r\nFileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\r\n    data.append(next(self.dataset_iter))\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/datasets/iterable_dataset.py\", line 497, in __iter__\r\n    for key, example in self._iter():\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/datasets/iterable_dataset.py\", line 494, in _iter\r\n    yield from ex_iterable\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/datasets/iterable_dataset.py\", line 87, in __iter__\r\n    yield from self.generate_examples_fn(**self.kwargs)\r\n  File \"/Users/dlwh/.cache/huggingface/modules/datasets_modules/datasets/oscar/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2/oscar.py\", line 358, in _generate_examples\r\n    with gzip.open(open(filepath, \"rb\"), \"rt\", encoding=\"utf-8\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'https://s3.amazonaws.com/datasets.huggingface.co/oscar/1.0/unshuffled/deduplicated/en/en_part_1.txt.gz'\r\n\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/multiprocessing/popen_fork.py\", line 27, in poll\r\n    pid, sts = os.waitpid(self.pid, flag)\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 6932) is killed by signal: Terminated: 15. \r\n  0%|          | 0/1000 [00:02<?, ?it/s]\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 2.0.0\r\n- Platform: macOS-12.2-arm64-arm-64bit\r\n- Python version: 3.8.12\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3951/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3950", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3950/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3950/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3950/events", "html_url": "https://github.com/huggingface/datasets/issues/3950", "id": 1171560585, "node_id": "I_kwDODunzps5F1JiJ", "number": 3950, "title": "Streaming Datasets don't work with Transformers Trainer when dataloader_num_workers>1", "user": {"login": "dlwh", "id": 9633, "node_id": "MDQ6VXNlcjk2MzM=", "avatar_url": "https://avatars.githubusercontent.com/u/9633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dlwh", "html_url": "https://github.com/dlwh", "followers_url": "https://api.github.com/users/dlwh/followers", "following_url": "https://api.github.com/users/dlwh/following{/other_user}", "gists_url": "https://api.github.com/users/dlwh/gists{/gist_id}", "starred_url": "https://api.github.com/users/dlwh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dlwh/subscriptions", "organizations_url": "https://api.github.com/users/dlwh/orgs", "repos_url": "https://api.github.com/users/dlwh/repos", "events_url": "https://api.github.com/users/dlwh/events{/privacy}", "received_events_url": "https://api.github.com/users/dlwh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892877, "node_id": "MDU6TGFiZWwxOTM1ODkyODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-16T21:14:11Z", "updated_at": "2022-06-10T20:47:26Z", "closed_at": "2022-06-10T20:47:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nStreaming Datasets can't be pickled, so any interaction between them and multiprocessing results in a crash.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport transformers\r\nfrom transformers import Trainer, AutoModelForCausalLM, TrainingArguments\r\nimport datasets\r\n\r\nds = datasets.load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True).with_format(\"torch\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\r\nTrainer(model, train_dataset=ds, args=TrainingArguments(\"out\", max_steps=1000, dataloader_num_workers=4)).train()\r\n```\r\n## Expected results\r\nFor this code I'd expect a crash related to not having preprocessed the data, but instead we get a pickling error.\r\n\r\n## Actual results\r\n```\r\n  0%|          | 0/1000 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/Users/dlwh/src/mistral/src/stream_fork_crash.py\", line 7, in <module>\r\n    Trainer(model, train_dataset=ds, args=TrainingArguments(\"out\", max_steps=1000, dataloader_num_workers=4)).train()\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/transformers/trainer.py\", line 1339, in train\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 359, in __iter__\r\n    return self._get_iterator()\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 305, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 918, in __init__\r\n    w.start()\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/multiprocessing/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/multiprocessing/context.py\", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/Users/dlwh/.conda/envs/mistral/lib/python3.8/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'iterable_dataset.<locals>.TorchIterableDataset'\r\n  0%|          | 0/1000 [00:00<?, ?it/s]\r\n```\r\n\r\nThis immediate crash can be fixed by not using a local class to make the `TorchIterableDataset` (Note that you have to do with_format(\"torch\") or you get an exception because the dataset has no len) However, any lambdas etc used as maps will also trigger this crash. A more permanent fix would be to move away from multiprocessing and instead use something like pathos or multiprocessing_on_dill (https://stackoverflow.com/questions/19984152/what-can-multiprocessing-and-dill-do-together)\r\n\r\nNote that if you bypass this crash you get another crash. (I'll file a separate bug).\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 2.0.0\r\n- Platform: macOS-12.2-arm64-arm-64bit\r\n- Python version: 3.8.12\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3950/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3950/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3942", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3942/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3942/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3942/events", "html_url": "https://github.com/huggingface/datasets/issues/3942", "id": 1171177122, "node_id": "I_kwDODunzps5Fzr6i", "number": 3942, "title": "reddit_tifu dataset: Checksums didn't match for dataset source files", "user": {"login": "XingxingZhang", "id": 8507585, "node_id": "MDQ6VXNlcjg1MDc1ODU=", "avatar_url": "https://avatars.githubusercontent.com/u/8507585?v=4", "gravatar_id": "", "url": "https://api.github.com/users/XingxingZhang", "html_url": "https://github.com/XingxingZhang", "followers_url": "https://api.github.com/users/XingxingZhang/followers", "following_url": "https://api.github.com/users/XingxingZhang/following{/other_user}", "gists_url": "https://api.github.com/users/XingxingZhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/XingxingZhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/XingxingZhang/subscriptions", "organizations_url": "https://api.github.com/users/XingxingZhang/orgs", "repos_url": "https://api.github.com/users/XingxingZhang/repos", "events_url": "https://api.github.com/users/XingxingZhang/events{/privacy}", "received_events_url": "https://api.github.com/users/XingxingZhang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892865, "node_id": "MDU6TGFiZWwxOTM1ODkyODY1", "url": "https://api.github.com/repos/huggingface/datasets/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-03-16T15:23:30Z", "updated_at": "2022-03-16T15:57:43Z", "closed_at": "2022-03-16T15:39:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen loading the reddit_tifu dataset, it throws the exception \"Checksums didn't match for dataset source files\"\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nfrom datasets import load_dataset\r\n\r\nprint(datasets.__version__)\r\n\r\n# load_dataset('billsum')\r\n\r\nload_dataset('reddit_tifu', 'short')\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.17.0\r\n- Platform: mac os\r\n- Python version: Python 3.7.6\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3942/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3941", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3941/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3941/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3941/events", "html_url": "https://github.com/huggingface/datasets/issues/3941", "id": 1171132709, "node_id": "I_kwDODunzps5FzhEl", "number": 3941, "title": "billsum dataset: Checksums didn't match for dataset source files:", "user": {"login": "XingxingZhang", "id": 8507585, "node_id": "MDQ6VXNlcjg1MDc1ODU=", "avatar_url": "https://avatars.githubusercontent.com/u/8507585?v=4", "gravatar_id": "", "url": "https://api.github.com/users/XingxingZhang", "html_url": "https://github.com/XingxingZhang", "followers_url": "https://api.github.com/users/XingxingZhang/followers", "following_url": "https://api.github.com/users/XingxingZhang/following{/other_user}", "gists_url": "https://api.github.com/users/XingxingZhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/XingxingZhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/XingxingZhang/subscriptions", "organizations_url": "https://api.github.com/users/XingxingZhang/orgs", "repos_url": "https://api.github.com/users/XingxingZhang/repos", "events_url": "https://api.github.com/users/XingxingZhang/events{/privacy}", "received_events_url": "https://api.github.com/users/XingxingZhang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-03-16T14:52:08Z", "updated_at": "2022-03-16T15:57:08Z", "closed_at": "2022-03-16T15:46:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen loading the `billsum` dataset, it throws the exception \"Checksums didn't match for dataset source files\"\r\n\r\n```\r\n  File \"virtualenv_projects/codex/lib/python3.7/site-packages/datasets/utils/info_utils.py\", line 40, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=1g89WgFHMRbr4QrvA0ngh26PY081Nv3lx']\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nfrom datasets import load_dataset\r\n\r\nprint(datasets.__version__)\r\n\r\nload_dataset('billsum')\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.17.0\r\n\r\n- Platform: mac os\r\n- Python version: Python 3.7.6\r\n- PyArrow version: 3.0.0\r\n\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3941/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3939", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3939/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3939/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3939/events", "html_url": "https://github.com/huggingface/datasets/issues/3939", "id": 1170882331, "node_id": "I_kwDODunzps5Fyj8b", "number": 3939, "title": "Source links broken", "user": {"login": "qqaatw", "id": 24835382, "node_id": "MDQ6VXNlcjI0ODM1Mzgy", "avatar_url": "https://avatars.githubusercontent.com/u/24835382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qqaatw", "html_url": "https://github.com/qqaatw", "followers_url": "https://api.github.com/users/qqaatw/followers", "following_url": "https://api.github.com/users/qqaatw/following{/other_user}", "gists_url": "https://api.github.com/users/qqaatw/gists{/gist_id}", "starred_url": "https://api.github.com/users/qqaatw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qqaatw/subscriptions", "organizations_url": "https://api.github.com/users/qqaatw/orgs", "repos_url": "https://api.github.com/users/qqaatw/repos", "events_url": "https://api.github.com/users/qqaatw/events{/privacy}", "received_events_url": "https://api.github.com/users/qqaatw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2022-03-16T11:17:47Z", "updated_at": "2022-03-19T04:41:32Z", "closed_at": "2022-03-19T04:41:32Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThe source links of v2.0.0 docs are broken:\r\n\r\nFor exmaple, clicking the source button of this [class](https://huggingface.co/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.ClassLabel) will direct users to `https://github.com/huggingface/datasets/blob/v2.0.0/src/datasets/features/features.py#L747`\r\n\r\nhere, the `v2.0.0` should be `2.0.0`.\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\n\r\nRedirecting to this link: `https://github.com/huggingface/datasets/blob/2.0.0/src/datasets/features/features.py#L747`\r\n\r\n## Actual results\r\nDescribed above.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3939/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3939/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3929", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3929/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3929/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3929/events", "html_url": "https://github.com/huggingface/datasets/issues/3929", "id": 1170066235, "node_id": "I_kwDODunzps5Fvcs7", "number": 3929, "title": "Load a local dataset twice", "user": {"login": "caush", "id": 28349961, "node_id": "MDQ6VXNlcjI4MzQ5OTYx", "avatar_url": "https://avatars.githubusercontent.com/u/28349961?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caush", "html_url": "https://github.com/caush", "followers_url": "https://api.github.com/users/caush/followers", "following_url": "https://api.github.com/users/caush/following{/other_user}", "gists_url": "https://api.github.com/users/caush/gists{/gist_id}", "starred_url": "https://api.github.com/users/caush/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caush/subscriptions", "organizations_url": "https://api.github.com/users/caush/orgs", "repos_url": "https://api.github.com/users/caush/repos", "events_url": "https://api.github.com/users/caush/events{/privacy}", "received_events_url": "https://api.github.com/users/caush/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-15T18:59:26Z", "updated_at": "2022-03-16T09:55:09Z", "closed_at": "2022-03-16T09:54:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nLoad a local \"dataset\" composed of two csv files twice.\r\n\r\n## Steps to reproduce the bug\r\nPut the two joined files in a repository named \"Data\".\r\nThen in python:\r\n\r\nimport datasets as ds\r\nds.load_dataset('Data', data_files = {'file1.csv', 'file2.csv'})\r\n\r\n## Expected results\r\nShould give something like (because files have only one data row):\r\nTitle, clicks\r\nTruc et astuce, 123\r\nMachin, 12\r\n\r\n## Actual results\r\nGives \r\nTitle, clicks\r\nTruc et astuce, 123\r\nMachin, 12\r\nTruc et astuce, 123\r\nMachin, 12\r\n\r\n## Environment info\r\n[file1.csv](https://github.com/huggingface/datasets/files/8256322/file1.csv)\r\n[file2.csv](https://github.com/huggingface/datasets/files/8256323/file2.csv)\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-5.4.0-65-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.12\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.4.1", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3929/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3928", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3928/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3928/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3928/events", "html_url": "https://github.com/huggingface/datasets/issues/3928", "id": 1170017132, "node_id": "I_kwDODunzps5FvQts", "number": 3928, "title": "Frugal score deprecations", "user": {"login": "ierezell", "id": 30974685, "node_id": "MDQ6VXNlcjMwOTc0Njg1", "avatar_url": "https://avatars.githubusercontent.com/u/30974685?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ierezell", "html_url": "https://github.com/ierezell", "followers_url": "https://api.github.com/users/ierezell/followers", "following_url": "https://api.github.com/users/ierezell/following{/other_user}", "gists_url": "https://api.github.com/users/ierezell/gists{/gist_id}", "starred_url": "https://api.github.com/users/ierezell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ierezell/subscriptions", "organizations_url": "https://api.github.com/users/ierezell/orgs", "repos_url": "https://api.github.com/users/ierezell/repos", "events_url": "https://api.github.com/users/ierezell/events{/privacy}", "received_events_url": "https://api.github.com/users/ierezell/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-15T18:10:42Z", "updated_at": "2022-03-17T08:37:24Z", "closed_at": "2022-03-17T08:37:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe frugal score returns a really verbose output with warnings that can be easily changed. \r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets.load import load_metric\r\n\r\nfrugal = load_metric(\"frugalscore\")\r\nfrugal.compute(predictions=[\"Do you like spinachis\"], references=[\"Do you like spinach\"])\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n```\r\n{'scores': [0.9946]}\r\n```\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n```\r\nPyTorch: setting up devices\r\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 864.09ba/s]\r\nUsing amp half precision backend\r\nThe following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\r\n***** Running Prediction *****\r\n  Num examples = 1\r\n  Batch size = 64\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 4644.85it/s]\r\n {'scores': [0.9946]}\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.17.0\r\n- Platform: Linux-5.13.0-30-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3928/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3928/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3919", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3919/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3919/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3919/events", "html_url": "https://github.com/huggingface/datasets/issues/3919", "id": 1169497210, "node_id": "I_kwDODunzps5FtRx6", "number": 3919, "title": "AttributeError: 'DatasetDict' object has no attribute 'features'", "user": {"login": "jswapnil10", "id": 48145785, "node_id": "MDQ6VXNlcjQ4MTQ1Nzg1", "avatar_url": "https://avatars.githubusercontent.com/u/48145785?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jswapnil10", "html_url": "https://github.com/jswapnil10", "followers_url": "https://api.github.com/users/jswapnil10/followers", "following_url": "https://api.github.com/users/jswapnil10/following{/other_user}", "gists_url": "https://api.github.com/users/jswapnil10/gists{/gist_id}", "starred_url": "https://api.github.com/users/jswapnil10/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jswapnil10/subscriptions", "organizations_url": "https://api.github.com/users/jswapnil10/orgs", "repos_url": "https://api.github.com/users/jswapnil10/repos", "events_url": "https://api.github.com/users/jswapnil10/events{/privacy}", "received_events_url": "https://api.github.com/users/jswapnil10/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-03-15T10:46:59Z", "updated_at": "2022-03-17T04:16:14Z", "closed_at": "2022-03-17T04:16:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nReceiving the error when trying to check for Dataset features\r\n\r\n## Steps to reproduce the bug\r\nfrom datasets import Dataset\r\ndataset = Dataset.from_pandas(df[['id', 'words', 'bboxes', 'ner_tags', 'image_path']])\r\n\r\ndataset.features\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nGetting the following errror\r\n\r\nAttributeError: 'DatasetDict' object has no attribute 'features'\r\n\r\n## Environment info\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 1.18.4\r\n- Platform: Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9\r\n- Python version: 3.6.13\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3919/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3919/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3918", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3918/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3918/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3918/events", "html_url": "https://github.com/huggingface/datasets/issues/3918", "id": 1169366117, "node_id": "I_kwDODunzps5Fsxxl", "number": 3918, "title": "datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files", "user": {"login": "willowdong", "id": 51409295, "node_id": "MDQ6VXNlcjUxNDA5Mjk1", "avatar_url": "https://avatars.githubusercontent.com/u/51409295?v=4", "gravatar_id": "", "url": "https://api.github.com/users/willowdong", "html_url": "https://github.com/willowdong", "followers_url": "https://api.github.com/users/willowdong/followers", "following_url": "https://api.github.com/users/willowdong/following{/other_user}", "gists_url": "https://api.github.com/users/willowdong/gists{/gist_id}", "starred_url": "https://api.github.com/users/willowdong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/willowdong/subscriptions", "organizations_url": "https://api.github.com/users/willowdong/orgs", "repos_url": "https://api.github.com/users/willowdong/repos", "events_url": "https://api.github.com/users/willowdong/events{/privacy}", "received_events_url": "https://api.github.com/users/willowdong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892865, "node_id": "MDU6TGFiZWwxOTM1ODkyODY1", "url": "https://api.github.com/repos/huggingface/datasets/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-03-15T08:53:45Z", "updated_at": "2022-03-16T15:36:58Z", "closed_at": "2022-03-15T14:01:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nCan't load the dataset\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('multi_news')\r\ndataset_2=load_dataset(\"reddit_tifu\", \"long\")\r\n\r\n\r\n## Actual results\r\n raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=1ffWfITKFMJeqjT8loC8aiCLRNJpc_XnF']\r\n\r\n## Environment info\r\n- `datasets` version: 1.18.4\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.0\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3918/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3918/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3909", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3909/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3909/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3909/events", "html_url": "https://github.com/huggingface/datasets/issues/3909", "id": 1168578058, "node_id": "I_kwDODunzps5FpxYK", "number": 3909, "title": "Error loading file audio when downloading the Common Voice dataset directly from the Hub", "user": {"login": "aliceinland", "id": 30385910, "node_id": "MDQ6VXNlcjMwMzg1OTEw", "avatar_url": "https://avatars.githubusercontent.com/u/30385910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aliceinland", "html_url": "https://github.com/aliceinland", "followers_url": "https://api.github.com/users/aliceinland/followers", "following_url": "https://api.github.com/users/aliceinland/following{/other_user}", "gists_url": "https://api.github.com/users/aliceinland/gists{/gist_id}", "starred_url": "https://api.github.com/users/aliceinland/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aliceinland/subscriptions", "organizations_url": "https://api.github.com/users/aliceinland/orgs", "repos_url": "https://api.github.com/users/aliceinland/repos", "events_url": "https://api.github.com/users/aliceinland/events{/privacy}", "received_events_url": "https://api.github.com/users/aliceinland/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2022-03-14T15:53:50Z", "updated_at": "2023-03-02T15:31:27Z", "closed_at": "2023-03-02T15:31:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen loading the Common_Voice dataset, by downloading it directly from the Hugging Face hub, some files can not be opened. \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport torch\r\nimport torchaudio\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\r\nimport re\r\n\r\ntest_dataset = load_dataset(\"common_voice\", \"it\", split=\"test\")\r\n#test_dataset = load_dataset('csv', data_files = {'test': '/workspace/Dataset/Common_Voice/cv-corpus80/it/test.csv'})\r\nwer = load_metric(\"wer\")\r\n\r\nprocessor = Wav2Vec2Processor.from_pretrained(\"joorock12/wav2vec2-large-xlsr-italian\")\r\nmodel = Wav2Vec2ForCTC.from_pretrained(\"joorock12/wav2vec2-large-xlsr-italian\")\r\nmodel.to(\"cuda\")\r\n\r\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c\\'\\\ufffd]'\r\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\r\n```\r\n## Expected results\r\nThe common voice dataset downloaded and correctly loaded whit the use of the hugging face datasets library. \r\n\r\n## Actual results\r\nThe error is:\r\n```python\r\n0ex [00:00, ?ex/s]\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-48-ef87f4129e6e> in <module>\r\n      7     return batch\r\n      8 \r\n----> 9 test_dataset = test_dataset.map(speech_file_to_array_fn)\r\n\r\n/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   2107 \r\n   2108         if num_proc is None or num_proc == 1:\r\n-> 2109             return self._map_single(\r\n   2110                 function=function,\r\n   2111                 with_indices=with_indices,\r\n\r\n/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    516             self: \"Dataset\" = kwargs.pop(\"self\")\r\n    517         # apply actual function\r\n--> 518         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    519         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    520         for dataset in datasets:\r\n\r\n/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    483         }\r\n    484         # apply actual function\r\n--> 485         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    486         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    487         # re-apply format to the output\r\n\r\n/opt/conda/lib/python3.8/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    411             # Call actual function\r\n    412 \r\n--> 413             out = func(self, *args, **kwargs)\r\n    414 \r\n    415             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\r\n   2465                 if not batched:\r\n   2466                     for i, example in enumerate(pbar):\r\n-> 2467                         example = apply_function_on_filtered_inputs(example, i, offset=offset)\r\n   2468                         if update_data:\r\n   2469                             if i == 0:\r\n\r\n/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py in apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples, offset)\r\n   2372             if with_rank:\r\n   2373                 additional_args += (rank,)\r\n-> 2374             processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n   2375             if update_data is None:\r\n   2376                 # Check if the function returns updated examples\r\n\r\n/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py in decorated(item, *args, **kwargs)\r\n   2067                 )\r\n   2068                 # Use the LazyDict internally, while mapping the function\r\n-> 2069                 result = f(decorated_item, *args, **kwargs)\r\n   2070                 # Return a standard dict\r\n   2071                 return result.data if isinstance(result, LazyDict) else result\r\n\r\n<ipython-input-48-ef87f4129e6e> in speech_file_to_array_fn(batch)\r\n      3 def speech_file_to_array_fn(batch):\r\n      4     batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\r\n----> 5     speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\r\n      6     batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\r\n      7     return batch\r\n\r\n/opt/conda/lib/python3.8/site-packages/torchaudio/backend/sox_io_backend.py in load(filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n    150                 filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n    151         filepath = os.fspath(filepath)\r\n--> 152     return torch.ops.torchaudio.sox_io_load_audio_file(\r\n    153         filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n    154 \r\n\r\nRuntimeError: Error loading audio file: failed to open file common_voice_it_17415776.mp3  ```\r\n\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.18.4\r\n- Platform: Linux-5.4.0-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 7.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3909/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3909/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3906", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3906/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3906/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3906/events", "html_url": "https://github.com/huggingface/datasets/issues/3906", "id": 1168496328, "node_id": "I_kwDODunzps5FpdbI", "number": 3906, "title": "NonMatchingChecksumError on Spider dataset ", "user": {"login": "kolk", "id": 9049591, "node_id": "MDQ6VXNlcjkwNDk1OTE=", "avatar_url": "https://avatars.githubusercontent.com/u/9049591?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kolk", "html_url": "https://github.com/kolk", "followers_url": "https://api.github.com/users/kolk/followers", "following_url": "https://api.github.com/users/kolk/following{/other_user}", "gists_url": "https://api.github.com/users/kolk/gists{/gist_id}", "starred_url": "https://api.github.com/users/kolk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kolk/subscriptions", "organizations_url": "https://api.github.com/users/kolk/orgs", "repos_url": "https://api.github.com/users/kolk/repos", "events_url": "https://api.github.com/users/kolk/events{/privacy}", "received_events_url": "https://api.github.com/users/kolk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-03-14T14:54:53Z", "updated_at": "2022-03-15T07:09:51Z", "closed_at": "2022-03-15T07:09:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nFailure to generate dataset ```spider``` because of checksums error for dataset source files.\r\n\r\n## Steps to reproduce the bug\r\n```\r\nfrom datasets import load_dataset\r\nspider = load_dataset(\"spider\")\r\n```\r\n\r\n\r\n## Expected results\r\nChecksums should match for files from url ['https://drive.google.com/uc?export=download&id=1_AckYkinAnhqmRQtGsQgUKAnTHxxX5J0']\r\n\r\n## Actual results\r\n```\r\n>>> load_dataset(\"spider\")\r\nload_dataset(\"spider\")\r\nDownloading and preparing dataset spider/spider (download: 95.12 MiB, generated: 5.17 MiB, post-processed: Unknown size, total: 100.29 MiB) to /home/user/.cache/huggingface/datasets/spider/spider/1.0.0/79778ebea87c59b19411f1eb3eda317e9dd5f7788a556d837ef25c3ae6e5e8b7...\r\nTraceback (most recent call last):\r\n  File \"/home/user/py3_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-5-d4cb54197348>\", line 1, in <module>\r\n    load_dataset(\"spider\")\r\n  File \"/home/user/py3_env/lib/python3.8/site-packages/datasets/load.py\", line 1702, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/user/py3_env/lib/python3.8/site-packages/datasets/builder.py\", line 594, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/user/py3_env/lib/python3.8/site-packages/datasets/builder.py\", line 665, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/home/user/py3_env/lib/python3.8/site-packages/datasets/utils/info_utils.py\", line 40, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=1_AckYkinAnhqmRQtGsQgUKAnTHxxX5J0']\r\n```\r\n\r\n\r\n## Environment info\r\ndatasets version: 1.18.3\r\nPlatform: Ubuntu 20 LTS\r\nPython version: 3.8.10\r\nPyArrow version: 6.0.1 \r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3906/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3906/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3902", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3902/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3902/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3902/events", "html_url": "https://github.com/huggingface/datasets/issues/3902", "id": 1167403377, "node_id": "I_kwDODunzps5FlSlx", "number": 3902, "title": "Can't import datasets: partially initialized module 'fsspec' has no attribute 'utils'", "user": {"login": "arunasank", "id": 3166852, "node_id": "MDQ6VXNlcjMxNjY4NTI=", "avatar_url": "https://avatars.githubusercontent.com/u/3166852?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arunasank", "html_url": "https://github.com/arunasank", "followers_url": "https://api.github.com/users/arunasank/followers", "following_url": "https://api.github.com/users/arunasank/following{/other_user}", "gists_url": "https://api.github.com/users/arunasank/gists{/gist_id}", "starred_url": "https://api.github.com/users/arunasank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arunasank/subscriptions", "organizations_url": "https://api.github.com/users/arunasank/orgs", "repos_url": "https://api.github.com/users/arunasank/repos", "events_url": "https://api.github.com/users/arunasank/events{/privacy}", "received_events_url": "https://api.github.com/users/arunasank/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-03-12T21:22:03Z", "updated_at": "2023-02-09T14:53:49Z", "closed_at": "2022-03-22T07:10:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nUnable to import datasets\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import Dataset, DatasetDict\r\n```\r\n\r\n## Expected results\r\nThe import works without errors\r\n\r\n## Actual results\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-37-c8cfcbe62127> in <module>\r\n     11 # from tqdm import tqdm\r\n     12 # import torch\r\n---> 13 from datasets import Dataset\r\n     14 # from transformers import Trainer, TrainingArguments, AutoModel, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\r\n     15 # from sentence_transformers import SentenceTransformer\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/__init__.py in <module>\r\n     31     )\r\n     32 \r\n---> 33 from .arrow_dataset import Dataset, concatenate_datasets\r\n     34 from .arrow_reader import ArrowReader, ReadInstruction\r\n     35 from .arrow_writer import ArrowWriter\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py in <module>\r\n     46 )\r\n     47 \r\n---> 48 import fsspec\r\n     49 import numpy as np\r\n     50 import pandas as pd\r\n\r\n~/.local/lib/python3.8/site-packages/fsspec/__init__.py in <module>\r\n     10 from . import _version, caching\r\n     11 from .callbacks import Callback\r\n---> 12 from .core import get_fs_token_paths, open, open_files, open_local\r\n     13 from .exceptions import FSTimeoutError\r\n     14 from .mapping import FSMap, get_mapper\r\n\r\n~/.local/lib/python3.8/site-packages/fsspec/core.py in <module>\r\n     16     caches,\r\n     17 )\r\n---> 18 from .compression import compr\r\n     19 from .registry import filesystem, get_filesystem_class\r\n     20 from .utils import (\r\n\r\n~/.local/lib/python3.8/site-packages/fsspec/compression.py in <module>\r\n     68 \r\n     69 \r\n---> 70 register_compression(\"zip\", unzip, \"zip\")\r\n     71 register_compression(\"bz2\", BZ2File, \"bz2\")\r\n     72 \r\n\r\n~/.local/lib/python3.8/site-packages/fsspec/compression.py in register_compression(name, callback, extensions, force)\r\n     44 \r\n     45     for ext in extensions:\r\n---> 46         if ext in fsspec.utils.compressions and not force:\r\n     47             raise ValueError(\r\n     48                 \"Duplicate compression file extension: %s (%s)\" % (ext, name)\r\n\r\nAttributeError: partially initialized module 'fsspec' has no attribute 'utils' (most likely due to a circular import)\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.4\r\n- Platform: Jupyter notebook\r\n- Python version: 3.8.10\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3902/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3883", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3883/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3883/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3883/events", "html_url": "https://github.com/huggingface/datasets/issues/3883", "id": 1164663229, "node_id": "I_kwDODunzps5Fa1m9", "number": 3883, "title": "The metric Meteor doesn't work for nltk ==3.6.4", "user": {"login": "zhaowei-wang-nlp", "id": 22047467, "node_id": "MDQ6VXNlcjIyMDQ3NDY3", "avatar_url": "https://avatars.githubusercontent.com/u/22047467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhaowei-wang-nlp", "html_url": "https://github.com/zhaowei-wang-nlp", "followers_url": "https://api.github.com/users/zhaowei-wang-nlp/followers", "following_url": "https://api.github.com/users/zhaowei-wang-nlp/following{/other_user}", "gists_url": "https://api.github.com/users/zhaowei-wang-nlp/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhaowei-wang-nlp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhaowei-wang-nlp/subscriptions", "organizations_url": "https://api.github.com/users/zhaowei-wang-nlp/orgs", "repos_url": "https://api.github.com/users/zhaowei-wang-nlp/repos", "events_url": "https://api.github.com/users/zhaowei-wang-nlp/events{/privacy}", "received_events_url": "https://api.github.com/users/zhaowei-wang-nlp/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-10T02:28:27Z", "updated_at": "2022-03-10T09:03:39Z", "closed_at": "2022-03-10T09:03:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nUsing the metric Meteor with nltk == 3.6.4 gives a TypeError:\r\nTypeError: descriptor 'lower' for 'str' objects doesn't apply to a 'list' object\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nmetric = datasets.load_metric(\"meteor\")\r\npredictions = [\"hello world\"]\r\nreferences = [\"hello world\"]\r\nmetric.compute(predictions=predictions, references=references)\r\n```\r\n\r\n## Expected results\r\nTypeError: descriptor 'lower' for 'str' objects doesn't apply to a 'list' object\r\n\r\nI think this TypeError exists because input sentences are tokenized into lists of tokens and the str.lower() is applied to this list of tokens.\r\n\r\n## Actual results\r\nNo error but a meteor score\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: linux\r\n- Python version: 3.8.12\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3883/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3883/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3859", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3859/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3859/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3859/events", "html_url": "https://github.com/huggingface/datasets/issues/3859", "id": 1162559333, "node_id": "I_kwDODunzps5FSz9l", "number": 3859, "title": "Unable to dowload big_patent (FileNotFoundError)", "user": {"login": "slvcsl", "id": 25265140, "node_id": "MDQ6VXNlcjI1MjY1MTQw", "avatar_url": "https://avatars.githubusercontent.com/u/25265140?v=4", "gravatar_id": "", "url": "https://api.github.com/users/slvcsl", "html_url": "https://github.com/slvcsl", "followers_url": "https://api.github.com/users/slvcsl/followers", "following_url": "https://api.github.com/users/slvcsl/following{/other_user}", "gists_url": "https://api.github.com/users/slvcsl/gists{/gist_id}", "starred_url": "https://api.github.com/users/slvcsl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/slvcsl/subscriptions", "organizations_url": "https://api.github.com/users/slvcsl/orgs", "repos_url": "https://api.github.com/users/slvcsl/repos", "events_url": "https://api.github.com/users/slvcsl/events{/privacy}", "received_events_url": "https://api.github.com/users/slvcsl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892865, "node_id": "MDU6TGFiZWwxOTM1ODkyODY1", "url": "https://api.github.com/repos/huggingface/datasets/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-08T11:47:12Z", "updated_at": "2022-03-08T13:04:09Z", "closed_at": "2022-03-08T13:04:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to download some splits of the big_patent dataset, using the following code:\r\n\r\n`ds = load_dataset(\"big_patent\", \"g\", split=\"validation\", download_mode=\"force_redownload\")\r\n`\r\nHowever, this leads to a FileNotFoundError.\r\n\r\nFileNotFoundError                         Traceback (most recent call last)\r\n[<ipython-input-3-8d8a745706a9>](https://localhost:8080/#) in <module>()\r\n      1 from datasets import load_dataset\r\n----> 2 ds = load_dataset(\"big_patent\", \"g\", split=\"validation\", download_mode=\"force_redownload\")\r\n\r\n8 frames\r\n[/usr/local/lib/python3.7/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, script_version, **config_kwargs)\r\n   1705         ignore_verifications=ignore_verifications,\r\n   1706         try_from_hf_gcs=try_from_hf_gcs,\r\n-> 1707         use_auth_token=use_auth_token,\r\n   1708     )\r\n   1709 \r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/builder.py](https://localhost:8080/#) in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    593                     if not downloaded_from_gcs:\r\n    594                         self._download_and_prepare(\r\n--> 595                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    596                         )\r\n    597                     # Sync info\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/builder.py](https://localhost:8080/#) in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    659         split_dict = SplitDict(dataset_name=self.name)\r\n    660         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 661         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    662 \r\n    663         # Checksums verification\r\n\r\n[/root/.cache/huggingface/modules/datasets_modules/datasets/big_patent/bdefa7c0b39fba8bba1c6331b70b738e30d63c8ad4567f983ce315a5fef6131c/big_patent.py](https://localhost:8080/#) in _split_generators(self, dl_manager)\r\n    123         split_types = [\"train\", \"val\", \"test\"]\r\n    124         extract_paths = dl_manager.extract(\r\n--> 125             {k: os.path.join(dl_path, \"bigPatentData\", k + \".tar.gz\") for k in split_types}\r\n    126         )\r\n    127         extract_paths = {k: os.path.join(extract_paths[k], k) for k in split_types}\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/utils/download_manager.py](https://localhost:8080/#) in extract(self, path_or_paths, num_proc)\r\n    282         download_config.extract_compressed_file = True\r\n    283         extracted_paths = map_nested(\r\n--> 284             partial(cached_path, download_config=download_config), path_or_paths, num_proc=num_proc, disable_tqdm=False\r\n    285         )\r\n    286         path_or_paths = NestedDataStructure(path_or_paths)\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/utils/py_utils.py](https://localhost:8080/#) in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm)\r\n    260         mapped = [\r\n    261             _single_map_nested((function, obj, types, None, True))\r\n--> 262             for obj in utils.tqdm(iterable, disable=disable_tqdm)\r\n    263         ]\r\n    264     else:\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/utils/py_utils.py](https://localhost:8080/#) in <listcomp>(.0)\r\n    260         mapped = [\r\n    261             _single_map_nested((function, obj, types, None, True))\r\n--> 262             for obj in utils.tqdm(iterable, disable=disable_tqdm)\r\n    263         ]\r\n    264     else:\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/utils/py_utils.py](https://localhost:8080/#) in _single_map_nested(args)\r\n    194     # Singleton first to spare some computation\r\n    195     if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 196         return function(data_struct)\r\n    197 \r\n    198     # Reduce logging to keep things readable in multiprocessing with tqdm\r\n\r\n[/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py](https://localhost:8080/#) in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    314     elif is_local_path(url_or_filename):\r\n    315         # File, but it doesn't exist.\r\n--> 316         raise FileNotFoundError(f\"Local file {url_or_filename} doesn't exist\")\r\n    317     else:\r\n    318         # Something unknown\r\n\r\nFileNotFoundError: Local file /root/.cache/huggingface/datasets/downloads/extracted/ad068abb3e11f9f2f5440b62e37eb2b03ee515df9de1637c55cd1793b68668b2/bigPatentData/train.tar.gz doesn't exist\r\n\r\nI have tried this in a number of machines, including on Colab, so I think this is not environment dependent.\r\n\r\nHow do I load the bigPatent dataset?", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3859/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3859/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3855", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3855/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3855/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3855/events", "html_url": "https://github.com/huggingface/datasets/issues/3855", "id": 1162448589, "node_id": "I_kwDODunzps5FSY7N", "number": 3855, "title": "Bad error message when loading private dataset", "user": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-03-08T09:55:17Z", "updated_at": "2022-07-11T15:06:40Z", "closed_at": "2022-07-11T15:06:40Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nA pretty common behavior of an interaction between the Hub and datasets is the following.\r\nAn organization adds a dataset in private mode and wants to load it afterward.\r\n\r\n\r\n```python\r\nfrom transformers import load_dataset\r\n\r\nds = load_dataset(\"NewT5/dummy_data\", \"dummy\")\r\n```\r\n\r\nThis command then fails with:\r\n\r\n```bash\r\nFileNotFoundError: Couldn't find a dataset script at /home/patrick/NewT5/dummy_data/dummy_data.py or any data file in the same directory. Couldn't find 'NewT5/dummy_data' on the Hugging Face Hub either: FileNotFoundError: Dataset 'NewT5/dummy_data' doesn't exist on the Hub\r\n```\r\n\r\n**even though** the user has access to the website `NewT5/dummy_data` since she/he is part of the org. \r\n\r\nWe need to improve the error message here similar to how @sgugger, @LysandreJik and @julien-c have done it for transformers IMO.\r\n\r\n## Steps to reproduce the bug\r\n\r\nE.g. execute the following code to see the different error messages between `transformes` and `datasets`.\r\n\r\n1. Transformers\r\n```python\r\nfrom transformers import BertModel\r\n\r\nBertModel.from_pretrained(\"NewT5/dummy_model\")\r\n```\r\n\r\nThe error message is clearer here - it gives:\r\n\r\n```\r\nOSError: patrickvonplaten/gpt2-xl is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\r\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.\r\n```\r\n\r\nLet's maybe do the same for datasets? The PR was introduced to `transformers` here:\r\nhttps://github.com/huggingface/transformers/pull/15261\r\n\r\n## Expected results\r\n\r\nBetter error message\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.4.dev0\r\n- Platform: Linux-5.15.15-76051515-generic-x86_64-with-glibc2.34\r\n- Python version: 3.9.7\r\n- PyArrow version: 6.0.1\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3855/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3855/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3851", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3851/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3851/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3851/events", "html_url": "https://github.com/huggingface/datasets/issues/3851", "id": 1162137998, "node_id": "I_kwDODunzps5FRNGO", "number": 3851, "title": "Load audio dataset error", "user": {"login": "lemoner20", "id": 31890987, "node_id": "MDQ6VXNlcjMxODkwOTg3", "avatar_url": "https://avatars.githubusercontent.com/u/31890987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lemoner20", "html_url": "https://github.com/lemoner20", "followers_url": "https://api.github.com/users/lemoner20/followers", "following_url": "https://api.github.com/users/lemoner20/following{/other_user}", "gists_url": "https://api.github.com/users/lemoner20/gists{/gist_id}", "starred_url": "https://api.github.com/users/lemoner20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lemoner20/subscriptions", "organizations_url": "https://api.github.com/users/lemoner20/orgs", "repos_url": "https://api.github.com/users/lemoner20/repos", "events_url": "https://api.github.com/users/lemoner20/events{/privacy}", "received_events_url": "https://api.github.com/users/lemoner20/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2022-03-08T02:16:04Z", "updated_at": "2022-09-27T12:13:55Z", "closed_at": "2022-03-08T11:20:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Load audio dataset error\r\n\r\nHi, when I load audio dataset following https://huggingface.co/docs/datasets/audio_process and https://github.com/huggingface/datasets/tree/master/datasets/superb,\r\n```\r\nfrom datasets import load_dataset, load_metric, Audio\r\nraw_datasets = load_dataset(\"superb\", \"ks\", split=\"train\")\r\nprint(raw_datasets[0][\"audio\"])\r\n```\r\nfollowing errors occur \r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-169-3f8253239fa0> in <module>\r\n----> 1 raw_datasets[0][\"audio\"]\r\n\r\n/usr/lib/python3.6/site-packages/datasets/arrow_dataset.py in __getitem__(self, key)\r\n   1924         \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n   1925         return self._getitem(\r\n-> 1926             key,\r\n   1927         )\r\n   1928 \r\n\r\n/usr/lib/python3.6/site-packages/datasets/arrow_dataset.py in _getitem(self, key, decoded, **kwargs)\r\n   1909         pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n   1910         formatted_output = format_table(\r\n-> 1911             pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   1912         )\r\n   1913         return formatted_output\r\n\r\n/usr/lib/python3.6/site-packages/datasets/formatting/formatting.py in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    530     python_formatter = PythonFormatter(features=None)\r\n    531     if format_columns is None:\r\n--> 532         return formatter(pa_table, query_type=query_type)\r\n    533     elif query_type == \"column\":\r\n    534         if key in format_columns:\r\n\r\n/usr/lib/python3.6/site-packages/datasets/formatting/formatting.py in __call__(self, pa_table, query_type)\r\n    279     def __call__(self, pa_table: pa.Table, query_type: str) -> Union[RowFormat, ColumnFormat, BatchFormat]:\r\n    280         if query_type == \"row\":\r\n--> 281             return self.format_row(pa_table)\r\n    282         elif query_type == \"column\":\r\n    283             return self.format_column(pa_table)\r\n\r\n/usr/lib/python3.6/site-packages/datasets/formatting/formatting.py in format_row(self, pa_table)\r\n    310         row = self.python_arrow_extractor().extract_row(pa_table)\r\n    311         if self.decoded:\r\n--> 312             row = self.python_features_decoder.decode_row(row)\r\n    313         return row\r\n    314 \r\n\r\n/usr/lib/python3.6/site-packages/datasets/formatting/formatting.py in decode_row(self, row)\r\n    219 \r\n    220     def decode_row(self, row: dict) -> dict:\r\n--> 221         return self.features.decode_example(row) if self.features else row\r\n    222 \r\n    223     def decode_column(self, column: list, column_name: str) -> list:\r\n\r\n/usr/lib/python3.6/site-packages/datasets/features/features.py in decode_example(self, example)\r\n   1320             else value\r\n   1321             for column_name, (feature, value) in utils.zip_dict(\r\n-> 1322                 {key: value for key, value in self.items() if key in example}, example\r\n   1323             )\r\n   1324         }\r\n\r\n/usr/lib/python3.6/site-packages/datasets/features/features.py in <dictcomp>(.0)\r\n   1319             if self._column_requires_decoding[column_name]\r\n   1320             else value\r\n-> 1321             for column_name, (feature, value) in utils.zip_dict(\r\n   1322                 {key: value for key, value in self.items() if key in example}, example\r\n   1323             )\r\n\r\n/usr/lib/python3.6/site-packages/datasets/features/features.py in decode_nested_example(schema, obj)\r\n   1053     # Object with special decoding:\r\n   1054     elif isinstance(schema, (Audio, Image)):\r\n-> 1055         return schema.decode_example(obj) if obj is not None else None\r\n   1056     return obj\r\n   1057 \r\n\r\n/usr/lib/python3.6/site-packages/datasets/features/audio.py in decode_example(self, value)\r\n    100                 array, sampling_rate = self._decode_non_mp3_file_like(file)\r\n    101             else:\r\n--> 102                 array, sampling_rate = self._decode_non_mp3_path_like(path)\r\n    103         return {\"path\": path, \"array\": array, \"sampling_rate\": sampling_rate}\r\n    104 \r\n\r\n/usr/lib/python3.6/site-packages/datasets/features/audio.py in _decode_non_mp3_path_like(self, path)\r\n    143 \r\n    144         with xopen(path, \"rb\") as f:\r\n--> 145             array, sampling_rate = librosa.load(f, sr=self.sampling_rate, mono=self.mono)\r\n    146         return array, sampling_rate\r\n    147 \r\n\r\n/usr/lib/python3.6/site-packages/librosa/core/audio.py in load(path, sr, mono, offset, duration, dtype, res_type)\r\n    110 \r\n    111     y = []\r\n--> 112     with audioread.audio_open(os.path.realpath(path)) as input_file:\r\n    113         sr_native = input_file.samplerate\r\n    114         n_channels = input_file.channels\r\n\r\n/usr/lib/python3.6/posixpath.py in realpath(filename)\r\n    392     \"\"\"Return the canonical path of the specified filename, eliminating any\r\n    393 symbolic links encountered in the path.\"\"\"\r\n--> 394     filename = os.fspath(filename)\r\n    395     path, ok = _joinrealpath(filename[:0], filename, {})\r\n    396     return abspath(path)\r\n\r\nTypeError: expected str, bytes or os.PathLike object, not _io.BufferedReader\r\n```\r\n\r\n## Expected results\r\n```\r\n>>> raw_datasets[0][\"audio\"]\r\n{'array': array([-0.0005188 , -0.00109863,  0.00030518, ...,  0.01730347,\r\n        0.01623535,  0.01724243]),\r\n'path': '/root/.cache/huggingface/datasets/downloads/extracted/bb3a06b491a64aff422f307cd8116820b4f61d6f32fcadcfc554617e84383cb7/bed/026290a7_nohash_0.wav',\r\n'sampling_rate': 16000}\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3851/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3851/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3848", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3848/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3848/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3848/events", "html_url": "https://github.com/huggingface/datasets/issues/3848", "id": 1162076902, "node_id": "I_kwDODunzps5FQ-Lm", "number": 3848, "title": "NonMatchingChecksumError when checksum is None", "user": {"login": "jxmorris12", "id": 13238952, "node_id": "MDQ6VXNlcjEzMjM4OTUy", "avatar_url": "https://avatars.githubusercontent.com/u/13238952?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jxmorris12", "html_url": "https://github.com/jxmorris12", "followers_url": "https://api.github.com/users/jxmorris12/followers", "following_url": "https://api.github.com/users/jxmorris12/following{/other_user}", "gists_url": "https://api.github.com/users/jxmorris12/gists{/gist_id}", "starred_url": "https://api.github.com/users/jxmorris12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jxmorris12/subscriptions", "organizations_url": "https://api.github.com/users/jxmorris12/orgs", "repos_url": "https://api.github.com/users/jxmorris12/repos", "events_url": "https://api.github.com/users/jxmorris12/events{/privacy}", "received_events_url": "https://api.github.com/users/jxmorris12/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2022-03-08T00:24:12Z", "updated_at": "2022-03-15T14:37:26Z", "closed_at": "2022-03-15T12:28:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I ran into the following error when adding a new dataset:\r\n\r\n```bash\r\nexpected_checksums = {'https://adversarialglue.github.io/dataset/dev.zip': {'checksum': None, 'num_bytes': 40662}}\r\nrecorded_checksums = {'https://adversarialglue.github.io/dataset/dev.zip': {'checksum': 'efb4cbd3aa4a87bfaffc310ae951981cc0a36c6c71c6425dd74e5b55f2f325c9', 'num_bytes': 40662}}\r\nverification_name = 'dataset source files'\r\n\r\n    def verify_checksums(expected_checksums: Optional[dict], recorded_checksums: dict, verification_name=None):\r\n        if expected_checksums is None:\r\n            logger.info(\"Unable to verify checksums.\")\r\n            return\r\n        if len(set(expected_checksums) - set(recorded_checksums)) > 0:\r\n            raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))\r\n        if len(set(recorded_checksums) - set(expected_checksums)) > 0:\r\n            raise UnexpectedDownloadedFile(str(set(recorded_checksums) - set(expected_checksums)))\r\n        bad_urls = [url for url in expected_checksums if expected_checksums[url] != recorded_checksums[url]]\r\n        for_verification_name = \" for \" + verification_name if verification_name is not None else \"\"\r\n        if len(bad_urls) > 0:\r\n            error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n>           raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\nE           datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\nE           ['https://adversarialglue.github.io/dataset/dev.zip']\r\n\r\nsrc/datasets/utils/info_utils.py:40: NonMatchingChecksumError\r\n```\r\n\r\n## Expected results\r\nThe dataset downloads correctly, and there is no error.\r\n\r\n## Actual results\r\nDatasets library is looking for a checksum of None, and it gets a non-None checksum, and throws an error. This is clearly a bug.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3848/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3841", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3841/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3841/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3841/events", "html_url": "https://github.com/huggingface/datasets/issues/3841", "id": 1161203842, "node_id": "I_kwDODunzps5FNpCC", "number": 3841, "title": "Pyright reportPrivateImportUsage when `from datasets import load_dataset` ", "user": {"login": "lkhphuc", "id": 12573521, "node_id": "MDQ6VXNlcjEyNTczNTIx", "avatar_url": "https://avatars.githubusercontent.com/u/12573521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lkhphuc", "html_url": "https://github.com/lkhphuc", "followers_url": "https://api.github.com/users/lkhphuc/followers", "following_url": "https://api.github.com/users/lkhphuc/following{/other_user}", "gists_url": "https://api.github.com/users/lkhphuc/gists{/gist_id}", "starred_url": "https://api.github.com/users/lkhphuc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lkhphuc/subscriptions", "organizations_url": "https://api.github.com/users/lkhphuc/orgs", "repos_url": "https://api.github.com/users/lkhphuc/repos", "events_url": "https://api.github.com/users/lkhphuc/events{/privacy}", "received_events_url": "https://api.github.com/users/lkhphuc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-03-07T10:24:04Z", "updated_at": "2023-02-18T19:14:03Z", "closed_at": "2023-02-13T13:48:41Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nPyright complains about module not exported. \r\n\r\n## Steps to reproduce the bug\r\nUse an editor/IDE with Pyright Language server with default configuration:\r\n```python\r\nfrom datasets import load_dataset\r\n```\r\n\r\n## Expected results\r\nNo complain from Pyright\r\n\r\n## Actual results\r\nPyright complain below:\r\n```\r\n`load_dataset` is not exported from module \"datasets\"\r\n Import from \"datasets.load\" instead [reportPrivateImportUsage] \r\n```\r\nImporting from `datasets.load` does indeed solves the problem but I believe importing directly from top level `datasets` is the intended usage per the documentation.\r\n\r\n## Environment info\r\n- `datasets` version: 1.18.3\r\n- Platform: macOS-12.2.1-arm64-arm-64bit\r\n- Python version: 3.9.10\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3841/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3841/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3839", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3839/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3839/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3839/events", "html_url": "https://github.com/huggingface/datasets/issues/3839", "id": 1161183482, "node_id": "I_kwDODunzps5FNkD6", "number": 3839, "title": "CI is broken for Windows", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-03-07T10:06:42Z", "updated_at": "2022-05-20T14:13:43Z", "closed_at": "2022-03-07T10:07:24Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nSee: https://app.circleci.com/pipelines/github/huggingface/datasets/10292/workflows/83de4a55-bff7-43ec-96f7-0c335af5c050/jobs/63355\r\n\r\n```\r\n___________________ test_datasetdict_from_text_split[test] ____________________\r\n[gw0] win32 -- Python 3.7.11 C:\\tools\\miniconda3\\envs\\py37\\python.exe\r\n\r\nsplit = 'test'\r\ntext_path = 'C:\\\\Users\\\\circleci\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-circleci\\\\pytest-0\\\\popen-gw0\\\\data6\\\\dataset.txt'\r\ntmp_path = WindowsPath('C:/Users/circleci/AppData/Local/Temp/pytest-of-circleci/pytest-0/popen-gw0/test_datasetdict_from_text_spl7')\r\n\r\n    @pytest.mark.parametrize(\"split\", [None, NamedSplit(\"train\"), \"train\", \"test\"])\r\n    def test_datasetdict_from_text_split(split, text_path, tmp_path):\r\n        if split:\r\n            path = {split: text_path}\r\n        else:\r\n            split = \"train\"\r\n            path = {\"train\": text_path, \"test\": text_path}\r\n        cache_dir = tmp_path / \"cache\"\r\n        expected_features = {\"text\": \"string\"}\r\n>       dataset = TextDatasetReader(path, cache_dir=cache_dir).read()\r\n\r\ntests\\io\\test_text.py:118: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\datasets\\io\\text.py:43: in read\r\n    use_auth_token=use_auth_token,\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\datasets\\builder.py:588: in download_and_prepare\r\n    self._download_prepared_from_hf_gcs(dl_manager.download_config)\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\datasets\\builder.py:630: in _download_prepared_from_hf_gcs\r\n    reader.download_from_hf_gcs(download_config, relative_data_dir)\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\datasets\\arrow_reader.py:260: in download_from_hf_gcs\r\n    downloaded_dataset_info = cached_path(remote_dataset_info.replace(os.sep, \"/\"))\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\datasets\\utils\\file_utils.py:301: in cached_path\r\n    download_desc=download_config.download_desc,\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\datasets\\utils\\file_utils.py:560: in get_from_cache\r\n    headers=headers,\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\datasets\\utils\\file_utils.py:476: in http_head\r\n    max_retries=max_retries,\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\datasets\\utils\\file_utils.py:397: in _request_with_retry\r\n    response = requests.request(method=method.upper(), url=url, timeout=timeout, **params)\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\requests\\api.py:61: in request\r\n    return session.request(method=method, url=url, **kwargs)\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\requests\\sessions.py:529: in request\r\n    resp = self.send(prep, **send_kwargs)\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\requests\\sessions.py:645: in send\r\n    r = adapter.send(request, **kwargs)\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\responses\\__init__.py:840: in unbound_on_send\r\n    return self._on_request(adapter, request, *a, **kwargs)\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\responses\\__init__.py:780: in _on_request\r\n    match, match_failed_reasons = self._find_match(request)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <responses.RequestsMock object at 0x000002048AD70588>\r\nrequest = <PreparedRequest [HEAD]>\r\n\r\n    def _find_first_match(self, request):\r\n        match_failed_reasons = []\r\n>       for i, match in enumerate(self._matches):\r\nE       AttributeError: 'RequestsMock' object has no attribute '_matches'\r\n\r\nC:\\tools\\miniconda3\\envs\\py37\\lib\\site-packages\\moto\\core\\models.py:289: AttributeError\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3839/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3835", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3835/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3835/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3835/events", "html_url": "https://github.com/huggingface/datasets/issues/3835", "id": 1161029205, "node_id": "I_kwDODunzps5FM-ZV", "number": 3835, "title": "The link given on the gigaword does not work", "user": {"login": "martin6336", "id": 26357784, "node_id": "MDQ6VXNlcjI2MzU3Nzg0", "avatar_url": "https://avatars.githubusercontent.com/u/26357784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martin6336", "html_url": "https://github.com/martin6336", "followers_url": "https://api.github.com/users/martin6336/followers", "following_url": "https://api.github.com/users/martin6336/following{/other_user}", "gists_url": "https://api.github.com/users/martin6336/gists{/gist_id}", "starred_url": "https://api.github.com/users/martin6336/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martin6336/subscriptions", "organizations_url": "https://api.github.com/users/martin6336/orgs", "repos_url": "https://api.github.com/users/martin6336/repos", "events_url": "https://api.github.com/users/martin6336/events{/privacy}", "received_events_url": "https://api.github.com/users/martin6336/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-03-07T07:56:42Z", "updated_at": "2022-03-15T12:30:23Z", "closed_at": "2022-03-15T12:30:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Dataset viewer issue for '*name of the dataset*'\r\n\r\n**Link:** *link to the dataset viewer page*\r\n\r\n*short description of the issue*\r\n\r\nAm I the one who added this dataset ? Yes-No\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3835/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3831", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3831/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3831/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3831/events", "html_url": "https://github.com/huggingface/datasets/issues/3831", "id": 1160501000, "node_id": "I_kwDODunzps5FK9cI", "number": 3831, "title": "when using to_tf_dataset with shuffle is true, not all completed batches are made", "user": {"login": "greenned", "id": 42107709, "node_id": "MDQ6VXNlcjQyMTA3NzA5", "avatar_url": "https://avatars.githubusercontent.com/u/42107709?v=4", "gravatar_id": "", "url": "https://api.github.com/users/greenned", "html_url": "https://github.com/greenned", "followers_url": "https://api.github.com/users/greenned/followers", "following_url": "https://api.github.com/users/greenned/following{/other_user}", "gists_url": "https://api.github.com/users/greenned/gists{/gist_id}", "starred_url": "https://api.github.com/users/greenned/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/greenned/subscriptions", "organizations_url": "https://api.github.com/users/greenned/orgs", "repos_url": "https://api.github.com/users/greenned/repos", "events_url": "https://api.github.com/users/greenned/events{/privacy}", "received_events_url": "https://api.github.com/users/greenned/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-03-06T02:43:50Z", "updated_at": "2022-03-08T15:18:56Z", "closed_at": "2022-03-08T15:18:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nwhen converting a dataset to tf_dataset by using to_tf_dataset with shuffle true, the remainder is not converted to one batch\r\n\r\n## Steps to reproduce the bug\r\nthis is the sample code below\r\nhttps://colab.research.google.com/drive/1_oRXWsR38ElO1EYF9ayFoCU7Ou1AAej4?usp=sharing\r\n\r\n\r\n## Expected results\r\nregardless of shuffle is true or not, 67 rows dataset should be 5 batches when batch size is 16.\r\n\r\n## Actual results\r\n4 batches\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.12\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3831/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3828", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3828/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3828/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3828/events", "html_url": "https://github.com/huggingface/datasets/issues/3828", "id": 1160064029, "node_id": "I_kwDODunzps5FJSwd", "number": 3828, "title": "The Pile's _FEATURE spec seems to be incorrect", "user": {"login": "dlwh", "id": 9633, "node_id": "MDQ6VXNlcjk2MzM=", "avatar_url": "https://avatars.githubusercontent.com/u/9633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dlwh", "html_url": "https://github.com/dlwh", "followers_url": "https://api.github.com/users/dlwh/followers", "following_url": "https://api.github.com/users/dlwh/following{/other_user}", "gists_url": "https://api.github.com/users/dlwh/gists{/gist_id}", "starred_url": "https://api.github.com/users/dlwh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dlwh/subscriptions", "organizations_url": "https://api.github.com/users/dlwh/orgs", "repos_url": "https://api.github.com/users/dlwh/repos", "events_url": "https://api.github.com/users/dlwh/events{/privacy}", "received_events_url": "https://api.github.com/users/dlwh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-03-04T21:25:32Z", "updated_at": "2022-03-08T09:30:49Z", "closed_at": "2022-03-08T09:30:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nIf you look at https://huggingface.co/datasets/the_pile/blob/main/the_pile.py:\r\n\r\nFor \"all\"\r\n*  the pile_set_name is never set for data\r\n* there's actually an id field inside of \"meta\" \r\n\r\nFor subcorpora pubmed_central and hacker_news:\r\n* the meta is specified to be a string, but it's actually a dict with an id field inside.\r\n\r\n## Steps to reproduce the bug\r\n\r\n\r\n## Expected results\r\nFeature spec should match the data I'd think?\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3828/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3828/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3823", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3823/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3823/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3823/events", "html_url": "https://github.com/huggingface/datasets/issues/3823", "id": 1159497844, "node_id": "I_kwDODunzps5FHIh0", "number": 3823, "title": "500 internal server error when trying to open a dataset composed of Zarr stores", "user": {"login": "jacobbieker", "id": 7170359, "node_id": "MDQ6VXNlcjcxNzAzNTk=", "avatar_url": "https://avatars.githubusercontent.com/u/7170359?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacobbieker", "html_url": "https://github.com/jacobbieker", "followers_url": "https://api.github.com/users/jacobbieker/followers", "following_url": "https://api.github.com/users/jacobbieker/following{/other_user}", "gists_url": "https://api.github.com/users/jacobbieker/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacobbieker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacobbieker/subscriptions", "organizations_url": "https://api.github.com/users/jacobbieker/orgs", "repos_url": "https://api.github.com/users/jacobbieker/repos", "events_url": "https://api.github.com/users/jacobbieker/events{/privacy}", "received_events_url": "https://api.github.com/users/jacobbieker/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-03-04T10:37:14Z", "updated_at": "2022-03-08T09:47:39Z", "closed_at": "2022-03-08T09:47:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe dataset [openclimatefix/mrms](https://huggingface.co/datasets/openclimatefix/mrms) gives a 500 server error when trying to open it on the website, or through code.\r\n\r\nThe dataset doesn't have a loading script yet, and I did push two [xarray](https://docs.xarray.dev/en/stable/) Zarr stores of data there recentlyish. The Zarr stores are composed of lots of small files, which I am guessing is probably the problem, as we have another [OCF dataset](https://huggingface.co/datasets/openclimatefix/eumetsat_uk_hrv) using xarray and Zarr, but with the Zarr stored on GCP public datasets instead of directly in HF datasets, and that one opens fine. \r\n\r\nIn general, we were hoping to use HF datasets to release some more public geospatial datasets as benchmarks, which are commonly stored as Zarr stores as they can be compressed well and deal with the multi-dimensional data and coordinates fairly easily compared to other formats, but with this error, I'm assuming we should try a different format? \r\n\r\nFor context, we are trying to have complete public model+data reimplementations of some SOTA weather and solar nowcasting models, like [MetNet, MetNet-2,](https://github.com/openclimatefix/metnet) [DGMR](https://github.com/openclimatefix/skillful_nowcasting), and [others](https://github.com/openclimatefix/graph_weather), which all have large, complex datasets.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"openclimatefix/mrms\")\r\n```\r\n\r\n## Expected results\r\nThe dataset should be downloaded or open up\r\n\r\n## Actual results\r\nA 500 internal server error\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.15.25-1-MANJARO-x86_64-with-glibc2.35\r\n- Python version: 3.9.10\r\n- PyArrow version: 7.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3823/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3820", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3820/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3820/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3820/events", "html_url": "https://github.com/huggingface/datasets/issues/3820", "id": 1159106603, "node_id": "I_kwDODunzps5FFpAr", "number": 3820, "title": "`pubmed_qa` checksum mismatch", "user": {"login": "jon-tow", "id": 41410219, "node_id": "MDQ6VXNlcjQxNDEwMjE5", "avatar_url": "https://avatars.githubusercontent.com/u/41410219?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jon-tow", "html_url": "https://github.com/jon-tow", "followers_url": "https://api.github.com/users/jon-tow/followers", "following_url": "https://api.github.com/users/jon-tow/following{/other_user}", "gists_url": "https://api.github.com/users/jon-tow/gists{/gist_id}", "starred_url": "https://api.github.com/users/jon-tow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jon-tow/subscriptions", "organizations_url": "https://api.github.com/users/jon-tow/orgs", "repos_url": "https://api.github.com/users/jon-tow/repos", "events_url": "https://api.github.com/users/jon-tow/events{/privacy}", "received_events_url": "https://api.github.com/users/jon-tow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892865, "node_id": "MDU6TGFiZWwxOTM1ODkyODY1", "url": "https://api.github.com/repos/huggingface/datasets/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-03-04T00:28:08Z", "updated_at": "2022-03-04T09:42:32Z", "closed_at": "2022-03-04T09:42:32Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading [`pubmed_qa`](https://huggingface.co/datasets/pubmed_qa) results in a mismatched checksum error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nimport datasets\r\ntry:\r\n    datasets.load_dataset(\"pubmed_qa\", \"pqa_labeled\")\r\nexcept Exception as e:\r\n    print(e)\r\n\r\ntry:\r\n    datasets.load_dataset(\"pubmed_qa\", \"pqa_unlabeled\")\r\nexcept Exception as e:\r\n    print(e)\r\n\r\ntry:\r\n    datasets.load_dataset(\"pubmed_qa\", \"pqa_artificial\")\r\nexcept Exception as e:\r\n    print(e)\r\n```\r\n\r\n## Expected results\r\nSuccessful download.\r\n\r\n## Actual results\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/load.py\", line 1702, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/builder.py\", line 594, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/builder.py\", line 665, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/usr/local/lib/python3.9/site-packages/datasets/utils/info_utils.py\", line 40, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=1RsGLINVce-0GsDkCLDuLZmoLuzfmoCuQ', 'https://drive.google.com/uc?export=download&id=15v1x6aQDlZymaHGP7cZJZZYFfeJt2NdS']\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: macOS\r\n- Python version: 3.8.1\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3820/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3809", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3809/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3809/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3809/events", "html_url": "https://github.com/huggingface/datasets/issues/3809", "id": 1158143480, "node_id": "I_kwDODunzps5FB934", "number": 3809, "title": "Checksums didn't match for datasets on Google Drive", "user": {"login": "muelletm", "id": 11507045, "node_id": "MDQ6VXNlcjExNTA3MDQ1", "avatar_url": "https://avatars.githubusercontent.com/u/11507045?v=4", "gravatar_id": "", "url": "https://api.github.com/users/muelletm", "html_url": "https://github.com/muelletm", "followers_url": "https://api.github.com/users/muelletm/followers", "following_url": "https://api.github.com/users/muelletm/following{/other_user}", "gists_url": "https://api.github.com/users/muelletm/gists{/gist_id}", "starred_url": "https://api.github.com/users/muelletm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/muelletm/subscriptions", "organizations_url": "https://api.github.com/users/muelletm/orgs", "repos_url": "https://api.github.com/users/muelletm/repos", "events_url": "https://api.github.com/users/muelletm/events{/privacy}", "received_events_url": "https://api.github.com/users/muelletm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892865, "node_id": "MDU6TGFiZWwxOTM1ODkyODY1", "url": "https://api.github.com/repos/huggingface/datasets/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-03-03T09:01:10Z", "updated_at": "2022-03-03T09:24:58Z", "closed_at": "2022-03-03T09:24:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nDatasets hosted on Google Drive do not seem to work right now.\r\nLoading them fails with a checksum error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfor dataset in [\"head_qa\", \"yelp_review_full\"]:\r\n  try:\r\n    load_dataset(dataset)\r\n  except Exception as exception:\r\n    print(\"Error\", dataset, exception)\r\n```\r\n\r\nHere is a [colab](https://colab.research.google.com/drive/1wOtHBmL8I65NmUYakzPV5zhVCtHhi7uQ#scrollTo=cDzdCLlk-Bo4).\r\n\r\n## Expected results\r\n\r\nThe datasets should be loaded.\r\n\r\n## Actual results\r\n\r\n```\r\nDownloading and preparing dataset head_qa/es (download: 75.69 MiB, generated: 2.86 MiB, post-processed: Unknown size, total: 78.55 MiB) to /root/.cache/huggingface/datasets/head_qa/es/1.1.0/583ab408e8baf54aab378c93715fadc4d8aa51b393e27c3484a877e2ac0278e9...\r\nError head_qa Checksums didn't match for dataset source files:\r\n['https://drive.google.com/u/0/uc?export=download&id=1a_95N5zQQoUCq8IBNVZgziHbeM-QxG2t']\r\nDownloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/13c31a618ba62568ec8572a222a283dfc29a6517776a3ac5945fb508877dde43...\r\nError yelp_review_full Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbZlU4dXhHTFhZQU0']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.12\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3809/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3808", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3808/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3808/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3808/events", "html_url": "https://github.com/huggingface/datasets/issues/3808", "id": 1157650043, "node_id": "I_kwDODunzps5FAFZ7", "number": 3808, "title": "Pre-Processing Cache Fails when using a Factory pattern", "user": {"login": "Helw150", "id": 9847335, "node_id": "MDQ6VXNlcjk4NDczMzU=", "avatar_url": "https://avatars.githubusercontent.com/u/9847335?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Helw150", "html_url": "https://github.com/Helw150", "followers_url": "https://api.github.com/users/Helw150/followers", "following_url": "https://api.github.com/users/Helw150/following{/other_user}", "gists_url": "https://api.github.com/users/Helw150/gists{/gist_id}", "starred_url": "https://api.github.com/users/Helw150/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Helw150/subscriptions", "organizations_url": "https://api.github.com/users/Helw150/orgs", "repos_url": "https://api.github.com/users/Helw150/repos", "events_url": "https://api.github.com/users/Helw150/events{/privacy}", "received_events_url": "https://api.github.com/users/Helw150/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-03-02T20:18:43Z", "updated_at": "2022-03-10T23:01:47Z", "closed_at": "2022-03-10T23:01:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nIf you utilize a pre-processing function which is created using a factory pattern, the function hash changes on each run (even if the function is identical) and therefore the data will be reproduced each time.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n    def preprocess_function_factory(augmentation=None):\r\n        def preprocess_function(examples):\r\n            # Tokenize the texts\r\n            if augmentation:\r\n                conversions1 = [\r\n                    augmentation(example)\r\n                    for example in examples[sentence1_key]\r\n                ]\r\n                if sentence2_key is None:\r\n                    args = (conversions1,)\r\n                else:\r\n                    conversions2 = [\r\n                        augmentation(example)\r\n                        for example in examples[sentence2_key]\r\n                    ]\r\n                    args = (conversions1, conversions2)\r\n            else:\r\n                args = (\r\n                    (examples[sentence1_key],)\r\n                    if sentence2_key is None\r\n                    else (examples[sentence1_key], examples[sentence2_key])\r\n                )\r\n            result = tokenizer(\r\n                *args, padding=padding, max_length=max_seq_length, truncation=True\r\n            )\r\n\r\n            # Map labels to IDs (not necessary for GLUE tasks)\r\n            if label_to_id is not None and \"label\" in examples:\r\n                result[\"label\"] = [\r\n                    (label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]\r\n                ]\r\n            return result\r\n\r\n        return preprocess_function\r\n\r\ncapitalize = lambda x: x.capitalize()\r\npreprocess_function = preprocess_function_factory(augmentation=capitalize)\r\nprint(hash(preprocess_function)) # This will change on each run\r\nraw_datasets = raw_datasets.map(\r\n        preprocess_function,\r\n        batched=True,\r\n        load_from_cache_file=True,\r\n        desc=\"Running transformation and tokenizer on dataset\",\r\n)\r\n```\r\n\r\n## Expected results\r\nRunning the code twice will cause the cache to be re-used.\r\n\r\n## Actual results\r\nRunning the code twice causes the whole dataset to be re-processed\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3808/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3807", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3807/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3807/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3807/events", "html_url": "https://github.com/huggingface/datasets/issues/3807", "id": 1157531812, "node_id": "I_kwDODunzps5E_oik", "number": 3807, "title": "NonMatchingChecksumError in xcopa dataset", "user": {"login": "afcruzs-ms", "id": 93286455, "node_id": "U_kgDOBY9wNw", "avatar_url": "https://avatars.githubusercontent.com/u/93286455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/afcruzs-ms", "html_url": "https://github.com/afcruzs-ms", "followers_url": "https://api.github.com/users/afcruzs-ms/followers", "following_url": "https://api.github.com/users/afcruzs-ms/following{/other_user}", "gists_url": "https://api.github.com/users/afcruzs-ms/gists{/gist_id}", "starred_url": "https://api.github.com/users/afcruzs-ms/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/afcruzs-ms/subscriptions", "organizations_url": "https://api.github.com/users/afcruzs-ms/orgs", "repos_url": "https://api.github.com/users/afcruzs-ms/repos", "events_url": "https://api.github.com/users/afcruzs-ms/events{/privacy}", "received_events_url": "https://api.github.com/users/afcruzs-ms/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2022-03-02T18:10:19Z", "updated_at": "2022-05-20T06:00:42Z", "closed_at": "2022-03-03T17:40:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading the xcopa dataset doesn't work, it fails due to a mismatch in the checksum.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"xcopa\", \"it\")\r\n```\r\n\r\n## Expected results\r\nThe dataset should be loaded correctly.\r\n\r\n## Actual results\r\nFails with:\r\n```python\r\nin verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://github.com/cambridgeltl/xcopa/archive/master.zip']\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3, and 1.18.4.dev0\r\n- Platform:\r\n- Python version: 3.8\r\n- PyArrow version:\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3807/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3795", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3795/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3795/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3795/events", "html_url": "https://github.com/huggingface/datasets/issues/3795", "id": 1153261281, "node_id": "I_kwDODunzps5EvV7h", "number": 3795, "title": "can not flatten natural_questions dataset", "user": {"login": "Hannibal046", "id": 38466901, "node_id": "MDQ6VXNlcjM4NDY2OTAx", "avatar_url": "https://avatars.githubusercontent.com/u/38466901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hannibal046", "html_url": "https://github.com/Hannibal046", "followers_url": "https://api.github.com/users/Hannibal046/followers", "following_url": "https://api.github.com/users/Hannibal046/following{/other_user}", "gists_url": "https://api.github.com/users/Hannibal046/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hannibal046/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hannibal046/subscriptions", "organizations_url": "https://api.github.com/users/Hannibal046/orgs", "repos_url": "https://api.github.com/users/Hannibal046/repos", "events_url": "https://api.github.com/users/Hannibal046/events{/privacy}", "received_events_url": "https://api.github.com/users/Hannibal046/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-02-27T13:57:40Z", "updated_at": "2022-03-21T14:36:12Z", "closed_at": "2022-03-21T14:36:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nafter downloading the natural_questions dataset, can not flatten the dataset considering there are `long answer` and `short answer` in `annotations`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('natural_questions',cache_dir = 'data/dataset_cache_dir')\r\ndataset['train'].flatten()\r\n```\r\n\r\n## Expected results\r\na dataset with `long_answer` as features\r\n\r\n## Actual results\r\nTraceback (most recent call last):\r\n  File \"temp.py\", line 5, in <module>\r\n    dataset['train'].flatten()\r\n  File \"/Users/hannibal046/anaconda3/lib/python3.8/site-packages/datasets/fingerprint.py\", line 413, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/Users/hannibal046/anaconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1296, in flatten\r\n    dataset._data = update_metadata_with_features(dataset._data, dataset.features)\r\n  File \"/Users/hannibal046/anaconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 536, in update_metadata_with_features\r\n    features = Features({col_name: features[col_name] for col_name in table.column_names})\r\n  File \"/Users/hannibal046/anaconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 536, in <dictcomp>\r\n    features = Features({col_name: features[col_name] for col_name in table.column_names})\r\nKeyError: 'annotations.long_answer'\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.13\r\n- Platform: MBP\r\n- Python version: 3.8\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3795/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3795/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3786", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3786/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3786/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3786/events", "html_url": "https://github.com/huggingface/datasets/issues/3786", "id": 1150233067, "node_id": "I_kwDODunzps5Ejynr", "number": 3786, "title": "Bug downloading Virus scan warning page from Google Drive URLs", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-02-25T09:32:23Z", "updated_at": "2022-03-03T09:25:59Z", "closed_at": "2022-02-25T11:56:35Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nRecently, some issues were reported with URLs from Google Drive, where we were downloading the Virus scan warning page instead of the data file itself.\r\n\r\nSee:\r\n- #3758 \r\n- #3773\r\n- #3784\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3786/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3786/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3784", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3784/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3784/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3784/events", "html_url": "https://github.com/huggingface/datasets/issues/3784", "id": 1150057955, "node_id": "I_kwDODunzps5EjH3j", "number": 3784, "title": "Unable to Download CNN-Dailymail Dataset ", "user": {"login": "AngadSethi", "id": 58678541, "node_id": "MDQ6VXNlcjU4Njc4NTQx", "avatar_url": "https://avatars.githubusercontent.com/u/58678541?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AngadSethi", "html_url": "https://github.com/AngadSethi", "followers_url": "https://api.github.com/users/AngadSethi/followers", "following_url": "https://api.github.com/users/AngadSethi/following{/other_user}", "gists_url": "https://api.github.com/users/AngadSethi/gists{/gist_id}", "starred_url": "https://api.github.com/users/AngadSethi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AngadSethi/subscriptions", "organizations_url": "https://api.github.com/users/AngadSethi/orgs", "repos_url": "https://api.github.com/users/AngadSethi/repos", "events_url": "https://api.github.com/users/AngadSethi/events{/privacy}", "received_events_url": "https://api.github.com/users/AngadSethi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "AngadSethi", "id": 58678541, "node_id": "MDQ6VXNlcjU4Njc4NTQx", "avatar_url": "https://avatars.githubusercontent.com/u/58678541?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AngadSethi", "html_url": "https://github.com/AngadSethi", "followers_url": "https://api.github.com/users/AngadSethi/followers", "following_url": "https://api.github.com/users/AngadSethi/following{/other_user}", "gists_url": "https://api.github.com/users/AngadSethi/gists{/gist_id}", "starred_url": "https://api.github.com/users/AngadSethi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AngadSethi/subscriptions", "organizations_url": "https://api.github.com/users/AngadSethi/orgs", "repos_url": "https://api.github.com/users/AngadSethi/repos", "events_url": "https://api.github.com/users/AngadSethi/events{/privacy}", "received_events_url": "https://api.github.com/users/AngadSethi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "AngadSethi", "id": 58678541, "node_id": "MDQ6VXNlcjU4Njc4NTQx", "avatar_url": "https://avatars.githubusercontent.com/u/58678541?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AngadSethi", "html_url": "https://github.com/AngadSethi", "followers_url": "https://api.github.com/users/AngadSethi/followers", "following_url": "https://api.github.com/users/AngadSethi/following{/other_user}", "gists_url": "https://api.github.com/users/AngadSethi/gists{/gist_id}", "starred_url": "https://api.github.com/users/AngadSethi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AngadSethi/subscriptions", "organizations_url": "https://api.github.com/users/AngadSethi/orgs", "repos_url": "https://api.github.com/users/AngadSethi/repos", "events_url": "https://api.github.com/users/AngadSethi/events{/privacy}", "received_events_url": "https://api.github.com/users/AngadSethi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2022-02-25T05:24:47Z", "updated_at": "2022-03-03T14:05:17Z", "closed_at": "2022-03-03T14:05:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am unable to download the CNN-Dailymail dataset. Upon closer investigation, I realised why this was happening:\r\n- The dataset sits in Google Drive, and both the CNN and DM datasets are large.\r\n- Google is unable to scan the folder for viruses, **so the link which would originally download the dataset, now downloads the source code of this web page:**\r\n![image](https://user-images.githubusercontent.com/58678541/155658435-c2f497d7-7601-4332-94b1-18a62dd96422.png)\r\n- **This leads to the following error**:\r\n```python\r\nNotADirectoryError: [Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories'\r\n```\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\ndataset = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\r\n```\r\n\r\n## Expected results\r\nThat the dataset is downloaded and processed just like other datasets.\r\n\r\n## Actual results\r\nHit with this error:\r\n```python\r\nNotADirectoryError: [Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.12\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3784/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3773", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3773/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3773/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3773/events", "html_url": "https://github.com/huggingface/datasets/issues/3773", "id": 1146758335, "node_id": "I_kwDODunzps5EWiS_", "number": 3773, "title": "Checksum mismatch for the reddit_tifu dataset", "user": {"login": "anna-kay", "id": 56791604, "node_id": "MDQ6VXNlcjU2NzkxNjA0", "avatar_url": "https://avatars.githubusercontent.com/u/56791604?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anna-kay", "html_url": "https://github.com/anna-kay", "followers_url": "https://api.github.com/users/anna-kay/followers", "following_url": "https://api.github.com/users/anna-kay/following{/other_user}", "gists_url": "https://api.github.com/users/anna-kay/gists{/gist_id}", "starred_url": "https://api.github.com/users/anna-kay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anna-kay/subscriptions", "organizations_url": "https://api.github.com/users/anna-kay/orgs", "repos_url": "https://api.github.com/users/anna-kay/repos", "events_url": "https://api.github.com/users/anna-kay/events{/privacy}", "received_events_url": "https://api.github.com/users/anna-kay/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2022-02-22T10:57:07Z", "updated_at": "2022-02-25T19:27:49Z", "closed_at": "2022-02-22T12:38:44Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nA checksum occurs when downloading the reddit_tifu data (both long & short).\r\n\r\n## Steps to reproduce the bug\r\nreddit_tifu_dataset = load_dataset('reddit_tifu', 'long')\r\n\r\n## Expected results\r\nThe expected result is for the dataset to be downloaded and cached locally.\r\n\r\n## Actual results\r\n  File \"/.../lib/python3.9/site-packages/datasets/utils/info_utils.py\", line 40, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=1ffWfITKFMJeqjT8loC8aiCLRNJpc_XnF']\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.13.0-30-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3773/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3773/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3763", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3763/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3763/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3763/events", "html_url": "https://github.com/huggingface/datasets/issues/3763", "id": 1145099878, "node_id": "I_kwDODunzps5EQNZm", "number": 3763, "title": "It's not possible download `20200501.pt` dataset", "user": {"login": "jvanz", "id": 1514798, "node_id": "MDQ6VXNlcjE1MTQ3OTg=", "avatar_url": "https://avatars.githubusercontent.com/u/1514798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jvanz", "html_url": "https://github.com/jvanz", "followers_url": "https://api.github.com/users/jvanz/followers", "following_url": "https://api.github.com/users/jvanz/following{/other_user}", "gists_url": "https://api.github.com/users/jvanz/gists{/gist_id}", "starred_url": "https://api.github.com/users/jvanz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jvanz/subscriptions", "organizations_url": "https://api.github.com/users/jvanz/orgs", "repos_url": "https://api.github.com/users/jvanz/repos", "events_url": "https://api.github.com/users/jvanz/events{/privacy}", "received_events_url": "https://api.github.com/users/jvanz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-02-20T18:34:58Z", "updated_at": "2022-02-21T12:06:12Z", "closed_at": "2022-02-21T09:25:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe dataset `20200501.pt` is broken. \r\n\r\nThe available datasets: https://dumps.wikimedia.org/ptwiki/\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"wikipedia\", \"20200501.pt\", beam_runner='DirectRunner')\r\n```\r\n\r\n## Expected results\r\nI expect to download the dataset locally. \r\n\r\n## Actual results\r\n```\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"wikipedia\", \"20200501.pt\", beam_runner='DirectRunner')\r\nDownloading and preparing dataset wikipedia/20200501.pt to /home/jvanz/.cache/huggingface/datasets/wikipedia/20200501.pt/1.0.0/009f923d9b6dd00c00c8cdc7f408f2b47f45dd4f5fb7982a21f9448f4afbe475...\r\n/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/apache_beam/__init__.py:79: UserWarning: This version of Apache Beam has not been sufficiently tested on Python 3.9. You may encounter bugs or missing features.\r\n  warnings.warn(\r\n  0%|                                                                                                                                                                                                                   | 0/1 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/load.py\", line 1702, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/builder.py\", line 594, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/builder.py\", line 1245, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/builder.py\", line 661, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/jvanz/.cache/huggingface/modules/datasets_modules/datasets/wikipedia/009f923d9b6dd00c00c8cdc7f408f2b47f45dd4f5fb7982a21f9448f4afbe475/wikipedia.py\", line 420, in _split_generators\r\n    downloaded_files = dl_manager.download_and_extract({\"info\": info_url})\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/utils/download_manager.py\", line 307, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/utils/download_manager.py\", line 195, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 260, in map_nested\r\n    mapped = [\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 261, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True))\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 196, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/utils/download_manager.py\", line 216, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/utils/file_utils.py\", line 298, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/home/jvanz/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/datasets/utils/file_utils.py\", line 612, in get_from_cache\r\n    raise FileNotFoundError(f\"Couldn't find file at {url}\")\r\nFileNotFoundError: Couldn't find file at https://dumps.wikimedia.org/ptwiki/20200501/dumpstatus.json\r\n\r\n```\r\n\r\n## Environment info\r\n```\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.3.18-150300.59.49-default-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 6.0.1\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3763/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3763/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3758", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3758/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3758/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3758/events", "html_url": "https://github.com/huggingface/datasets/issues/3758", "id": 1143366393, "node_id": "I_kwDODunzps5EJmL5", "number": 3758, "title": "head_qa file missing", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-02-18T16:32:43Z", "updated_at": "2022-02-28T14:29:18Z", "closed_at": "2022-02-21T14:39:19Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nA file for the `head_qa` dataset is missing (https://drive.google.com/u/0/uc?export=download&id=1a_95N5zQQoUCq8IBNVZgziHbeM-QxG2t/HEAD_EN/train_HEAD_EN.json)\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> load_dataset(\"head_qa\", name=\"en\")\r\n```\r\n\r\n## Expected results\r\n\r\nThe dataset should be loaded\r\n\r\n## Actual results\r\n\r\n```\r\nDownloading and preparing dataset head_qa/en (download: 75.69 MiB, generated: 2.69 MiB, post-processed: Unknown size, total: 78.38 MiB) to /home/slesage/.cache/huggingface/datasets/head_qa/en/1.1.0/583ab408e8baf54aab378c93715fadc4d8aa51b393e27c3484a877e2ac0278e9...\r\nDownloading data: 2.21kB [00:00, 2.05MB/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1729, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/builder.py\", line 594, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/builder.py\", line 665, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/utils/info_utils.py\", line 40, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/u/0/uc?export=download&id=1a_95N5zQQoUCq8IBNVZgziHbeM-QxG2t']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.18.4.dev0\r\n- Platform: Linux-5.11.0-1028-aws-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3758/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3758/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3756", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3756/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3756/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3756/events", "html_url": "https://github.com/huggingface/datasets/issues/3756", "id": 1143273825, "node_id": "I_kwDODunzps5EJPlh", "number": 3756, "title": "Images get decoded when using `map()` with `input_columns` argument on a dataset", "user": {"login": "kklemon", "id": 1430243, "node_id": "MDQ6VXNlcjE0MzAyNDM=", "avatar_url": "https://avatars.githubusercontent.com/u/1430243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kklemon", "html_url": "https://github.com/kklemon", "followers_url": "https://api.github.com/users/kklemon/followers", "following_url": "https://api.github.com/users/kklemon/following{/other_user}", "gists_url": "https://api.github.com/users/kklemon/gists{/gist_id}", "starred_url": "https://api.github.com/users/kklemon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kklemon/subscriptions", "organizations_url": "https://api.github.com/users/kklemon/orgs", "repos_url": "https://api.github.com/users/kklemon/repos", "events_url": "https://api.github.com/users/kklemon/events{/privacy}", "received_events_url": "https://api.github.com/users/kklemon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-02-18T15:35:38Z", "updated_at": "2022-12-13T16:59:06Z", "closed_at": "2022-12-13T16:59:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThe `datasets.features.Image` feature class decodes image data by default. Expectedly, when indexing a dataset or using the `map()` method, images are returned as PIL Image instances.\r\n\r\nHowever, when calling `map()` and setting a specific data column with the `input_columns` argument, the image data is passed as raw byte representation to the mapping function.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfrom torchvision import transforms\r\nfrom PIL.Image import Image\r\n\r\ndataset = load_dataset('mnist', split='train')\r\n\r\ndef transform_all_columns(example):\r\n    # example['image'] is encoded as PIL Image\r\n    assert isinstance(example['image'], Image)\r\n    return example\r\n\r\ndef transform_image_column(image):\r\n    # image is decoded here and represented as raw bytes\r\n    assert isinstance(image, Image)\r\n    return image\r\n\r\n# single-sample dataset for debugging purposes\r\ndev = dataset.select([0])\r\n\r\ndev.map(transform_all_columns)\r\ndev.map(transform_image_column, input_columns='image')\r\n```\r\n\r\n## Expected results\r\n\r\nImage data should be passed in decoded form, i.e. as PIL Image objects to the mapping function unless the `decode` attribute on the image feature is set to `False`.\r\n\r\n## Actual results\r\n\r\nThe mapping function receives images as raw byte data.\r\n\r\n## Environment info\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.11.0-49-generic-x86_64-with-glibc2.32\r\n- Python version: 3.8.0b4\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3756/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3756/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3754", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3754/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3754/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3754/events", "html_url": "https://github.com/huggingface/datasets/issues/3754", "id": 1142886536, "node_id": "I_kwDODunzps5EHxCI", "number": 3754, "title": "Overflowing indices in `select`", "user": {"login": "lvwerra", "id": 8264887, "node_id": "MDQ6VXNlcjgyNjQ4ODc=", "avatar_url": "https://avatars.githubusercontent.com/u/8264887?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lvwerra", "html_url": "https://github.com/lvwerra", "followers_url": "https://api.github.com/users/lvwerra/followers", "following_url": "https://api.github.com/users/lvwerra/following{/other_user}", "gists_url": "https://api.github.com/users/lvwerra/gists{/gist_id}", "starred_url": "https://api.github.com/users/lvwerra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lvwerra/subscriptions", "organizations_url": "https://api.github.com/users/lvwerra/orgs", "repos_url": "https://api.github.com/users/lvwerra/repos", "events_url": "https://api.github.com/users/lvwerra/events{/privacy}", "received_events_url": "https://api.github.com/users/lvwerra/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-02-18T11:30:52Z", "updated_at": "2022-02-18T11:38:23Z", "closed_at": "2022-02-18T11:38:23Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nThe `Dataset.select` function seems to accept indices that are larger than the dataset size and seems to effectively use `index %len(ds)`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import Dataset\r\n\r\nds = Dataset.from_dict({\"test\": [1,2,3]})\r\nds = ds.select(range(5))\r\n\r\nprint(ds)\r\nprint()\r\nprint(ds[\"test\"])\r\n```\r\nResult:\r\n```python\r\nDataset({\r\n    features: ['test'],\r\n    num_rows: 5\r\n})\r\n\r\n[1, 2, 3, 1, 2]\r\n```\r\n\r\nThis behaviour is not documented and can lead to unexpected behaviour when for example taking a sample larger than the dataset and thus creating a lot of duplicates.\r\n\r\n## Expected results\r\nIt think this should throw an error or at least a very big warning:\r\n```python\r\nIndexError: Invalid key: 5 is out of bounds for size 3\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.18.3\r\n- Platform: macOS-12.0.1-x86_64-i386-64bit\r\n- Python version: 3.9.10\r\n- PyArrow version: 7.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3754/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3754/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3750", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3750/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3750/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3750/events", "html_url": "https://github.com/huggingface/datasets/issues/3750", "id": 1142408331, "node_id": "I_kwDODunzps5EF8SL", "number": 3750, "title": "`NonMatchingSplitsSizesError` for cats_vs_dogs dataset", "user": {"login": "jaketae", "id": 25360440, "node_id": "MDQ6VXNlcjI1MzYwNDQw", "avatar_url": "https://avatars.githubusercontent.com/u/25360440?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jaketae", "html_url": "https://github.com/jaketae", "followers_url": "https://api.github.com/users/jaketae/followers", "following_url": "https://api.github.com/users/jaketae/following{/other_user}", "gists_url": "https://api.github.com/users/jaketae/gists{/gist_id}", "starred_url": "https://api.github.com/users/jaketae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jaketae/subscriptions", "organizations_url": "https://api.github.com/users/jaketae/orgs", "repos_url": "https://api.github.com/users/jaketae/repos", "events_url": "https://api.github.com/users/jaketae/events{/privacy}", "received_events_url": "https://api.github.com/users/jaketae/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-02-18T05:46:39Z", "updated_at": "2022-02-18T14:56:11Z", "closed_at": "2022-02-18T14:56:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nCannot download cats_vs_dogs dataset due to `NonMatchingSplitsSizesError`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"cats_vs_dogs\")\r\n```\r\n\r\n## Expected results\r\n\r\nLoading is successful.\r\n\r\n## Actual results\r\n\r\n```\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=7503250, num_examples=23422, dataset_name='cats_vs_dogs'), 'recorded': SplitInfo(name='train', num_bytes=7262410, num_examples=23410, dataset_name='cats_vs_dogs')}]\r\n```\r\n\r\n## Environment info\r\n\r\nReproduced on a fresh [Colab notebook](https://colab.research.google.com/drive/13GTvrSJbBGvL2ybDdXCBZwATd6FOkMub?usp=sharing).\r\n\r\n## Additional Context\r\n\r\nOriginally reported in https://github.com/huggingface/transformers/issues/15698.\r\n\r\ncc @mariosasko ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3750/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3750/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3739", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3739/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3739/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3739/events", "html_url": "https://github.com/huggingface/datasets/issues/3739", "id": 1140329189, "node_id": "I_kwDODunzps5D-Arl", "number": 3739, "title": "Pubmed dataset does not work in streaming mode", "user": {"login": "abhi-mosaic", "id": 77638579, "node_id": "MDQ6VXNlcjc3NjM4NTc5", "avatar_url": "https://avatars.githubusercontent.com/u/77638579?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhi-mosaic", "html_url": "https://github.com/abhi-mosaic", "followers_url": "https://api.github.com/users/abhi-mosaic/followers", "following_url": "https://api.github.com/users/abhi-mosaic/following{/other_user}", "gists_url": "https://api.github.com/users/abhi-mosaic/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhi-mosaic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhi-mosaic/subscriptions", "organizations_url": "https://api.github.com/users/abhi-mosaic/orgs", "repos_url": "https://api.github.com/users/abhi-mosaic/repos", "events_url": "https://api.github.com/users/abhi-mosaic/events{/privacy}", "received_events_url": "https://api.github.com/users/abhi-mosaic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-02-16T17:13:37Z", "updated_at": "2022-02-18T14:42:13Z", "closed_at": "2022-02-18T14:42:13Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nTrying to use the `pubmed` dataset with `streaming=True` fails.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\npubmed_train = datasets.load_dataset('pubmed', split='train', streaming=True)\r\nprint (next(iter(pubmed_train)))\r\n```\r\n\r\n## Expected results\r\nI would expect to see the first training sample from the pubmed dataset. \r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/abhinav/Documents/mosaicml/mosaicml_venv/lib/python3.8/site-packages/datasets/iterable_dataset.py\", line 367, in __iter__\r\n    for key, example in self._iter():\r\n  File \"/Users/abhinav/Documents/mosaicml/mosaicml_venv/lib/python3.8/site-packages/datasets/iterable_dataset.py\", line 364, in _iter\r\n    yield from ex_iterable\r\n  File \"/Users/abhinav/Documents/mosaicml/mosaicml_venv/lib/python3.8/site-packages/datasets/iterable_dataset.py\", line 79, in __iter__\r\n    for key, example in self.generate_examples_fn(**self.kwargs):\r\n  File \"/Users/abhinav/.cache/huggingface/modules/datasets_modules/datasets/pubmed/9715addf10c42a7877a2149ae0c5f2fddabefc775cd1bd9b03ac3f012b86ce46/pubmed.py\", line 373, in _generate_examples\r\n    tree = etree.parse(filename)\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/xml/etree/ElementTree.py\", line 1202, in parse\r\n    tree.parse(source, parser)\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/xml/etree/ElementTree.py\", line 584, in parse\r\n    source = open(source, \"rb\")\r\nFileNotFoundError: [Errno 2] No such file or directory: 'gzip://pubmed21n0001.xml::ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/pubmed21n0001.xml.gz'\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.2\r\n- Platform: macOS-11.4-x86_64-i386-64bit\r\n- Python version: 3.8.2\r\n- PyArrow version: 6.0.0\r\n\r\n## Comments\r\nThe error looks like an issue with `open` vs. `xopen` inside the `xml` package. It looks like it's trying to open the remote source URL, which has been edited with prefix `gzip://...`. \r\n\r\nMaybe there can be an explicit `xopen` before passing the raw data to `etree`, something like:\r\n\r\n```python\r\n# Before\r\ntree = etree.parse(filename)\r\nroot = tree.getroot()\r\n\r\n# After\r\nwith xopen(filename) as f:\r\n  data_str = f.read()\r\nroot = etree.fromstring(data_str)\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3739/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3733", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3733/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3733/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3733/events", "html_url": "https://github.com/huggingface/datasets/issues/3733", "id": 1140011378, "node_id": "I_kwDODunzps5D8zFy", "number": 3733, "title": "Bugs in NewsQA dataset", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-02-16T13:17:37Z", "updated_at": "2022-02-17T07:54:25Z", "closed_at": "2022-02-17T07:54:25Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nNewsQA dataset has the following bugs:\r\n- the field `validated_answers` is an exact copy of the field `answers` but with the addition of `'count': [0]` to each dict\r\n- the field `badQuestion` does not appear in `answers` nor `validated_answers`\r\n\r\n## Steps to reproduce the bug\r\nBy inspecting the dataset script we can see that:\r\n- the parsing of `validated_answers` is a copy-paste of the one for `answers`\r\n- the `badQuestion` field is ignored in the parsing of both `answers` and `validated_answers`\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3733/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3733/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3730", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3730/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3730/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3730/events", "html_url": "https://github.com/huggingface/datasets/issues/3730", "id": 1139545613, "node_id": "I_kwDODunzps5D7BYN", "number": 3730, "title": "Checksum Error when loading multi-news dataset", "user": {"login": "byw2", "id": 60560991, "node_id": "MDQ6VXNlcjYwNTYwOTkx", "avatar_url": "https://avatars.githubusercontent.com/u/60560991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byw2", "html_url": "https://github.com/byw2", "followers_url": "https://api.github.com/users/byw2/followers", "following_url": "https://api.github.com/users/byw2/following{/other_user}", "gists_url": "https://api.github.com/users/byw2/gists{/gist_id}", "starred_url": "https://api.github.com/users/byw2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byw2/subscriptions", "organizations_url": "https://api.github.com/users/byw2/orgs", "repos_url": "https://api.github.com/users/byw2/repos", "events_url": "https://api.github.com/users/byw2/events{/privacy}", "received_events_url": "https://api.github.com/users/byw2/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-02-16T05:11:08Z", "updated_at": "2022-02-16T20:05:06Z", "closed_at": "2022-02-16T08:48:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen using the load_dataset function from datasets module to load the Multi-News dataset, does not load the dataset but throws Checksum Error instead.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"multi_news\")\r\n```\r\n\r\n\r\n## Expected results\r\nShould download and load Multi-News dataset.\r\n\r\n## Actual results\r\nThrows the following error and cannot load data successfully:\r\n\r\n```\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=1vRY2wM6rlOZrf9exGTm5pXj5ExlVwJ0C']\r\n```\r\n\r\nCould this issue please be looked at? Thanks!", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3730/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3729", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3729/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3729/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3729/events", "html_url": "https://github.com/huggingface/datasets/issues/3729", "id": 1139398442, "node_id": "I_kwDODunzps5D6dcq", "number": 3729, "title": "Wrong number of examples when loading a text dataset", "user": {"login": "kg-nlp", "id": 58376804, "node_id": "MDQ6VXNlcjU4Mzc2ODA0", "avatar_url": "https://avatars.githubusercontent.com/u/58376804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kg-nlp", "html_url": "https://github.com/kg-nlp", "followers_url": "https://api.github.com/users/kg-nlp/followers", "following_url": "https://api.github.com/users/kg-nlp/following{/other_user}", "gists_url": "https://api.github.com/users/kg-nlp/gists{/gist_id}", "starred_url": "https://api.github.com/users/kg-nlp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kg-nlp/subscriptions", "organizations_url": "https://api.github.com/users/kg-nlp/orgs", "repos_url": "https://api.github.com/users/kg-nlp/repos", "events_url": "https://api.github.com/users/kg-nlp/events{/privacy}", "received_events_url": "https://api.github.com/users/kg-nlp/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-02-16T01:13:31Z", "updated_at": "2022-03-15T16:16:09Z", "closed_at": "2022-03-15T16:16:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nwhen I use load_dataset to read a txt file  I find  that the number of the samples is incorrect\r\n\r\n## Steps to reproduce the bug\r\n```\r\nfr = open('train.txt','r',encoding='utf-8').readlines()\r\nprint(len(fr))  # 1199637\r\n\r\ndatasets = load_dataset('text', data_files={'train': ['train.txt']}, streaming=False)\r\nprint(len(datasets['train']))  # 1199649\r\n```\r\nI also use command line operation to verify it\r\n```\r\n$ wc -l train.txt \r\n1199637 train.txt\r\n```\r\n\r\n## Expected results\r\nplease fix that issue \r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.3\r\n- Platform:windows&linux\r\n- Python version:3.7\r\n- PyArrow version:6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3729/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3729/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3724", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3724/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3724/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3724/events", "html_url": "https://github.com/huggingface/datasets/issues/3724", "id": 1138827681, "node_id": "I_kwDODunzps5D4SGh", "number": 3724, "title": "Bug while streaming CSV dataset with pandas 1.4", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-02-15T15:16:19Z", "updated_at": "2022-02-15T16:55:44Z", "closed_at": "2022-02-15T16:55:44Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nIf we upgrade to pandas `1.4`, the patching of the pandas module is no longer working\r\n```\r\nAttributeError: '_PatchedModuleObj' object has no attribute '__version__'\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```\r\npip install pandas==1.4\r\n```\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"lvwerra/red-wine\", split=\"train\", streaming=True)\r\nitem = next(iter(ds))\r\nitem\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3724/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3717", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3717/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3717/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3717/events", "html_url": "https://github.com/huggingface/datasets/issues/3717", "id": 1137183015, "node_id": "I_kwDODunzps5DyAkn", "number": 3717, "title": "wrong condition in `Features ClassLabel encode_example`", "user": {"login": "Tudyx", "id": 56633664, "node_id": "MDQ6VXNlcjU2NjMzNjY0", "avatar_url": "https://avatars.githubusercontent.com/u/56633664?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tudyx", "html_url": "https://github.com/Tudyx", "followers_url": "https://api.github.com/users/Tudyx/followers", "following_url": "https://api.github.com/users/Tudyx/following{/other_user}", "gists_url": "https://api.github.com/users/Tudyx/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tudyx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tudyx/subscriptions", "organizations_url": "https://api.github.com/users/Tudyx/orgs", "repos_url": "https://api.github.com/users/Tudyx/repos", "events_url": "https://api.github.com/users/Tudyx/events{/privacy}", "received_events_url": "https://api.github.com/users/Tudyx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-02-14T11:44:35Z", "updated_at": "2022-02-14T15:09:36Z", "closed_at": "2022-02-14T15:07:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThe `encode_example` function in *features.py* seems to have a wrong condition.\r\n\r\n```python\r\nif not -1 <= example_data < self.num_classes:\r\n    raise ValueError(f\"Class label {example_data:d} greater than configured num_classes {self.num_classes}\")\r\n```\r\n\r\n## Expected results\r\n\r\nThe `not - 1` condition change the result of the condition. For instance, if  `example_data`  equals 4 and ` self.num_classes` equals 4 too, `example_data < self.num_classes` will give `False` as expected  . But if i add the  `not - 1`  condition, `not -1 <= example_data < self.num_classes` will give `True` and raise an exception.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.18.3\r\n- Python version: 3.8.10\r\n- PyArrow version: 7.00\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3717/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3707", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3707/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3707/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3707/events", "html_url": "https://github.com/huggingface/datasets/issues/3707", "id": 1132741903, "node_id": "I_kwDODunzps5DhEUP", "number": 3707, "title": "`.select`: unexpected behavior with `indices`", "user": {"login": "gabegma", "id": 36087158, "node_id": "MDQ6VXNlcjM2MDg3MTU4", "avatar_url": "https://avatars.githubusercontent.com/u/36087158?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gabegma", "html_url": "https://github.com/gabegma", "followers_url": "https://api.github.com/users/gabegma/followers", "following_url": "https://api.github.com/users/gabegma/following{/other_user}", "gists_url": "https://api.github.com/users/gabegma/gists{/gist_id}", "starred_url": "https://api.github.com/users/gabegma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gabegma/subscriptions", "organizations_url": "https://api.github.com/users/gabegma/orgs", "repos_url": "https://api.github.com/users/gabegma/repos", "events_url": "https://api.github.com/users/gabegma/events{/privacy}", "received_events_url": "https://api.github.com/users/gabegma/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-02-11T15:20:01Z", "updated_at": "2022-02-14T19:19:21Z", "closed_at": "2022-02-14T19:19:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe `.select` method will not throw when sending `indices` bigger than the dataset length; `indices` will be wrapped instead. This behavior is not documented anywhere, and is not intuitive. \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import Dataset\r\nds = Dataset.from_dict({\"text\": [\"d\", \"e\", \"f\"], \"label\": [4, 5, 6]})\r\nres1 = ds.select([1, 2, 3])['text']\r\nres2 = ds.select([1000])['text']\r\n```\r\n\r\n## Expected results\r\nBoth results should throw an `Error`.\r\n\r\n## Actual results\r\n`res1` will give `['e', 'f', 'd']`\r\n`res2` will give `['e']`\r\n\r\n## Environment info\r\nBug found from this environment:\r\n- `datasets` version: 1.16.1\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.7\r\n- PyArrow version: 6.0.1\r\n\r\nIt was also replicated on `master`.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3707/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3707/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3706", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3706/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3706/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3706/events", "html_url": "https://github.com/huggingface/datasets/issues/3706", "id": 1132218874, "node_id": "I_kwDODunzps5DfEn6", "number": 3706, "title": "Unable to load dataset 'big_patent'", "user": {"login": "ankitk2109", "id": 26432753, "node_id": "MDQ6VXNlcjI2NDMyNzUz", "avatar_url": "https://avatars.githubusercontent.com/u/26432753?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ankitk2109", "html_url": "https://github.com/ankitk2109", "followers_url": "https://api.github.com/users/ankitk2109/followers", "following_url": "https://api.github.com/users/ankitk2109/following{/other_user}", "gists_url": "https://api.github.com/users/ankitk2109/gists{/gist_id}", "starred_url": "https://api.github.com/users/ankitk2109/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ankitk2109/subscriptions", "organizations_url": "https://api.github.com/users/ankitk2109/orgs", "repos_url": "https://api.github.com/users/ankitk2109/repos", "events_url": "https://api.github.com/users/ankitk2109/events{/privacy}", "received_events_url": "https://api.github.com/users/ankitk2109/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-02-11T09:48:34Z", "updated_at": "2022-02-14T15:26:03Z", "closed_at": "2022-02-14T15:26:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nUnable to load the \"big_patent\" dataset\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset('big_patent', 'd', 'validation')\r\n```\r\n\r\n## Expected results\r\nDownload big_patents' validation split from the 'd' subset\r\n\r\n## Getting an error saying:\r\n{FileNotFoundError}Local file ..\\huggingface\\datasets\\downloads\\6159313604f4f2c01e7d1cac52139343b6c07f73f6de348d09be6213478455c5\\bigPatentData\\train.tar.gz doesn't exist\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:1.18.3\r\n- Platform: Windows\r\n- Python version:3.8\r\n- PyArrow version:7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3706/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3704", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3704/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3704/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3704/events", "html_url": "https://github.com/huggingface/datasets/issues/3704", "id": 1132042631, "node_id": "I_kwDODunzps5DeZmH", "number": 3704, "title": "OSCAR-2109 datasets are misaligned and truncated", "user": {"login": "adrianeboyd", "id": 5794899, "node_id": "MDQ6VXNlcjU3OTQ4OTk=", "avatar_url": "https://avatars.githubusercontent.com/u/5794899?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adrianeboyd", "html_url": "https://github.com/adrianeboyd", "followers_url": "https://api.github.com/users/adrianeboyd/followers", "following_url": "https://api.github.com/users/adrianeboyd/following{/other_user}", "gists_url": "https://api.github.com/users/adrianeboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/adrianeboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adrianeboyd/subscriptions", "organizations_url": "https://api.github.com/users/adrianeboyd/orgs", "repos_url": "https://api.github.com/users/adrianeboyd/repos", "events_url": "https://api.github.com/users/adrianeboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/adrianeboyd/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2022-02-11T08:14:59Z", "updated_at": "2022-03-17T18:01:04Z", "closed_at": "2022-03-16T16:21:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThe `oscar-corpus/OSCAR-2109` data appears to be misaligned and truncated by the dataset builder for subsets that contain more than one part and for cases where the texts contain non-unix newlines.\r\n\r\n## Steps to reproduce the bug\r\n\r\nA few examples, although I'm not sure how deterministic the particular (mis)alignment is in various configurations:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"oscar-corpus/OSCAR-2109\", \"deduplicated_fi\", split=\"train\", use_auth_token=True)\r\nentry = dataset[0]\r\n# entry[\"text\"] is from fi_part_3.txt.gz\r\n# entry[\"meta\"] is from fi_meta_part_2.jsonl.gz\r\n\r\ndataset = load_dataset(\"oscar-corpus/OSCAR-2109\", \"deduplicated_no\", split=\"train\", use_auth_token=True)\r\nentry = dataset[900000]\r\n# entry[\"text\"] is from no_part_3.txt.gz and contains a blank line\r\n# entry[\"meta\"] is from no_meta_part_1.jsonl.gz\r\n\r\ndataset = load_dataset(\"oscar-corpus/OSCAR-2109\", \"deduplicated_mk\", split=\"train\", streaming=True, use_auth_token=True)\r\n# 9088 texts in the dataset are empty\r\n```\r\n\r\nFor `deduplicated_fi`, all exported raw texts from the dataset are 17GB rather than 20GB as reported in the data splits overview table. The token count with `wc -w` for the raw texts is 2,067,556,874 rather than the expected 2,357,264,196 from the data splits table.\r\n\r\nFor `deduplicated_no` all exported raw texts contain 624,040,887 rather than the expected 776,354,517 tokens.\r\n\r\nFor `deduplicated_mk` it is 122,236,936 rather than 134,544,934 tokens. \r\n\r\nI'm not expecting the `wc -w` counts to line up exactly with the data splits table, but for comparison the `wc -w` count for `deduplicated_mk` on the raw texts is 134,545,424.\r\n\r\n## Issues\r\n\r\n* The meta / text files are not paired correctly when loading, so the extracted texts do not have the right offsets, the metadata is not associated with the correct text, and the text files may not be processed to the end or may be processed beyond the end (empty texts).\r\n* The line count offset is not reset per file so the texts aren't aligned to the right offsets in any parts beyond the first part, leading to truncation when in effect blank lines are not skipped.\r\n* Non-unix newline characters are treated as newlines when reading the text files while the metadata only counts unix newlines for its line offsets, leading to further misalignments between the metadata and the extracted texts, and which also results in truncation.\r\n\r\n## Expected results\r\n\r\nAll texts from the OSCAR release are extracted according to the metadata and aligned with the correct metadata.\r\n\r\n## Fixes\r\n\r\nNot necessarily the exact fixes/checks you may want to use (I didn't test all languages or do any cross-platform testing, I'm not sure all the details are compatible with streaming), however to highlight the issues:\r\n\r\n```diff\r\ndiff --git a/OSCAR-2109.py b/OSCAR-2109.py\r\nindex bbac1076..5eee8de7 100644\r\n--- a/OSCAR-2109.py\r\n+++ b/OSCAR-2109.py\r\n@@ -20,6 +20,7 @@\r\n import collections\r\n import gzip\r\n import json\r\n+import os\r\n \r\n import datasets\r\n \r\n@@ -387,9 +388,20 @@ class Oscar2109(datasets.GeneratorBasedBuilder):\r\n         with open(checksum_file, encoding=\"utf-8\") as f:\r\n             data_filenames = [line.split()[1] for line in f if line]\r\n             data_urls = [self.config.base_data_path + data_filename for data_filename in data_filenames]\r\n-        text_files = dl_manager.download([url for url in data_urls if url.endswith(\".txt.gz\")])\r\n-        metadata_files = dl_manager.download([url for url in data_urls if url.endswith(\".jsonl.gz\")])\r\n+        # sort filenames so corresponding parts are aligned\r\n+        text_files = sorted(dl_manager.download([url for url in data_urls if url.endswith(\".txt.gz\")]))\r\n+        metadata_files = sorted(dl_manager.download([url for url in data_urls if url.endswith(\".jsonl.gz\")]))\r\n+        assert len(text_files) == len(metadata_files)\r\n         metadata_and_text_files = list(zip(metadata_files, text_files))\r\n+        for meta_path, text_path in metadata_and_text_files:\r\n+            # check that meta/text part numbers are the same\r\n+            if \"part\" in os.path.basename(text_path):\r\n+                assert (\r\n+                    os.path.basename(text_path).replace(\".txt.gz\", \"\").split(\"_\")[-1]\r\n+                    == os.path.basename(meta_path).replace(\".jsonl.gz\", \"\").split(\"_\")[-1]\r\n+                )\r\n+            else:\r\n+                assert len(metadata_and_text_files) == 1\r\n         return [\r\n             datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"metadata_and_text_files\": metadata_and_text_files}),\r\n         ]\r\n@@ -397,10 +409,14 @@ class Oscar2109(datasets.GeneratorBasedBuilder):\r\n     def _generate_examples(self, metadata_and_text_files):\r\n         \"\"\"This function returns the examples in the raw (text) form by iterating on all the files.\"\"\"\r\n         id_ = 0\r\n-        offset = 0\r\n         for meta_path, text_path in metadata_and_text_files:\r\n+            # line offsets are per text file\r\n+            offset = 0\r\n             logger.info(\"generating examples from = %s\", text_path)\r\n-            with gzip.open(open(text_path, \"rb\"), \"rt\", encoding=\"utf-8\") as text_f:\r\n+            # some texts contain non-Unix newlines that should not be\r\n+            # interpreted as line breaks for the line counts in the metadata\r\n+            # with readline()\r\n+            with gzip.open(open(text_path, \"rb\"), \"rt\", encoding=\"utf-8\", newline=\"\\n\") as text_f:\r\n                 with gzip.open(open(meta_path, \"rb\"), \"rt\", encoding=\"utf-8\") as meta_f:\r\n                     for line in meta_f:\r\n                         # read meta\r\n@@ -411,7 +427,12 @@ class Oscar2109(datasets.GeneratorBasedBuilder):\r\n                             offset += 1\r\n                             text_f.readline()\r\n                         # read text\r\n-                        text = \"\".join([text_f.readline() for _ in range(meta[\"nb_sentences\"])]).rstrip()\r\n+                        text_lines = [text_f.readline() for _ in range(meta[\"nb_sentences\"])]\r\n+                        # all lines contain text (no blank lines or EOF)\r\n+                        assert all(text_lines)\r\n+                        assert \"\\n\" not in text_lines\r\n                         offset += meta[\"nb_sentences\"]\r\n+                        # only strip the trailing newline\r\n+                        text = \"\".join(text_lines).rstrip(\"\\n\")\r\n                         yield id_, {\"id\": id_, \"text\": text, \"meta\": meta}\r\n                         id_ += 1\r\n```\r\n\r\nI've tested this with a number of smaller deduplicated languages with 1-20 parts and the resulting datasets looked correct in terms of word count and size when compared to the data splits table and raw texts, and the text/metadata alignments were correct in all my spot checks. However, there are many many languages I didn't test and I'm not sure that there aren't any texts containing blank lines in the corpus, for instance. For the cases I tested, the assertions related to blank lines and EOF made it easier to verify that the text and metadata were aligned as intended, since there would be little chance of spurious alignments of variable-length texts across so much data.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3704/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3704/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3700", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3700/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3700/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3700/events", "html_url": "https://github.com/huggingface/datasets/issues/3700", "id": 1130252496, "node_id": "I_kwDODunzps5DXkjQ", "number": 3700, "title": "Unable to load a dataset", "user": {"login": "PaulchauvinAI", "id": 97964230, "node_id": "U_kgDOBdbQxg", "avatar_url": "https://avatars.githubusercontent.com/u/97964230?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PaulchauvinAI", "html_url": "https://github.com/PaulchauvinAI", "followers_url": "https://api.github.com/users/PaulchauvinAI/followers", "following_url": "https://api.github.com/users/PaulchauvinAI/following{/other_user}", "gists_url": "https://api.github.com/users/PaulchauvinAI/gists{/gist_id}", "starred_url": "https://api.github.com/users/PaulchauvinAI/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PaulchauvinAI/subscriptions", "organizations_url": "https://api.github.com/users/PaulchauvinAI/orgs", "repos_url": "https://api.github.com/users/PaulchauvinAI/repos", "events_url": "https://api.github.com/users/PaulchauvinAI/events{/privacy}", "received_events_url": "https://api.github.com/users/PaulchauvinAI/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-02-10T15:05:53Z", "updated_at": "2022-02-11T22:56:39Z", "closed_at": "2022-02-11T22:56:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nUnable to load a dataset from Huggingface that I have just saved.\r\n\r\n\r\n## Steps to reproduce the bug\r\nOn Google colab\r\n`! pip install datasets `\r\n`from datasets import load_dataset`\r\n`my_path = \"wiki_dataset\"`\r\n`dataset = load_dataset('wikipedia', \"20200501.fr\")`\r\n`dataset.save_to_disk(my_path)`\r\n`dataset = load_dataset(my_path)`\r\n\r\n\r\n## Expected results\r\nLoading the dataset\r\n\r\n## Actual results\r\nValueError: Couldn't cast\r\n_data_files: list<item: struct<filename: string>>\r\n  child 0, item: struct<filename: string>\r\n      child 0, filename: string\r\n_fingerprint: string\r\n_format_columns: null\r\n_format_kwargs: struct<>\r\n_format_type: null\r\n_indexes: struct<>\r\n_output_all_columns: bool\r\n_split: string\r\nto\r\n{'builder_name': Value(dtype='string', id=None), 'citation': Value(dtype='string', id=None), 'config_name': Value(dtype='string', id=None), 'dataset_size': Value(dtype='int64', id=None), 'description': Value(dtype='string', id=None), 'download_checksums': {}, 'download_size': Value(dtype='int64', id=None), 'features': {'title': {'dtype': Value(dtype='string', id=None), 'id': Value(dtype='null', id=None), '_type': Value(dtype='string', id=None)}, 'text': {'dtype': Value(dtype='string', id=None), 'id': Value(dtype='null', id=None), '_type': Value(dtype='string', id=None)}}, 'homepage': Value(dtype='string', id=None), 'license': Value(dtype='string', id=None), 'post_processed': Value(dtype='null', id=None), 'post_processing_size': Value(dtype='null', id=None), 'size_in_bytes': Value(dtype='int64', id=None), 'splits': {'train': {'name': Value(dtype='string', id=None), 'num_bytes': Value(dtype='int64', id=None), 'num_examples': Value(dtype='int64', id=None), 'dataset_name': Value(dtype='string', id=None)}}, 'supervised_keys': Value(dtype='null', id=None), 'task_templates': Value(dtype='null', id=None), 'version': {'version_str': Value(dtype='string', id=None), 'description': Value(dtype='string', id=None), 'major': Value(dtype='int64', id=None), 'minor': Value(dtype='int64', id=None), 'patch': Value(dtype='int64', id=None)}}\r\nbecause column names don't match\r\n\r\n## Environment info\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.12\r\n- PyArrow version: 6.0.1\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3700/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3700/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3688", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3688/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3688/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3688/events", "html_url": "https://github.com/huggingface/datasets/issues/3688", "id": 1127218321, "node_id": "I_kwDODunzps5DL_yR", "number": 3688, "title": "Pyarrow version error", "user": {"login": "Zaker237", "id": 49993443, "node_id": "MDQ6VXNlcjQ5OTkzNDQz", "avatar_url": "https://avatars.githubusercontent.com/u/49993443?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zaker237", "html_url": "https://github.com/Zaker237", "followers_url": "https://api.github.com/users/Zaker237/followers", "following_url": "https://api.github.com/users/Zaker237/following{/other_user}", "gists_url": "https://api.github.com/users/Zaker237/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zaker237/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zaker237/subscriptions", "organizations_url": "https://api.github.com/users/Zaker237/orgs", "repos_url": "https://api.github.com/users/Zaker237/repos", "events_url": "https://api.github.com/users/Zaker237/events{/privacy}", "received_events_url": "https://api.github.com/users/Zaker237/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-02-08T12:53:59Z", "updated_at": "2022-02-09T06:35:33Z", "closed_at": "2022-02-09T06:35:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI installed datasets(version 1.17.0, 1.18.0, 1.18.3) but i'm right now nor able to import it because of pyarrow. when i try to import it, i get the following error:\r\n`To use datasets, the module pyarrow>=3.0.0 is required, and the current version of pyarrow doesn't match this condition`.\r\ni tryed with all version of pyarrow execpt `4.0.0` but still get the same error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-19-652e886d387f> in <module>\r\n----> 1 import datasets\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\datasets\\__init__.py in <module>\r\n     26 \r\n     27 \r\n---> 28 if _version.parse(pyarrow.__version__).major < 3:\r\n     29     raise ImportWarning(\r\n     30         \"To use `datasets`, the module `pyarrow>=3.0.0` is required, and the current version of `pyarrow` doesn't match this condition.\\n\"\r\n\r\nAttributeError: 'Version' object has no attribute 'major'\r\n\r\n## Environment info\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\alex\\appdata\\local\\continuum\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\alex\\appdata\\local\\continuum\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Alex\\AppData\\Local\\Continuum\\anaconda3\\Scripts\\datasets-cli.exe\\__main__.py\", line 5, in <module>\r\n  File \"c:\\users\\alex\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\datasets\\__init__.py\", line 28, in <module>\r\n    if _version.parse(pyarrow.__version__).major < 3:\r\nAttributeError: 'Version' object has no attribute 'major'\r\n\r\n- `datasets` version:\r\n- Platform: Linux(Ubuntu) and Windows: conda on the both\r\n- Python version: 3.7\r\n- PyArrow version: 7.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3688/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3688/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3686", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3686/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3686/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3686/events", "html_url": "https://github.com/huggingface/datasets/issues/3686", "id": 1127137290, "node_id": "I_kwDODunzps5DLsAK", "number": 3686, "title": "`Translation` features cannot be `flatten`ed", "user": {"login": "SBrandeis", "id": 33657802, "node_id": "MDQ6VXNlcjMzNjU3ODAy", "avatar_url": "https://avatars.githubusercontent.com/u/33657802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SBrandeis", "html_url": "https://github.com/SBrandeis", "followers_url": "https://api.github.com/users/SBrandeis/followers", "following_url": "https://api.github.com/users/SBrandeis/following{/other_user}", "gists_url": "https://api.github.com/users/SBrandeis/gists{/gist_id}", "starred_url": "https://api.github.com/users/SBrandeis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SBrandeis/subscriptions", "organizations_url": "https://api.github.com/users/SBrandeis/orgs", "repos_url": "https://api.github.com/users/SBrandeis/repos", "events_url": "https://api.github.com/users/SBrandeis/events{/privacy}", "received_events_url": "https://api.github.com/users/SBrandeis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-02-08T11:33:48Z", "updated_at": "2022-03-18T17:28:13Z", "closed_at": "2022-03-18T17:28:13Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\n(`Dataset.flatten`)[https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_dataset.py#L1265] fails for columns with feature (`Translation`)[https://github.com/huggingface/datasets/blob/3edbeb0ec6519b79f1119adc251a1a6b379a2c12/src/datasets/features/translation.py#L8]\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"europa_ecdc_tm\", \"en2fr\", split=\"train[:10]\")\r\nprint(dataset.features)\r\n# {'translation': Translation(languages=['en', 'fr'], id=None)}\r\nprint(dataset[0])\r\n# {'translation': {'en': 'Vaccination against hepatitis C is not yet available.', 'fr': 'Aucune vaccination contre l\u2019h\u00e9patite C n\u2019est encore disponible.'}}\r\n\r\ndataset.flatten()\r\n```\r\n\r\n## Expected results\r\n\r\n`dataset.flatten` should flatten the `Translation` column as if it were a dict of `Value(\"string\")`\r\n\r\n```python\r\ndataset[0]\r\n# {'translation.en':  'Vaccination against hepatitis C is not yet available.', 'translation.fr': 'Aucune vaccination contre l\u2019h\u00e9patite C n\u2019est encore disponible.' }\r\ndataset.features\r\n# {'translation.en': Value(\"string\"), 'translation.fr': Value(\"string\")}\r\n```\r\n\r\n## Actual results\r\n\r\n```python\r\nIn [31]: dset.flatten()\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-31-bb88eb5276ee> in <module>\r\n----> 1 dset.flatten()\r\n\r\n[...]\\site-packages\\datasets\\fingerprint.py in wrapper(*args, **kwargs)\r\n    411             # Call actual function\r\n    412\r\n--> 413             out = func(self, *args, **kwargs)\r\n    414\r\n    415             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n[...]\\site-packages\\datasets\\arrow_dataset.py in flatten(self, new_fingerprint, max_depth)\r\n   1294                 break\r\n   1295         dataset.info.features = self.features.flatten(max_depth=max_depth)\r\n-> 1296         dataset._data = update_metadata_with_features(dataset._data, dataset.features)\r\n   1297         logger.info(f'Flattened dataset from depth {depth} to depth {1 if depth + 1 < max_depth else \"unknown\"}.')\r\n   1298         dataset._fingerprint = new_fingerprint\r\n\r\n[...]\\site-packages\\datasets\\arrow_dataset.py in update_metadata_with_features(table, features)\r\n    534 def update_metadata_with_features(table: Table, features: Features):\r\n    535     \"\"\"To be used in dataset transforms that modify the features of the dataset, in order to update the features stored in the metadata of its schema.\"\"\"\r\n--> 536     features = Features({col_name: features[col_name] for col_name in table.column_names})\r\n    537     if table.schema.metadata is None or b\"huggingface\" not in table.schema.metadata:\r\n    538         pa_metadata = ArrowWriter._build_metadata(DatasetInfo(features=features))\r\n\r\n[...]\\site-packages\\datasets\\arrow_dataset.py in <dictcomp>(.0)\r\n    534 def update_metadata_with_features(table: Table, features: Features):\r\n    535     \"\"\"To be used in dataset transforms that modify the features of the dataset, in order to update the features stored in the metadata of its schema.\"\"\"\r\n--> 536     features = Features({col_name: features[col_name] for col_name in table.column_names})\r\n    537     if table.schema.metadata is None or b\"huggingface\" not in table.schema.metadata:\r\n    538         pa_metadata = ArrowWriter._build_metadata(DatasetInfo(features=features))\r\n\r\nKeyError: 'translation.en'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.18.3\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3686/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3686/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3677", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3677/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3677/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3677/events", "html_url": "https://github.com/huggingface/datasets/issues/3677", "id": 1123192866, "node_id": "I_kwDODunzps5C8pAi", "number": 3677, "title": "Discovery cannot be streamed anymore", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-02-03T15:02:03Z", "updated_at": "2022-02-10T16:51:24Z", "closed_at": "2022-02-10T16:51:24Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\niterable_dataset = load_dataset(\"discovery\", name=\"discovery\", split=\"train\", streaming=True)\r\nlist(iterable_dataset.take(1))\r\n```\r\n\r\n## Expected results\r\n\r\nThe first row of the train split.\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/iterable_dataset.py\", line 365, in __iter__\r\n    for key, example in self._iter():\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/iterable_dataset.py\", line 362, in _iter\r\n    yield from ex_iterable\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/iterable_dataset.py\", line 272, in __iter__\r\n    yield from islice(self.ex_iterable, self.n)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/iterable_dataset.py\", line 79, in __iter__\r\n    yield from self.generate_examples_fn(**self.kwargs)\r\n  File \"/home/slesage/.cache/huggingface/modules/datasets_modules/datasets/discovery/542fab7a9ddc1d9726160355f7baa06a1ccc44c40bc8e12c09e9bc743aca43a2/discovery.py\", line 333, in _generate_examples\r\n    with open(data_file, encoding=\"utf8\") as f:\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/streaming.py\", line 64, in wrapper\r\n    return function(*args, use_auth_token=use_auth_token, **kwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/utils/streaming_download_manager.py\", line 369, in xopen\r\n    file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/fsspec/core.py\", line 456, in open\r\n    return open_files(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/fsspec/core.py\", line 288, in open_files\r\n    fs, fs_token, paths = get_fs_token_paths(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/fsspec/core.py\", line 611, in get_fs_token_paths\r\n    fs = filesystem(protocol, **inkwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/fsspec/registry.py\", line 253, in filesystem\r\n    return cls(**storage_options)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/fsspec/spec.py\", line 68, in __call__\r\n    obj = super().__call__(*args, **kwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/fsspec/implementations/zip.py\", line 57, in __init__\r\n    self.zip = zipfile.ZipFile(self.fo)\r\n  File \"/home/slesage/.pyenv/versions/3.9.6/lib/python3.9/zipfile.py\", line 1257, in __init__\r\n    self._RealGetContents()\r\n  File \"/home/slesage/.pyenv/versions/3.9.6/lib/python3.9/zipfile.py\", line 1320, in _RealGetContents\r\n    endrec = _EndRecData(fp)\r\n  File \"/home/slesage/.pyenv/versions/3.9.6/lib/python3.9/zipfile.py\", line 263, in _EndRecData\r\n    fpin.seek(0, 2)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/fsspec/implementations/http.py\", line 676, in seek\r\n    raise ValueError(\"Cannot seek streaming HTTP file\")\r\nValueError: Cannot seek streaming HTTP file\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.18.3\r\n- Platform: Linux-5.11.0-1027-aws-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3677/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3673", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3673/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3673/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3673/events", "html_url": "https://github.com/huggingface/datasets/issues/3673", "id": 1123010520, "node_id": "I_kwDODunzps5C78fY", "number": 3673, "title": "`load_dataset(\"snli\")` is different from dataset viewer", "user": {"login": "pietrolesci", "id": 61748653, "node_id": "MDQ6VXNlcjYxNzQ4NjUz", "avatar_url": "https://avatars.githubusercontent.com/u/61748653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietrolesci", "html_url": "https://github.com/pietrolesci", "followers_url": "https://api.github.com/users/pietrolesci/followers", "following_url": "https://api.github.com/users/pietrolesci/following{/other_user}", "gists_url": "https://api.github.com/users/pietrolesci/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietrolesci/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietrolesci/subscriptions", "organizations_url": "https://api.github.com/users/pietrolesci/orgs", "repos_url": "https://api.github.com/users/pietrolesci/repos", "events_url": "https://api.github.com/users/pietrolesci/events{/privacy}", "received_events_url": "https://api.github.com/users/pietrolesci/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 3470211881, "node_id": "LA_kwDODunzps7O1zsp", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset-viewer", "name": "dataset-viewer", "color": "E5583E", "default": false, "description": "Related to the dataset viewer on huggingface.co"}], "state": "closed", "locked": false, "assignee": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2022-02-03T12:10:43Z", "updated_at": "2022-02-16T11:22:31Z", "closed_at": "2022-02-11T17:01:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe dataset that is downloaded from the Hub via `load_dataset(\"snli\")` is different from what is available in the dataset viewer. In the viewer the labels are not encoded (i.e., \"neutral\", \"entailment\", \"contradiction\"), while the downloaded dataset shows the encoded labels (i.e., 0, 1, 2).\r\n\r\nIs this expected? \r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform: Ubuntu 20.4\r\n- Python version: 3.7\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3673/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3673/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3668", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3668/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3668/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3668/events", "html_url": "https://github.com/huggingface/datasets/issues/3668", "id": 1122261736, "node_id": "I_kwDODunzps5C5Fro", "number": 3668, "title": " Couldn't cast array of type string error with cast_column", "user": {"login": "R4ZZ3", "id": 25264037, "node_id": "MDQ6VXNlcjI1MjY0MDM3", "avatar_url": "https://avatars.githubusercontent.com/u/25264037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/R4ZZ3", "html_url": "https://github.com/R4ZZ3", "followers_url": "https://api.github.com/users/R4ZZ3/followers", "following_url": "https://api.github.com/users/R4ZZ3/following{/other_user}", "gists_url": "https://api.github.com/users/R4ZZ3/gists{/gist_id}", "starred_url": "https://api.github.com/users/R4ZZ3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/R4ZZ3/subscriptions", "organizations_url": "https://api.github.com/users/R4ZZ3/orgs", "repos_url": "https://api.github.com/users/R4ZZ3/repos", "events_url": "https://api.github.com/users/R4ZZ3/events{/privacy}", "received_events_url": "https://api.github.com/users/R4ZZ3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-02-02T18:33:29Z", "updated_at": "2022-07-19T13:36:24Z", "closed_at": "2022-07-19T13:36:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\n\r\nIn OVH cloud during Huggingface Robust-speech-recognition event on a AI training notebook instance using jupyter lab and running jupyter notebook When using the dataset.cast_column(\"audio\",Audio(sampling_rate=16_000))\r\nmethod I get error\r\n![image](https://user-images.githubusercontent.com/25264037/152214027-9c42a71a-dd24-463c-a346-57e0287e5a8f.png)\r\n\r\nThis was working with datasets version 1.17.1.dev0\r\nbut now with version 1.18.3 produces the error above.\r\n\r\n## Steps to reproduce the bug\r\n\r\nload dataset:\r\n![image](https://user-images.githubusercontent.com/25264037/152216145-159553b6-cddc-4f0b-8607-7e76b600e22a.png)\r\n\r\n\r\nremove columns:\r\n![image](https://user-images.githubusercontent.com/25264037/152214707-7c7e89d1-87d8-4b4f-8cfc-5d7223d35644.png)\r\n\r\nrun my fix_path function.\r\nThis also creates the audio column that is referring to the absolute file path of the audio\r\n\r\n![image](https://user-images.githubusercontent.com/25264037/152214773-51f71ccf-d31b-4449-b63a-1af56436e49f.png)\r\n\r\nThen I concatenate few other datasets and finally try the cast_column method\r\n![image](https://user-images.githubusercontent.com/25264037/152215032-f341ec86-9d6d-48c9-943b-e2efe37a4d98.png)\r\n\r\nbut get error:\r\n![image](https://user-images.githubusercontent.com/25264037/152215073-b85bd057-98e8-413c-9b05-51e9805f2c24.png)\r\n\r\n\r\n\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3\r\n- Platform: \r\nOVH Cloud, AI Training section, container for Huggingface Robust Speech Recognition event image(baaastijn/ovh_huggingface)\r\n![image](https://user-images.githubusercontent.com/25264037/152215161-b4ff7bfb-2736-4afb-9223-761a3338d23c.png)\r\n\r\n- Python version: 3.8.8\r\n- PyArrow version:\r\n![image](https://user-images.githubusercontent.com/25264037/152215936-4d365760-557e-456b-b5eb-ad1d15cf5073.png)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3668/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3663", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3663/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3663/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3663/events", "html_url": "https://github.com/huggingface/datasets/issues/3663", "id": 1121067647, "node_id": "I_kwDODunzps5C0iJ_", "number": 3663, "title": "[Audio] Path of Common Voice cannot be used for audio loading anymore", "user": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, {"login": "polinaeterna", "id": 16348744, "node_id": "MDQ6VXNlcjE2MzQ4NzQ0", "avatar_url": "https://avatars.githubusercontent.com/u/16348744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polinaeterna", "html_url": "https://github.com/polinaeterna", "followers_url": "https://api.github.com/users/polinaeterna/followers", "following_url": "https://api.github.com/users/polinaeterna/following{/other_user}", "gists_url": "https://api.github.com/users/polinaeterna/gists{/gist_id}", "starred_url": "https://api.github.com/users/polinaeterna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polinaeterna/subscriptions", "organizations_url": "https://api.github.com/users/polinaeterna/orgs", "repos_url": "https://api.github.com/users/polinaeterna/repos", "events_url": "https://api.github.com/users/polinaeterna/events{/privacy}", "received_events_url": "https://api.github.com/users/polinaeterna/received_events", "type": "User", "site_admin": false}, {"login": "anton-l", "id": 26864830, "node_id": "MDQ6VXNlcjI2ODY0ODMw", "avatar_url": "https://avatars.githubusercontent.com/u/26864830?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anton-l", "html_url": "https://github.com/anton-l", "followers_url": "https://api.github.com/users/anton-l/followers", "following_url": "https://api.github.com/users/anton-l/following{/other_user}", "gists_url": "https://api.github.com/users/anton-l/gists{/gist_id}", "starred_url": "https://api.github.com/users/anton-l/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anton-l/subscriptions", "organizations_url": "https://api.github.com/users/anton-l/orgs", "repos_url": "https://api.github.com/users/anton-l/repos", "events_url": "https://api.github.com/users/anton-l/events{/privacy}", "received_events_url": "https://api.github.com/users/anton-l/received_events", "type": "User", "site_admin": false}, {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 19, "created_at": "2022-02-01T18:40:10Z", "updated_at": "2022-09-21T15:03:09Z", "closed_at": "2022-09-21T14:56:22Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfrom torchaudio import load\r\n\r\nds = load_dataset(\"common_voice\", \"ab\", split=\"train\")\r\n\r\n# both of the following commands fail at the moment\r\nload(ds[0][\"audio\"][\"path\"])\r\nload(ds[0][\"path\"])\r\n```\r\n\r\n## Expected results\r\n\r\nThe path should be the complete absolute path to the downloaded audio file not some relative path.\r\n\r\n\r\n## Actual results\r\n\r\n```bash\r\n~/hugging_face/venv_3.9/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py in load(filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n    150                 filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n    151         filepath = os.fspath(filepath)\r\n--> 152     return torch.ops.torchaudio.sox_io_load_audio_file(\r\n    153         filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n    154\r\n\r\nRuntimeError: Error loading audio file: failed to open file cv-corpus-6.1-2020-12-11/ab/clips/common_voice_ab_19904194.mp3\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.3.dev0\r\n- Platform: Linux-5.4.0-96-generic-x86_64-with-glibc2.27\r\n- Python version: 3.9.1\r\n- PyArrow version: 3.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3663/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3663/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3656", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3656/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3656/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3656/events", "html_url": "https://github.com/huggingface/datasets/issues/3656", "id": 1120510823, "node_id": "I_kwDODunzps5CyaNn", "number": 3656, "title": "checksum error subjqa dataset", "user": {"login": "RensDimmendaal", "id": 9828683, "node_id": "MDQ6VXNlcjk4Mjg2ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/9828683?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RensDimmendaal", "html_url": "https://github.com/RensDimmendaal", "followers_url": "https://api.github.com/users/RensDimmendaal/followers", "following_url": "https://api.github.com/users/RensDimmendaal/following{/other_user}", "gists_url": "https://api.github.com/users/RensDimmendaal/gists{/gist_id}", "starred_url": "https://api.github.com/users/RensDimmendaal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RensDimmendaal/subscriptions", "organizations_url": "https://api.github.com/users/RensDimmendaal/orgs", "repos_url": "https://api.github.com/users/RensDimmendaal/repos", "events_url": "https://api.github.com/users/RensDimmendaal/events{/privacy}", "received_events_url": "https://api.github.com/users/RensDimmendaal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-02-01T10:53:33Z", "updated_at": "2022-02-10T10:56:59Z", "closed_at": "2022-02-10T10:56:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nI get a checksum error when loading the `subjqa` dataset (used in the transformers book).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nsubjqa = load_dataset(\"subjqa\",\"electronics\")\r\n```\r\n\r\n## Expected results\r\nLoading the dataset\r\n\r\n## Actual results\r\n\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n\r\n<ipython-input-2-d2857d460155> in <module>()\r\n      2 from datasets import load_dataset\r\n      3 \r\n----> 4 subjqa = load_dataset(\"subjqa\",\"electronics\")\r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://github.com/lewtun/SubjQA/archive/refs/heads/master.zip']\r\n```\r\n\r\n## Environment info\r\n\r\nGoogle colab\r\n\r\n- `datasets` version: 1.18.2\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.12\r\n- PyArrow version: 3.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3656/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3656/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3655", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3655/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3655/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3655/events", "html_url": "https://github.com/huggingface/datasets/issues/3655", "id": 1119801077, "node_id": "I_kwDODunzps5Cvs71", "number": 3655, "title": "Pubmed dataset not reachable", "user": {"login": "abhi-mosaic", "id": 77638579, "node_id": "MDQ6VXNlcjc3NjM4NTc5", "avatar_url": "https://avatars.githubusercontent.com/u/77638579?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhi-mosaic", "html_url": "https://github.com/abhi-mosaic", "followers_url": "https://api.github.com/users/abhi-mosaic/followers", "following_url": "https://api.github.com/users/abhi-mosaic/following{/other_user}", "gists_url": "https://api.github.com/users/abhi-mosaic/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhi-mosaic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhi-mosaic/subscriptions", "organizations_url": "https://api.github.com/users/abhi-mosaic/orgs", "repos_url": "https://api.github.com/users/abhi-mosaic/repos", "events_url": "https://api.github.com/users/abhi-mosaic/events{/privacy}", "received_events_url": "https://api.github.com/users/abhi-mosaic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2022-01-31T18:45:47Z", "updated_at": "2022-12-19T19:18:10Z", "closed_at": "2022-02-14T14:15:41Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nTrying to use the `pubmed` dataset fails to reach / download the source files.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\npubmed_train = datasets.load_dataset('pubmed', split='train')\r\n```\r\n\r\n## Expected results\r\nShould begin downloading the pubmed dataset.\r\n\r\n## Actual results\r\n```\r\nConnectionError: Couldn't reach ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/pubmed21n0865.xml.gz (InvalidSchema(\"No connection adapters were found for 'ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/pubmed21n0865.xml.gz'\"))\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.18.2\r\n- Platform: macOS-11.4-x86_64-i386-64bit\r\n- Python version: 3.8.2\r\n- PyArrow version: 6.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3655/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3640", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3640/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3640/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3640/events", "html_url": "https://github.com/huggingface/datasets/issues/3640", "id": 1116133769, "node_id": "I_kwDODunzps5ChtmJ", "number": 3640, "title": "Issues with custom dataset in Wav2Vec2", "user": {"login": "peregilk", "id": 9079808, "node_id": "MDQ6VXNlcjkwNzk4MDg=", "avatar_url": "https://avatars.githubusercontent.com/u/9079808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peregilk", "html_url": "https://github.com/peregilk", "followers_url": "https://api.github.com/users/peregilk/followers", "following_url": "https://api.github.com/users/peregilk/following{/other_user}", "gists_url": "https://api.github.com/users/peregilk/gists{/gist_id}", "starred_url": "https://api.github.com/users/peregilk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peregilk/subscriptions", "organizations_url": "https://api.github.com/users/peregilk/orgs", "repos_url": "https://api.github.com/users/peregilk/repos", "events_url": "https://api.github.com/users/peregilk/events{/privacy}", "received_events_url": "https://api.github.com/users/peregilk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-01-27T12:09:05Z", "updated_at": "2022-01-27T12:29:48Z", "closed_at": "2022-01-27T12:29:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "We are training Vav2Vec using the run_speech_recognition_ctc_bnb.py-script.\r\n\r\nThis is working fine with Common Voice, however using our custom dataset and data loader at [NbAiLab/NPSC]( https://huggingface.co/datasets/NbAiLab/NPSC) it crashes after roughly 1 epoch with the following stack trace:\r\n\r\n![image](https://user-images.githubusercontent.com/9079808/151355893-6d5887cc-ca19-4b12-948a-124eb6dac372.png)\r\n\r\n\r\nWe are able to work around the issue, for instance by adding this check in line#222 in transformers/models/wav2vec2/modeling_wav2vec2.py:\r\n```python\r\nif input_length - (mask_length - 1) < num_masked_span:\r\n           num_masked_span = input_length - (mask_length - 1)\r\n```\r\nInterestingly, these are the variable values before the adjustment:\r\n```\r\ninput_length=10\r\nmask_length=10\r\nnum_masked_span=2\r\n````\r\nAfter adjusting num_masked_spin to 1, the training script runs. The issue is also fixed by setting \u201creplace=True\u201d in the same function.\r\n\r\nDo you have any  idea what is causing this, and how to fix this error permanently? If you do not think this is an Datasets issue, feel free to move the issue.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3640/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3640/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3639", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3639/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3639/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3639/events", "html_url": "https://github.com/huggingface/datasets/issues/3639", "id": 1116021420, "node_id": "I_kwDODunzps5ChSKs", "number": 3639, "title": "same value of precision, recall, f1 score at each epoch for classification task. ", "user": {"login": "Dhanachandra", "id": 10828657, "node_id": "MDQ6VXNlcjEwODI4NjU3", "avatar_url": "https://avatars.githubusercontent.com/u/10828657?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dhanachandra", "html_url": "https://github.com/Dhanachandra", "followers_url": "https://api.github.com/users/Dhanachandra/followers", "following_url": "https://api.github.com/users/Dhanachandra/following{/other_user}", "gists_url": "https://api.github.com/users/Dhanachandra/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dhanachandra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dhanachandra/subscriptions", "organizations_url": "https://api.github.com/users/Dhanachandra/orgs", "repos_url": "https://api.github.com/users/Dhanachandra/repos", "events_url": "https://api.github.com/users/Dhanachandra/events{/privacy}", "received_events_url": "https://api.github.com/users/Dhanachandra/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-01-27T10:14:16Z", "updated_at": "2022-02-24T09:02:18Z", "closed_at": "2022-02-24T09:02:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "**1st Epoch:** \r\n1/27/2022 09:30:48 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow.59it/s]\r\n01/27/2022 09:30:48 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow\r\n01/27/2022 09:30:49 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow\r\nPRECISION:  {'precision': 0.7612903225806451}\r\nRECALL:  {'recall': 0.7612903225806451}\r\nF1:  {'f1': 0.7612903225806451}\r\n{'eval_loss': 1.4658324718475342, 'eval_accuracy': 0.7612903118133545, 'eval_runtime': 30.0054, 'eval_samples_per_second': 46.492, 'eval_steps_per_second': 46.492, 'epoch': 3.0} \r\n**4th Epoch:**\r\n1/27/2022 09:56:55 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow.92it/s]\r\n01/27/2022 09:56:56 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow\r\n01/27/2022 09:56:56 - INFO - datasets.metric - Removing /home/ubuntu/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow\r\nPRECISION:  {'precision': 0.7698924731182796}\r\nRECALL:  {'recall': 0.7698924731182796}\r\nF1:  {'f1': 0.7698924731182796}\r\n\r\n\r\n## Environment info\r\n!git clone https://github.com/huggingface/transformers\r\n%cd transformers\r\n!pip install .\r\n!pip install -r /content/transformers/examples/pytorch/token-classification/requirements.txt\r\n!pip install datasets", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3639/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3637", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3637/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3637/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3637/events", "html_url": "https://github.com/huggingface/datasets/issues/3637", "id": 1115526438, "node_id": "I_kwDODunzps5CfZUm", "number": 3637, "title": "[TypeError: Couldn't cast array of type] Cannot load dataset in v1.18", "user": {"login": "lewtun", "id": 26859204, "node_id": "MDQ6VXNlcjI2ODU5MjA0", "avatar_url": "https://avatars.githubusercontent.com/u/26859204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lewtun", "html_url": "https://github.com/lewtun", "followers_url": "https://api.github.com/users/lewtun/followers", "following_url": "https://api.github.com/users/lewtun/following{/other_user}", "gists_url": "https://api.github.com/users/lewtun/gists{/gist_id}", "starred_url": "https://api.github.com/users/lewtun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lewtun/subscriptions", "organizations_url": "https://api.github.com/users/lewtun/orgs", "repos_url": "https://api.github.com/users/lewtun/repos", "events_url": "https://api.github.com/users/lewtun/events{/privacy}", "received_events_url": "https://api.github.com/users/lewtun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-01-26T21:38:02Z", "updated_at": "2022-02-09T16:15:53Z", "closed_at": "2022-02-09T16:15:53Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to load the [`GEM/RiSAWOZ` dataset](https://huggingface.co/datasets/GEM/RiSAWOZ) in `datasets` v1.18.1 and am running into a type error when casting the features. The strange thing is that I can load the dataset with v1.17.0. Note that the error is also present if I install from `master` too.\r\n\r\nAs far as I can tell, the dataset loading script is correct and the problematic features [here](https://huggingface.co/datasets/GEM/RiSAWOZ/blob/main/RiSAWOZ.py#L237) also look fine to me.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndset = load_dataset(\"GEM/RiSAWOZ\")\r\n```\r\n\r\n## Expected results\r\nI can load the dataset without error.\r\n\r\n## Actual results\r\n\r\n<details><summary>Traceback</summary>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/builder.py in _prepare_split(self, split_generator)\r\n   1083                     example = self.info.features.encode_example(record)\r\n-> 1084                     writer.write(example, key)\r\n   1085             finally:\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/arrow_writer.py in write(self, example, key, writer_batch_size)\r\n    445 \r\n--> 446             self.write_examples_on_file()\r\n    447 \r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/arrow_writer.py in write_examples_on_file(self)\r\n    403             batch_examples[col] = [row[0][col] for row in self.current_examples]\r\n--> 404         self.write_batch(batch_examples=batch_examples)\r\n    405         self.current_examples = []\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)\r\n    496             typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\r\n--> 497             arrays.append(pa.array(typed_sequence))\r\n    498             inferred_features[col] = typed_sequence.get_inferred_type()\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/arrow_writer.py in __arrow_array__(self, type)\r\n    204                 # We only do it if trying_type is False - since this is what the user asks for.\r\n--> 205                 out = cast_array_to_feature(out, type, allow_number_to_str=not self.trying_type)\r\n    206             return out\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    943             array = _sanitize(array)\r\n--> 944         return func(array, *args, **kwargs)\r\n    945 \r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    919         else:\r\n--> 920             return func(array, *args, **kwargs)\r\n    921 \r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1064         if isinstance(feature, list):\r\n-> 1065             return pa.ListArray.from_arrays(array.offsets, _c(array.values, feature[0]))\r\n   1066         elif isinstance(feature, Sequence):\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    943             array = _sanitize(array)\r\n--> 944         return func(array, *args, **kwargs)\r\n    945 \r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    919         else:\r\n--> 920             return func(array, *args, **kwargs)\r\n    921 \r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1059         if isinstance(feature, dict) and set(field.name for field in array.type) == set(feature):\r\n-> 1060             arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n   1061             return pa.StructArray.from_arrays(arrays, names=list(feature))\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in <listcomp>(.0)\r\n   1059         if isinstance(feature, dict) and set(field.name for field in array.type) == set(feature):\r\n-> 1060             arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n   1061             return pa.StructArray.from_arrays(arrays, names=list(feature))\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    943             array = _sanitize(array)\r\n--> 944         return func(array, *args, **kwargs)\r\n    945 \r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    919         else:\r\n--> 920             return func(array, *args, **kwargs)\r\n    921 \r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1059         if isinstance(feature, dict) and set(field.name for field in array.type) == set(feature):\r\n-> 1060             arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n   1061             return pa.StructArray.from_arrays(arrays, names=list(feature))\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in <listcomp>(.0)\r\n   1059         if isinstance(feature, dict) and set(field.name for field in array.type) == set(feature):\r\n-> 1060             arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n   1061             return pa.StructArray.from_arrays(arrays, names=list(feature))\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    943             array = _sanitize(array)\r\n--> 944         return func(array, *args, **kwargs)\r\n    945 \r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    919         else:\r\n--> 920             return func(array, *args, **kwargs)\r\n    921 \r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1086         return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n-> 1087     raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\n   1088 \r\n\r\nTypeError: Couldn't cast array of type\r\nstruct<\u533b\u9662-3.0T MRI: string, \u533b\u9662-CT: string, \u533b\u9662-DSA: string, \u533b\u9662-\u516c\u4ea4\u7ebf\u8def: string, \u533b\u9662-\u533a\u57df: string, \u533b\u9662-\u540d\u79f0: string, \u533b\u9662-\u5730\u5740: string, \u533b\u9662-\u5730\u94c1\u53ef\u8fbe: string, \u533b\u9662-\u5730\u94c1\u7ebf\u8def: string, \u533b\u9662-\u6027\u8d28: string, \u533b\u9662-\u6302\u53f7\u65f6\u95f4: string, \u533b\u9662-\u7535\u8bdd: string, \u533b\u9662-\u7b49\u7ea7: string, \u533b\u9662-\u7c7b\u522b: string, \u533b\u9662-\u91cd\u70b9\u79d1\u5ba4: string, \u533b\u9662-\u95e8\u8bca\u65f6\u95f4: string, \u5929\u6c14-\u57ce\u5e02: string, \u5929\u6c14-\u5929\u6c14: string, \u5929\u6c14-\u65e5\u671f: string, \u5929\u6c14-\u6e29\u5ea6: string, \u5929\u6c14-\u7d2b\u5916\u7ebf\u5f3a\u5ea6: string, \u5929\u6c14-\u98ce\u529b\u98ce\u5411: string, \u65c5\u6e38\u666f\u70b9-\u533a\u57df: string, \u65c5\u6e38\u666f\u70b9-\u540d\u79f0: string, \u65c5\u6e38\u666f\u70b9-\u5730\u5740: string, \u65c5\u6e38\u666f\u70b9-\u5f00\u653e\u65f6\u95f4: string, \u65c5\u6e38\u666f\u70b9-\u662f\u5426\u5730\u94c1\u76f4\u8fbe: string, \u65c5\u6e38\u666f\u70b9-\u666f\u70b9\u7c7b\u578b: string, \u65c5\u6e38\u666f\u70b9-\u6700\u9002\u5408\u4eba\u7fa4: string, \u65c5\u6e38\u666f\u70b9-\u6d88\u8d39: string, \u65c5\u6e38\u666f\u70b9-\u7279\u70b9: string, \u65c5\u6e38\u666f\u70b9-\u7535\u8bdd\u53f7\u7801: string, \u65c5\u6e38\u666f\u70b9-\u8bc4\u5206: string, \u65c5\u6e38\u666f\u70b9-\u95e8\u7968\u4ef7\u683c: string, \u6c7d\u8f66-\u4ef7\u683c(\u4e07\u5143): string, \u6c7d\u8f66-\u5012\u8f66\u5f71\u50cf: string, \u6c7d\u8f66-\u52a8\u529b\u6c34\u5e73: string, \u6c7d\u8f66-\u5382\u5546: string, \u6c7d\u8f66-\u53d1\u52a8\u673a\u6392\u91cf(L): string, \u6c7d\u8f66-\u53d1\u52a8\u673a\u9a6c\u529b(Ps): string, \u6c7d\u8f66-\u540d\u79f0: string, \u6c7d\u8f66-\u5b9a\u901f\u5de1\u822a: string, \u6c7d\u8f66-\u5de1\u822a\u7cfb\u7edf: string, \u6c7d\u8f66-\u5ea7\u4f4d\u6570: string, \u6c7d\u8f66-\u5ea7\u6905\u52a0\u70ed: string, \u6c7d\u8f66-\u5ea7\u6905\u901a\u98ce: string, \u6c7d\u8f66-\u6240\u5c5e\u4ef7\u683c\u533a\u95f4: string, \u6c7d\u8f66-\u6cb9\u8017\u6c34\u5e73: string, \u6c7d\u8f66-\u73af\u4fdd\u6807\u51c6: string, \u6c7d\u8f66-\u7ea7\u522b: string, \u6c7d\u8f66-\u7efc\u5408\u6cb9\u8017(L/100km): string, \u6c7d\u8f66-\u80fd\u6e90\u7c7b\u578b: string, \u6c7d\u8f66-\u8f66\u578b: string, \u6c7d\u8f66-\u8f66\u7cfb: string, \u6c7d\u8f66-\u8f66\u8eab\u5c3a\u5bf8(mm): string, \u6c7d\u8f66-\u9a71\u52a8\u65b9\u5f0f: string, \u6c7d\u8f66-\u9a7e\u9a76\u8f85\u52a9\u5f71\u50cf: string, \u706b\u8f66-\u51fa\u53d1\u5730: string, \u706b\u8f66-\u51fa\u53d1\u65f6\u95f4: string, \u706b\u8f66-\u5230\u8fbe\u65f6\u95f4: string, \u706b\u8f66-\u5750\u5e2d: string, \u706b\u8f66-\u65e5\u671f: string, \u706b\u8f66-\u65f6\u957f: string, \u706b\u8f66-\u76ee\u7684\u5730: string, \u706b\u8f66-\u7968\u4ef7: string, \u706b\u8f66-\u8231\u4f4d\u6863\u6b21: string, \u706b\u8f66-\u8f66\u578b: string, \u706b\u8f66-\u8f66\u6b21\u4fe1\u606f: string, \u7535\u5f71-\u4e3b\u6f14: string, \u7535\u5f71-\u4e3b\u6f14\u540d\u5355: string, \u7535\u5f71-\u5177\u4f53\u4e0a\u6620\u65f6\u95f4: string, \u7535\u5f71-\u5236\u7247\u56fd\u5bb6/\u5730\u533a: string, \u7535\u5f71-\u5bfc\u6f14: string, \u7535\u5f71-\u5e74\u4ee3: string, \u7535\u5f71-\u7247\u540d: string, \u7535\u5f71-\u7247\u957f: string, \u7535\u5f71-\u7c7b\u578b: string, \u7535\u5f71-\u8c46\u74e3\u8bc4\u5206: string, \u7535\u8111-CPU: string, \u7535\u8111-CPU\u578b\u53f7: string, \u7535\u8111-\u4ea7\u54c1\u7c7b\u522b: string, \u7535\u8111-\u4ef7\u683c: string, \u7535\u8111-\u4ef7\u683c\u533a\u95f4: string, \u7535\u8111-\u5185\u5b58\u5bb9\u91cf: string, \u7535\u8111-\u5206\u7c7b: string, \u7535\u8111-\u54c1\u724c: string, \u7535\u8111-\u5546\u54c1\u540d\u79f0: string, \u7535\u8111-\u5c4f\u5e55\u5c3a\u5bf8: string, \u7535\u8111-\u5f85\u673a\u65f6\u957f: string, \u7535\u8111-\u663e\u5361\u578b\u53f7: string, \u7535\u8111-\u663e\u5361\u7c7b\u522b: string, \u7535\u8111-\u6e38\u620f\u6027\u80fd: string, \u7535\u8111-\u7279\u6027: string, \u7535\u8111-\u786c\u76d8\u5bb9\u91cf: string, \u7535\u8111-\u7cfb\u5217: string, \u7535\u8111-\u7cfb\u7edf: string, \u7535\u8111-\u8272\u7cfb: string, \u7535\u8111-\u88f8\u673a\u91cd\u91cf: string, \u7535\u89c6\u5267-\u4e3b\u6f14: string, \u7535\u89c6\u5267-\u4e3b\u6f14\u540d\u5355: string, \u7535\u89c6\u5267-\u5236\u7247\u56fd\u5bb6/\u5730\u533a: string, \u7535\u89c6\u5267-\u5355\u96c6\u7247\u957f: string, \u7535\u89c6\u5267-\u5bfc\u6f14: string, \u7535\u89c6\u5267-\u5e74\u4ee3: string, \u7535\u89c6\u5267-\u7247\u540d: string, \u7535\u89c6\u5267-\u7c7b\u578b: string, \u7535\u89c6\u5267-\u8c46\u74e3\u8bc4\u5206: string, \u7535\u89c6\u5267-\u96c6\u6570: string, \u7535\u89c6\u5267-\u9996\u64ad\u65f6\u95f4: string, \u8f85\u5bfc\u73ed-\u4e0a\u8bfe\u65b9\u5f0f: string, \u8f85\u5bfc\u73ed-\u4e0a\u8bfe\u65f6\u95f4: string, \u8f85\u5bfc\u73ed-\u4e0b\u8bfe\u65f6\u95f4: string, \u8f85\u5bfc\u73ed-\u4ef7\u683c: string, \u8f85\u5bfc\u73ed-\u533a\u57df: string, \u8f85\u5bfc\u73ed-\u5e74\u7ea7: string, \u8f85\u5bfc\u73ed-\u5f00\u59cb\u65e5\u671f: string, \u8f85\u5bfc\u73ed-\u6559\u5ba4\u5730\u70b9: string, \u8f85\u5bfc\u73ed-\u6559\u5e08: string, \u8f85\u5bfc\u73ed-\u6559\u5e08\u7f51\u5740: string, \u8f85\u5bfc\u73ed-\u65f6\u6bb5: string, \u8f85\u5bfc\u73ed-\u6821\u533a: string, \u8f85\u5bfc\u73ed-\u6bcf\u5468: string, \u8f85\u5bfc\u73ed-\u73ed\u53f7: string, \u8f85\u5bfc\u73ed-\u79d1\u76ee: string, \u8f85\u5bfc\u73ed-\u7ed3\u675f\u65e5\u671f: string, \u8f85\u5bfc\u73ed-\u8bfe\u65f6: string, \u8f85\u5bfc\u73ed-\u8bfe\u6b21: string, \u8f85\u5bfc\u73ed-\u8bfe\u7a0b\u7f51\u5740: string, \u8f85\u5bfc\u73ed-\u96be\u5ea6: string, \u901a\u7528-\u4ea7\u54c1\u7c7b\u522b: string, \u901a\u7528-\u4ef7\u683c\u533a\u95f4: string, \u901a\u7528-\u54c1\u724c: string, \u901a\u7528-\u7cfb\u5217: string, \u9152\u5e97-\u4ef7\u4f4d: string, \u9152\u5e97-\u505c\u8f66\u573a: string, \u9152\u5e97-\u533a\u57df: string, \u9152\u5e97-\u540d\u79f0: string, \u9152\u5e97-\u5730\u5740: string, \u9152\u5e97-\u623f\u578b: string, \u9152\u5e97-\u623f\u8d39: string, \u9152\u5e97-\u661f\u7ea7: string, \u9152\u5e97-\u7535\u8bdd\u53f7\u7801: string, \u9152\u5e97-\u8bc4\u5206: string, \u9152\u5e97-\u9152\u5e97\u7c7b\u578b: string, \u98de\u673a-\u51c6\u70b9\u7387: string, \u98de\u673a-\u51fa\u53d1\u5730: string, \u98de\u673a-\u5230\u8fbe\u65f6\u95f4: string, \u98de\u673a-\u65e5\u671f: string, \u98de\u673a-\u76ee\u7684\u5730: string, \u98de\u673a-\u7968\u4ef7: string, \u98de\u673a-\u822a\u73ed\u4fe1\u606f: string, \u98de\u673a-\u8231\u4f4d\u6863\u6b21: string, \u98de\u673a-\u8d77\u98de\u65f6\u95f4: string, \u9910\u5385-\u4eba\u5747\u6d88\u8d39: string, \u9910\u5385-\u4ef7\u4f4d: string, \u9910\u5385-\u533a\u57df: string, \u9910\u5385-\u540d\u79f0: string, \u9910\u5385-\u5730\u5740: string, \u9910\u5385-\u63a8\u8350\u83dc: string, \u9910\u5385-\u662f\u5426\u5730\u94c1\u76f4\u8fbe: string, \u9910\u5385-\u7535\u8bdd\u53f7\u7801: string, \u9910\u5385-\u83dc\u7cfb: string, \u9910\u5385-\u8425\u4e1a\u65f6\u95f4: string, \u9910\u5385-\u8bc4\u5206: string>\r\nto\r\n{'\u65c5\u6e38\u666f\u70b9-\u540d\u79f0': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u533a\u57df': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u666f\u70b9\u7c7b\u578b': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u6700\u9002\u5408\u4eba\u7fa4': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u6d88\u8d39': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u662f\u5426\u5730\u94c1\u76f4\u8fbe': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u95e8\u7968\u4ef7\u683c': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u7535\u8bdd\u53f7\u7801': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u5730\u5740': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u8bc4\u5206': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u5f00\u653e\u65f6\u95f4': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u7279\u70b9': Value(dtype='string', id=None), '\u9910\u5385-\u540d\u79f0': Value(dtype='string', id=None), '\u9910\u5385-\u533a\u57df': Value(dtype='string', id=None), '\u9910\u5385-\u83dc\u7cfb': Value(dtype='string', id=None), '\u9910\u5385-\u4ef7\u4f4d': Value(dtype='string', id=None), '\u9910\u5385-\u662f\u5426\u5730\u94c1\u76f4\u8fbe': Value(dtype='string', id=None), '\u9910\u5385-\u4eba\u5747\u6d88\u8d39': Value(dtype='string', id=None), '\u9910\u5385-\u5730\u5740': Value(dtype='string', id=None), '\u9910\u5385-\u7535\u8bdd\u53f7\u7801': Value(dtype='string', id=None), '\u9910\u5385-\u8bc4\u5206': Value(dtype='string', id=None), '\u9910\u5385-\u8425\u4e1a\u65f6\u95f4': Value(dtype='string', id=None), '\u9910\u5385-\u63a8\u8350\u83dc': Value(dtype='string', id=None), '\u9152\u5e97-\u540d\u79f0': Value(dtype='string', id=None), '\u9152\u5e97-\u533a\u57df': Value(dtype='string', id=None), '\u9152\u5e97-\u661f\u7ea7': Value(dtype='string', id=None), '\u9152\u5e97-\u4ef7\u4f4d': Value(dtype='string', id=None), '\u9152\u5e97-\u9152\u5e97\u7c7b\u578b': Value(dtype='string', id=None), '\u9152\u5e97-\u623f\u578b': Value(dtype='string', id=None), '\u9152\u5e97-\u505c\u8f66\u573a': Value(dtype='string', id=None), '\u9152\u5e97-\u623f\u8d39': Value(dtype='string', id=None), '\u9152\u5e97-\u5730\u5740': Value(dtype='string', id=None), '\u9152\u5e97-\u7535\u8bdd\u53f7\u7801': Value(dtype='string', id=None), '\u9152\u5e97-\u8bc4\u5206': Value(dtype='string', id=None), '\u7535\u8111-\u54c1\u724c': Value(dtype='string', id=None), '\u7535\u8111-\u4ea7\u54c1\u7c7b\u522b': Value(dtype='string', id=None), '\u7535\u8111-\u5206\u7c7b': Value(dtype='string', id=None), '\u7535\u8111-\u5185\u5b58\u5bb9\u91cf': Value(dtype='string', id=None), '\u7535\u8111-\u5c4f\u5e55\u5c3a\u5bf8': Value(dtype='string', id=None), '\u7535\u8111-CPU': Value(dtype='string', id=None), '\u7535\u8111-\u4ef7\u683c\u533a\u95f4': Value(dtype='string', id=None), '\u7535\u8111-\u7cfb\u5217': Value(dtype='string', id=None), '\u7535\u8111-\u5546\u54c1\u540d\u79f0': Value(dtype='string', id=None), '\u7535\u8111-\u7cfb\u7edf': Value(dtype='string', id=None), '\u7535\u8111-\u6e38\u620f\u6027\u80fd': Value(dtype='string', id=None), '\u7535\u8111-CPU\u578b\u53f7': Value(dtype='string', id=None), '\u7535\u8111-\u88f8\u673a\u91cd\u91cf': Value(dtype='string', id=None), '\u7535\u8111-\u663e\u5361\u7c7b\u522b': Value(dtype='string', id=None), '\u7535\u8111-\u663e\u5361\u578b\u53f7': Value(dtype='string', id=None), '\u7535\u8111-\u7279\u6027': Value(dtype='string', id=None), '\u7535\u8111-\u8272\u7cfb': Value(dtype='string', id=None), '\u7535\u8111-\u5f85\u673a\u65f6\u957f': Value(dtype='string', id=None), '\u7535\u8111-\u786c\u76d8\u5bb9\u91cf': Value(dtype='string', id=None), '\u7535\u8111-\u4ef7\u683c': Value(dtype='string', id=None), '\u706b\u8f66-\u51fa\u53d1\u5730': Value(dtype='string', id=None), '\u706b\u8f66-\u76ee\u7684\u5730': Value(dtype='string', id=None), '\u706b\u8f66-\u65e5\u671f': Value(dtype='string', id=None), '\u706b\u8f66-\u8f66\u578b': Value(dtype='string', id=None), '\u706b\u8f66-\u5750\u5e2d': Value(dtype='string', id=None), '\u706b\u8f66-\u8f66\u6b21\u4fe1\u606f': Value(dtype='string', id=None), '\u706b\u8f66-\u65f6\u957f': Value(dtype='string', id=None), '\u706b\u8f66-\u51fa\u53d1\u65f6\u95f4': Value(dtype='string', id=None), '\u706b\u8f66-\u5230\u8fbe\u65f6\u95f4': Value(dtype='string', id=None), '\u706b\u8f66-\u7968\u4ef7': Value(dtype='string', id=None), '\u98de\u673a-\u51fa\u53d1\u5730': Value(dtype='string', id=None), '\u98de\u673a-\u76ee\u7684\u5730': Value(dtype='string', id=None), '\u98de\u673a-\u65e5\u671f': Value(dtype='string', id=None), '\u98de\u673a-\u8231\u4f4d\u6863\u6b21': Value(dtype='string', id=None), '\u98de\u673a-\u822a\u73ed\u4fe1\u606f': Value(dtype='string', id=None), '\u98de\u673a-\u8d77\u98de\u65f6\u95f4': Value(dtype='string', id=None), '\u98de\u673a-\u5230\u8fbe\u65f6\u95f4': Value(dtype='string', id=None), '\u98de\u673a-\u7968\u4ef7': Value(dtype='string', id=None), '\u98de\u673a-\u51c6\u70b9\u7387': Value(dtype='string', id=None), '\u5929\u6c14-\u57ce\u5e02': Value(dtype='string', id=None), '\u5929\u6c14-\u65e5\u671f': Value(dtype='string', id=None), '\u5929\u6c14-\u5929\u6c14': Value(dtype='string', id=None), '\u5929\u6c14-\u6e29\u5ea6': Value(dtype='string', id=None), '\u5929\u6c14-\u98ce\u529b\u98ce\u5411': Value(dtype='string', id=None), '\u5929\u6c14-\u7d2b\u5916\u7ebf\u5f3a\u5ea6': Value(dtype='string', id=None), '\u7535\u5f71-\u5236\u7247\u56fd\u5bb6/\u5730\u533a': Value(dtype='string', id=None), '\u7535\u5f71-\u7c7b\u578b': Value(dtype='string', id=None), '\u7535\u5f71-\u5e74\u4ee3': Value(dtype='string', id=None), '\u7535\u5f71-\u4e3b\u6f14': Value(dtype='string', id=None), '\u7535\u5f71-\u5bfc\u6f14': Value(dtype='string', id=None), '\u7535\u5f71-\u7247\u540d': Value(dtype='string', id=None), '\u7535\u5f71-\u4e3b\u6f14\u540d\u5355': Value(dtype='string', id=None), '\u7535\u5f71-\u5177\u4f53\u4e0a\u6620\u65f6\u95f4': Value(dtype='string', id=None), '\u7535\u5f71-\u7247\u957f': Value(dtype='string', id=None), '\u7535\u5f71-\u8c46\u74e3\u8bc4\u5206': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u5236\u7247\u56fd\u5bb6/\u5730\u533a': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u7c7b\u578b': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u5e74\u4ee3': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u4e3b\u6f14': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u5bfc\u6f14': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u7247\u540d': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u4e3b\u6f14\u540d\u5355': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u9996\u64ad\u65f6\u95f4': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u96c6\u6570': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u5355\u96c6\u7247\u957f': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u8c46\u74e3\u8bc4\u5206': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u73ed\u53f7': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u96be\u5ea6': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u79d1\u76ee': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u5e74\u7ea7': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u533a\u57df': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u6821\u533a': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u4e0a\u8bfe\u65b9\u5f0f': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u5f00\u59cb\u65e5\u671f': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u7ed3\u675f\u65e5\u671f': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u6bcf\u5468': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u4e0a\u8bfe\u65f6\u95f4': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u4e0b\u8bfe\u65f6\u95f4': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u65f6\u6bb5': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u8bfe\u6b21': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u8bfe\u65f6': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u6559\u5ba4\u5730\u70b9': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u6559\u5e08': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u4ef7\u683c': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u8bfe\u7a0b\u7f51\u5740': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u6559\u5e08\u7f51\u5740': Value(dtype='string', id=None), '\u6c7d\u8f66-\u540d\u79f0': Value(dtype='string', id=None), '\u6c7d\u8f66-\u8f66\u578b': Value(dtype='string', id=None), '\u6c7d\u8f66-\u7ea7\u522b': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5ea7\u4f4d\u6570': Value(dtype='string', id=None), '\u6c7d\u8f66-\u8f66\u8eab\u5c3a\u5bf8(mm)': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5382\u5546': Value(dtype='string', id=None), '\u6c7d\u8f66-\u80fd\u6e90\u7c7b\u578b': Value(dtype='string', id=None), '\u6c7d\u8f66-\u53d1\u52a8\u673a\u6392\u91cf(L)': Value(dtype='string', id=None), '\u6c7d\u8f66-\u53d1\u52a8\u673a\u9a6c\u529b(Ps)': Value(dtype='string', id=None), '\u6c7d\u8f66-\u9a71\u52a8\u65b9\u5f0f': Value(dtype='string', id=None), '\u6c7d\u8f66-\u7efc\u5408\u6cb9\u8017(L/100km)': Value(dtype='string', id=None), '\u6c7d\u8f66-\u73af\u4fdd\u6807\u51c6': Value(dtype='string', id=None), '\u6c7d\u8f66-\u9a7e\u9a76\u8f85\u52a9\u5f71\u50cf': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5de1\u822a\u7cfb\u7edf': Value(dtype='string', id=None), '\u6c7d\u8f66-\u4ef7\u683c(\u4e07\u5143)': Value(dtype='string', id=None), '\u6c7d\u8f66-\u8f66\u7cfb': Value(dtype='string', id=None), '\u6c7d\u8f66-\u52a8\u529b\u6c34\u5e73': Value(dtype='string', id=None), '\u6c7d\u8f66-\u6cb9\u8017\u6c34\u5e73': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5012\u8f66\u5f71\u50cf': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5b9a\u901f\u5de1\u822a': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5ea7\u6905\u52a0\u70ed': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5ea7\u6905\u901a\u98ce': Value(dtype='string', id=None), '\u6c7d\u8f66-\u6240\u5c5e\u4ef7\u683c\u533a\u95f4': Value(dtype='string', id=None), '\u533b\u9662-\u540d\u79f0': Value(dtype='string', id=None), '\u533b\u9662-\u7b49\u7ea7': Value(dtype='string', id=None), '\u533b\u9662-\u7c7b\u522b': Value(dtype='string', id=None), '\u533b\u9662-\u6027\u8d28': Value(dtype='string', id=None), '\u533b\u9662-\u533a\u57df': Value(dtype='string', id=None), '\u533b\u9662-\u5730\u5740': Value(dtype='string', id=None), '\u533b\u9662-\u7535\u8bdd': Value(dtype='string', id=None), '\u533b\u9662-\u6302\u53f7\u65f6\u95f4': Value(dtype='string', id=None), '\u533b\u9662-\u95e8\u8bca\u65f6\u95f4': Value(dtype='string', id=None), '\u533b\u9662-\u516c\u4ea4\u7ebf\u8def': Value(dtype='string', id=None), '\u533b\u9662-\u5730\u94c1\u53ef\u8fbe': Value(dtype='string', id=None), '\u533b\u9662-\u5730\u94c1\u7ebf\u8def': Value(dtype='string', id=None), '\u533b\u9662-\u91cd\u70b9\u79d1\u5ba4': Value(dtype='string', id=None), '\u533b\u9662-CT': Value(dtype='string', id=None), '\u533b\u9662-3.0T MRI': Value(dtype='string', id=None), '\u533b\u9662-DSA': Value(dtype='string', id=None)}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_44306/2896005239.py in <module>\r\n----> 1 dset = load_dataset(\"GEM/RiSAWOZ\")\r\n      2 dset\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, script_version, **config_kwargs)\r\n   1692 \r\n   1693     # Download and prepare data\r\n-> 1694     builder_instance.download_and_prepare(\r\n   1695         download_config=download_config,\r\n   1696         download_mode=download_mode,\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    593                             logger.warning(\"HF google storage unreachable. Downloading and preparing it from source\")\r\n    594                     if not downloaded_from_gcs:\r\n--> 595                         self._download_and_prepare(\r\n    596                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    597                         )\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    682             try:\r\n    683                 # Prepare split will record examples associated to the split\r\n--> 684                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    685             except OSError as e:\r\n    686                 raise OSError(\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/builder.py in _prepare_split(self, split_generator)\r\n   1084                     writer.write(example, key)\r\n   1085             finally:\r\n-> 1086                 num_examples, num_bytes = writer.finalize()\r\n   1087 \r\n   1088         split_generator.split_info.num_examples = num_examples\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/arrow_writer.py in finalize(self, close_stream)\r\n    525             # Re-intializing to empty list for next batch\r\n    526             self.hkey_record = []\r\n--> 527         self.write_examples_on_file()\r\n    528         if self.pa_writer is None:\r\n    529             if self.schema:\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/arrow_writer.py in write_examples_on_file(self)\r\n    402             # Since current_examples contains (example, key) tuples\r\n    403             batch_examples[col] = [row[0][col] for row in self.current_examples]\r\n--> 404         self.write_batch(batch_examples=batch_examples)\r\n    405         self.current_examples = []\r\n    406 \r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)\r\n    495             col_try_type = try_features[col] if try_features is not None and col in try_features else None\r\n    496             typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\r\n--> 497             arrays.append(pa.array(typed_sequence))\r\n    498             inferred_features[col] = typed_sequence.get_inferred_type()\r\n    499         schema = inferred_features.arrow_schema if self.pa_writer is None else self.schema\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/arrow_writer.py in __arrow_array__(self, type)\r\n    203                 # Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\r\n    204                 # We only do it if trying_type is False - since this is what the user asks for.\r\n--> 205                 out = cast_array_to_feature(out, type, allow_number_to_str=not self.trying_type)\r\n    206             return out\r\n    207         except (TypeError, pa.lib.ArrowInvalid) as e:  # handle type errors and overflows\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    942         if pa.types.is_list(array.type) and config.PYARROW_VERSION < version.parse(\"4.0.0\"):\r\n    943             array = _sanitize(array)\r\n--> 944         return func(array, *args, **kwargs)\r\n    945 \r\n    946     return wrapper\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    918             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n    919         else:\r\n--> 920             return func(array, *args, **kwargs)\r\n    921 \r\n    922     return wrapper\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1063         # feature must be either [subfeature] or Sequence(subfeature)\r\n   1064         if isinstance(feature, list):\r\n-> 1065             return pa.ListArray.from_arrays(array.offsets, _c(array.values, feature[0]))\r\n   1066         elif isinstance(feature, Sequence):\r\n   1067             if feature.length > -1:\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    942         if pa.types.is_list(array.type) and config.PYARROW_VERSION < version.parse(\"4.0.0\"):\r\n    943             array = _sanitize(array)\r\n--> 944         return func(array, *args, **kwargs)\r\n    945 \r\n    946     return wrapper\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    918             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n    919         else:\r\n--> 920             return func(array, *args, **kwargs)\r\n    921 \r\n    922     return wrapper\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1058             }\r\n   1059         if isinstance(feature, dict) and set(field.name for field in array.type) == set(feature):\r\n-> 1060             arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n   1061             return pa.StructArray.from_arrays(arrays, names=list(feature))\r\n   1062     elif pa.types.is_list(array.type):\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in <listcomp>(.0)\r\n   1058             }\r\n   1059         if isinstance(feature, dict) and set(field.name for field in array.type) == set(feature):\r\n-> 1060             arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n   1061             return pa.StructArray.from_arrays(arrays, names=list(feature))\r\n   1062     elif pa.types.is_list(array.type):\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    942         if pa.types.is_list(array.type) and config.PYARROW_VERSION < version.parse(\"4.0.0\"):\r\n    943             array = _sanitize(array)\r\n--> 944         return func(array, *args, **kwargs)\r\n    945 \r\n    946     return wrapper\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    918             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n    919         else:\r\n--> 920             return func(array, *args, **kwargs)\r\n    921 \r\n    922     return wrapper\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1058             }\r\n   1059         if isinstance(feature, dict) and set(field.name for field in array.type) == set(feature):\r\n-> 1060             arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n   1061             return pa.StructArray.from_arrays(arrays, names=list(feature))\r\n   1062     elif pa.types.is_list(array.type):\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in <listcomp>(.0)\r\n   1058             }\r\n   1059         if isinstance(feature, dict) and set(field.name for field in array.type) == set(feature):\r\n-> 1060             arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n   1061             return pa.StructArray.from_arrays(arrays, names=list(feature))\r\n   1062     elif pa.types.is_list(array.type):\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    942         if pa.types.is_list(array.type) and config.PYARROW_VERSION < version.parse(\"4.0.0\"):\r\n    943             array = _sanitize(array)\r\n--> 944         return func(array, *args, **kwargs)\r\n    945 \r\n    946     return wrapper\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in wrapper(array, *args, **kwargs)\r\n    918             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n    919         else:\r\n--> 920             return func(array, *args, **kwargs)\r\n    921 \r\n    922     return wrapper\r\n\r\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/datasets/table.py in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1085     elif not isinstance(feature, (Sequence, dict, list, tuple)):\r\n   1086         return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n-> 1087     raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\n   1088 \r\n   1089 \r\n\r\nTypeError: Couldn't cast array of type\r\nstruct<\u533b\u9662-3.0T MRI: string, \u533b\u9662-CT: string, \u533b\u9662-DSA: string, \u533b\u9662-\u516c\u4ea4\u7ebf\u8def: string, \u533b\u9662-\u533a\u57df: string, \u533b\u9662-\u540d\u79f0: string, \u533b\u9662-\u5730\u5740: string, \u533b\u9662-\u5730\u94c1\u53ef\u8fbe: string, \u533b\u9662-\u5730\u94c1\u7ebf\u8def: string, \u533b\u9662-\u6027\u8d28: string, \u533b\u9662-\u6302\u53f7\u65f6\u95f4: string, \u533b\u9662-\u7535\u8bdd: string, \u533b\u9662-\u7b49\u7ea7: string, \u533b\u9662-\u7c7b\u522b: string, \u533b\u9662-\u91cd\u70b9\u79d1\u5ba4: string, \u533b\u9662-\u95e8\u8bca\u65f6\u95f4: string, \u5929\u6c14-\u57ce\u5e02: string, \u5929\u6c14-\u5929\u6c14: string, \u5929\u6c14-\u65e5\u671f: string, \u5929\u6c14-\u6e29\u5ea6: string, \u5929\u6c14-\u7d2b\u5916\u7ebf\u5f3a\u5ea6: string, \u5929\u6c14-\u98ce\u529b\u98ce\u5411: string, \u65c5\u6e38\u666f\u70b9-\u533a\u57df: string, \u65c5\u6e38\u666f\u70b9-\u540d\u79f0: string, \u65c5\u6e38\u666f\u70b9-\u5730\u5740: string, \u65c5\u6e38\u666f\u70b9-\u5f00\u653e\u65f6\u95f4: string, \u65c5\u6e38\u666f\u70b9-\u662f\u5426\u5730\u94c1\u76f4\u8fbe: string, \u65c5\u6e38\u666f\u70b9-\u666f\u70b9\u7c7b\u578b: string, \u65c5\u6e38\u666f\u70b9-\u6700\u9002\u5408\u4eba\u7fa4: string, \u65c5\u6e38\u666f\u70b9-\u6d88\u8d39: string, \u65c5\u6e38\u666f\u70b9-\u7279\u70b9: string, \u65c5\u6e38\u666f\u70b9-\u7535\u8bdd\u53f7\u7801: string, \u65c5\u6e38\u666f\u70b9-\u8bc4\u5206: string, \u65c5\u6e38\u666f\u70b9-\u95e8\u7968\u4ef7\u683c: string, \u6c7d\u8f66-\u4ef7\u683c(\u4e07\u5143): string, \u6c7d\u8f66-\u5012\u8f66\u5f71\u50cf: string, \u6c7d\u8f66-\u52a8\u529b\u6c34\u5e73: string, \u6c7d\u8f66-\u5382\u5546: string, \u6c7d\u8f66-\u53d1\u52a8\u673a\u6392\u91cf(L): string, \u6c7d\u8f66-\u53d1\u52a8\u673a\u9a6c\u529b(Ps): string, \u6c7d\u8f66-\u540d\u79f0: string, \u6c7d\u8f66-\u5b9a\u901f\u5de1\u822a: string, \u6c7d\u8f66-\u5de1\u822a\u7cfb\u7edf: string, \u6c7d\u8f66-\u5ea7\u4f4d\u6570: string, \u6c7d\u8f66-\u5ea7\u6905\u52a0\u70ed: string, \u6c7d\u8f66-\u5ea7\u6905\u901a\u98ce: string, \u6c7d\u8f66-\u6240\u5c5e\u4ef7\u683c\u533a\u95f4: string, \u6c7d\u8f66-\u6cb9\u8017\u6c34\u5e73: string, \u6c7d\u8f66-\u73af\u4fdd\u6807\u51c6: string, \u6c7d\u8f66-\u7ea7\u522b: string, \u6c7d\u8f66-\u7efc\u5408\u6cb9\u8017(L/100km): string, \u6c7d\u8f66-\u80fd\u6e90\u7c7b\u578b: string, \u6c7d\u8f66-\u8f66\u578b: string, \u6c7d\u8f66-\u8f66\u7cfb: string, \u6c7d\u8f66-\u8f66\u8eab\u5c3a\u5bf8(mm): string, \u6c7d\u8f66-\u9a71\u52a8\u65b9\u5f0f: string, \u6c7d\u8f66-\u9a7e\u9a76\u8f85\u52a9\u5f71\u50cf: string, \u706b\u8f66-\u51fa\u53d1\u5730: string, \u706b\u8f66-\u51fa\u53d1\u65f6\u95f4: string, \u706b\u8f66-\u5230\u8fbe\u65f6\u95f4: string, \u706b\u8f66-\u5750\u5e2d: string, \u706b\u8f66-\u65e5\u671f: string, \u706b\u8f66-\u65f6\u957f: string, \u706b\u8f66-\u76ee\u7684\u5730: string, \u706b\u8f66-\u7968\u4ef7: string, \u706b\u8f66-\u8231\u4f4d\u6863\u6b21: string, \u706b\u8f66-\u8f66\u578b: string, \u706b\u8f66-\u8f66\u6b21\u4fe1\u606f: string, \u7535\u5f71-\u4e3b\u6f14: string, \u7535\u5f71-\u4e3b\u6f14\u540d\u5355: string, \u7535\u5f71-\u5177\u4f53\u4e0a\u6620\u65f6\u95f4: string, \u7535\u5f71-\u5236\u7247\u56fd\u5bb6/\u5730\u533a: string, \u7535\u5f71-\u5bfc\u6f14: string, \u7535\u5f71-\u5e74\u4ee3: string, \u7535\u5f71-\u7247\u540d: string, \u7535\u5f71-\u7247\u957f: string, \u7535\u5f71-\u7c7b\u578b: string, \u7535\u5f71-\u8c46\u74e3\u8bc4\u5206: string, \u7535\u8111-CPU: string, \u7535\u8111-CPU\u578b\u53f7: string, \u7535\u8111-\u4ea7\u54c1\u7c7b\u522b: string, \u7535\u8111-\u4ef7\u683c: string, \u7535\u8111-\u4ef7\u683c\u533a\u95f4: string, \u7535\u8111-\u5185\u5b58\u5bb9\u91cf: string, \u7535\u8111-\u5206\u7c7b: string, \u7535\u8111-\u54c1\u724c: string, \u7535\u8111-\u5546\u54c1\u540d\u79f0: string, \u7535\u8111-\u5c4f\u5e55\u5c3a\u5bf8: string, \u7535\u8111-\u5f85\u673a\u65f6\u957f: string, \u7535\u8111-\u663e\u5361\u578b\u53f7: string, \u7535\u8111-\u663e\u5361\u7c7b\u522b: string, \u7535\u8111-\u6e38\u620f\u6027\u80fd: string, \u7535\u8111-\u7279\u6027: string, \u7535\u8111-\u786c\u76d8\u5bb9\u91cf: string, \u7535\u8111-\u7cfb\u5217: string, \u7535\u8111-\u7cfb\u7edf: string, \u7535\u8111-\u8272\u7cfb: string, \u7535\u8111-\u88f8\u673a\u91cd\u91cf: string, \u7535\u89c6\u5267-\u4e3b\u6f14: string, \u7535\u89c6\u5267-\u4e3b\u6f14\u540d\u5355: string, \u7535\u89c6\u5267-\u5236\u7247\u56fd\u5bb6/\u5730\u533a: string, \u7535\u89c6\u5267-\u5355\u96c6\u7247\u957f: string, \u7535\u89c6\u5267-\u5bfc\u6f14: string, \u7535\u89c6\u5267-\u5e74\u4ee3: string, \u7535\u89c6\u5267-\u7247\u540d: string, \u7535\u89c6\u5267-\u7c7b\u578b: string, \u7535\u89c6\u5267-\u8c46\u74e3\u8bc4\u5206: string, \u7535\u89c6\u5267-\u96c6\u6570: string, \u7535\u89c6\u5267-\u9996\u64ad\u65f6\u95f4: string, \u8f85\u5bfc\u73ed-\u4e0a\u8bfe\u65b9\u5f0f: string, \u8f85\u5bfc\u73ed-\u4e0a\u8bfe\u65f6\u95f4: string, \u8f85\u5bfc\u73ed-\u4e0b\u8bfe\u65f6\u95f4: string, \u8f85\u5bfc\u73ed-\u4ef7\u683c: string, \u8f85\u5bfc\u73ed-\u533a\u57df: string, \u8f85\u5bfc\u73ed-\u5e74\u7ea7: string, \u8f85\u5bfc\u73ed-\u5f00\u59cb\u65e5\u671f: string, \u8f85\u5bfc\u73ed-\u6559\u5ba4\u5730\u70b9: string, \u8f85\u5bfc\u73ed-\u6559\u5e08: string, \u8f85\u5bfc\u73ed-\u6559\u5e08\u7f51\u5740: string, \u8f85\u5bfc\u73ed-\u65f6\u6bb5: string, \u8f85\u5bfc\u73ed-\u6821\u533a: string, \u8f85\u5bfc\u73ed-\u6bcf\u5468: string, \u8f85\u5bfc\u73ed-\u73ed\u53f7: string, \u8f85\u5bfc\u73ed-\u79d1\u76ee: string, \u8f85\u5bfc\u73ed-\u7ed3\u675f\u65e5\u671f: string, \u8f85\u5bfc\u73ed-\u8bfe\u65f6: string, \u8f85\u5bfc\u73ed-\u8bfe\u6b21: string, \u8f85\u5bfc\u73ed-\u8bfe\u7a0b\u7f51\u5740: string, \u8f85\u5bfc\u73ed-\u96be\u5ea6: string, \u901a\u7528-\u4ea7\u54c1\u7c7b\u522b: string, \u901a\u7528-\u4ef7\u683c\u533a\u95f4: string, \u901a\u7528-\u54c1\u724c: string, \u901a\u7528-\u7cfb\u5217: string, \u9152\u5e97-\u4ef7\u4f4d: string, \u9152\u5e97-\u505c\u8f66\u573a: string, \u9152\u5e97-\u533a\u57df: string, \u9152\u5e97-\u540d\u79f0: string, \u9152\u5e97-\u5730\u5740: string, \u9152\u5e97-\u623f\u578b: string, \u9152\u5e97-\u623f\u8d39: string, \u9152\u5e97-\u661f\u7ea7: string, \u9152\u5e97-\u7535\u8bdd\u53f7\u7801: string, \u9152\u5e97-\u8bc4\u5206: string, \u9152\u5e97-\u9152\u5e97\u7c7b\u578b: string, \u98de\u673a-\u51c6\u70b9\u7387: string, \u98de\u673a-\u51fa\u53d1\u5730: string, \u98de\u673a-\u5230\u8fbe\u65f6\u95f4: string, \u98de\u673a-\u65e5\u671f: string, \u98de\u673a-\u76ee\u7684\u5730: string, \u98de\u673a-\u7968\u4ef7: string, \u98de\u673a-\u822a\u73ed\u4fe1\u606f: string, \u98de\u673a-\u8231\u4f4d\u6863\u6b21: string, \u98de\u673a-\u8d77\u98de\u65f6\u95f4: string, \u9910\u5385-\u4eba\u5747\u6d88\u8d39: string, \u9910\u5385-\u4ef7\u4f4d: string, \u9910\u5385-\u533a\u57df: string, \u9910\u5385-\u540d\u79f0: string, \u9910\u5385-\u5730\u5740: string, \u9910\u5385-\u63a8\u8350\u83dc: string, \u9910\u5385-\u662f\u5426\u5730\u94c1\u76f4\u8fbe: string, \u9910\u5385-\u7535\u8bdd\u53f7\u7801: string, \u9910\u5385-\u83dc\u7cfb: string, \u9910\u5385-\u8425\u4e1a\u65f6\u95f4: string, \u9910\u5385-\u8bc4\u5206: string>\r\nto\r\n{'\u65c5\u6e38\u666f\u70b9-\u540d\u79f0': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u533a\u57df': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u666f\u70b9\u7c7b\u578b': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u6700\u9002\u5408\u4eba\u7fa4': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u6d88\u8d39': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u662f\u5426\u5730\u94c1\u76f4\u8fbe': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u95e8\u7968\u4ef7\u683c': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u7535\u8bdd\u53f7\u7801': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u5730\u5740': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u8bc4\u5206': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u5f00\u653e\u65f6\u95f4': Value(dtype='string', id=None), '\u65c5\u6e38\u666f\u70b9-\u7279\u70b9': Value(dtype='string', id=None), '\u9910\u5385-\u540d\u79f0': Value(dtype='string', id=None), '\u9910\u5385-\u533a\u57df': Value(dtype='string', id=None), '\u9910\u5385-\u83dc\u7cfb': Value(dtype='string', id=None), '\u9910\u5385-\u4ef7\u4f4d': Value(dtype='string', id=None), '\u9910\u5385-\u662f\u5426\u5730\u94c1\u76f4\u8fbe': Value(dtype='string', id=None), '\u9910\u5385-\u4eba\u5747\u6d88\u8d39': Value(dtype='string', id=None), '\u9910\u5385-\u5730\u5740': Value(dtype='string', id=None), '\u9910\u5385-\u7535\u8bdd\u53f7\u7801': Value(dtype='string', id=None), '\u9910\u5385-\u8bc4\u5206': Value(dtype='string', id=None), '\u9910\u5385-\u8425\u4e1a\u65f6\u95f4': Value(dtype='string', id=None), '\u9910\u5385-\u63a8\u8350\u83dc': Value(dtype='string', id=None), '\u9152\u5e97-\u540d\u79f0': Value(dtype='string', id=None), '\u9152\u5e97-\u533a\u57df': Value(dtype='string', id=None), '\u9152\u5e97-\u661f\u7ea7': Value(dtype='string', id=None), '\u9152\u5e97-\u4ef7\u4f4d': Value(dtype='string', id=None), '\u9152\u5e97-\u9152\u5e97\u7c7b\u578b': Value(dtype='string', id=None), '\u9152\u5e97-\u623f\u578b': Value(dtype='string', id=None), '\u9152\u5e97-\u505c\u8f66\u573a': Value(dtype='string', id=None), '\u9152\u5e97-\u623f\u8d39': Value(dtype='string', id=None), '\u9152\u5e97-\u5730\u5740': Value(dtype='string', id=None), '\u9152\u5e97-\u7535\u8bdd\u53f7\u7801': Value(dtype='string', id=None), '\u9152\u5e97-\u8bc4\u5206': Value(dtype='string', id=None), '\u7535\u8111-\u54c1\u724c': Value(dtype='string', id=None), '\u7535\u8111-\u4ea7\u54c1\u7c7b\u522b': Value(dtype='string', id=None), '\u7535\u8111-\u5206\u7c7b': Value(dtype='string', id=None), '\u7535\u8111-\u5185\u5b58\u5bb9\u91cf': Value(dtype='string', id=None), '\u7535\u8111-\u5c4f\u5e55\u5c3a\u5bf8': Value(dtype='string', id=None), '\u7535\u8111-CPU': Value(dtype='string', id=None), '\u7535\u8111-\u4ef7\u683c\u533a\u95f4': Value(dtype='string', id=None), '\u7535\u8111-\u7cfb\u5217': Value(dtype='string', id=None), '\u7535\u8111-\u5546\u54c1\u540d\u79f0': Value(dtype='string', id=None), '\u7535\u8111-\u7cfb\u7edf': Value(dtype='string', id=None), '\u7535\u8111-\u6e38\u620f\u6027\u80fd': Value(dtype='string', id=None), '\u7535\u8111-CPU\u578b\u53f7': Value(dtype='string', id=None), '\u7535\u8111-\u88f8\u673a\u91cd\u91cf': Value(dtype='string', id=None), '\u7535\u8111-\u663e\u5361\u7c7b\u522b': Value(dtype='string', id=None), '\u7535\u8111-\u663e\u5361\u578b\u53f7': Value(dtype='string', id=None), '\u7535\u8111-\u7279\u6027': Value(dtype='string', id=None), '\u7535\u8111-\u8272\u7cfb': Value(dtype='string', id=None), '\u7535\u8111-\u5f85\u673a\u65f6\u957f': Value(dtype='string', id=None), '\u7535\u8111-\u786c\u76d8\u5bb9\u91cf': Value(dtype='string', id=None), '\u7535\u8111-\u4ef7\u683c': Value(dtype='string', id=None), '\u706b\u8f66-\u51fa\u53d1\u5730': Value(dtype='string', id=None), '\u706b\u8f66-\u76ee\u7684\u5730': Value(dtype='string', id=None), '\u706b\u8f66-\u65e5\u671f': Value(dtype='string', id=None), '\u706b\u8f66-\u8f66\u578b': Value(dtype='string', id=None), '\u706b\u8f66-\u5750\u5e2d': Value(dtype='string', id=None), '\u706b\u8f66-\u8f66\u6b21\u4fe1\u606f': Value(dtype='string', id=None), '\u706b\u8f66-\u65f6\u957f': Value(dtype='string', id=None), '\u706b\u8f66-\u51fa\u53d1\u65f6\u95f4': Value(dtype='string', id=None), '\u706b\u8f66-\u5230\u8fbe\u65f6\u95f4': Value(dtype='string', id=None), '\u706b\u8f66-\u7968\u4ef7': Value(dtype='string', id=None), '\u98de\u673a-\u51fa\u53d1\u5730': Value(dtype='string', id=None), '\u98de\u673a-\u76ee\u7684\u5730': Value(dtype='string', id=None), '\u98de\u673a-\u65e5\u671f': Value(dtype='string', id=None), '\u98de\u673a-\u8231\u4f4d\u6863\u6b21': Value(dtype='string', id=None), '\u98de\u673a-\u822a\u73ed\u4fe1\u606f': Value(dtype='string', id=None), '\u98de\u673a-\u8d77\u98de\u65f6\u95f4': Value(dtype='string', id=None), '\u98de\u673a-\u5230\u8fbe\u65f6\u95f4': Value(dtype='string', id=None), '\u98de\u673a-\u7968\u4ef7': Value(dtype='string', id=None), '\u98de\u673a-\u51c6\u70b9\u7387': Value(dtype='string', id=None), '\u5929\u6c14-\u57ce\u5e02': Value(dtype='string', id=None), '\u5929\u6c14-\u65e5\u671f': Value(dtype='string', id=None), '\u5929\u6c14-\u5929\u6c14': Value(dtype='string', id=None), '\u5929\u6c14-\u6e29\u5ea6': Value(dtype='string', id=None), '\u5929\u6c14-\u98ce\u529b\u98ce\u5411': Value(dtype='string', id=None), '\u5929\u6c14-\u7d2b\u5916\u7ebf\u5f3a\u5ea6': Value(dtype='string', id=None), '\u7535\u5f71-\u5236\u7247\u56fd\u5bb6/\u5730\u533a': Value(dtype='string', id=None), '\u7535\u5f71-\u7c7b\u578b': Value(dtype='string', id=None), '\u7535\u5f71-\u5e74\u4ee3': Value(dtype='string', id=None), '\u7535\u5f71-\u4e3b\u6f14': Value(dtype='string', id=None), '\u7535\u5f71-\u5bfc\u6f14': Value(dtype='string', id=None), '\u7535\u5f71-\u7247\u540d': Value(dtype='string', id=None), '\u7535\u5f71-\u4e3b\u6f14\u540d\u5355': Value(dtype='string', id=None), '\u7535\u5f71-\u5177\u4f53\u4e0a\u6620\u65f6\u95f4': Value(dtype='string', id=None), '\u7535\u5f71-\u7247\u957f': Value(dtype='string', id=None), '\u7535\u5f71-\u8c46\u74e3\u8bc4\u5206': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u5236\u7247\u56fd\u5bb6/\u5730\u533a': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u7c7b\u578b': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u5e74\u4ee3': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u4e3b\u6f14': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u5bfc\u6f14': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u7247\u540d': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u4e3b\u6f14\u540d\u5355': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u9996\u64ad\u65f6\u95f4': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u96c6\u6570': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u5355\u96c6\u7247\u957f': Value(dtype='string', id=None), '\u7535\u89c6\u5267-\u8c46\u74e3\u8bc4\u5206': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u73ed\u53f7': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u96be\u5ea6': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u79d1\u76ee': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u5e74\u7ea7': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u533a\u57df': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u6821\u533a': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u4e0a\u8bfe\u65b9\u5f0f': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u5f00\u59cb\u65e5\u671f': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u7ed3\u675f\u65e5\u671f': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u6bcf\u5468': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u4e0a\u8bfe\u65f6\u95f4': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u4e0b\u8bfe\u65f6\u95f4': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u65f6\u6bb5': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u8bfe\u6b21': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u8bfe\u65f6': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u6559\u5ba4\u5730\u70b9': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u6559\u5e08': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u4ef7\u683c': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u8bfe\u7a0b\u7f51\u5740': Value(dtype='string', id=None), '\u8f85\u5bfc\u73ed-\u6559\u5e08\u7f51\u5740': Value(dtype='string', id=None), '\u6c7d\u8f66-\u540d\u79f0': Value(dtype='string', id=None), '\u6c7d\u8f66-\u8f66\u578b': Value(dtype='string', id=None), '\u6c7d\u8f66-\u7ea7\u522b': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5ea7\u4f4d\u6570': Value(dtype='string', id=None), '\u6c7d\u8f66-\u8f66\u8eab\u5c3a\u5bf8(mm)': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5382\u5546': Value(dtype='string', id=None), '\u6c7d\u8f66-\u80fd\u6e90\u7c7b\u578b': Value(dtype='string', id=None), '\u6c7d\u8f66-\u53d1\u52a8\u673a\u6392\u91cf(L)': Value(dtype='string', id=None), '\u6c7d\u8f66-\u53d1\u52a8\u673a\u9a6c\u529b(Ps)': Value(dtype='string', id=None), '\u6c7d\u8f66-\u9a71\u52a8\u65b9\u5f0f': Value(dtype='string', id=None), '\u6c7d\u8f66-\u7efc\u5408\u6cb9\u8017(L/100km)': Value(dtype='string', id=None), '\u6c7d\u8f66-\u73af\u4fdd\u6807\u51c6': Value(dtype='string', id=None), '\u6c7d\u8f66-\u9a7e\u9a76\u8f85\u52a9\u5f71\u50cf': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5de1\u822a\u7cfb\u7edf': Value(dtype='string', id=None), '\u6c7d\u8f66-\u4ef7\u683c(\u4e07\u5143)': Value(dtype='string', id=None), '\u6c7d\u8f66-\u8f66\u7cfb': Value(dtype='string', id=None), '\u6c7d\u8f66-\u52a8\u529b\u6c34\u5e73': Value(dtype='string', id=None), '\u6c7d\u8f66-\u6cb9\u8017\u6c34\u5e73': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5012\u8f66\u5f71\u50cf': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5b9a\u901f\u5de1\u822a': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5ea7\u6905\u52a0\u70ed': Value(dtype='string', id=None), '\u6c7d\u8f66-\u5ea7\u6905\u901a\u98ce': Value(dtype='string', id=None), '\u6c7d\u8f66-\u6240\u5c5e\u4ef7\u683c\u533a\u95f4': Value(dtype='string', id=None), '\u533b\u9662-\u540d\u79f0': Value(dtype='string', id=None), '\u533b\u9662-\u7b49\u7ea7': Value(dtype='string', id=None), '\u533b\u9662-\u7c7b\u522b': Value(dtype='string', id=None), '\u533b\u9662-\u6027\u8d28': Value(dtype='string', id=None), '\u533b\u9662-\u533a\u57df': Value(dtype='string', id=None), '\u533b\u9662-\u5730\u5740': Value(dtype='string', id=None), '\u533b\u9662-\u7535\u8bdd': Value(dtype='string', id=None), '\u533b\u9662-\u6302\u53f7\u65f6\u95f4': Value(dtype='string', id=None), '\u533b\u9662-\u95e8\u8bca\u65f6\u95f4': Value(dtype='string', id=None), '\u533b\u9662-\u516c\u4ea4\u7ebf\u8def': Value(dtype='string', id=None), '\u533b\u9662-\u5730\u94c1\u53ef\u8fbe': Value(dtype='string', id=None), '\u533b\u9662-\u5730\u94c1\u7ebf\u8def': Value(dtype='string', id=None), '\u533b\u9662-\u91cd\u70b9\u79d1\u5ba4': Value(dtype='string', id=None), '\u533b\u9662-CT': Value(dtype='string', id=None), '\u533b\u9662-3.0T MRI': Value(dtype='string', id=None), '\u533b\u9662-DSA': Value(dtype='string', id=None)}\r\n```\r\n\r\n</details>\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.1\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3637/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3637/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3634", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3634/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3634/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3634/events", "html_url": "https://github.com/huggingface/datasets/issues/3634", "id": 1115133279, "node_id": "I_kwDODunzps5Cd5Vf", "number": 3634, "title": "Dataset.shuffle(seed=None) gives fixed row permutation", "user": {"login": "elisno", "id": 18127060, "node_id": "MDQ6VXNlcjE4MTI3MDYw", "avatar_url": "https://avatars.githubusercontent.com/u/18127060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elisno", "html_url": "https://github.com/elisno", "followers_url": "https://api.github.com/users/elisno/followers", "following_url": "https://api.github.com/users/elisno/following{/other_user}", "gists_url": "https://api.github.com/users/elisno/gists{/gist_id}", "starred_url": "https://api.github.com/users/elisno/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elisno/subscriptions", "organizations_url": "https://api.github.com/users/elisno/orgs", "repos_url": "https://api.github.com/users/elisno/repos", "events_url": "https://api.github.com/users/elisno/events{/privacy}", "received_events_url": "https://api.github.com/users/elisno/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-01-26T15:13:08Z", "updated_at": "2022-01-27T18:16:07Z", "closed_at": "2022-01-27T18:16:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nRepeated attempts to `shuffle` a dataset without specifying a seed give the same results.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\n\r\n# Some toy example\r\ndata = datasets.Dataset.from_dict(\r\n    {\"feature\": [1, 2, 3, 4, 5], \"label\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\r\n)\r\n\r\n# Doesn't work as expected\r\nprint(\"Shuffle dataset\")\r\nfor _ in range(3):\r\n    print(data.shuffle(seed=None)[:])\r\n\r\n# This seems to work with pandas\r\nprint(\"\\nShuffle via pandas\")\r\nfor _ in range(3):\r\n    df = data.to_pandas().sample(frac=1.0)\r\n    print(datasets.Dataset.from_pandas(df, preserve_index=False)[:])\r\n\r\n```\r\n\r\n## Expected results\r\nI assumed that the default setting would initialize a new/random state of a `np.random.BitGenerator` (see [docs](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=shuffle#datasets.Dataset.shuffle)).\r\n\r\nWouldn't that reshuffle the rows each time I call `data.shuffle()`?\r\n\r\n## Actual results\r\n\r\n```bash\r\nShuffle dataset\r\n{'feature': [5, 1, 3, 2, 4], 'label': ['e', 'a', 'c', 'b', 'd']}\r\n{'feature': [5, 1, 3, 2, 4], 'label': ['e', 'a', 'c', 'b', 'd']}\r\n{'feature': [5, 1, 3, 2, 4], 'label': ['e', 'a', 'c', 'b', 'd']}\r\n\r\nShuffle via pandas\r\n{'feature': [4, 2, 3, 1, 5], 'label': ['d', 'b', 'c', 'a', 'e']}\r\n{'feature': [2, 5, 3, 4, 1], 'label': ['b', 'e', 'c', 'd', 'a']}\r\n{'feature': [5, 2, 3, 1, 4], 'label': ['e', 'b', 'c', 'a', 'd']}\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.18.0\r\n- Platform: Linux-5.13.0-27-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.12\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3634/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3632", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3632/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3632/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3632/events", "html_url": "https://github.com/huggingface/datasets/issues/3632", "id": 1115027185, "node_id": "I_kwDODunzps5Cdfbx", "number": 3632, "title": "Adding CC-100: Monolingual Datasets from Web Crawl Data (Datasets links are invalid)", "user": {"login": "AnzorGozalishvili", "id": 55232459, "node_id": "MDQ6VXNlcjU1MjMyNDU5", "avatar_url": "https://avatars.githubusercontent.com/u/55232459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnzorGozalishvili", "html_url": "https://github.com/AnzorGozalishvili", "followers_url": "https://api.github.com/users/AnzorGozalishvili/followers", "following_url": "https://api.github.com/users/AnzorGozalishvili/following{/other_user}", "gists_url": "https://api.github.com/users/AnzorGozalishvili/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnzorGozalishvili/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnzorGozalishvili/subscriptions", "organizations_url": "https://api.github.com/users/AnzorGozalishvili/orgs", "repos_url": "https://api.github.com/users/AnzorGozalishvili/repos", "events_url": "https://api.github.com/users/AnzorGozalishvili/events{/privacy}", "received_events_url": "https://api.github.com/users/AnzorGozalishvili/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-01-26T13:35:37Z", "updated_at": "2022-02-10T06:58:11Z", "closed_at": "2022-02-10T06:58:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nThe dataset links are no longer valid for CC-100. It seems that the website which was keeping these files are no longer accessible and therefore this dataset became unusable. \r\nCheck out the dataset [homepage](http://data.statmt.org/cc-100/)  which isn't accessible.\r\nAlso the URLs for dataset file per language isn't accessible: http://data.statmt.org/cc-100/<language code here>.txt.xz (language codes: am, sr, ka, etc.)\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"cc100\", \"ka\")\r\n```\r\nIt throws 503 error.\r\n\r\n## Expected results\r\nIt should successfully download and load dataset but it throws an exception because the dataset files are no longer accessible.\r\n\r\n\r\n## Environment info\r\nRun from google colab. Just installed the library using pip:\r\n```!pip install -U datasets```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3632/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3632/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3631", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3631/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3631/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3631/events", "html_url": "https://github.com/huggingface/datasets/issues/3631", "id": 1114833662, "node_id": "I_kwDODunzps5CcwL-", "number": 3631, "title": "Labels conflict when loading a local CSV file.", "user": {"login": "pichljan", "id": 8571301, "node_id": "MDQ6VXNlcjg1NzEzMDE=", "avatar_url": "https://avatars.githubusercontent.com/u/8571301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pichljan", "html_url": "https://github.com/pichljan", "followers_url": "https://api.github.com/users/pichljan/followers", "following_url": "https://api.github.com/users/pichljan/following{/other_user}", "gists_url": "https://api.github.com/users/pichljan/gists{/gist_id}", "starred_url": "https://api.github.com/users/pichljan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pichljan/subscriptions", "organizations_url": "https://api.github.com/users/pichljan/orgs", "repos_url": "https://api.github.com/users/pichljan/repos", "events_url": "https://api.github.com/users/pichljan/events{/privacy}", "received_events_url": "https://api.github.com/users/pichljan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-01-26T10:00:33Z", "updated_at": "2022-02-11T23:02:31Z", "closed_at": "2022-02-11T23:02:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to load a local CSV file with a separate file containing label names. It is successfully loaded for the first time, but when I try to load it again, there is a conflict between provided labels and the cached dataset info. Disabling caching globally and/or using `download_mode=\"force_redownload\"` did not help.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset('csv', data_files='data/my_data.csv',\r\n                        features=Features(text=Value(dtype='string'),\r\n                                          label=ClassLabel(names_file='data/my_data_labels.txt')))\r\n```\r\n`my_data.csv` file has the following structure:\r\n```\r\ntext,label\r\n\"example1\",0\r\n\"example2\",1\r\n...\r\n```\r\nand the `my_data_labels.txt` looks like this:\r\n```\r\nlabel1\r\nlabel2\r\n...\r\n```\r\n\r\n## Expected results\r\nSuccessfully loaded dataset.\r\n\r\n## Actual results\r\n```python\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/load.py\", line 1706, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/builder.py\", line 766, in as_dataset\r\n    datasets = utils.map_nested(\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 261, in map_nested\r\n    mapped = [\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 262, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True))\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 197, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/builder.py\", line 797, in _build_single_dataset\r\n    ds = self._as_dataset(\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/builder.py\", line 872, in _as_dataset\r\n    return Dataset(fingerprint=fingerprint, **dataset_kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 638, in __init__\r\n    inferred_features = Features.from_arrow_schema(arrow_table.schema)\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/features/features.py\", line 1242, in from_arrow_schema\r\n    return Features.from_dict(metadata[\"info\"][\"features\"])\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/features/features.py\", line 1271, in from_dict\r\n    obj = generate_from_dict(dic)\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/features/features.py\", line 1076, in generate_from_dict\r\n    return {key: generate_from_dict(value) for key, value in obj.items()}\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/features/features.py\", line 1076, in <dictcomp>\r\n    return {key: generate_from_dict(value) for key, value in obj.items()}\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/features/features.py\", line 1083, in generate_from_dict\r\n    return class_type(**{k: v for k, v in obj.items() if k in field_names})\r\n  File \"<string>\", line 7, in __init__\r\n  File \"/usr/local/lib/python3.8/site-packages/datasets/features/features.py\", line 776, in __post_init__\r\n    raise ValueError(\"Please provide either names or names_file but not both.\")\r\nValueError: Please provide either names or names_file but not both.\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.18.0\r\n- Python version: 3.8.2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3631/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3631/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3626", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3626/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3626/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3626/events", "html_url": "https://github.com/huggingface/datasets/issues/3626", "id": 1113534436, "node_id": "I_kwDODunzps5CXy_k", "number": 3626, "title": "The Pile cannot connect to host", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-01-25T07:43:33Z", "updated_at": "2022-02-14T08:40:58Z", "closed_at": "2022-02-14T08:40:58Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nThe Pile had issues with their previous host server and have mirrored its content to another server.\r\n\r\nThe new URL server should be updated.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3626/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3626/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3621", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3621/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3621/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3621/events", "html_url": "https://github.com/huggingface/datasets/issues/3621", "id": 1112720434, "node_id": "I_kwDODunzps5CUsQy", "number": 3621, "title": "Consider adding `ipywidgets` as a dependency.", "user": {"login": "koaning", "id": 1019791, "node_id": "MDQ6VXNlcjEwMTk3OTE=", "avatar_url": "https://avatars.githubusercontent.com/u/1019791?v=4", "gravatar_id": "", "url": "https://api.github.com/users/koaning", "html_url": "https://github.com/koaning", "followers_url": "https://api.github.com/users/koaning/followers", "following_url": "https://api.github.com/users/koaning/following{/other_user}", "gists_url": "https://api.github.com/users/koaning/gists{/gist_id}", "starred_url": "https://api.github.com/users/koaning/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/koaning/subscriptions", "organizations_url": "https://api.github.com/users/koaning/orgs", "repos_url": "https://api.github.com/users/koaning/repos", "events_url": "https://api.github.com/users/koaning/events{/privacy}", "received_events_url": "https://api.github.com/users/koaning/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-01-24T14:27:11Z", "updated_at": "2022-02-24T09:04:36Z", "closed_at": "2022-02-24T09:04:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I install `datasets` in a fresh virtualenv with jupyterlab I always see this error. \r\n\r\n```\r\nImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\r\n```\r\n\r\nIt's a bit of a nuisance, because I need to run shut down the jupyterlab server in order to install the required dependency. Might it be an option to just include it as a dependency here? ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3621/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3621/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3615", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3615/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3615/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3615/events", "html_url": "https://github.com/huggingface/datasets/issues/3615", "id": 1111576876, "node_id": "I_kwDODunzps5CQVEs", "number": 3615, "title": "Dataset BnL Historical Newspapers does not work in streaming mode", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-01-22T14:12:59Z", "updated_at": "2022-02-04T14:05:21Z", "closed_at": "2022-02-04T14:05:21Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen trying to load in streaming mode, it \"hangs\"...\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nds = load_dataset(\"bnl_newspapers\", split=\"train\", streaming=True)\r\n```\r\n\r\n## Expected results\r\nThe code should be optimized, so that it works fast in streaming mode.\r\n\r\nCC: @davanstrien \r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3615/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3611", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3611/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3611/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3611/events", "html_url": "https://github.com/huggingface/datasets/issues/3611", "id": 1110399096, "node_id": "I_kwDODunzps5CL1h4", "number": 3611, "title": "Indexing bug after dataset.select()", "user": {"login": "kamalkraj", "id": 17096858, "node_id": "MDQ6VXNlcjE3MDk2ODU4", "avatar_url": "https://avatars.githubusercontent.com/u/17096858?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kamalkraj", "html_url": "https://github.com/kamalkraj", "followers_url": "https://api.github.com/users/kamalkraj/followers", "following_url": "https://api.github.com/users/kamalkraj/following{/other_user}", "gists_url": "https://api.github.com/users/kamalkraj/gists{/gist_id}", "starred_url": "https://api.github.com/users/kamalkraj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kamalkraj/subscriptions", "organizations_url": "https://api.github.com/users/kamalkraj/orgs", "repos_url": "https://api.github.com/users/kamalkraj/repos", "events_url": "https://api.github.com/users/kamalkraj/events{/privacy}", "received_events_url": "https://api.github.com/users/kamalkraj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-01-21T12:09:30Z", "updated_at": "2022-01-27T18:16:22Z", "closed_at": "2022-01-27T18:16:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n \r\nDataset indexing is not working as expected after `dataset.select(range(100))`\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nimport datasets\r\n\r\ntask_to_keys = {\r\n    \"cola\": (\"sentence\", None),\r\n    \"mnli\": (\"premise\", \"hypothesis\"),\r\n    \"mrpc\": (\"sentence1\", \"sentence2\"),\r\n    \"qnli\": (\"question\", \"sentence\"),\r\n    \"qqp\": (\"question1\", \"question2\"),\r\n    \"rte\": (\"sentence1\", \"sentence2\"),\r\n    \"sst2\": (\"sentence\", None),\r\n    \"stsb\": (\"sentence1\", \"sentence2\"),\r\n    \"wnli\": (\"sentence1\", \"sentence2\"),\r\n}\r\n\r\ntask_name = \"sst2\"\r\nraw_datasets = datasets.load_dataset(\"glue\", task_name)\r\n\r\n\r\ntrain_dataset = raw_datasets[\"train\"]\r\n\r\nprint(\"before select: \",train_dataset[-2:])\r\n# before select:  {'sentence': ['a patient viewer ', 'this new jangle of noise , mayhem and stupidity must be a serious contender for the title . '], 'label': [1, 0], 'idx': [67347, 67348]}\r\n\r\ntrain_dataset = train_dataset.select(range(100))\r\n\r\nprint(\"after select: \",train_dataset[-2:])\r\n# after select:  {'sentence': [], 'label': [], 'idx': []}\r\n\r\n```\r\n\r\nlink to colab: https://colab.research.google.com/drive/1LngeRC9f0jE7eSQ4Kh1cIeb411lRXQD-?usp=sharing\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\nshowing 98, 99 index data\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\nempty\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.17.0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.12\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3611/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3611/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3610", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3610/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3610/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3610/events", "html_url": "https://github.com/huggingface/datasets/issues/3610", "id": 1109777314, "node_id": "I_kwDODunzps5CJdui", "number": 3610, "title": "Checksum error when trying to load amazon_review dataset", "user": {"login": "rifoag", "id": 32415171, "node_id": "MDQ6VXNlcjMyNDE1MTcx", "avatar_url": "https://avatars.githubusercontent.com/u/32415171?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rifoag", "html_url": "https://github.com/rifoag", "followers_url": "https://api.github.com/users/rifoag/followers", "following_url": "https://api.github.com/users/rifoag/following{/other_user}", "gists_url": "https://api.github.com/users/rifoag/gists{/gist_id}", "starred_url": "https://api.github.com/users/rifoag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rifoag/subscriptions", "organizations_url": "https://api.github.com/users/rifoag/orgs", "repos_url": "https://api.github.com/users/rifoag/repos", "events_url": "https://api.github.com/users/rifoag/events{/privacy}", "received_events_url": "https://api.github.com/users/rifoag/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-01-20T21:20:32Z", "updated_at": "2022-01-21T13:22:31Z", "closed_at": "2022-01-21T13:22:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\nI am getting the issue when trying to load dataset using\r\n```\r\ndataset = load_dataset(\"amazon_polarity\")\r\n```\r\n\r\n## Expected results\r\ndataset loaded\r\n\r\n## Actual results\r\n```\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-3-b4758ba980ae> in <module>()\r\n----> 1 dataset = load_dataset(\"amazon_polarity\")\r\n      2 dataset.set_format(type='pandas')\r\n      3 content_series = dataset['train']['content']\r\n      4 label_series = dataset['train']['label']\r\n      5 df = pd.concat([content_series, label_series], axis=1)\r\n\r\n3 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/u/0/uc?id=0Bz8a_Dbh9QhbaW12WVVZS2drcnM&export=download']\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.17.0\r\n- Platform: Google colab\r\n- Python version: 3.7.12", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3610/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3606", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3606/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3606/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3606/events", "html_url": "https://github.com/huggingface/datasets/issues/3606", "id": 1108918701, "node_id": "I_kwDODunzps5CGMGt", "number": 3606, "title": "audio column not saved correctly after resampling", "user": {"login": "laphang", "id": 24724502, "node_id": "MDQ6VXNlcjI0NzI0NTAy", "avatar_url": "https://avatars.githubusercontent.com/u/24724502?v=4", "gravatar_id": "", "url": "https://api.github.com/users/laphang", "html_url": "https://github.com/laphang", "followers_url": "https://api.github.com/users/laphang/followers", "following_url": "https://api.github.com/users/laphang/following{/other_user}", "gists_url": "https://api.github.com/users/laphang/gists{/gist_id}", "starred_url": "https://api.github.com/users/laphang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/laphang/subscriptions", "organizations_url": "https://api.github.com/users/laphang/orgs", "repos_url": "https://api.github.com/users/laphang/repos", "events_url": "https://api.github.com/users/laphang/events{/privacy}", "received_events_url": "https://api.github.com/users/laphang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-01-20T06:37:10Z", "updated_at": "2022-01-23T01:41:01Z", "closed_at": "2022-01-23T01:24:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nAfter resampling the audio column, saving with save_to_disk doesn't seem to save with the correct type. \r\n\r\n## Steps to reproduce the bug\r\n- load a subset of common voice dataset (48Khz)\r\n- resample audio column to 16Khz\r\n- save with save_to_disk()\r\n- load with load_from_disk()\r\n\r\n## Expected results\r\nI expected that after saving the data, and then loading it back in, the audio column has the correct dataset.Audio type (i.e. same as before saving it)\r\n{'accent': Value(dtype='string', id=None),\r\n 'age': Value(dtype='string', id=None),\r\n 'audio': Audio(sampling_rate=16000, mono=True, _storage_dtype='string', id=None),\r\n 'client_id': Value(dtype='string', id=None),\r\n 'down_votes': Value(dtype='int64', id=None),\r\n 'gender': Value(dtype='string', id=None),\r\n 'locale': Value(dtype='string', id=None),\r\n 'path': Value(dtype='string', id=None),\r\n 'segment': Value(dtype='string', id=None),\r\n 'sentence': Value(dtype='string', id=None),\r\n 'up_votes': Value(dtype='int64', id=None)}\r\n\r\n## Actual results\r\nAudio column does not have the right type\r\n{'accent': Value(dtype='string', id=None),\r\n 'age': Value(dtype='string', id=None),\r\n 'audio': {'bytes': Value(dtype='binary', id=None),\r\n  'path': Value(dtype='string', id=None)},\r\n 'client_id': Value(dtype='string', id=None),\r\n 'down_votes': Value(dtype='int64', id=None),\r\n 'gender': Value(dtype='string', id=None),\r\n 'locale': Value(dtype='string', id=None),\r\n 'path': Value(dtype='string', id=None),\r\n 'segment': Value(dtype='string', id=None),\r\n 'sentence': Value(dtype='string', id=None),\r\n 'up_votes': Value(dtype='int64', id=None)}\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.17.0\r\n- Platform: linux\r\n- Python version:\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3606/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3599", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3599/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3599/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3599/events", "html_url": "https://github.com/huggingface/datasets/issues/3599", "id": 1108111607, "node_id": "I_kwDODunzps5CDHD3", "number": 3599, "title": "The `add_column()` method does not work if used on dataset sliced with `select()`", "user": {"login": "ThGouzias", "id": 59422506, "node_id": "MDQ6VXNlcjU5NDIyNTA2", "avatar_url": "https://avatars.githubusercontent.com/u/59422506?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ThGouzias", "html_url": "https://github.com/ThGouzias", "followers_url": "https://api.github.com/users/ThGouzias/followers", "following_url": "https://api.github.com/users/ThGouzias/following{/other_user}", "gists_url": "https://api.github.com/users/ThGouzias/gists{/gist_id}", "starred_url": "https://api.github.com/users/ThGouzias/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ThGouzias/subscriptions", "organizations_url": "https://api.github.com/users/ThGouzias/orgs", "repos_url": "https://api.github.com/users/ThGouzias/repos", "events_url": "https://api.github.com/users/ThGouzias/events{/privacy}", "received_events_url": "https://api.github.com/users/ThGouzias/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-01-19T13:36:50Z", "updated_at": "2022-01-28T15:35:57Z", "closed_at": "2022-01-28T15:35:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I posted this as a question on the forums ([here](https://discuss.huggingface.co/t/add-column-does-not-work-if-used-on-dataset-sliced-with-select/13893)): \r\n\r\nI have a dataset with 2000 entries\r\n\r\n> dataset = Dataset.from_dict({'colA': list(range(2000))})\r\n\r\nand from which I want to extract the first one thousand rows, create a new dataset with these and also add a new column to it:\r\n\r\n> dataset2 = dataset.select(list(range(1000)))\r\n> final_dataset = dataset2.add_column('colB', list(range(1000)))\r\n\r\nThis gives an error\r\n>ArrowInvalid: Added column's length must match table's length. Expected length 2000 but got length 1000\r\n\r\nSo it looks like even though it is a dataset with 1000 rows, it \"remembers\" the shape of the one it was sliced from.\r\n\r\n## Actual results\r\n```\r\nArrowInvalid                              Traceback (most recent call last)\r\n<ipython-input-138-e806860f3ce3> in <module>\r\n----> 1 final_dataset = dataset2.add_column('colB', list(range(1000)))\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    468         }\r\n    469         # apply actual function\r\n--> 470         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    471         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    472         # re-apply format to the output\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    404             # Call actual function\r\n    405 \r\n--> 406             out = func(self, *args, **kwargs)\r\n    407 \r\n    408             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py in add_column(self, name, column, new_fingerprint)\r\n   3343         column_table = InMemoryTable.from_pydict({name: column})\r\n   3344         # Concatenate tables horizontally\r\n-> 3345         table = ConcatenationTable.from_tables([self._data, column_table], axis=1)\r\n   3346         # Update features\r\n   3347         info = self.info.copy()\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/table.py in from_tables(cls, tables, axis)\r\n    729             table_blocks = to_blocks(table)\r\n    730             blocks = _extend_blocks(blocks, table_blocks, axis=axis)\r\n--> 731         return cls.from_blocks(blocks)\r\n    732 \r\n    733     @property\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/table.py in from_blocks(cls, blocks)\r\n    668     @classmethod\r\n    669     def from_blocks(cls, blocks: TableBlockContainer) -> \"ConcatenationTable\":\r\n--> 670         blocks = cls._consolidate_blocks(blocks)\r\n    671         if isinstance(blocks, TableBlock):\r\n    672             table = blocks\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/table.py in _consolidate_blocks(cls, blocks)\r\n    664             return cls._merge_blocks(blocks, axis=0)\r\n    665         else:\r\n--> 666             return cls._merge_blocks(blocks)\r\n    667 \r\n    668     @classmethod\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/table.py in _merge_blocks(cls, blocks, axis)\r\n    650                 merged_blocks += list(block_group)\r\n    651         else:  # both\r\n--> 652             merged_blocks = [cls._merge_blocks(row_block, axis=1) for row_block in blocks]\r\n    653             if all(len(row_block) == 1 for row_block in merged_blocks):\r\n    654                 merged_blocks = cls._merge_blocks(\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/table.py in <listcomp>(.0)\r\n    650                 merged_blocks += list(block_group)\r\n    651         else:  # both\r\n--> 652             merged_blocks = [cls._merge_blocks(row_block, axis=1) for row_block in blocks]\r\n    653             if all(len(row_block) == 1 for row_block in merged_blocks):\r\n    654                 merged_blocks = cls._merge_blocks(\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/table.py in _merge_blocks(cls, blocks, axis)\r\n    647             for is_in_memory, block_group in groupby(blocks, key=lambda x: isinstance(x, InMemoryTable)):\r\n    648                 if is_in_memory:\r\n--> 649                     block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\r\n    650                 merged_blocks += list(block_group)\r\n    651         else:  # both\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/table.py in _concat_blocks(blocks, axis)\r\n    626                 else:\r\n    627                     for name, col in zip(table.column_names, table.columns):\r\n--> 628                         pa_table = pa_table.append_column(name, col)\r\n    629             return pa_table\r\n    630         else:\r\n\r\n~/.local/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.append_column()\r\n\r\n~/.local/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.add_column()\r\n\r\n~/.local/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/.local/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Added column's length must match table's length. Expected length 2000 but got length 1000\r\n```\r\n\r\nA solution provided by @mariosasko is to use `dataset2.flatten_indices()` after the `select()` and before attempting to add the new column:\r\n\r\n> dataset = Dataset.from_dict({'colA': list(range(2000))})\r\n> dataset2 = dataset.select(list(range(1000)))\r\n> dataset2 = dataset2.flatten_indices()\r\n> final_dataset = dataset2.add_column('colB', list(range(1000)))\r\n\r\nwhich works.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.13.2 (note: also checked with version 1.17.0, still the same error)\r\n- Platform: Ubuntu 20.04.3\r\n- Python version: 3.8.10\r\n- PyArrow version: 6.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3599/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3599/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3598", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3598/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3598/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3598/events", "html_url": "https://github.com/huggingface/datasets/issues/3598", "id": 1108107199, "node_id": "I_kwDODunzps5CDF-_", "number": 3598, "title": "Readme info not being parsed to show on Dataset card page", "user": {"login": "davidcanovas", "id": 79796807, "node_id": "MDQ6VXNlcjc5Nzk2ODA3", "avatar_url": "https://avatars.githubusercontent.com/u/79796807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidcanovas", "html_url": "https://github.com/davidcanovas", "followers_url": "https://api.github.com/users/davidcanovas/followers", "following_url": "https://api.github.com/users/davidcanovas/following{/other_user}", "gists_url": "https://api.github.com/users/davidcanovas/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidcanovas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidcanovas/subscriptions", "organizations_url": "https://api.github.com/users/davidcanovas/orgs", "repos_url": "https://api.github.com/users/davidcanovas/repos", "events_url": "https://api.github.com/users/davidcanovas/events{/privacy}", "received_events_url": "https://api.github.com/users/davidcanovas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-01-19T13:32:29Z", "updated_at": "2022-01-21T10:20:01Z", "closed_at": "2022-01-21T10:20:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe info contained in the README.md file is not being shown in the dataset main page. Basic info and table of contents are properly formatted in the README.\r\n\r\n## Steps to reproduce the bug\r\n\r\n# Sample code to reproduce the bug\r\nThe README file is this one: https://huggingface.co/datasets/softcatala/Tilde-MODEL-Catalan/blob/main/README.md\r\n\r\n## Expected results\r\nREADME info should appear in the Dataset card page.\r\n\r\n## Actual results\r\nNothing is shown. However, labels are parsed and shown successfully.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3598/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3597", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3597/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3597/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3597/events", "html_url": "https://github.com/huggingface/datasets/issues/3597", "id": 1108092864, "node_id": "I_kwDODunzps5CDCfA", "number": 3597, "title": "ERROR: File \"setup.py\" or \"setup.cfg\" not found. Directory cannot be installed in editable mode: /content", "user": {"login": "amitkml", "id": 49492030, "node_id": "MDQ6VXNlcjQ5NDkyMDMw", "avatar_url": "https://avatars.githubusercontent.com/u/49492030?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amitkml", "html_url": "https://github.com/amitkml", "followers_url": "https://api.github.com/users/amitkml/followers", "following_url": "https://api.github.com/users/amitkml/following{/other_user}", "gists_url": "https://api.github.com/users/amitkml/gists{/gist_id}", "starred_url": "https://api.github.com/users/amitkml/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amitkml/subscriptions", "organizations_url": "https://api.github.com/users/amitkml/orgs", "repos_url": "https://api.github.com/users/amitkml/repos", "events_url": "https://api.github.com/users/amitkml/events{/privacy}", "received_events_url": "https://api.github.com/users/amitkml/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-01-19T13:19:28Z", "updated_at": "2022-08-05T12:35:51Z", "closed_at": "2022-02-14T08:46:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Bug\r\nThe install of streaming dataset is giving following error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n! git clone https://github.com/huggingface/datasets.git\r\n! cd datasets\r\n! pip install -e \".[streaming]\"\r\n```\r\n\r\n## Actual results\r\nCloning into 'datasets'...\r\nremote: Enumerating objects: 50816, done.\r\nremote: Counting objects: 100% (2356/2356), done.\r\nremote: Compressing objects: 100% (1606/1606), done.\r\nremote: Total 50816 (delta 834), reused 1741 (delta 525), pack-reused 48460\r\nReceiving objects: 100% (50816/50816), 72.47 MiB | 27.68 MiB/s, done.\r\nResolving deltas: 100% (22541/22541), done.\r\nChecking out files: 100% (6722/6722), done.\r\nERROR: File \"setup.py\" or \"setup.cfg\" not found. Directory cannot be installed in editable mode: /content\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3597/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3596", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3596/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3596/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3596/events", "html_url": "https://github.com/huggingface/datasets/issues/3596", "id": 1107345338, "node_id": "I_kwDODunzps5CAL-6", "number": 3596, "title": "Loss of cast `Image` feature on certain dataset method", "user": {"login": "davanstrien", "id": 8995957, "node_id": "MDQ6VXNlcjg5OTU5NTc=", "avatar_url": "https://avatars.githubusercontent.com/u/8995957?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davanstrien", "html_url": "https://github.com/davanstrien", "followers_url": "https://api.github.com/users/davanstrien/followers", "following_url": "https://api.github.com/users/davanstrien/following{/other_user}", "gists_url": "https://api.github.com/users/davanstrien/gists{/gist_id}", "starred_url": "https://api.github.com/users/davanstrien/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davanstrien/subscriptions", "organizations_url": "https://api.github.com/users/davanstrien/orgs", "repos_url": "https://api.github.com/users/davanstrien/repos", "events_url": "https://api.github.com/users/davanstrien/events{/privacy}", "received_events_url": "https://api.github.com/users/davanstrien/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2022-01-18T20:44:01Z", "updated_at": "2022-01-21T18:07:28Z", "closed_at": "2022-01-21T18:07:28Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen an a column is cast to an `Image` feature, the cast type appears to be lost during certain operations. I first noticed this when using the `push_to_hub` method on a dataset that contained urls pointing to images which had been cast to an `image`. This also happens when using select on a dataset which has had a column cast to an `Image`.\r\n\r\nI suspect this might be related to https://github.com/huggingface/datasets/pull/3556 but I don't believe that pull request fixes this issue.  \r\n\r\n## Steps to reproduce the bug\r\n\r\nAn example of casting a url to an image followed by using the `select` method:\r\n\r\n```python\r\nfrom datasets import Dataset\r\nfrom datasets import features\r\nurl = \"https://cf.ltkcdn.net/cats/images/std-lg/246866-1200x816-grey-white-kitten.webp\"\r\ndata_dict = {\"url\": [url]*2}\r\ndataset = Dataset.from_dict(data_dict)\r\ndataset =  dataset.cast_column('url',features.Image())\r\nsample = dataset.select([1])\r\n```\r\n\r\n[example notebook](https://gist.github.com/davanstrien/06e53f4383c28ae77ce1b30d0eaf0d70#file-potential_casting_bug-ipynb)\r\n\r\n## Expected results\r\nThe cast value is maintained when further methods are applied to the dataset. \r\n\r\n## Actual results\r\n```python\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-12-47f393bc2d0d> in <module>()\r\n----> 1 sample = dataset.select([1])\r\n\r\n4 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    487         }\r\n    488         # apply actual function\r\n--> 489         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    490         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    491         # re-apply format to the output\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    409             # Call actual function\r\n    410 \r\n--> 411             out = func(self, *args, **kwargs)\r\n    412 \r\n    413             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in select(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\r\n   2772             )\r\n   2773         else:\r\n-> 2774             return self._new_dataset_with_indices(indices_buffer=buf_writer.getvalue(), fingerprint=new_fingerprint)\r\n   2775 \r\n   2776     @transmit_format\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in _new_dataset_with_indices(self, indices_cache_file_name, indices_buffer, fingerprint)\r\n   2688             split=self.split,\r\n   2689             indices_table=indices_table,\r\n-> 2690             fingerprint=fingerprint,\r\n   2691         )\r\n   2692 \r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in __init__(self, arrow_table, info, split, indices_table, fingerprint)\r\n    664         if self.info.features.type != inferred_features.type:\r\n    665             raise ValueError(\r\n--> 666                 f\"External features info don't match the dataset:\\nGot\\n{self.info.features}\\nwith type\\n{self.info.features.type}\\n\\nbut expected something like\\n{inferred_features}\\nwith type\\n{inferred_features.type}\"\r\n    667             )\r\n    668 \r\n\r\nValueError: External features info don't match the dataset:\r\nGot\r\n{'url': Image(id=None)}\r\nwith type\r\nstruct<url: extension<arrow.py_extension_type<ImageExtensionType>>>\r\n\r\nbut expected something like\r\n{'url': Value(dtype='string', id=None)}\r\nwith type\r\nstruct<url: string>\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.17.1.dev0\r\n- Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.12\r\n- PyArrow version: 3.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3596/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3587", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3587/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3587/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3587/events", "html_url": "https://github.com/huggingface/datasets/issues/3587", "id": 1106719182, "node_id": "I_kwDODunzps5B9zHO", "number": 3587, "title": "No module named 'fsspec.archive'", "user": {"login": "shuuchen", "id": 13246825, "node_id": "MDQ6VXNlcjEzMjQ2ODI1", "avatar_url": "https://avatars.githubusercontent.com/u/13246825?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shuuchen", "html_url": "https://github.com/shuuchen", "followers_url": "https://api.github.com/users/shuuchen/followers", "following_url": "https://api.github.com/users/shuuchen/following{/other_user}", "gists_url": "https://api.github.com/users/shuuchen/gists{/gist_id}", "starred_url": "https://api.github.com/users/shuuchen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shuuchen/subscriptions", "organizations_url": "https://api.github.com/users/shuuchen/orgs", "repos_url": "https://api.github.com/users/shuuchen/repos", "events_url": "https://api.github.com/users/shuuchen/events{/privacy}", "received_events_url": "https://api.github.com/users/shuuchen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-01-18T10:17:01Z", "updated_at": "2022-08-11T09:57:54Z", "closed_at": "2022-01-18T10:33:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nCannot import datasets after installation.\r\n\r\n## Steps to reproduce the bug\r\n```shell\r\n$ python\r\nPython 3.9.7 (default, Sep 16 2021, 13:09:58)\r\n[GCC 7.5.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import datasets\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/shuchen/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/__init__.py\", line 34, in <module>\r\n    from .arrow_dataset import Dataset, concatenate_datasets\r\n  File \"/home/shuchen/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 61, in <module>\r\n    from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n  File \"/home/shuchen/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/arrow_writer.py\", line 28, in <module>\r\n    from .features import (\r\n  File \"/home/shuchen/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/features/__init__.py\", line 2, in <module>\r\n    from .audio import Audio\r\n  File \"/home/shuchen/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/features/audio.py\", line 7, in <module>\r\n    from ..utils.streaming_download_manager import xopen\r\n  File \"/home/shuchen/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/utils/streaming_download_manager.py\", line 18, in <module>\r\n    from ..filesystems import COMPRESSION_FILESYSTEMS\r\n  File \"/home/shuchen/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/filesystems/__init__.py\", line 6, in <module>\r\n    from . import compression\r\n  File \"/home/shuchen/miniconda3/envs/hf/lib/python3.9/site-packages/datasets/filesystems/compression.py\", line 5, in <module>\r\n    from fsspec.archive import AbstractArchiveFileSystem\r\nModuleNotFoundError: No module named 'fsspec.archive'\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3587/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3585", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3585/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3585/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3585/events", "html_url": "https://github.com/huggingface/datasets/issues/3585", "id": 1105821470, "node_id": "I_kwDODunzps5B6X8e", "number": 3585, "title": "Datasets streaming + map doesn't work for `Audio`", "user": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892865, "node_id": "MDU6TGFiZWwxOTM1ODkyODY1", "url": "https://api.github.com/repos/huggingface/datasets/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-01-17T12:55:42Z", "updated_at": "2022-01-20T13:28:00Z", "closed_at": "2022-01-20T13:28:00Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen using audio datasets in streaming mode, applying a `map(...)` before iterating leads to an error as the key `array` does not exist anymore.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"common_voice\", \"en\", streaming=True, split=\"train\")\r\n\r\n\r\ndef map_fn(batch):\r\n    print(\"audio keys\", batch[\"audio\"].keys())\r\n    batch[\"audio\"] = batch[\"audio\"][\"array\"][:100]\r\n    return batch\r\n\r\n\r\nds = ds.map(map_fn) \r\n\r\nsample = next(iter(ds))\r\n```\r\n\r\nI think the audio is somehow decoded before `.map(...)` is actually called.\r\n\r\n## Expected results\r\n\r\nIMO, the above code snippet should work.\r\n\r\n## Actual results\r\n\r\n```bash\r\naudio keys dict_keys(['path', 'bytes'])\r\nTraceback (most recent call last):\r\n  File \"./run_audio.py\", line 15, in <module>\r\n    sample = next(iter(ds))\r\n  File \"/home/patrick/python_bin/datasets/iterable_dataset.py\", line 341, in __iter__\r\n    for key, example in self._iter():\r\n  File \"/home/patrick/python_bin/datasets/iterable_dataset.py\", line 338, in _iter\r\n    yield from ex_iterable\r\n  File \"/home/patrick/python_bin/datasets/iterable_dataset.py\", line 192, in __iter__\r\n    yield key, self.function(example)\r\n  File \"./run_audio.py\", line 9, in map_fn\r\n    batch[\"input\"] = batch[\"audio\"][\"array\"][:100]\r\nKeyError: 'array'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.17.1.dev0\r\n- Platform: Linux-5.3.0-64-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.12\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3585/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3585/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3582", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3582/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3582/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3582/events", "html_url": "https://github.com/huggingface/datasets/issues/3582", "id": 1104877303, "node_id": "I_kwDODunzps5B2xb3", "number": 3582, "title": "conll 2003 dataset source url is no longer valid", "user": {"login": "rcanand", "id": 303900, "node_id": "MDQ6VXNlcjMwMzkwMA==", "avatar_url": "https://avatars.githubusercontent.com/u/303900?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rcanand", "html_url": "https://github.com/rcanand", "followers_url": "https://api.github.com/users/rcanand/followers", "following_url": "https://api.github.com/users/rcanand/following{/other_user}", "gists_url": "https://api.github.com/users/rcanand/gists{/gist_id}", "starred_url": "https://api.github.com/users/rcanand/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rcanand/subscriptions", "organizations_url": "https://api.github.com/users/rcanand/orgs", "repos_url": "https://api.github.com/users/rcanand/repos", "events_url": "https://api.github.com/users/rcanand/events{/privacy}", "received_events_url": "https://api.github.com/users/rcanand/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2067388877, "node_id": "MDU6TGFiZWwyMDY3Mzg4ODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset%20bug", "name": "dataset bug", "color": "2edb81", "default": false, "description": "A bug in a dataset script provided in the library"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2022-01-15T23:04:17Z", "updated_at": "2022-07-20T13:06:40Z", "closed_at": "2022-01-21T16:57:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading `conll2003` dataset fails because it was removed (just yesterday 1/14/2022) from the location it is looking for.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n\r\nload_dataset(\"conll2003\")\r\n```\r\n\r\n## Expected results\r\nThe dataset should load.\r\n\r\n## Actual results\r\nIt is looking for the dataset at `https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt` but it was removed from there yesterday (see [commit](https://github.com/davidsbatista/NER-datasets/commit/9d8f45cc7331569af8eb3422bbe1c97cbebd5690) that removed the file and related [issue](https://github.com/davidsbatista/NER-datasets/issues/8)). \r\n\r\n- We should replace this with an alternate valid location.\r\n- this is being referenced in the huggingface course chapter 7 [colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb), which is also broken.\r\n\r\n```python\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-4-27c956bec93c> in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 raw_datasets = load_dataset(\"conll2003\")\r\n\r\n11 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params)\r\n    610             )\r\n    611         elif response is not None and response.status_code == 404:\r\n--> 612             raise FileNotFoundError(f\"Couldn't find file at {url}\")\r\n    613         _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\r\n    614         if head_error is not None:\r\n\r\nFileNotFoundError: Couldn't find file at https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3582/reactions", "total_count": 5, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 5, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3582/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3563", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3563/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3563/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3563/events", "html_url": "https://github.com/huggingface/datasets/issues/3563", "id": 1099070368, "node_id": "I_kwDODunzps5Bgnug", "number": 3563, "title": "Dataset.from_pandas preserves useless index", "user": {"login": "Sorrow321", "id": 20703486, "node_id": "MDQ6VXNlcjIwNzAzNDg2", "avatar_url": "https://avatars.githubusercontent.com/u/20703486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Sorrow321", "html_url": "https://github.com/Sorrow321", "followers_url": "https://api.github.com/users/Sorrow321/followers", "following_url": "https://api.github.com/users/Sorrow321/following{/other_user}", "gists_url": "https://api.github.com/users/Sorrow321/gists{/gist_id}", "starred_url": "https://api.github.com/users/Sorrow321/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Sorrow321/subscriptions", "organizations_url": "https://api.github.com/users/Sorrow321/orgs", "repos_url": "https://api.github.com/users/Sorrow321/repos", "events_url": "https://api.github.com/users/Sorrow321/events{/privacy}", "received_events_url": "https://api.github.com/users/Sorrow321/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-01-11T12:07:07Z", "updated_at": "2022-01-12T16:11:27Z", "closed_at": "2022-01-12T16:11:27Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nLet's say that you want to create a Dataset object from pandas dataframe. Most likely you will write something like this:\r\n\r\n```\r\nimport pandas as pd\r\nfrom datasets import Dataset\r\n\r\n\r\ndf = pd.read_csv('some_dataset.csv')\r\n# Some DataFrame preprocessing code...\r\ndataset = Dataset.from_pandas(df)\r\n```\r\nIf your preprocessing code contain indexing operations like this:\r\n```\r\ndf = df[df.col1 == some_value]\r\n```\r\nthen your df.index can be changed from (default) ```RangeIndex(start=0, stop=16590, step=1)``` to something like this ```Int64Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,\r\n                9,\r\n            ...\r\n            83979, 83980, 83981, 83982, 83983, 83984, 83985, 83986, 83987,\r\n            83988],\r\n           dtype='int64', length=16590)```\r\n\r\nIn this case, PyArrow (by default) will preserve this non-standard index. In the result, your dataset object will have the extra field that you likely don't want to have: '__index_level_0__'.\r\n\r\nYou can easily fix this by just adding extra argument ```preserve_index=False``` to call of ```InMemoryTable.from_pandas``` in ```arrow_dataset.py```.\r\n\r\nIf you approve that this isn't desirable behavior, I can make a PR fixing that.\r\n\r\n## Environment info\r\n- `datasets` version: 1.16.1\r\n- Platform: Linux-5.11.0-44-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 6.0.1\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3563/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3563/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3561", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3561/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3561/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3561/events", "html_url": "https://github.com/huggingface/datasets/issues/3561", "id": 1098328870, "node_id": "I_kwDODunzps5Bdysm", "number": 3561, "title": "Cannot load \u2018bookcorpusopen\u2019", "user": {"login": "HUIYINXUE", "id": 54684403, "node_id": "MDQ6VXNlcjU0Njg0NDAz", "avatar_url": "https://avatars.githubusercontent.com/u/54684403?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HUIYINXUE", "html_url": "https://github.com/HUIYINXUE", "followers_url": "https://api.github.com/users/HUIYINXUE/followers", "following_url": "https://api.github.com/users/HUIYINXUE/following{/other_user}", "gists_url": "https://api.github.com/users/HUIYINXUE/gists{/gist_id}", "starred_url": "https://api.github.com/users/HUIYINXUE/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HUIYINXUE/subscriptions", "organizations_url": "https://api.github.com/users/HUIYINXUE/orgs", "repos_url": "https://api.github.com/users/HUIYINXUE/repos", "events_url": "https://api.github.com/users/HUIYINXUE/events{/privacy}", "received_events_url": "https://api.github.com/users/HUIYINXUE/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2067388877, "node_id": "MDU6TGFiZWwyMDY3Mzg4ODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset%20bug", "name": "dataset bug", "color": "2edb81", "default": false, "description": "A bug in a dataset script provided in the library"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-01-10T20:17:18Z", "updated_at": "2022-02-14T09:19:27Z", "closed_at": "2022-02-14T09:18:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nCannot load 'bookcorpusopen'\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = load_dataset('bookcorpusopen')\r\n```\r\nor\r\n```python\r\ndataset = load_dataset('bookcorpusopen',script_version='master')\r\n```\r\n\r\n## Actual results\r\nConnectionError: Couldn't reach https://the-eye.eu/public/AI/pile_preliminary_components/books1.tar.gz\r\n\r\n## Environment info\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux version 3.10.0-1160.45.1.el7.x86_64\r\n- Python version:  3.6.13\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3561/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3555", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3555/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3555/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3555/events", "html_url": "https://github.com/huggingface/datasets/issues/3555", "id": 1097736982, "node_id": "I_kwDODunzps5BbiMW", "number": 3555, "title": "DuplicatedKeysError when loading tweet_qa dataset", "user": {"login": "LeonieWeissweiler", "id": 30300891, "node_id": "MDQ6VXNlcjMwMzAwODkx", "avatar_url": "https://avatars.githubusercontent.com/u/30300891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LeonieWeissweiler", "html_url": "https://github.com/LeonieWeissweiler", "followers_url": "https://api.github.com/users/LeonieWeissweiler/followers", "following_url": "https://api.github.com/users/LeonieWeissweiler/following{/other_user}", "gists_url": "https://api.github.com/users/LeonieWeissweiler/gists{/gist_id}", "starred_url": "https://api.github.com/users/LeonieWeissweiler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LeonieWeissweiler/subscriptions", "organizations_url": "https://api.github.com/users/LeonieWeissweiler/orgs", "repos_url": "https://api.github.com/users/LeonieWeissweiler/repos", "events_url": "https://api.github.com/users/LeonieWeissweiler/events{/privacy}", "received_events_url": "https://api.github.com/users/LeonieWeissweiler/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-01-10T10:53:11Z", "updated_at": "2022-01-12T15:17:33Z", "closed_at": "2022-01-12T15:13:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "When loading the tweet_qa dataset with `load_dataset('tweet_qa')`, the following error occurs: \r\n\r\n`DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 2a167f9e016ba338e1813fed275a6a1e\r\nKeys should be unique and deterministic in nature\r\n`\r\nMight be related to issues #2433 and #2333\r\n\r\n- `datasets` version: 1.17.0\r\n- Python version: 3.8.5\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3555/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3554", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3554/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3554/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3554/events", "html_url": "https://github.com/huggingface/datasets/issues/3554", "id": 1097711367, "node_id": "I_kwDODunzps5Bbb8H", "number": 3554, "title": "ImportError: cannot import name 'is_valid_waiter_error'", "user": {"login": "danielbellhv", "id": 84714841, "node_id": "MDQ6VXNlcjg0NzE0ODQx", "avatar_url": "https://avatars.githubusercontent.com/u/84714841?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielbellhv", "html_url": "https://github.com/danielbellhv", "followers_url": "https://api.github.com/users/danielbellhv/followers", "following_url": "https://api.github.com/users/danielbellhv/following{/other_user}", "gists_url": "https://api.github.com/users/danielbellhv/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielbellhv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielbellhv/subscriptions", "organizations_url": "https://api.github.com/users/danielbellhv/orgs", "repos_url": "https://api.github.com/users/danielbellhv/repos", "events_url": "https://api.github.com/users/danielbellhv/events{/privacy}", "received_events_url": "https://api.github.com/users/danielbellhv/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-01-10T10:32:04Z", "updated_at": "2022-02-14T09:35:57Z", "closed_at": "2022-02-14T09:35:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Based on [SO post](https://stackoverflow.com/q/70606147/17840900).\r\n\r\nI'm following along to this [Notebook][1], cell \"**Loading the dataset**\".\r\n\r\nKernel: `conda_pytorch_p36`.\r\n\r\nI run:\r\n```\r\n! pip install datasets transformers optimum[intel]\r\n```\r\n\r\nOutput:\r\n```\r\nRequirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.17.0)\r\nRequirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.15.0)\r\nRequirement already satisfied: optimum[intel] in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.3)\r\nRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (1.19.5)\r\nRequirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (0.3.4)\r\nRequirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (4.62.3)\r\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (0.2.1)\r\nRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (21.3)\r\nRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (6.0.1)\r\nRequirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (1.1.5)\r\nRequirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (2.0.2)\r\nRequirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (3.8.1)\r\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (2021.11.1)\r\nRequirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (0.8)\r\nRequirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (0.70.12.2)\r\nRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (4.5.0)\r\nRequirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (2.25.1)\r\nRequirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (5.4.1)\r\nRequirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2021.4.4)\r\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.10.3)\r\nRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\r\nRequirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.46)\r\nRequirement already satisfied: torch>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum[intel]) (1.10.1)\r\nRequirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum[intel]) (1.8)\r\nRequirement already satisfied: coloredlogs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum[intel]) (15.0.1)\r\nRequirement already satisfied: pycocotools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum[intel]) (2.0.3)\r\nRequirement already satisfied: neural-compressor>=1.7 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum[intel]) (1.9)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.0)\r\nRequirement already satisfied: sigopt in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (8.2.0)\r\nRequirement already satisfied: opencv-python in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (4.5.1.48)\r\nRequirement already satisfied: cryptography in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (3.4.7)\r\nRequirement already satisfied: py-cpuinfo in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (8.0.0)\r\nRequirement already satisfied: gevent in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (21.1.2)\r\nRequirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (0.7.5)\r\nRequirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (5.8.0)\r\nRequirement already satisfied: gevent-websocket in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (0.10.1)\r\nRequirement already satisfied: hyperopt in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (0.2.7)\r\nRequirement already satisfied: Flask in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (2.0.1)\r\nRequirement already satisfied: prettytable in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (2.5.0)\r\nRequirement already satisfied: Flask-SocketIO in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (5.1.1)\r\nRequirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (0.24.2)\r\nRequirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (8.4.0)\r\nRequirement already satisfied: Flask-Cors in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from neural-compressor>=1.7->optimum[intel]) (3.0.10)\r\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->datasets) (2.4.7)\r\nRequirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (4.0.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.5)\r\nRequirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.10)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->datasets) (1.6.3)\r\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->datasets) (2.0.9)\r\nRequirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->datasets) (21.2.0)\r\nRequirement already satisfied: asynctest==0.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->datasets) (0.13.0)\r\nRequirement already satisfied: idna-ssl>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->datasets) (1.1.0)\r\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->datasets) (4.0.1)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->datasets) (1.2.0)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->datasets) (1.2.0)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp->datasets) (5.1.0)\r\nRequirement already satisfied: humanfriendly>=9.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from coloredlogs->optimum[intel]) (10.0)\r\nRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.4.1)\r\nRequirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets) (2.8.1)\r\nRequirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets) (2021.1)\r\nRequirement already satisfied: matplotlib>=2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pycocotools->optimum[intel]) (3.3.4)\r\nRequirement already satisfied: cython>=0.27.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pycocotools->optimum[intel]) (0.29.23)\r\nRequirement already satisfied: setuptools>=18.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pycocotools->optimum[intel]) (52.0.0.post20210125)\r\nRequirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\r\nRequirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (8.0.1)\r\nRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\r\nRequirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sympy->optimum[intel]) (1.2.1)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools->optimum[intel]) (1.3.1)\r\nRequirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib>=2.1.0->pycocotools->optimum[intel]) (0.10.0)\r\nRequirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cryptography->neural-compressor>=1.7->optimum[intel]) (1.14.5)\r\nRequirement already satisfied: Werkzeug>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Flask->neural-compressor>=1.7->optimum[intel]) (2.0.2)\r\nRequirement already satisfied: Jinja2>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Flask->neural-compressor>=1.7->optimum[intel]) (3.0.1)\r\nRequirement already satisfied: itsdangerous>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Flask->neural-compressor>=1.7->optimum[intel]) (2.0.1)\r\nRequirement already satisfied: python-socketio>=5.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Flask-SocketIO->neural-compressor>=1.7->optimum[intel]) (5.5.0)\r\nRequirement already satisfied: zope.event in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gevent->neural-compressor>=1.7->optimum[intel]) (4.5.0)\r\nRequirement already satisfied: greenlet<2.0,>=0.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gevent->neural-compressor>=1.7->optimum[intel]) (1.1.0)\r\nRequirement already satisfied: zope.interface in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gevent->neural-compressor>=1.7->optimum[intel]) (5.4.0)\r\nRequirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt->neural-compressor>=1.7->optimum[intel]) (0.18.2)\r\nRequirement already satisfied: cloudpickle in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt->neural-compressor>=1.7->optimum[intel]) (1.6.0)\r\nRequirement already satisfied: networkx>=2.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt->neural-compressor>=1.7->optimum[intel]) (2.5)\r\nRequirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt->neural-compressor>=1.7->optimum[intel]) (1.5.3)\r\nRequirement already satisfied: py4j in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt->neural-compressor>=1.7->optimum[intel]) (0.10.7)\r\nRequirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from prettytable->neural-compressor>=1.7->optimum[intel]) (0.2.5)\r\nRequirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from schema->neural-compressor>=1.7->optimum[intel]) (0.6.0.post1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->neural-compressor>=1.7->optimum[intel]) (2.1.0)\r\nRequirement already satisfied: pyOpenSSL>=20.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sigopt->neural-compressor>=1.7->optimum[intel]) (20.0.1)\r\nRequirement already satisfied: pypng>=0.0.20 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sigopt->neural-compressor>=1.7->optimum[intel]) (0.0.21)\r\nRequirement already satisfied: kubernetes<13.0.0,>=12.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sigopt->neural-compressor>=1.7->optimum[intel]) (12.0.1)\r\nRequirement already satisfied: rsa<5.0.0,>=4.7 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sigopt->neural-compressor>=1.7->optimum[intel]) (4.7.2)\r\nRequirement already satisfied: boto3<2.0.0,==1.16.34 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sigopt->neural-compressor>=1.7->optimum[intel]) (1.16.34)\r\nRequirement already satisfied: Pint<0.17.0,>=0.16.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sigopt->neural-compressor>=1.7->optimum[intel]) (0.16.1)\r\nRequirement already satisfied: GitPython>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sigopt->neural-compressor>=1.7->optimum[intel]) (3.1.18)\r\nRequirement already satisfied: backoff<2.0.0,>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sigopt->neural-compressor>=1.7->optimum[intel]) (1.11.1)\r\nRequirement already satisfied: ipython>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sigopt->neural-compressor>=1.7->optimum[intel]) (7.16.1)\r\nRequirement already satisfied: docker<5.0.0,>=4.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sigopt->neural-compressor>=1.7->optimum[intel]) (4.4.4)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3<2.0.0,==1.16.34->sigopt->neural-compressor>=1.7->optimum[intel]) (0.10.0)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3<2.0.0,==1.16.34->sigopt->neural-compressor>=1.7->optimum[intel]) (0.3.7)\r\nRequirement already satisfied: botocore<1.20.0,>=1.19.34 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3<2.0.0,==1.16.34->sigopt->neural-compressor>=1.7->optimum[intel]) (1.19.63)\r\nRequirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cffi>=1.12->cryptography->neural-compressor>=1.7->optimum[intel]) (2.20)\r\nRequirement already satisfied: websocket-client>=0.32.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from docker<5.0.0,>=4.4.0->sigopt->neural-compressor>=1.7->optimum[intel]) (0.58.0)\r\nRequirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from GitPython>=2.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (4.0.9)\r\nRequirement already satisfied: traitlets>=4.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (4.3.3)\r\nRequirement already satisfied: jedi>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (0.17.2)\r\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (3.0.19)\r\nRequirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (0.2.0)\r\nRequirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (2.9.0)\r\nRequirement already satisfied: pexpect in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (4.8.0)\r\nRequirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (5.0.9)\r\nRequirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (0.7.5)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Jinja2>=3.0->Flask->neural-compressor>=1.7->optimum[intel]) (2.0.1)\r\nRequirement already satisfied: google-auth>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kubernetes<13.0.0,>=12.0.1->sigopt->neural-compressor>=1.7->optimum[intel]) (1.30.2)\r\nRequirement already satisfied: requests-oauthlib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kubernetes<13.0.0,>=12.0.1->sigopt->neural-compressor>=1.7->optimum[intel]) (1.3.0)\r\nRequirement already satisfied: importlib-resources in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Pint<0.17.0,>=0.16.0->sigopt->neural-compressor>=1.7->optimum[intel]) (5.4.0)\r\nRequirement already satisfied: python-engineio>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-socketio>=5.0.2->Flask-SocketIO->neural-compressor>=1.7->optimum[intel]) (4.3.0)\r\nRequirement already satisfied: bidict>=0.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-socketio>=5.0.2->Flask-SocketIO->neural-compressor>=1.7->optimum[intel]) (0.21.4)\r\nRequirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from rsa<5.0.0,>=4.7->sigopt->neural-compressor>=1.7->optimum[intel]) (0.4.8)\r\nRequirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (5.0.0)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes<13.0.0,>=12.0.1->sigopt->neural-compressor>=1.7->optimum[intel]) (0.2.8)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes<13.0.0,>=12.0.1->sigopt->neural-compressor>=1.7->optimum[intel]) (4.2.2)\r\nRequirement already satisfied: parso<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from jedi>=0.10->ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (0.7.1)\r\nRequirement already satisfied: ipython-genutils in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from traitlets>=4.2->ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (0.2.0)\r\nRequirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pexpect->ipython>=5.0.0->sigopt->neural-compressor>=1.7->optimum[intel]) (0.7.0)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib->kubernetes<13.0.0,>=12.0.1->sigopt->neural-compressor>=1.7->optimum[intel]) (3.1.1)\r\n```\r\n\r\n---\r\n\r\n**Cell:**\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\n```\r\nOR\r\n```python\r\nimport datasets\r\n```\r\n\r\n**Traceback:**\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-7-34fb7ba3338d> in <module>\r\n----> 1 from datasets import load_dataset, load_metric\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/__init__.py in <module>\r\n     32     )\r\n     33 \r\n---> 34 from .arrow_dataset import Dataset, concatenate_datasets\r\n     35 from .arrow_reader import ArrowReader, ReadInstruction\r\n     36 from .arrow_writer import ArrowWriter\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/arrow_dataset.py in <module>\r\n     59 from . import config, utils\r\n     60 from .arrow_reader import ArrowReader\r\n---> 61 from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n     62 from .features import ClassLabel, Features, FeatureType, Sequence, Value, _ArrayXD, pandas_types_mapper\r\n     63 from .filesystems import extract_path_from_uri, is_remote_filesystem\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/arrow_writer.py in <module>\r\n     26 \r\n     27 from . import config, utils\r\n---> 28 from .features import (\r\n     29     Features,\r\n     30     ImageExtensionType,\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/features/__init__.py in <module>\r\n      1 # flake8: noqa\r\n----> 2 from .audio import Audio\r\n      3 from .features import *\r\n      4 from .features import (\r\n      5     _ArrayXD,\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/features/audio.py in <module>\r\n      5 import pyarrow as pa\r\n      6 \r\n----> 7 from ..utils.streaming_download_manager import xopen\r\n      8 \r\n      9 \r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/utils/streaming_download_manager.py in <module>\r\n     16 \r\n     17 from .. import config\r\n---> 18 from ..filesystems import COMPRESSION_FILESYSTEMS\r\n     19 from .download_manager import DownloadConfig, map_nested\r\n     20 from .file_utils import (\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/filesystems/__init__.py in <module>\r\n     11 \r\n     12 if _has_s3fs:\r\n---> 13     from .s3filesystem import S3FileSystem  # noqa: F401\r\n     14 \r\n     15 COMPRESSION_FILESYSTEMS: List[compression.BaseCompressedFileFileSystem] = [\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/filesystems/s3filesystem.py in <module>\r\n----> 1 import s3fs\r\n      2 \r\n      3 \r\n      4 class S3FileSystem(s3fs.S3FileSystem):\r\n      5     \"\"\"\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/s3fs/__init__.py in <module>\r\n----> 1 from .core import S3FileSystem, S3File\r\n      2 from .mapping import S3Map\r\n      3 \r\n      4 from ._version import get_versions\r\n      5 \r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/s3fs/core.py in <module>\r\n     12 from fsspec.asyn import AsyncFileSystem, sync, sync_wrapper\r\n     13 \r\n---> 14 import aiobotocore\r\n     15 import botocore\r\n     16 import aiobotocore.session\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/aiobotocore/__init__.py in <module>\r\n----> 1 from .session import get_session, AioSession\r\n      2 \r\n      3 __all__ = ['get_session', 'AioSession']\r\n      4 __version__ = '1.3.0'\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/aiobotocore/session.py in <module>\r\n      4 from botocore import retryhandler, translate\r\n      5 from botocore.exceptions import PartialCredentialsError\r\n----> 6 from .client import AioClientCreator, AioBaseClient\r\n      7 from .hooks import AioHierarchicalEmitter\r\n      8 from .parsers import AioResponseParserFactory\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/aiobotocore/client.py in <module>\r\n     11 from .args import AioClientArgsCreator\r\n     12 from .utils import AioS3RegionRedirector\r\n---> 13 from . import waiter\r\n     14 \r\n     15 history_recorder = get_global_history_recorder()\r\n\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/aiobotocore/waiter.py in <module>\r\n      4 from botocore.exceptions import ClientError\r\n      5 from botocore.waiter import WaiterModel  # noqa: F401, lgtm[py/unused-import]\r\n----> 6 from botocore.waiter import Waiter, xform_name, logger, WaiterError, \\\r\n      7     NormalizedOperationMethod as _NormalizedOperationMethod, is_valid_waiter_error\r\n      8 from botocore.docs.docstring import WaiterDocstring\r\n\r\nImportError: cannot import name 'is_valid_waiter_error'\r\n```\r\n\r\nPlease let me know if there's anything else I can add to post.\r\n\r\n  [1]: https://github.com/huggingface/notebooks/blob/master/examples/text_classification_quantization_inc.ipynb", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3554/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3553", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3553/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3553/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3553/events", "html_url": "https://github.com/huggingface/datasets/issues/3553", "id": 1097252275, "node_id": "I_kwDODunzps5BZr2z", "number": 3553, "title": "set_format(\"np\") no longer works for Image data", "user": {"login": "cgarciae", "id": 5862228, "node_id": "MDQ6VXNlcjU4NjIyMjg=", "avatar_url": "https://avatars.githubusercontent.com/u/5862228?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cgarciae", "html_url": "https://github.com/cgarciae", "followers_url": "https://api.github.com/users/cgarciae/followers", "following_url": "https://api.github.com/users/cgarciae/following{/other_user}", "gists_url": "https://api.github.com/users/cgarciae/gists{/gist_id}", "starred_url": "https://api.github.com/users/cgarciae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cgarciae/subscriptions", "organizations_url": "https://api.github.com/users/cgarciae/orgs", "repos_url": "https://api.github.com/users/cgarciae/repos", "events_url": "https://api.github.com/users/cgarciae/events{/privacy}", "received_events_url": "https://api.github.com/users/cgarciae/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-01-09T17:18:13Z", "updated_at": "2022-10-14T12:03:55Z", "closed_at": "2022-10-14T12:03:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n`dataset.set_format(\"np\")` no longer works for image data, previously you could load the MNIST like this:\r\n\r\n```python\r\ndataset = load_dataset(\"mnist\")\r\ndataset.set_format(\"np\")\r\nX_train = dataset[\"train\"][\"image\"][..., None] # <== No longer a numpy array\r\n```\r\nbut now it doesn't work, `set_format(\"np\")` seems to have no effect and the dataset just returns a list/array of PIL images instead of numpy arrays as requested.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3553/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3553/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3550", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3550/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3550/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3550/events", "html_url": "https://github.com/huggingface/datasets/issues/3550", "id": 1096522377, "node_id": "I_kwDODunzps5BW5qJ", "number": 3550, "title": "Bug in `openbookqa` dataset", "user": {"login": "lucadiliello", "id": 23355969, "node_id": "MDQ6VXNlcjIzMzU1OTY5", "avatar_url": "https://avatars.githubusercontent.com/u/23355969?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucadiliello", "html_url": "https://github.com/lucadiliello", "followers_url": "https://api.github.com/users/lucadiliello/followers", "following_url": "https://api.github.com/users/lucadiliello/following{/other_user}", "gists_url": "https://api.github.com/users/lucadiliello/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucadiliello/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucadiliello/subscriptions", "organizations_url": "https://api.github.com/users/lucadiliello/orgs", "repos_url": "https://api.github.com/users/lucadiliello/repos", "events_url": "https://api.github.com/users/lucadiliello/events{/privacy}", "received_events_url": "https://api.github.com/users/lucadiliello/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2067388877, "node_id": "MDU6TGFiZWwyMDY3Mzg4ODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset%20bug", "name": "dataset bug", "color": "2edb81", "default": false, "description": "A bug in a dataset script provided in the library"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-01-07T17:32:57Z", "updated_at": "2022-05-04T06:33:00Z", "closed_at": "2022-05-04T06:32:19Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nDataset entries contains a typo.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> obqa = load_dataset('openbookqa', 'main')\r\n>>> obqa['train'][0]\r\n```\r\n\r\n## Expected results\r\n```python\r\n{'id': '7-980', 'question_stem': 'The sun is responsible for', 'choices': {'text': ['puppies learning new tricks', 'children growing up and getting old', 'flowers wilting in a vase', 'plants sprouting, blooming and wilting'], 'label': ['A', 'B', 'C', 'D']}, 'answerKey': 'D'}\r\n```\r\n\r\n## Actual results\r\n```python\r\n{'id': '7-980', 'question_stem': 'The sun is responsible for', 'choices': {'text': ['puppies learning new tricks', 'children growing up and getting old', 'flowers wilting in a vase', 'plants sprouting, blooming and wilting'], 'label': ['puppies learning new tricks', 'children growing up and getting old', 'flowers wilting in a vase', 'plants sprouting, blooming and wilting']}, 'answerKey': 'D'}\r\n```\r\n\r\nThe bug is present in all configs and all splits.\r\n\r\n## Environment info\r\n- `datasets` version: 1.17.0\r\n- Platform: Linux-5.4.0-1057-aws-x86_64-with-glibc2.27\r\n- Python version: 3.9.7\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3550/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3550/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3531", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3531/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3531/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3531/events", "html_url": "https://github.com/huggingface/datasets/issues/3531", "id": 1094033280, "node_id": "I_kwDODunzps5BNZ-A", "number": 3531, "title": "Give clearer instructions to add the YAML tags", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-01-05T06:44:20Z", "updated_at": "2022-01-17T15:54:36Z", "closed_at": "2022-01-17T15:54:36Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAs reported by @julien-c, many community datasets contain the line `YAML tags:` at the top of the YAML section in the header of the README file.  See e.g.: https://huggingface.co/datasets/bigscience/P3/commit/a03bea08cf4d58f268b469593069af6aeb15de32\r\n\r\nMaybe we should give clearer instruction/hints in the README template.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3531/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3531/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3522", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3522/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3522/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3522/events", "html_url": "https://github.com/huggingface/datasets/issues/3522", "id": 1093807586, "node_id": "I_kwDODunzps5BMi3i", "number": 3522, "title": "wmt19 is broken (zh-en)", "user": {"login": "AjayP13", "id": 5404177, "node_id": "MDQ6VXNlcjU0MDQxNzc=", "avatar_url": "https://avatars.githubusercontent.com/u/5404177?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AjayP13", "html_url": "https://github.com/AjayP13", "followers_url": "https://api.github.com/users/AjayP13/followers", "following_url": "https://api.github.com/users/AjayP13/following{/other_user}", "gists_url": "https://api.github.com/users/AjayP13/gists{/gist_id}", "starred_url": "https://api.github.com/users/AjayP13/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AjayP13/subscriptions", "organizations_url": "https://api.github.com/users/AjayP13/orgs", "repos_url": "https://api.github.com/users/AjayP13/repos", "events_url": "https://api.github.com/users/AjayP13/events{/privacy}", "received_events_url": "https://api.github.com/users/AjayP13/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2067388877, "node_id": "MDU6TGFiZWwyMDY3Mzg4ODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset%20bug", "name": "dataset bug", "color": "2edb81", "default": false, "description": "A bug in a dataset script provided in the library"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-01-04T22:33:45Z", "updated_at": "2022-05-06T16:27:37Z", "closed_at": "2022-05-06T16:27:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"wmt19\", 'zh-en')\r\n```\r\n\r\n## Expected results\r\nThe dataset should download.\r\n\r\n## Actual results\r\n`ConnectionError: Couldn't reach ftp://cwmt-wmt:cwmt-wmt@datasets.nju.edu.cn/parallel/casia2015.zip`\r\n\r\n## Environment info\r\n- `datasets` version: 1.15.1\r\n- Platform: Linux\r\n- Python version: 3.8 \r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3522/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3515", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3515/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3515/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3515/events", "html_url": "https://github.com/huggingface/datasets/issues/3515", "id": 1092624695, "node_id": "I_kwDODunzps5BICE3", "number": 3515, "title": "`ExpectedMoreDownloadedFiles` for `evidence_infer_treatment`", "user": {"login": "VictorSanh", "id": 16107619, "node_id": "MDQ6VXNlcjE2MTA3NjE5", "avatar_url": "https://avatars.githubusercontent.com/u/16107619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VictorSanh", "html_url": "https://github.com/VictorSanh", "followers_url": "https://api.github.com/users/VictorSanh/followers", "following_url": "https://api.github.com/users/VictorSanh/following{/other_user}", "gists_url": "https://api.github.com/users/VictorSanh/gists{/gist_id}", "starred_url": "https://api.github.com/users/VictorSanh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VictorSanh/subscriptions", "organizations_url": "https://api.github.com/users/VictorSanh/orgs", "repos_url": "https://api.github.com/users/VictorSanh/repos", "events_url": "https://api.github.com/users/VictorSanh/events{/privacy}", "received_events_url": "https://api.github.com/users/VictorSanh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2067388877, "node_id": "MDU6TGFiZWwyMDY3Mzg4ODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset%20bug", "name": "dataset bug", "color": "2edb81", "default": false, "description": "A bug in a dataset script provided in the library"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-01-03T15:58:38Z", "updated_at": "2022-02-14T13:21:43Z", "closed_at": "2022-02-14T13:21:43Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to load a dataset called `evidence_infer_treatment`. The first subset (`1.1`) works fine but the second returns an error (`2.0`). It downloads a file but crashes during the checksums.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> load_dataset(\"evidence_infer_treatment\", \"2.0\")\r\nDownloading and preparing dataset evidence_infer_treatment/2.0 (download: 34.84 MiB, generated: 91.46 MiB, post-processed: Unknown size, total: 126.30 MiB) to /home/victor_huggingface_co/.cache/huggingface/datasets/evidence_infer_treatment/2.0/2.0.0/6812655bfd26cbaa58c84eab098bf6403694b06c6ae2ded603c55681868a1e24...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/load.py\", line 1669, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/builder.py\", line 594, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/builder.py\", line 664, in _download_and_prepare\r\n    self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/utils/info_utils.py\", line 33, in verify_checksums\r\n    raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))\r\ndatasets.utils.info_utils.ExpectedMoreDownloadedFiles: {'http://evidence-inference.ebm-nlp.com/v2.0.tar.gz'}\r\n```\r\n\r\nI did try to pass the argument `ignore_verifications=True` but run into an error when trying to build the dataset:\r\n```python\r\n>>> load_dataset(\"evidence_infer_treatment\", \"2.0\", ignore_verifications=True, download_mode=\"force_redownload\")\r\nDownloading and preparing dataset evidence_infer_treatment/2.0 (download: 34.84 MiB, generated: 91.46 MiB, post-processed: Unknown size, total: 126.30 MiB) to /home/victor_huggingface_co/.cache/huggingface/datasets/evidence_infer_treatment/2.0/2.0.0/6812655bfd26cbaa58c84eab098bf6403694b06c6ae2ded603c55681868a1e24...\r\nDownloading: 164MB [00:23, 6.98MB/s] \r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/load.py\", line 1669, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/builder.py\", line 594, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/builder.py\", line 681, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/builder.py\", line 1080, in _prepare_split\r\n    example = self.info.features.encode_example(record)\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/features/features.py\", line 1032, in encode_example\r\n    return encode_nested_example(self, example)\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/features/features.py\", line 807, in encode_nested_example\r\n    k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/features/features.py\", line 807, in <dictcomp>\r\n    k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/features/features.py\", line 829, in encode_nested_example\r\n    list_dict[k] = [encode_nested_example(dict_tuples[0], o) for o in dict_tuples[1:]]\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/features/features.py\", line 829, in <listcomp>\r\n    list_dict[k] = [encode_nested_example(dict_tuples[0], o) for o in dict_tuples[1:]]\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/features/features.py\", line 828, in encode_nested_example\r\n    for k, dict_tuples in utils.zip_dict(schema.feature, *obj):\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 136, in zip_dict\r\n    yield key, tuple(d[key] for d in dicts)\r\n  File \"/home/victor_huggingface_co/miniconda3/envs/promptsource/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 136, in <genexpr>\r\n    yield key, tuple(d[key] for d in dicts)\r\nKeyError: ''\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.16.1\r\n- Platform: Linux-5.0.0-1020-gcp-x86_64-with-debian-buster-sid\r\n- Python version: 3.7.11\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3515/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3515/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3505", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3505/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3505/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3505/events", "html_url": "https://github.com/huggingface/datasets/issues/3505", "id": 1091150820, "node_id": "I_kwDODunzps5BCaPk", "number": 3505, "title": "cast_column function not working with map function in streaming mode for Audio features", "user": {"login": "ashu5644", "id": 8268102, "node_id": "MDQ6VXNlcjgyNjgxMDI=", "avatar_url": "https://avatars.githubusercontent.com/u/8268102?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ashu5644", "html_url": "https://github.com/ashu5644", "followers_url": "https://api.github.com/users/ashu5644/followers", "following_url": "https://api.github.com/users/ashu5644/following{/other_user}", "gists_url": "https://api.github.com/users/ashu5644/gists{/gist_id}", "starred_url": "https://api.github.com/users/ashu5644/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ashu5644/subscriptions", "organizations_url": "https://api.github.com/users/ashu5644/orgs", "repos_url": "https://api.github.com/users/ashu5644/repos", "events_url": "https://api.github.com/users/ashu5644/events{/privacy}", "received_events_url": "https://api.github.com/users/ashu5644/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-12-30T14:52:01Z", "updated_at": "2022-01-18T19:54:07Z", "closed_at": "2022-01-18T19:54:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to use Audio class for loading audio features using custom dataset. I am able to cast 'audio' feature into 'Audio' format with cast_column function. On using map function, I am not getting 'Audio' casted feature but getting path of audio file only.\r\nI am getting features of 'audio' of string type with load_dataset call. After using cast_column 'audio' feature is converted into 'Audio' type. But in map function I am not able to get Audio type for audio feature & getting string type data containing path of file only. So I am not able to use processor in encode function.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import load_dataset, Audio\r\nfrom transformers import Wav2Vec2Processor\r\n\r\ndef encode(batch, processor):\r\n    print(\"Audio: \",batch['audio'])\r\n    batch[\"input_values\"] = processor(batch[\"audio\"]['array'], sampling_rate=16000).input_values\r\n    return batch\r\n\r\ndef print_ds(ds):\r\n    iterator = iter(ds)\r\n    for d in iterator:\r\n        print(\"Data: \",d)\r\n        break\r\n\r\nprocessor = Wav2Vec2Processor.from_pretrained(pretrained_model_path)\r\n\r\ndataset = load_dataset(\"custom_dataset.py\",\"train\",data_files={'train':'train_path.txt'},\r\n                                data_dir=\"data\", streaming=True, split=\"train\")\r\n\r\nprint(\"Features: \",dataset.features)\r\n\r\nprint_ds(dataset)\r\n\r\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\r\n\r\nprint(\"Features: \",dataset.features)\r\n\r\nprint_ds(dataset)\r\n\r\ndataset = dataset.map(lambda x: encode(x,processor))\r\n\r\nprint(\"Features: \",dataset.features)\r\n\r\nprint_ds(dataset)\r\n\r\n\r\n\r\n```\r\n\r\n## Expected results\r\n\r\nmap function not printing Audio type features be used with processor function and getting error in processor call due to this.\r\n\r\n## Actual results\r\n\r\n# after load_dataset call\r\nFeatures:  {'sentence': Value(dtype='string', id=None), 'audio': Value(dtype='string', id=None)}\r\nData:  {'sentence': '\u0914\u0930 \u0905\u092a\u0928\u0947 \u092a\u0947\u091f \u0915\u094b \u092e\u093e\u0901 \u0915\u0940 \u0938\u094d\u0935\u093e\u0926\u093f\u0937\u094d\u091f \u0917\u0930\u092e\u0917\u0930\u092e \u091c\u0932\u0947\u092c\u093f\u092f\u093e\u0901 \u0939\u095c\u092a\u0924\u0947\\n', 'audio': 'data/0116_003.wav'}\r\n\r\n# after cast_column call\r\nFeatures:  {'sentence': Value(dtype='string', id=None), 'audio': Audio(sampling_rate=16000, mono=True, _storage_dtype='string', id=None)}\r\nData:  {'sentence': '\u0914\u0930 \u0905\u092a\u0928\u0947 \u092a\u0947\u091f \u0915\u094b \u092e\u093e\u0901 \u0915\u0940 \u0938\u094d\u0935\u093e\u0926\u093f\u0937\u094d\u091f \u0917\u0930\u092e\u0917\u0930\u092e \u091c\u0932\u0947\u092c\u093f\u092f\u093e\u0901 \u0939\u095c\u092a\u0924\u0947\\n', 'audio': {'path': 'data/0116_003.wav', 'array': array([ 1.2662281e-06,  1.0264218e-06, -1.3615092e-06, ...,\r\n        1.3017889e-02,  1.0085563e-02,  4.8155054e-03], dtype=float32), 'sampling_rate': 16000}}\r\n\r\n# after map call\r\nFeatures:  None\r\nAudio:  data/0116_003.wav\r\n\r\nTraceback (most recent call last):\r\n  File \"demo2.py\", line 36, in <module>\r\n    print_ds(dataset)\r\n  File \"demo2.py\", line 11, in print_ds\r\n    for d in iterator:\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/iterable_dataset.py\", line 341, in __iter__\r\n    for key, example in self._iter():\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/iterable_dataset.py\", line 338, in _iter\r\n    yield from ex_iterable\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/iterable_dataset.py\", line 192, in __iter__\r\n    yield key, self.function(example)\r\n  File \"demo2.py\", line 32, in <lambda>\r\n    dataset = dataset.map(lambda x: batch_encode(x,processor))\r\n  File \"demo2.py\", line 6, in batch_encode\r\n    batch[\"input_values\"] = processor(batch[\"audio\"]['array'], sampling_rate=16000).input_values\r\nTypeError: string indices must be integers\r\n## Environment info\r\n\r\n- `datasets` version: 1.17.0\r\n- Platform: Linux-4.14.243 with-debian-bullseye-sid\r\n- Python version: 3.7.9\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3505/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3505/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3504", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3504/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3504/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3504/events", "html_url": "https://github.com/huggingface/datasets/issues/3504", "id": 1090682230, "node_id": "I_kwDODunzps5BAn12", "number": 3504, "title": "Unable to download PUBMED_title_abstracts_2019_baseline.jsonl.zst", "user": {"login": "ToddMorrill", "id": 12600692, "node_id": "MDQ6VXNlcjEyNjAwNjky", "avatar_url": "https://avatars.githubusercontent.com/u/12600692?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ToddMorrill", "html_url": "https://github.com/ToddMorrill", "followers_url": "https://api.github.com/users/ToddMorrill/followers", "following_url": "https://api.github.com/users/ToddMorrill/following{/other_user}", "gists_url": "https://api.github.com/users/ToddMorrill/gists{/gist_id}", "starred_url": "https://api.github.com/users/ToddMorrill/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ToddMorrill/subscriptions", "organizations_url": "https://api.github.com/users/ToddMorrill/orgs", "repos_url": "https://api.github.com/users/ToddMorrill/repos", "events_url": "https://api.github.com/users/ToddMorrill/events{/privacy}", "received_events_url": "https://api.github.com/users/ToddMorrill/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2067388877, "node_id": "MDU6TGFiZWwyMDY3Mzg4ODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset%20bug", "name": "dataset bug", "color": "2edb81", "default": false, "description": "A bug in a dataset script provided in the library"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2021-12-29T18:23:20Z", "updated_at": "2022-02-18T07:49:00Z", "closed_at": "2022-02-17T15:04:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am unable to download the PubMed dataset from the link provided in the [Hugging Face Course (Chapter 5 Section 4)](https://huggingface.co/course/chapter5/4?fw=pt).\r\n\r\nhttps://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst \r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import load_dataset\r\n\r\n# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\r\ndata_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\r\npubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\r\npubmed_dataset\r\n```\r\n\r\nI also tried with `wget` as follows.\r\n```\r\nwget https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\r\n```\r\n\r\n## Expected results\r\nI expect to be able to download this file.\r\n\r\n## Actual results\r\nTraceback\r\n```\r\n---------------------------------------------------------------------------\r\ntimeout                                   Traceback (most recent call last)\r\n/usr/lib/python3/dist-packages/urllib3/connection.py in _new_conn(self)\r\n    158         try:\r\n--> 159             conn = connection.create_connection(\r\n    160                 (self._dns_host, self.port), self.timeout, **extra_kw\r\n\r\n/usr/lib/python3/dist-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)\r\n     83     if err is not None:\r\n---> 84         raise err\r\n     85 \r\n\r\n/usr/lib/python3/dist-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)\r\n     73                 sock.bind(source_address)\r\n---> 74             sock.connect(sa)\r\n     75             return sock\r\n\r\ntimeout: timed out\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConnectTimeoutError                       Traceback (most recent call last)\r\n/usr/lib/python3/dist-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    664             # Make the request on the httplib connection object.\r\n--> 665             httplib_response = self._make_request(\r\n    666                 conn,\r\n\r\n/usr/lib/python3/dist-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\r\n    375         try:\r\n--> 376             self._validate_conn(conn)\r\n    377         except (SocketTimeout, BaseSSLError) as e:\r\n\r\n/usr/lib/python3/dist-packages/urllib3/connectionpool.py in _validate_conn(self, conn)\r\n    995         if not getattr(conn, \"sock\", None):  # AppEngine might not have  `.sock`\r\n--> 996             conn.connect()\r\n    997 \r\n\r\n/usr/lib/python3/dist-packages/urllib3/connection.py in connect(self)\r\n    313         # Add certificate verification\r\n--> 314         conn = self._new_conn()\r\n    315         hostname = self.host\r\n\r\n/usr/lib/python3/dist-packages/urllib3/connection.py in _new_conn(self)\r\n    163         except SocketTimeout:\r\n--> 164             raise ConnectTimeoutError(\r\n    165                 self,\r\n\r\nConnectTimeoutError: (<urllib3.connection.VerifiedHTTPSConnection object at 0x7f06dd698850>, 'Connection to the-eye.eu timed out. (connect timeout=10.0)')\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nMaxRetryError                             Traceback (most recent call last)\r\n/usr/lib/python3/dist-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\r\n    438             if not chunked:\r\n--> 439                 resp = conn.urlopen(\r\n    440                     method=request.method,\r\n\r\n/usr/lib/python3/dist-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    718 \r\n--> 719             retries = retries.increment(\r\n    720                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n\r\n/usr/lib/python3/dist-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\r\n    435         if new_retry.is_exhausted():\r\n--> 436             raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\n    437 \r\n\r\nMaxRetryError: HTTPSConnectionPool(host='the-eye.eu', port=443): Max retries exceeded with url: /public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f06dd698850>, 'Connection to the-eye.eu timed out. (connect timeout=10.0)'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConnectTimeout                            Traceback (most recent call last)\r\n/tmp/ipykernel_15104/606583593.py in <module>\r\n      3 # This takes a few minutes to run, so go grab a tea or coffee while you wait :)\r\n      4 data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\r\n----> 5 pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\r\n      6 pubmed_dataset\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, script_version, **config_kwargs)\r\n   1655 \r\n   1656     # Create a dataset builder\r\n-> 1657     builder_instance = load_dataset_builder(\r\n   1658         path=path,\r\n   1659         name=name,\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/load.py in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, script_version, **config_kwargs)\r\n   1492         download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1493         download_config.use_auth_token = use_auth_token\r\n-> 1494     dataset_module = dataset_module_factory(\r\n   1495         path, revision=revision, download_config=download_config, download_mode=download_mode, data_files=data_files\r\n   1496     )\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/load.py in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_files, **download_kwargs)\r\n   1116     # Try packaged\r\n   1117     if path in _PACKAGED_DATASETS_MODULES:\r\n-> 1118         return PackagedDatasetModuleFactory(\r\n   1119             path, data_files=data_files, download_config=download_config, download_mode=download_mode\r\n   1120         ).get_module()\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/load.py in get_module(self)\r\n    773             else get_patterns_locally(str(Path().resolve()))\r\n    774         )\r\n--> 775         data_files = DataFilesDict.from_local_or_remote(patterns, use_auth_token=self.downnload_config.use_auth_token)\r\n    776         module_path, hash = _PACKAGED_DATASETS_MODULES[self.name]\r\n    777         builder_kwargs = {\"hash\": hash, \"data_files\": data_files}\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/data_files.py in from_local_or_remote(cls, patterns, base_path, allowed_extensions, use_auth_token)\r\n    576         for key, patterns_for_key in patterns.items():\r\n    577             out[key] = (\r\n--> 578                 DataFilesList.from_local_or_remote(\r\n    579                     patterns_for_key,\r\n    580                     base_path=base_path,\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/data_files.py in from_local_or_remote(cls, patterns, base_path, allowed_extensions, use_auth_token)\r\n    545         base_path = base_path if base_path is not None else str(Path().resolve())\r\n    546         data_files = resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)\r\n--> 547         origin_metadata = _get_origin_metadata_locally_or_by_urls(data_files, use_auth_token=use_auth_token)\r\n    548         return cls(data_files, origin_metadata)\r\n    549 \r\n\r\n~/.local/lib/python3.8/site-packages/datasets/data_files.py in _get_origin_metadata_locally_or_by_urls(data_files, max_workers, use_auth_token)\r\n    492     data_files: List[Union[Path, Url]], max_workers=64, use_auth_token: Optional[Union[bool, str]] = None\r\n    493 ) -> Tuple[str]:\r\n--> 494     return thread_map(\r\n    495         partial(_get_single_origin_metadata_locally_or_by_urls, use_auth_token=use_auth_token),\r\n    496         data_files,\r\n\r\n~/.local/lib/python3.8/site-packages/tqdm/contrib/concurrent.py in thread_map(fn, *iterables, **tqdm_kwargs)\r\n     92     \"\"\"\r\n     93     from concurrent.futures import ThreadPoolExecutor\r\n---> 94     return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\r\n     95 \r\n     96 \r\n\r\n~/.local/lib/python3.8/site-packages/tqdm/contrib/concurrent.py in _executor_map(PoolExecutor, fn, *iterables, **tqdm_kwargs)\r\n     74             map_args.update(chunksize=chunksize)\r\n     75         with PoolExecutor(**pool_kwargs) as ex:\r\n---> 76             return list(tqdm_class(ex.map(fn, *iterables, **map_args), **kwargs))\r\n     77 \r\n     78 \r\n\r\n~/.local/lib/python3.8/site-packages/tqdm/notebook.py in __iter__(self)\r\n    252     def __iter__(self):\r\n    253         try:\r\n--> 254             for obj in super(tqdm_notebook, self).__iter__():\r\n    255                 # return super(tqdm...) will not catch exception\r\n    256                 yield obj\r\n\r\n~/.local/lib/python3.8/site-packages/tqdm/std.py in __iter__(self)\r\n   1171         # (note: keep this check outside the loop for performance)\r\n   1172         if self.disable:\r\n-> 1173             for obj in iterable:\r\n   1174                 yield obj\r\n   1175             return\r\n\r\n/usr/lib/python3.8/concurrent/futures/_base.py in result_iterator()\r\n    617                     # Careful not to keep a reference to the popped future\r\n    618                     if timeout is None:\r\n--> 619                         yield fs.pop().result()\r\n    620                     else:\r\n    621                         yield fs.pop().result(end_time - time.monotonic())\r\n\r\n/usr/lib/python3.8/concurrent/futures/_base.py in result(self, timeout)\r\n    442                     raise CancelledError()\r\n    443                 elif self._state == FINISHED:\r\n--> 444                     return self.__get_result()\r\n    445                 else:\r\n    446                     raise TimeoutError()\r\n\r\n/usr/lib/python3.8/concurrent/futures/_base.py in __get_result(self)\r\n    387         if self._exception:\r\n    388             try:\r\n--> 389                 raise self._exception\r\n    390             finally:\r\n    391                 # Break a reference cycle with the exception in self._exception\r\n\r\n/usr/lib/python3.8/concurrent/futures/thread.py in run(self)\r\n     55 \r\n     56         try:\r\n---> 57             result = self.fn(*self.args, **self.kwargs)\r\n     58         except BaseException as exc:\r\n     59             self.future.set_exception(exc)\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/data_files.py in _get_single_origin_metadata_locally_or_by_urls(data_file, use_auth_token)\r\n    483     if isinstance(data_file, Url):\r\n    484         data_file = str(data_file)\r\n--> 485         return (request_etag(data_file, use_auth_token=use_auth_token),)\r\n    486     else:\r\n    487         data_file = str(data_file.resolve())\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/utils/file_utils.py in request_etag(url, use_auth_token)\r\n    489 def request_etag(url: str, use_auth_token: Optional[Union[str, bool]] = None) -> Optional[str]:\r\n    490     headers = get_authentication_headers_for_url(url, use_auth_token=use_auth_token)\r\n--> 491     response = http_head(url, headers=headers, max_retries=3)\r\n    492     response.raise_for_status()\r\n    493     etag = response.headers.get(\"ETag\") if response.ok else None\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/utils/file_utils.py in http_head(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\r\n    474     headers = copy.deepcopy(headers) or {}\r\n    475     headers[\"user-agent\"] = get_datasets_user_agent(user_agent=headers.get(\"user-agent\"))\r\n--> 476     response = _request_with_retry(\r\n    477         method=\"HEAD\",\r\n    478         url=url,\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/utils/file_utils.py in _request_with_retry(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\r\n    407         except (requests.exceptions.ConnectTimeout, requests.exceptions.ConnectionError) as err:\r\n    408             if tries > max_retries:\r\n--> 409                 raise err\r\n    410             else:\r\n    411                 logger.info(f\"{method} request to {url} timed out, retrying... [{tries/max_retries}]\")\r\n\r\n~/.local/lib/python3.8/site-packages/datasets/utils/file_utils.py in _request_with_retry(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\r\n    403         tries += 1\r\n    404         try:\r\n--> 405             response = requests.request(method=method.upper(), url=url, timeout=timeout, **params)\r\n    406             success = True\r\n    407         except (requests.exceptions.ConnectTimeout, requests.exceptions.ConnectionError) as err:\r\n\r\n/usr/lib/python3/dist-packages/requests/api.py in request(method, url, **kwargs)\r\n     58     # cases, and look like a memory leak in others.\r\n     59     with sessions.Session() as session:\r\n---> 60         return session.request(method=method, url=url, **kwargs)\r\n     61 \r\n     62 \r\n\r\n/usr/lib/python3/dist-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\r\n    531         }\r\n    532         send_kwargs.update(settings)\r\n--> 533         resp = self.send(prep, **send_kwargs)\r\n    534 \r\n    535         return resp\r\n\r\n/usr/lib/python3/dist-packages/requests/sessions.py in send(self, request, **kwargs)\r\n    644 \r\n    645         # Send the request\r\n--> 646         r = adapter.send(request, **kwargs)\r\n    647 \r\n    648         # Total elapsed time of the request (approximately)\r\n\r\n/usr/lib/python3/dist-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\r\n    502                 # TODO: Remove this in 3.0.0: see #2811\r\n    503                 if not isinstance(e.reason, NewConnectionError):\r\n--> 504                     raise ConnectTimeout(e, request=request)\r\n    505 \r\n    506             if isinstance(e.reason, ResponseError):\r\n\r\nConnectTimeout: HTTPSConnectionPool(host='the-eye.eu', port=443): Max retries exceeded with url: /public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f06dd698850>, 'Connection to the-eye.eu timed out. (connect timeout=10.0)'))\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.17.0\r\n- Platform: Linux-5.11.0-43-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 6.0.1", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3504/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3503", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3503/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3503/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3503/events", "html_url": "https://github.com/huggingface/datasets/issues/3503", "id": 1090472735, "node_id": "I_kwDODunzps5A_0sf", "number": 3503, "title": "Batched in filter throws error", "user": {"login": "gpucce", "id": 32967787, "node_id": "MDQ6VXNlcjMyOTY3Nzg3", "avatar_url": "https://avatars.githubusercontent.com/u/32967787?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gpucce", "html_url": "https://github.com/gpucce", "followers_url": "https://api.github.com/users/gpucce/followers", "following_url": "https://api.github.com/users/gpucce/following{/other_user}", "gists_url": "https://api.github.com/users/gpucce/gists{/gist_id}", "starred_url": "https://api.github.com/users/gpucce/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gpucce/subscriptions", "organizations_url": "https://api.github.com/users/gpucce/orgs", "repos_url": "https://api.github.com/users/gpucce/repos", "events_url": "https://api.github.com/users/gpucce/events{/privacy}", "received_events_url": "https://api.github.com/users/gpucce/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "thomasw21", "id": 24695242, "node_id": "MDQ6VXNlcjI0Njk1MjQy", "avatar_url": "https://avatars.githubusercontent.com/u/24695242?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomasw21", "html_url": "https://github.com/thomasw21", "followers_url": "https://api.github.com/users/thomasw21/followers", "following_url": "https://api.github.com/users/thomasw21/following{/other_user}", "gists_url": "https://api.github.com/users/thomasw21/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomasw21/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomasw21/subscriptions", "organizations_url": "https://api.github.com/users/thomasw21/orgs", "repos_url": "https://api.github.com/users/thomasw21/repos", "events_url": "https://api.github.com/users/thomasw21/events{/privacy}", "received_events_url": "https://api.github.com/users/thomasw21/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "thomasw21", "id": 24695242, "node_id": "MDQ6VXNlcjI0Njk1MjQy", "avatar_url": "https://avatars.githubusercontent.com/u/24695242?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomasw21", "html_url": "https://github.com/thomasw21", "followers_url": "https://api.github.com/users/thomasw21/followers", "following_url": "https://api.github.com/users/thomasw21/following{/other_user}", "gists_url": "https://api.github.com/users/thomasw21/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomasw21/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomasw21/subscriptions", "organizations_url": "https://api.github.com/users/thomasw21/orgs", "repos_url": "https://api.github.com/users/thomasw21/repos", "events_url": "https://api.github.com/users/thomasw21/events{/privacy}", "received_events_url": "https://api.github.com/users/thomasw21/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-12-29T12:01:04Z", "updated_at": "2022-01-04T10:24:27Z", "closed_at": "2022-01-04T10:24:27Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I hope this is really a bug, I could not find it among the open issues\r\n\r\n## Describe the bug\r\nusing `batched=False` in DataSet.filter throws error\r\n```python\r\nTypeError: filter() got an unexpected keyword argument 'batched'\r\n```\r\nbut in the docs it is lister as an argument.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ntask = \"mnli\"\r\nmax_length = 128\r\ntokenizer = AutoTokenizer.from_pretrained(\"./pretrained_models/pretrained_models_drozd/sl250.m.gsic.titech.ac.jp:8000/21.11.17_06.30.32_roberta-base_a0057/checkpoints/smpl_400M/hf/\")\r\n\r\ndataset = load_dataset(\"glue\", task)\r\n\r\ntask_to_keys = {\r\n    \"cola\": (\"sentence\", None),\r\n    \"mnli\": (\"premise\", \"hypothesis\"),\r\n    \"mnli-mm\": (\"premise\", \"hypothesis\"),\r\n    \"mrpc\": (\"sentence1\", \"sentence2\"),\r\n    \"qnli\": (\"question\", \"sentence\"),\r\n    \"qqp\": (\"question1\", \"question2\"),\r\n    \"rte\": (\"sentence1\", \"sentence2\"),\r\n    \"sst2\": (\"sentence\", None),\r\n    \"stsb\": (\"sentence1\", \"sentence2\"),\r\n    \"wnli\": (\"sentence1\", \"sentence2\"),\r\n}\r\n\r\n##### tokenization_parameters\r\nsentence1_key, sentence2_key = task_to_keys[task]\r\ndef preprocess_function(examples, max_length):\r\n    if sentence2_key is None:\r\n        return tokenizer(\r\n            examples[sentence1_key], truncation=True, max_length=max_length\r\n        )\r\n    return tokenizer(\r\n        examples[sentence1_key],\r\n        examples[sentence2_key],\r\n        truncation=False,\r\n        padding=\"max_length\",\r\n        max_length=max_length,\r\n    )\r\n\r\nencoded_dataset = dataset.map(\r\n    lambda x: preprocess_function(x, max_length=max_length), batched=False\r\n)\r\n\r\nencoded_dataset.filter(lambda x: len(x['input_ids']) <= max_length, batched=False)\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.16.1, 1.17.0\r\n- Platform: ubuntu\r\n- Python version: 3.8.12\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3503/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3503/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3497", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3497/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3497/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3497/events", "html_url": "https://github.com/huggingface/datasets/issues/3497", "id": 1090050148, "node_id": "I_kwDODunzps5A-Nhk", "number": 3497, "title": "Changing sampling rate in audio dataset and subsequently mapping with `num_proc > 1` leads to weird bug", "user": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-12-28T18:03:49Z", "updated_at": "2022-01-21T13:22:27Z", "closed_at": "2022-01-21T13:22:27Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Running: \r\n\r\n```python\r\nfrom datasets import load_dataset, DatasetDict\r\nimport datasets\r\nfrom transformers import AutoFeatureExtractor\r\n\r\nraw_datasets = DatasetDict()\r\n\r\nraw_datasets[\"train\"] = load_dataset(\"common_voice\", \"ab\", split=\"train\")\r\n\r\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\r\n\r\nraw_datasets = raw_datasets.cast_column(\r\n    \"audio\", datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\r\n)\r\n\r\nnum_workers = 16\r\n\r\ndef prepare_dataset(batch):\r\n    sample = batch[\"audio\"]\r\n\r\n    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\r\n    batch[\"input_values\"] = inputs.input_values[0]\r\n    batch[\"input_length\"] = len(batch[\"input_values\"])\r\n    return batch\r\n\r\nraw_datasets.map(\r\n    prepare_dataset,\r\n    remove_columns=next(iter(raw_datasets.values())).column_names,\r\n    num_proc=16,\r\n    desc=\"preprocess datasets\",\r\n)\r\n```\r\n\r\ngives\r\n\r\n```bash\r\n  File \"/home/patrick/experiments/run_bug.py\", line 25, in <module>\r\n    raw_datasets.map(\r\n  File \"/home/patrick/python_bin/datasets/dataset_dict.py\", line 492, in map\r\n    {\r\n  File \"/home/patrick/python_bin/datasets/dataset_dict.py\", line 493, in <dictcomp>\r\n    k: dataset.map(\r\n  File \"/home/patrick/python_bin/datasets/arrow_dataset.py\", line 2139, in map\r\n    shards = [\r\n  File \"/home/patrick/python_bin/datasets/arrow_dataset.py\", line 2140, in <listcomp>\r\n    self.shard(num_shards=num_proc, index=rank, contiguous=True, keep_in_memory=keep_in_memory)\r\n  File \"/home/patrick/python_bin/datasets/arrow_dataset.py\", line 3164, in shard\r\n    return self.select(\r\n  File \"/home/patrick/python_bin/datasets/arrow_dataset.py\", line 485, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/patrick/python_bin/datasets/fingerprint.py\", line 411, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/patrick/python_bin/datasets/arrow_dataset.py\", line 2756, in select\r\n    return self._new_dataset_with_indices(indices_buffer=buf_writer.getvalue(), fingerprint=new_fingerprint)\r\n  File \"/home/patrick/python_bin/datasets/arrow_dataset.py\", line 2667, in _new_dataset_with_indices\r\n    return Dataset(\r\n  File \"/home/patrick/python_bin/datasets/arrow_dataset.py\", line 659, in __init__\r\n    raise ValueError(\r\nValueError: External features info don't match the dataset:\r\nGot\r\n{'client_id': Value(dtype='string', id=None), 'path': Value(dtype='string', id=None), 'audio': Audio(sampling_rate=16000, mono=True, _storage_dtype='string', id=None), 'sentence': Value(dtype='string', id=None), 'up_votes': Value(dtype='int64', id=None), 'down_votes': Value(dtype='int64', id=None), 'age': Value(dtype='string', id=None), 'gender': Value(dtype='string', id=None), 'accent': Value(dtype='string', id=None), 'locale': Value(dtype='string', id=None), 'segment': Value(dtype='string', id=None)}\r\nwith type\r\nstruct<client_id: string, path: string, audio: string, sentence: string, up_votes: int64, down_votes: int64, age: string, gender: string, accent: string, locale: string, segment: string>\r\n\r\nbut expected something like\r\n{'client_id': Value(dtype='string', id=None), 'path': Value(dtype='string', id=None), 'audio': {'path': Value(dtype='string', id=None), 'bytes': Value(dtype='binary', id=None)}, 'sentence': Value(dtype='string', id=None), 'up_votes': Value(dtype='int64', id=None), 'down_votes': Value(dtype='int64', id=None), 'age': Value(dtype='string', id=None), 'gender': Value(dtype='string', id=None), 'accent': Value(dtype='string', id=None), 'locale': Value(dtype='string', id=None), 'segment': Value(dtype='string', id=None)}\r\nwith type\r\nstruct<client_id: string, path: string, audio: struct<path: string, bytes: binary>, sentence: string, up_votes: int64, down_votes: int64, age: string, gender: string, accent: string, locale: string, segment: string>\r\n```\r\n\r\nVersions:\r\n\r\n```python\r\n- `datasets` version: 1.16.2.dev0\r\n- Platform: Linux-5.15.8-76051508-generic-x86_64-with-glibc2.33\r\n- Python version: 3.9.7\r\n- PyArrow version: 6.0.1\r\n```\r\n\r\nand `transformers`:\r\n\r\n```\r\n- `transformers` version: 4.16.0.dev0\r\n- Platform: Linux-5.15.8-76051508-generic-x86_64-with-glibc2.33\r\n- Python version: 3.9.7\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3497/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3497/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3480", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3480/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3480/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3480/events", "html_url": "https://github.com/huggingface/datasets/issues/3480", "id": 1088267110, "node_id": "I_kwDODunzps5A3aNm", "number": 3480, "title": "the compression format requested when saving a dataset in json format is not respected", "user": {"login": "SaulLu", "id": 55560583, "node_id": "MDQ6VXNlcjU1NTYwNTgz", "avatar_url": "https://avatars.githubusercontent.com/u/55560583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SaulLu", "html_url": "https://github.com/SaulLu", "followers_url": "https://api.github.com/users/SaulLu/followers", "following_url": "https://api.github.com/users/SaulLu/following{/other_user}", "gists_url": "https://api.github.com/users/SaulLu/gists{/gist_id}", "starred_url": "https://api.github.com/users/SaulLu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SaulLu/subscriptions", "organizations_url": "https://api.github.com/users/SaulLu/orgs", "repos_url": "https://api.github.com/users/SaulLu/repos", "events_url": "https://api.github.com/users/SaulLu/events{/privacy}", "received_events_url": "https://api.github.com/users/SaulLu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-12-24T09:23:51Z", "updated_at": "2022-01-05T13:03:35Z", "closed_at": "2022-01-05T13:03:35Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nIn the documentation of the `to_json` method, it is stated in the parameters that \r\n> **to_json_kwargs \u2013 Parameters to pass to pandas\u2019s pandas.DataFrame.to_json.\r\n\r\nhowever when we pass for example `compression=\"gzip\"`, the saved file is not compressed.\r\n\r\nWould you also have expected compression to be applied? :relaxed: \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nmy_dict = {\"a\": [1, 2, 3], \"b\": [1, 2, 3]}\r\n```\r\n### Result with datasets\r\n```python\r\nfrom datasets import Dataset\r\n\r\ndataset = Dataset.from_dict(my_dict)\r\ndataset.to_json(\"dic_with_datasets.jsonl.gz\", compression=\"gzip\")\r\n!cat dic_with_datasets.jsonl.gz\r\n```\r\noutput\r\n```\r\n{\"a\":1,\"b\":1}\r\n{\"a\":2,\"b\":2}\r\n{\"a\":3,\"b\":3}\r\n```\r\nNote: I would expected to see binary data here\r\n\r\n### Result with pandas\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(my_dict)\r\ndf.to_json(\"dic_with_pandas.jsonl.gz\", lines=True, orient=\"records\", compression=\"gzip\")\r\n!cat dic_with_pandas.jsonl.gz\r\n```\r\noutput\r\n```\r\n4\ufffd\ufffda\u0002\ufffddic_with_pandas.jsonl\ufffd\ufffdVJT\ufffd2\ufffdQJ\u0002\ufffd\ufffd\\\ufffd \ufffd\u0011\ufffdg\u0004\ufffd\u0019\ufffdy\u01b5\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u0003\ufffd\ufffd\u000e\ufffd)\ufffd\ufffd\ufffd\r\n```\r\nNote: It looks like binary data\r\n\r\n## Expected results\r\n\r\nI would have expected that the saved result with datasets would also be a binary file\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.16.1\r\n- Platform: Linux-4.18.0-193.70.1.el8_2.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.8.11\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3480/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3479", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3479/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3479/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3479/events", "html_url": "https://github.com/huggingface/datasets/issues/3479", "id": 1088232880, "node_id": "I_kwDODunzps5A3R2w", "number": 3479, "title": "Dataset preview is not available (I think for all Hugging Face datasets)", "user": {"login": "Abirate", "id": 66887439, "node_id": "MDQ6VXNlcjY2ODg3NDM5", "avatar_url": "https://avatars.githubusercontent.com/u/66887439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Abirate", "html_url": "https://github.com/Abirate", "followers_url": "https://api.github.com/users/Abirate/followers", "following_url": "https://api.github.com/users/Abirate/following{/other_user}", "gists_url": "https://api.github.com/users/Abirate/gists{/gist_id}", "starred_url": "https://api.github.com/users/Abirate/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Abirate/subscriptions", "organizations_url": "https://api.github.com/users/Abirate/orgs", "repos_url": "https://api.github.com/users/Abirate/repos", "events_url": "https://api.github.com/users/Abirate/events{/privacy}", "received_events_url": "https://api.github.com/users/Abirate/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 3470211881, "node_id": "LA_kwDODunzps7O1zsp", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset-viewer", "name": "dataset-viewer", "color": "E5583E", "default": false, "description": "Related to the dataset viewer on huggingface.co"}], "state": "closed", "locked": false, "assignee": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2021-12-24T08:18:48Z", "updated_at": "2021-12-24T14:27:46Z", "closed_at": "2021-12-24T14:27:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Dataset viewer issue for '*french_book_reviews*'\r\n\r\n**Link:** https://huggingface.co/datasets/Abirate/french_book_reviews\r\n\r\n**short description of the issue**\r\nFor my dataset, the dataset preview is no longer functional (it used to work: The dataset had been added the day before and it was fine...)  \r\nAnd, after looking over the datasets, I discovered that this issue affects all Hugging Face datasets (as of yesterday, December 23, 2021, around 10 p.m. (CET)).  \r\n**Am I the one who added this dataset** :  Yes   \r\n\r\n**Note**: here a screenshot showing the issue\r\n![Dataset preview is not available for my dataset](https://user-images.githubusercontent.com/66887439/147333078-60734578-420d-4e91-8691-a90afeaa8948.jpg)  \r\n\r\n**And here for glue dataset :**  \r\n\r\n![Dataset preview is not available for other Hugging Face datasets(glue)](https://user-images.githubusercontent.com/66887439/147333492-26fa530c-befd-4992-8361-70c51397a25a.jpg)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3479/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3479/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3473", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3473/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3473/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3473/events", "html_url": "https://github.com/huggingface/datasets/issues/3473", "id": 1086937610, "node_id": "I_kwDODunzps5AyVoK", "number": 3473, "title": "Iterating over a vision dataset doesn't decode the images", "user": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 3608941089, "node_id": "LA_kwDODunzps7XHBIh", "url": "https://api.github.com/repos/huggingface/datasets/labels/vision", "name": "vision", "color": "bfdadc", "default": false, "description": "Vision datasets"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2021-12-22T15:26:32Z", "updated_at": "2021-12-27T14:13:21Z", "closed_at": "2021-12-23T15:21:57Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nIf I load `mnist` and I iterate over the dataset, the images are not decoded, and the dictionary with the bytes is returned.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nimport PIL\r\n\r\nmnist = load_dataset(\"mnist\", split=\"train\")\r\n\r\nfirst_image = mnist[0][\"image\"]\r\nassert isinstance(first_image, PIL.PngImagePlugin.PngImageFile)  # passes\r\nfirst_image = next(iter(mnist))[\"image\"]\r\nassert isinstance(first_image, PIL.PngImagePlugin.PngImageFile)  # fails\r\n```\r\n\r\n## Expected results\r\n\r\nThe image should be decoded, as a PIL Image\r\n\r\n## Actual results\r\n\r\nWe get a dictionary\r\n```\r\n{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00..., 'path': None}\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.17.1.dev0\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.2\r\n- PyArrow version: 6.0.0\r\n\r\nThe bug also exists in 1.17.0\r\n\r\n## Investigation\r\n\r\nI think the issue is that decoding is disabled in `__iter__`:\r\n\r\nhttps://github.com/huggingface/datasets/blob/dfe5b73387c5e27de6a16b0caeb39d3b9ded66d6/src/datasets/arrow_dataset.py#L1651-L1661\r\n\r\nDo you remember why it was disabled in the first place @albertvillanova ?\r\nAlso cc @mariosasko @NielsRogge \r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3473/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3465", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3465/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3465/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3465/events", "html_url": "https://github.com/huggingface/datasets/issues/3465", "id": 1085400432, "node_id": "I_kwDODunzps5AseVw", "number": 3465, "title": "Unable to load 'cnn_dailymail' dataset", "user": {"login": "talha1503", "id": 42352729, "node_id": "MDQ6VXNlcjQyMzUyNzI5", "avatar_url": "https://avatars.githubusercontent.com/u/42352729?v=4", "gravatar_id": "", "url": "https://api.github.com/users/talha1503", "html_url": "https://github.com/talha1503", "followers_url": "https://api.github.com/users/talha1503/followers", "following_url": "https://api.github.com/users/talha1503/following{/other_user}", "gists_url": "https://api.github.com/users/talha1503/gists{/gist_id}", "starred_url": "https://api.github.com/users/talha1503/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/talha1503/subscriptions", "organizations_url": "https://api.github.com/users/talha1503/orgs", "repos_url": "https://api.github.com/users/talha1503/repos", "events_url": "https://api.github.com/users/talha1503/events{/privacy}", "received_events_url": "https://api.github.com/users/talha1503/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892865, "node_id": "MDU6TGFiZWwxOTM1ODkyODY1", "url": "https://api.github.com/repos/huggingface/datasets/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}, {"id": 2067388877, "node_id": "MDU6TGFiZWwyMDY3Mzg4ODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset%20bug", "name": "dataset bug", "color": "2edb81", "default": false, "description": "A bug in a dataset script provided in the library"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-12-21T03:32:21Z", "updated_at": "2022-02-17T14:13:57Z", "closed_at": "2022-02-17T14:13:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI wanted to load cnn_dailymail dataset from huggingface datasets on Google Colab, but I am getting an error while loading it.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('cnn_dailymail', '3.0.0', ignore_verifications = True)\r\n```\r\n\r\n## Expected results\r\nExpecting to load 'cnn_dailymail' dataset.\r\n\r\n## Actual results\r\n`NotADirectoryError: [Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories'`\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.16.1\r\n- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.12\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3465/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3459", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3459/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3459/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3459/events", "html_url": "https://github.com/huggingface/datasets/issues/3459", "id": 1084969672, "node_id": "I_kwDODunzps5Aq1LI", "number": 3459, "title": "dataset.filter overwriting previously set dataset._indices values, resulting in the wrong elements being selected.", "user": {"login": "mmajurski", "id": 9354454, "node_id": "MDQ6VXNlcjkzNTQ0NTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/9354454?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mmajurski", "html_url": "https://github.com/mmajurski", "followers_url": "https://api.github.com/users/mmajurski/followers", "following_url": "https://api.github.com/users/mmajurski/following{/other_user}", "gists_url": "https://api.github.com/users/mmajurski/gists{/gist_id}", "starred_url": "https://api.github.com/users/mmajurski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mmajurski/subscriptions", "organizations_url": "https://api.github.com/users/mmajurski/orgs", "repos_url": "https://api.github.com/users/mmajurski/repos", "events_url": "https://api.github.com/users/mmajurski/events{/privacy}", "received_events_url": "https://api.github.com/users/mmajurski/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-12-20T16:16:49Z", "updated_at": "2021-12-20T16:34:57Z", "closed_at": "2021-12-20T16:34:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen using dataset.select to select a subset of a dataset, dataset._indices are set to indicate which elements are now considered in the dataset.\r\nThe same thing happens when you shuffle the dataset; dataset._indices are set to indicate what the new order of the data is.\r\nHowever, if you then use a dataset.filter, that filter interacts with those dataset._indices values in a non-intuitive manner.\r\nhttps://huggingface.co/docs/datasets/_modules/datasets/arrow_dataset.html#Dataset.filter\r\n\r\nEffectively, it looks like the original set of _indices were discared and overwritten by the set created during the filter operation.\r\n\r\nI think this is actually an issue with how the map function handles dataset._indices. Ideally it should use the _indices it gets passed, and then return an updated _indices which reflect the map transformation applied to the starting _indices.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = load_dataset('imdb', split='train', keep_in_memory=True)\r\n\r\ndataset = dataset.shuffle(keep_in_memory=True)\r\n\r\ndataset = dataset.select(range(0, 10), keep_in_memory=True)\r\nprint(\"initial 10 elements\")\r\nprint(dataset['label'])   # -> [1, 1, 0, 1, 0, 0, 0, 1, 0, 0]\r\n\r\ndataset = dataset.filter(lambda x: x['label'] == 0, keep_in_memory=True)\r\nprint(\"filtered 10 elements looking for label 0\")\r\nprint(dataset['label'])  # -> [1, 1, 1, 1, 1, 1]\r\n```\r\n\r\n## Actual results\r\n```\r\n$ python indices_bug.py\r\ninitial 10 elements\r\n[1, 1, 0, 1, 0, 0, 0, 1, 0, 0]\r\nfiltered 10 elements looking for label 0\r\n[1, 1, 1, 1, 1, 1]\r\n```\r\n\r\nThis code block first shuffles the dataset (to get a mix of label 0 and label 1).\r\nThen it selects just the first 10 elements (the number of elements does not matter, 10 is just easy to visualize). The important part is that you select some subset of the dataset. \r\nFinally, a filter is applied to pull out just the elements with `label == 0`.\r\n\r\nThe bug is that you cannot combine any dataset operation which sets the dataset._indices with filter.\r\nIn this case I have 2, shuffle and subset.\r\n\r\nIf you just use a single dataset._indices operation (in this case shuffle) the bug still shows up.\r\n\r\nThe shuffle sets the dataset._indices and then filter uses those indices in the map, then overwrites dataset._indices with the filter results.\r\n```python\r\ndataset = load_dataset('imdb', split='train', keep_in_memory=True)\r\n\r\ndataset = dataset.shuffle(keep_in_memory=True)\r\n\r\ndataset = dataset.filter(lambda x: x['label'] == 0, keep_in_memory=True)\r\n\r\ndataset = dataset.select(range(0, 10), keep_in_memory=True)\r\nprint(dataset['label']) # -> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\r\n```\r\n\r\n## Expected results\r\nIn an ideal world, the dataset filter would respect any dataset._indices values which had previously been set.\r\n\r\nIf you use dataset.filter with the base dataset (where dataset._indices has not been set) then the filter command works as expected.\r\n\r\n## Environment info\r\nHere are the commands required to rebuild the conda environment from scratch.\r\n```\r\n# create a virtual environment\r\nconda create -n dataset_indices python=3.8 -y\r\n\r\n# activate the virtual environment\r\nconda activate dataset_indices\r\n\r\n# install huggingface datasets\r\nconda install datasets\r\n```\r\n\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-41-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.12\r\n- PyArrow version: 3.0.0\r\n\r\n\r\n### Full Conda Environment\r\n```\r\n$ conda env export\r\nname: dasaset_indices\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=4.5=1_gnu\r\n  - abseil-cpp=20210324.2=h2531618_0\r\n  - aiohttp=3.8.1=py38h7f8727e_0\r\n  - aiosignal=1.2.0=pyhd3eb1b0_0\r\n  - arrow-cpp=3.0.0=py38h6b21186_4\r\n  - attrs=21.2.0=pyhd3eb1b0_0\r\n  - aws-c-common=0.4.57=he6710b0_1\r\n  - aws-c-event-stream=0.1.6=h2531618_5\r\n  - aws-checksums=0.1.9=he6710b0_0\r\n  - aws-sdk-cpp=1.8.185=hce553d0_0\r\n  - bcj-cffi=0.5.1=py38h295c915_0\r\n  - blas=1.0=mkl\r\n  - boost-cpp=1.73.0=h27cfd23_11\r\n  - bottleneck=1.3.2=py38heb32a55_1\r\n  - brotli=1.0.9=he6710b0_2\r\n  - brotli-python=1.0.9=py38heb0550a_2\r\n  - brotlicffi=1.0.9.2=py38h295c915_0\r\n  - brotlipy=0.7.0=py38h27cfd23_1003\r\n  - bzip2=1.0.8=h7b6447c_0\r\n  - c-ares=1.17.1=h27cfd23_0\r\n  - ca-certificates=2021.10.26=h06a4308_2\r\n  - certifi=2021.10.8=py38h06a4308_0\r\n  - cffi=1.14.6=py38h400218f_0\r\n  - conllu=4.4.1=pyhd3eb1b0_0\r\n  - cryptography=36.0.0=py38h9ce1e76_0\r\n  - dataclasses=0.8=pyh6d0b6a4_7\r\n  - dill=0.3.4=pyhd3eb1b0_0\r\n  - double-conversion=3.1.5=he6710b0_1\r\n  - et_xmlfile=1.1.0=py38h06a4308_0\r\n  - filelock=3.4.0=pyhd3eb1b0_0\r\n  - frozenlist=1.2.0=py38h7f8727e_0\r\n  - gflags=2.2.2=he6710b0_0\r\n  - glog=0.5.0=h2531618_0\r\n  - gmp=6.2.1=h2531618_2\r\n  - grpc-cpp=1.39.0=hae934f6_5\r\n  - huggingface_hub=0.0.17=pyhd3eb1b0_0\r\n  - icu=58.2=he6710b0_3\r\n  - idna=3.3=pyhd3eb1b0_0\r\n  - importlib-metadata=4.8.2=py38h06a4308_0\r\n  - importlib_metadata=4.8.2=hd3eb1b0_0\r\n  - intel-openmp=2021.4.0=h06a4308_3561\r\n  - krb5=1.19.2=hac12032_0\r\n  - ld_impl_linux-64=2.35.1=h7274673_9\r\n  - libboost=1.73.0=h3ff78a5_11\r\n  - libcurl=7.80.0=h0b77cf5_0\r\n  - libedit=3.1.20210910=h7f8727e_0\r\n  - libev=4.33=h7f8727e_1\r\n  - libevent=2.1.8=h1ba5d50_1\r\n  - libffi=3.3=he6710b0_2\r\n  - libgcc-ng=9.3.0=h5101ec6_17\r\n  - libgomp=9.3.0=h5101ec6_17\r\n  - libnghttp2=1.46.0=hce63b2e_0\r\n  - libprotobuf=3.17.2=h4ff587b_1\r\n  - libssh2=1.9.0=h1ba5d50_1\r\n  - libstdcxx-ng=9.3.0=hd4cf53a_17\r\n  - libthrift=0.14.2=hcc01f38_0\r\n  - libxml2=2.9.12=h03d6c58_0\r\n  - libxslt=1.1.34=hc22bd24_0\r\n  - lxml=4.6.3=py38h9120a33_0\r\n  - lz4-c=1.9.3=h295c915_1\r\n  - mkl=2021.4.0=h06a4308_640\r\n  - mkl-service=2.4.0=py38h7f8727e_0\r\n  - mkl_fft=1.3.1=py38hd3c417c_0\r\n  - mkl_random=1.2.2=py38h51133e4_0\r\n  - multiprocess=0.70.12.2=py38h7f8727e_0\r\n  - multivolumefile=0.2.3=pyhd3eb1b0_0\r\n  - ncurses=6.3=h7f8727e_2\r\n  - numexpr=2.7.3=py38h22e1b3c_1\r\n  - numpy=1.21.2=py38h20f2e39_0\r\n  - numpy-base=1.21.2=py38h79a1101_0\r\n  - openpyxl=3.0.9=pyhd3eb1b0_0\r\n  - openssl=1.1.1l=h7f8727e_0\r\n  - orc=1.6.9=ha97a36c_3\r\n  - packaging=21.3=pyhd3eb1b0_0\r\n  - pip=21.2.4=py38h06a4308_0\r\n  - py7zr=0.16.1=pyhd3eb1b0_1\r\n  - pycparser=2.21=pyhd3eb1b0_0\r\n  - pycryptodomex=3.10.1=py38h27cfd23_1\r\n  - pyopenssl=21.0.0=pyhd3eb1b0_1\r\n  - pyparsing=3.0.4=pyhd3eb1b0_0\r\n  - pyppmd=0.16.1=py38h295c915_0\r\n  - pysocks=1.7.1=py38h06a4308_0\r\n  - python=3.8.12=h12debd9_0\r\n  - python-dateutil=2.8.2=pyhd3eb1b0_0\r\n  - python-xxhash=2.0.2=py38h7f8727e_0\r\n  - pyzstd=0.14.4=py38h7f8727e_3\r\n  - re2=2020.11.01=h2531618_1\r\n  - readline=8.1=h27cfd23_0\r\n  - requests=2.26.0=pyhd3eb1b0_0\r\n  - setuptools=58.0.4=py38h06a4308_0\r\n  - six=1.16.0=pyhd3eb1b0_0\r\n  - snappy=1.1.8=he6710b0_0\r\n  - sqlite=3.36.0=hc218d9a_0\r\n  - texttable=1.6.4=pyhd3eb1b0_0\r\n  - tk=8.6.11=h1ccaba5_0\r\n  - typing_extensions=3.10.0.2=pyh06a4308_0\r\n  - uriparser=0.9.3=he6710b0_1\r\n  - utf8proc=2.6.1=h27cfd23_0\r\n  - wheel=0.37.0=pyhd3eb1b0_1\r\n  - xxhash=0.8.0=h7f8727e_3\r\n  - xz=5.2.5=h7b6447c_0\r\n  - zipp=3.6.0=pyhd3eb1b0_0\r\n  - zlib=1.2.11=h7f8727e_4\r\n  - zstd=1.4.9=haebb681_0\r\n  - pip:\r\n    - async-timeout==4.0.2\r\n    - charset-normalizer==2.0.9\r\n    - datasets==1.16.1\r\n    - fsspec==2021.11.1\r\n    - huggingface-hub==0.2.1\r\n    - multidict==5.2.0\r\n    - pandas==1.3.5\r\n    - pyarrow==6.0.1\r\n    - pytz==2021.3\r\n    - pyyaml==6.0\r\n    - tqdm==4.62.3\r\n    - typing-extensions==4.0.1\r\n    - urllib3==1.26.7\r\n    - yarl==1.7.2\r\n\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3459/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3453", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3453/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3453/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3453/events", "html_url": "https://github.com/huggingface/datasets/issues/3453", "id": 1084515911, "node_id": "I_kwDODunzps5ApGZH", "number": 3453, "title": "ValueError while iter_archive", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-12-20T08:46:18Z", "updated_at": "2021-12-20T10:04:59Z", "closed_at": "2021-12-20T10:04:59Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAfter the merge of:\r\n- #3443\r\n\r\nthe method `iter_archive` throws a ValueError:\r\n```\r\nValueError: read of closed file\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfor path, file in dl_manager.iter_archive(archive_path):\r\n    pass\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3453/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3453/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3447", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3447/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3447/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3447/events", "html_url": "https://github.com/huggingface/datasets/issues/3447", "id": 1082539790, "node_id": "I_kwDODunzps5Ahj8O", "number": 3447, "title": "HF_DATASETS_OFFLINE=1 didn't stop datasets.builder from downloading ", "user": {"login": "dunalduck0", "id": 51274745, "node_id": "MDQ6VXNlcjUxMjc0NzQ1", "avatar_url": "https://avatars.githubusercontent.com/u/51274745?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dunalduck0", "html_url": "https://github.com/dunalduck0", "followers_url": "https://api.github.com/users/dunalduck0/followers", "following_url": "https://api.github.com/users/dunalduck0/following{/other_user}", "gists_url": "https://api.github.com/users/dunalduck0/gists{/gist_id}", "starred_url": "https://api.github.com/users/dunalduck0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dunalduck0/subscriptions", "organizations_url": "https://api.github.com/users/dunalduck0/orgs", "repos_url": "https://api.github.com/users/dunalduck0/repos", "events_url": "https://api.github.com/users/dunalduck0/events{/privacy}", "received_events_url": "https://api.github.com/users/dunalduck0/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-12-16T18:51:13Z", "updated_at": "2022-02-17T14:16:27Z", "closed_at": "2022-02-17T14:16:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nAccording to https://huggingface.co/docs/datasets/loading_datasets.html#loading-a-dataset-builder, setting HF_DATASETS_OFFLINE to 1 should make datasets to \"run in full offline mode\". It didn't work for me. At the very beginning, datasets still tried to download \"custom data configuration\" for JSON, despite I have run the program once and cached all data into the same --cache_dir. \r\n\r\n\"Downloading\" is not an issue when running with local disk, but crashes often with cloud storage because (1) multiply GPU processes try to access the same file, AND (2) FileLocker fails to synchronize all processes, due to storage throttling. 99% of times, when the main process releases FileLocker, the file is not actually ready for access in cloud storage and thus triggers \"FileNotFound\" errors for all other processes. Well, another way to resolve the problem is to investigate super reliable cloud storage, but that's out of scope here.\r\n\r\n## Steps to reproduce the bug\r\n```\r\nexport HF_DATASETS_OFFLINE=1\r\npython run_clm.py --model_name_or_path=models/gpt-j-6B --train_file=trainpy.v2.train.json --validation_file=trainpy.v2.eval.json --cache_dir=datacache/trainpy.v2\r\n```\r\n\r\n## Expected results\r\ndatasets should stop all \"downloading\" behavior but reuse the cached JSON configuration. I think the problem here is part of the cache directory path, \"default-471372bed4b51b53\", is randomly generated, and it could change if some parameters changed. And I didn't find a way to use a fixed path to ensure datasets to reuse cached data every time.\r\n\r\n## Actual results\r\nThe logging shows datasets are still downloading into \"datacache/trainpy.v2/json/default-471372bed4b51b53/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426\". \r\n```\r\n12/16/2021 10:25:59 - WARNING - datasets.builder - Using custom data configuration default-471372bed4b51b53\r\n12/16/2021 10:25:59 - INFO - datasets.builder - Generating dataset json (datacache/trainpy.v2/json/default-471372bed4b51b53/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426)\r\nDownloading and preparing dataset json/default to  datacache/trainpy.v2/json/default-471372bed4b51b53/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 17623.13it/s]\r\n12/16/2021 10:25:59 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\r\n12/16/2021 10:26:00 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 1206.99it/s]\r\n12/16/2021 10:26:00 - INFO - datasets.utils.info_utils - Unable to verify checksums.\r\n12/16/2021 10:26:00 - INFO - datasets.builder - Generating split train\r\n12/16/2021 10:26:01 - INFO - datasets.builder - Generating split validation\r\n12/16/2021 10:26:02 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\r\nDataset json downloaded and prepared to  datacache/trainpy.v2/json/default-471372bed4b51b53/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426. Subsequent calls will reuse this data.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 53.54it/s]\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.16.1\r\n- Platform: Linux\r\n- Python version:  3.8.10\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3447/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3440", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3440/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3440/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3440/events", "html_url": "https://github.com/huggingface/datasets/issues/3440", "id": 1081528426, "node_id": "I_kwDODunzps5AdtBq", "number": 3440, "title": "datasets keeps reading from cached files, although I disabled it", "user": {"login": "dorost1234", "id": 79165106, "node_id": "MDQ6VXNlcjc5MTY1MTA2", "avatar_url": "https://avatars.githubusercontent.com/u/79165106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dorost1234", "html_url": "https://github.com/dorost1234", "followers_url": "https://api.github.com/users/dorost1234/followers", "following_url": "https://api.github.com/users/dorost1234/following{/other_user}", "gists_url": "https://api.github.com/users/dorost1234/gists{/gist_id}", "starred_url": "https://api.github.com/users/dorost1234/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dorost1234/subscriptions", "organizations_url": "https://api.github.com/users/dorost1234/orgs", "repos_url": "https://api.github.com/users/dorost1234/repos", "events_url": "https://api.github.com/users/dorost1234/events{/privacy}", "received_events_url": "https://api.github.com/users/dorost1234/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-12-15T21:26:22Z", "updated_at": "2022-02-24T09:12:22Z", "closed_at": "2022-02-24T09:12:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nHi,\r\nI am trying to avoid dataset library using cached files, I get the following bug when this tried to read the cached files. I tried to do the followings:\r\n```\r\nfrom datasets import set_caching_enabled\r\nset_caching_enabled(False)\r\n```\r\nalso force redownlaod:\r\n```\r\n download_mode='force_redownload'\r\n```\r\nbut none worked so far, this is on a cluster and on some of the machines this reads from the cached files, I really appreciate any idea on how to fully remove caching @lhoestq \r\nmany thanks\r\n\r\n```\r\nFile \"run_clm.py\", line 496, in <module>\r\n    main()\r\n  File \"run_clm.py\", line 419, in main\r\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\r\n  File \"/users/dara/codes/fewshot/debug/fewshot/third_party/trainers/trainer.py\", line 943, in train\r\n    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/transformers/trainer.py\", line 1445, in _maybe_log_save_evaluate\r\n    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\r\n  File \"/users/dara/codes/fewshot/debug/fewshot/third_party/trainers/trainer.py\", line 172, in evaluate\r\n    output = self.eval_loop(\r\n  File \"/users/dara/codes/fewshot/debug/fewshot/third_party/trainers/trainer.py\", line 241, in eval_loop\r\n    metrics = self.compute_pet_metrics(eval_datasets, model, self.extra_info[metric_key_prefix], task=task)\r\n  File \"/users/dara/codes/fewshot/debug/fewshot/third_party/trainers/trainer.py\", line 268, in compute_pet_metrics\r\n    centroids = self._compute_per_token_train_centroids(model, task=task)\r\n  File \"/users/dara/codes/fewshot/debug/fewshot/third_party/trainers/trainer.py\", line 353, in _compute_per_token_train_centroids\r\n    data = get_label_samples(self.get_per_task_train_dataset(task), label)\r\n  File \"/users/dara/codes/fewshot/debug/fewshot/third_party/trainers/trainer.py\", line 350, in get_label_samples\r\n    return dataset.filter(lambda example: int(example['labels']) == label)\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 470, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/fingerprint.py\", line 406, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2519, in filter\r\n    indices = self.map(\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2036, in map\r\n    return self._map_single(\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 503, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 470, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/fingerprint.py\", line 406, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2248, in _map_single\r\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 654, in from_file\r\n    return cls(\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 593, in __init__\r\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/features/features.py\", line 1092, in reorder_fields_as\r\n    return Features(recursive_reorder(self, other))\r\n  File \"/users/dara/conda/envs/multisuccess/lib/python3.8/site-packages/datasets/features/features.py\", line 1081, in recursive_reorder\r\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\r\nValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'candidates_ids': Sequence(feature=Value(dtype='null', id=None), length=-1, id=None), 'labels': Value(dtype='int64', id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'extra_fields': {}, 'task': Value(dtype='string', id=None)}\r\n\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform: linux \r\n- Python version: 3.8.12 \r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3440/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3423", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3423/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3423/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3423/events", "html_url": "https://github.com/huggingface/datasets/issues/3423", "id": 1078049638, "node_id": "I_kwDODunzps5AQbtm", "number": 3423, "title": "data duplicate when setting num_works > 1 with streaming data", "user": {"login": "cloudyuyuyu", "id": 16486492, "node_id": "MDQ6VXNlcjE2NDg2NDky", "avatar_url": "https://avatars.githubusercontent.com/u/16486492?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cloudyuyuyu", "html_url": "https://github.com/cloudyuyuyu", "followers_url": "https://api.github.com/users/cloudyuyuyu/followers", "following_url": "https://api.github.com/users/cloudyuyuyu/following{/other_user}", "gists_url": "https://api.github.com/users/cloudyuyuyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/cloudyuyuyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cloudyuyuyu/subscriptions", "organizations_url": "https://api.github.com/users/cloudyuyuyu/orgs", "repos_url": "https://api.github.com/users/cloudyuyuyu/repos", "events_url": "https://api.github.com/users/cloudyuyuyu/events{/privacy}", "received_events_url": "https://api.github.com/users/cloudyuyuyu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 3287858981, "node_id": "MDU6TGFiZWwzMjg3ODU4OTgx", "url": "https://api.github.com/repos/huggingface/datasets/labels/streaming", "name": "streaming", "color": "fef2c0", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2021-12-13T03:43:17Z", "updated_at": "2022-12-14T16:04:22Z", "closed_at": "2022-12-14T16:04:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe data is repeated num_works times when we load_dataset with streaming and set num_works > 1 when construct dataloader\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nimport pandas as pd\r\nimport numpy as np\r\nimport os\r\n\r\nfrom datasets import load_dataset\r\nfrom torch.utils.data import DataLoader\r\nfrom tqdm import tqdm\r\nimport shutil\r\n\r\nNUM_OF_USER = 1000000\r\nNUM_OF_ACTION = 50000\r\nNUM_OF_SEQUENCE = 10000\r\nNUM_OF_FILES = 32\r\nNUM_OF_WORKERS = 16\r\n\r\nif __name__ == \"__main__\":\r\n    shutil.rmtree(\"./dataset\")\r\n    for i in range(NUM_OF_FILES):\r\n        sequence_data = pd.DataFrame(\r\n            {\r\n                \"imei\": np.random.randint(1, NUM_OF_USER, size=NUM_OF_SEQUENCE),\r\n                \"sequence\": np.random.randint(1, NUM_OF_ACTION, size=NUM_OF_SEQUENCE)\r\n            }\r\n        )\r\n\r\n        if not os.path.exists(\"./dataset\"):\r\n            os.makedirs(\"./dataset\")\r\n\r\n        sequence_data.to_csv(f\"./dataset/sequence_data_{i}.csv\",\r\n                            \r\n index=False)\r\n\r\n    dataset = load_dataset(\"csv\",\r\n                           data_files=[os.path.join(\"./dataset\",file) for file in os.listdir(\"./dataset\") if file.endswith(\".csv\")],\r\n                           split=\"train\",\r\n                           streaming=True).with_format(\"torch\")\r\n    data_loader = DataLoader(dataset,\r\n                             batch_size=1024,\r\n                             num_workers=NUM_OF_WORKERS)\r\n    \r\n    result = pd.DataFrame()\r\n    for i, batch in tqdm(enumerate(data_loader)):\r\n        result = pd.concat([result, \r\n                           pd.DataFrame(batch)],\r\n                           axis=0)\r\n    result.to_csv(f\"num_work_{NUM_OF_WORKERS}.csv\", index=False)\r\n\r\n```\r\n\r\n## Expected results\r\ndata do not duplicate \r\n\r\n## Actual results\r\ndata duplicate NUM_OF_WORKERS = 16 \r\n![image](https://user-images.githubusercontent.com/16486492/145748707-9d2df25b-2f4f-4d7b-a83e-242be4fc8934.png)\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:datasets==1.14.0\r\n- Platform:transformers==4.11.3\r\n- Python version:3.8\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3423/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3423/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3422", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3422/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3422/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3422/events", "html_url": "https://github.com/huggingface/datasets/issues/3422", "id": 1078022619, "node_id": "I_kwDODunzps5AQVHb", "number": 3422, "title": "Error about load_metric", "user": {"login": "jiacheng-ye", "id": 30772464, "node_id": "MDQ6VXNlcjMwNzcyNDY0", "avatar_url": "https://avatars.githubusercontent.com/u/30772464?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiacheng-ye", "html_url": "https://github.com/jiacheng-ye", "followers_url": "https://api.github.com/users/jiacheng-ye/followers", "following_url": "https://api.github.com/users/jiacheng-ye/following{/other_user}", "gists_url": "https://api.github.com/users/jiacheng-ye/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiacheng-ye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiacheng-ye/subscriptions", "organizations_url": "https://api.github.com/users/jiacheng-ye/orgs", "repos_url": "https://api.github.com/users/jiacheng-ye/repos", "events_url": "https://api.github.com/users/jiacheng-ye/events{/privacy}", "received_events_url": "https://api.github.com/users/jiacheng-ye/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-12-13T02:49:51Z", "updated_at": "2022-01-07T14:06:47Z", "closed_at": "2022-01-07T14:06:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1371, in load_metric\r\n    metric = metric_cls(\r\nTypeError: 'NoneType' object is not callable\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nmetric = load_metric(\"glue\", \"sst2\")\r\n```\r\n\r\n\r\n## Environment info\r\n- `datasets` version: 1.16.1\r\n- Platform: Linux-4.15.0-161-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.3\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3422/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3422/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3415", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3415/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3415/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3415/events", "html_url": "https://github.com/huggingface/datasets/issues/3415", "id": 1076472534, "node_id": "I_kwDODunzps5AKarW", "number": 3415, "title": "Non-deterministic tests: CI tests randomly fail", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-12-10T06:08:59Z", "updated_at": "2022-03-31T16:38:51Z", "closed_at": "2022-03-31T16:38:51Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nSome CI tests fail randomly.\r\n\r\n1. In https://github.com/huggingface/datasets/pull/3375/commits/c10275fe36085601cb7bdb9daee9a8f1fc734f48, there were 3 failing tests, only on Linux:\r\n   ```\r\n   =========================== short test summary info ============================\r\n   FAILED tests/test_streaming_download_manager.py::test_streaming_dl_manager_get_extraction_protocol[https://drive.google.com/uc?export=download&id=1k92sUfpHxKq8PXWRr7Y5aNHXwOCNUmqh-zip]\r\n   FAILED tests/test_streaming_download_manager.py::test_streaming_gg_drive - Fi...\r\n   FAILED tests/test_streaming_download_manager.py::test_streaming_gg_drive_zipped\r\n   = 3 failed, 3553 passed, 2950 skipped, 2 xfailed, 1 xpassed, 125 warnings in 192.79s (0:03:12) =\r\n   ```\r\n\r\n2. After re-running the CI (without any change in the code) in https://github.com/huggingface/datasets/pull/3375/commits/57bfe1f342cd3c59d2510b992d5f06a0761eb147, there was only 1 failing test (one on Linux and a different one on Windows):\r\n   - On Linux:\r\n      ```\r\n      =========================== short test summary info ============================\r\n      FAILED tests/test_streaming_download_manager.py::test_streaming_gg_drive_zipped\r\n      = 1 failed, 3555 passed, 2950 skipped, 2 xfailed, 1 xpassed, 125 warnings in 199.76s (0:03:19) =\r\n      ```\r\n   - On Windows:\r\n      ```\r\n      =========================== short test summary info ===========================\r\n      FAILED tests/test_load.py::test_load_dataset_builder_for_community_dataset_without_script\r\n      = 1 failed, 3551 passed, 2954 skipped, 2 xfailed, 1 xpassed, 121 warnings in 478.58s (0:07:58) =\r\n      ```\r\n\r\n   The test `tests/test_streaming_download_manager.py::test_streaming_gg_drive_zipped` passes locally.\r\n\r\n3. After re-running again the CI (without any change in the code) in https://github.com/huggingface/datasets/pull/3375/commits/39f32f2119cf91b86867216bb5c356c586503c6a, ALL the tests passed.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3415/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3405", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3405/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3405/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3405/events", "html_url": "https://github.com/huggingface/datasets/issues/3405", "id": 1074360362, "node_id": "I_kwDODunzps5ACXAq", "number": 3405, "title": "ZIP format inference does not work when files located in a dir inside the archive", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-12-08T12:32:15Z", "updated_at": "2021-12-08T13:03:29Z", "closed_at": "2021-12-08T13:03:29Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen a zipped file contains archived files within a directory, the function `infer_module_for_data_files_in_archives` does not work.\r\n\r\nIt only works for files located in the root directory of the ZIP file.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ninfer_module_for_data_files_in_archives([\"path/to/zip/file.zip\"], False)\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3405/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3405/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3403", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3403/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3403/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3403/events", "html_url": "https://github.com/huggingface/datasets/issues/3403", "id": 1073622120, "node_id": "I_kwDODunzps4__ixo", "number": 3403, "title": "Cannot import name 'maybe_sync'", "user": {"login": "KMFODA", "id": 35491698, "node_id": "MDQ6VXNlcjM1NDkxNjk4", "avatar_url": "https://avatars.githubusercontent.com/u/35491698?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KMFODA", "html_url": "https://github.com/KMFODA", "followers_url": "https://api.github.com/users/KMFODA/followers", "following_url": "https://api.github.com/users/KMFODA/following{/other_user}", "gists_url": "https://api.github.com/users/KMFODA/gists{/gist_id}", "starred_url": "https://api.github.com/users/KMFODA/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KMFODA/subscriptions", "organizations_url": "https://api.github.com/users/KMFODA/orgs", "repos_url": "https://api.github.com/users/KMFODA/repos", "events_url": "https://api.github.com/users/KMFODA/events{/privacy}", "received_events_url": "https://api.github.com/users/KMFODA/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-12-07T17:57:59Z", "updated_at": "2021-12-17T07:00:35Z", "closed_at": "2021-12-17T07:00:35Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nCannot seem to import datasets when running run_summarizer.py script on a VM set up on ovhcloud\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n```\r\n\r\n## Expected results\r\nNo error\r\n\r\n## Actual results\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/__init__.py\", line 34, in <module>\r\n    from .arrow_dataset import Dataset, concatenate_datasets\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 48, in <module>\r\n    from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 27, in <module>\r\n    from .features import (\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/features/__init__.py\", line 2, in <module>\r\n    from .audio import Audio\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/features/audio.py\", line 8, in <module>\r\n    from ..utils.streaming_download_manager import xopen\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/utils/streaming_download_manager.py\", line 16, in <module>\r\n    from ..filesystems import COMPRESSION_FILESYSTEMS\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/filesystems/__init__.py\", line 13, in <module>\r\n    from .s3filesystem import S3FileSystem  # noqa: F401\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/filesystems/s3filesystem.py\", line 1, in <module>\r\n    import s3fs\r\n  File \"/opt/conda/lib/python3.7/site-packages/s3fs/__init__.py\", line 1, in <module>\r\n    from .core import S3FileSystem, S3File\r\n  File \"/opt/conda/lib/python3.7/site-packages/s3fs/core.py\", line 11, in <module>\r\n    from fsspec.asyn import AsyncFileSystem, sync, sync_wrapper, maybe_sync\r\nImportError: cannot import name 'maybe_sync' from 'fsspec.asyn' (/opt/conda/lib/python3.7/site-packages/fsspec/asyn.py)\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.16.0\r\n- Platform: OVH Cloud Tesla V100 Machine\r\n- Python version: 3.7.9\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3403/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3403/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3394", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3394/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3394/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3394/events", "html_url": "https://github.com/huggingface/datasets/issues/3394", "id": 1073396308, "node_id": "I_kwDODunzps4_-rpU", "number": 3394, "title": "Preserve all feature types when saving a dataset on the Hub with `push_to_hub`", "user": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-12-07T14:08:30Z", "updated_at": "2021-12-21T17:00:09Z", "closed_at": "2021-12-21T17:00:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Currently, if one of the dataset features is of type `ClassLabel`, saving the dataset with `push_to_hub` and reloading the dataset with `load_dataset` will return the feature of type `Value`. To fix this, we should do something similar to `save_to_disk` (which correctly preserves the types) and not only push the parquet files in `push_to_hub`, but also the dataset `info` (stored in a JSON file).", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3394/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3394/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3390", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3390/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3390/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3390/events", "html_url": "https://github.com/huggingface/datasets/issues/3390", "id": 1072462456, "node_id": "I_kwDODunzps4_7Hp4", "number": 3390, "title": "Loading dataset throws \"KeyError: 'Field \"builder_name\" does not exist in table schema'\"", "user": {"login": "R4ZZ3", "id": 25264037, "node_id": "MDQ6VXNlcjI1MjY0MDM3", "avatar_url": "https://avatars.githubusercontent.com/u/25264037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/R4ZZ3", "html_url": "https://github.com/R4ZZ3", "followers_url": "https://api.github.com/users/R4ZZ3/followers", "following_url": "https://api.github.com/users/R4ZZ3/following{/other_user}", "gists_url": "https://api.github.com/users/R4ZZ3/gists{/gist_id}", "starred_url": "https://api.github.com/users/R4ZZ3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/R4ZZ3/subscriptions", "organizations_url": "https://api.github.com/users/R4ZZ3/orgs", "repos_url": "https://api.github.com/users/R4ZZ3/repos", "events_url": "https://api.github.com/users/R4ZZ3/events{/privacy}", "received_events_url": "https://api.github.com/users/R4ZZ3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-12-06T18:22:49Z", "updated_at": "2021-12-06T20:22:05Z", "closed_at": "2021-12-06T20:22:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI have prepared dataset to datasets and now I am trying to load it back Finnish-NLP/voxpopuli_fi\r\nI get \"KeyError: 'Field \"builder_name\" does not exist in table schema'\"\r\n\r\nMy dataset folder and files should be like @patrickvonplaten  has here https://huggingface.co/datasets/flax-community/german-common-voice-processed\r\n\r\nHow my voxpopuli dataset looks like:\r\n![image](https://user-images.githubusercontent.com/25264037/144895598-b7d9ae91-b04a-4046-9f06-b71ff0824d13.png)\r\n\r\nPart of the processing (path column is the absolute path to audio files)\r\n```\r\ndef add_audio_column(example):\r\n    example['audio'] = example['path']\r\n    return example\r\n\r\nvoxpopuli = voxpopuli.map(add_audio_column)\r\nvoxpopuli.cast_column(\"audio\", Audio())\r\nvoxpopuli[\"audio\"] <-- to my knowledge this does load the local files and prepares those arrays\r\nvoxpopuli = voxpopuli.cast_column(\"audio\", Audio(sampling_rate=16_000)) resampling 16kHz\r\n```\r\n\r\nI have then saved it to disk_\r\n`voxpopuli.save_to_disk('/asr_disk/datasets_processed_new/voxpopuli')`\r\n\r\nand made folder structure same as @patrickvonplaten \r\nI also get same error while trying to load_dataset from his repo:\r\n![image](https://user-images.githubusercontent.com/25264037/144895872-e9b8f326-cf2b-46cf-9417-606a0ce14077.png)\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = load_dataset(\"Finnish-NLP/voxpopuli_fi\")\r\n```\r\n\r\n## Expected results\r\nDataset is loaded correctly and looks like in the first picture\r\n\r\n## Actual results\r\nLoading throws keyError:\r\nKeyError: 'Field \"builder_name\" does not exist in table schema'\r\n\r\n\r\nResources I have been trying to follow:\r\nhttps://huggingface.co/docs/datasets/audio_process.html \r\nhttps://huggingface.co/docs/datasets/share_dataset.html\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.16.2.dev0\r\n- Platform: Ubuntu 20.04.2 LTS\r\n- Python version: 3.8.12\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3390/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3381", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3381/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3381/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3381/events", "html_url": "https://github.com/huggingface/datasets/issues/3381", "id": 1071283879, "node_id": "I_kwDODunzps4_2n6n", "number": 3381, "title": "Unable to load audio_features from common_voice dataset", "user": {"login": "ashu5644", "id": 8268102, "node_id": "MDQ6VXNlcjgyNjgxMDI=", "avatar_url": "https://avatars.githubusercontent.com/u/8268102?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ashu5644", "html_url": "https://github.com/ashu5644", "followers_url": "https://api.github.com/users/ashu5644/followers", "following_url": "https://api.github.com/users/ashu5644/following{/other_user}", "gists_url": "https://api.github.com/users/ashu5644/gists{/gist_id}", "starred_url": "https://api.github.com/users/ashu5644/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ashu5644/subscriptions", "organizations_url": "https://api.github.com/users/ashu5644/orgs", "repos_url": "https://api.github.com/users/ashu5644/repos", "events_url": "https://api.github.com/users/ashu5644/events{/privacy}", "received_events_url": "https://api.github.com/users/ashu5644/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-12-04T19:59:11Z", "updated_at": "2021-12-06T17:52:42Z", "closed_at": "2021-12-06T17:52:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am not able to load audio features from common_voice dataset\r\n\r\n## Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\nimport torchaudio\r\n\r\ntest_dataset = load_dataset(\"common_voice\", \"hi\", split=\"test[:2%]\")\r\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\r\n\r\ndef speech_file_to_array_fn(batch):\r\n      speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\r\n      batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\r\n      return batch\r\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\r\n```\r\n## Expected results\r\n\r\nThis piece of code should return test_dataset after loading audio features.\r\n\r\n## Actual results\r\n\r\nReusing dataset common_voice (/home/jovyan/.cache/huggingface/datasets/common_voice/hi/6.1.0/b879a355caa529b11f2249400b61cadd0d9433f334d5c60f8c7216ccedfecfe1)\r\n/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py:341: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\r\n  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n  0%|                                                                                                                    | 0/3 [00:00<?, ?ex/s]formats: can't open input file `common_voice_hi_23795358.mp3': No such file or directory\r\n  0%|                                                                                                                    | 0/3 [00:00<?, ?ex/s]\r\nTraceback (most recent call last):\r\n  File \"demo_file.py\", line 23, in <module>\r\n    test_dataset = test_dataset.map(speech_file_to_array_fn)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2036, in map\r\n    desc=desc,\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 518, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 485, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py\", line 411, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2368, in _map_single\r\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2277, in apply_function_on_filtered_inputs\r\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1978, in decorated\r\n    result = f(decorated_item, *args, **kwargs)\r\n  File \"demo_file.py\", line 19, in speech_file_to_array_fn\r\n    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\r\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 154, in load\r\n    filepath, frame_offset, num_frames, normalize, channels_first, format)\r\nRuntimeError: Error loading audio file: failed to open file common_voice_hi_23795358.mp3\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.16.1\r\n- Platform: Linux-4.14.243 with-debian-bullseye-sid\r\n- Python version: 3.7.9\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3381/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3381/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3361", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3361/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3361/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3361/events", "html_url": "https://github.com/huggingface/datasets/issues/3361", "id": 1068736268, "node_id": "I_kwDODunzps4_s58M", "number": 3361, "title": "Jeopardy _URL access denied", "user": {"login": "tianjianjiang", "id": 4812544, "node_id": "MDQ6VXNlcjQ4MTI1NDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/4812544?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tianjianjiang", "html_url": "https://github.com/tianjianjiang", "followers_url": "https://api.github.com/users/tianjianjiang/followers", "following_url": "https://api.github.com/users/tianjianjiang/following{/other_user}", "gists_url": "https://api.github.com/users/tianjianjiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/tianjianjiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tianjianjiang/subscriptions", "organizations_url": "https://api.github.com/users/tianjianjiang/orgs", "repos_url": "https://api.github.com/users/tianjianjiang/repos", "events_url": "https://api.github.com/users/tianjianjiang/events{/privacy}", "received_events_url": "https://api.github.com/users/tianjianjiang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-12-01T18:21:33Z", "updated_at": "2021-12-11T12:50:23Z", "closed_at": "2021-12-06T11:16:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nhttp://skeeto.s3.amazonaws.com/share/JEOPARDY_QUESTIONS1.json.gz returns Access Denied now.\r\n\r\nHowever, https://drive.google.com/file/d/0BwT5wj_P7BKXb2hfM3d2RHU1ckE/view?usp=sharing from the original Reddit post https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/ may work.\r\n\r\n\r\n## Steps to reproduce the bug\r\n```shell\r\n> python\r\nPython 3.7.12 (default, Sep  5 2021, 08:34:29)\r\n[Clang 11.0.3 (clang-1103.0.32.62)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n```\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> load_dataset(\"jeopardy\")\r\n```\r\n\r\n## Expected results\r\nThe download completes.\r\n\r\n## Actual results\r\n```shell\r\nDownloading: 4.18kB [00:00, 1.60MB/s]\r\nDownloading: 2.03kB [00:00, 1.04MB/s]\r\nUsing custom data configuration default\r\nDownloading and preparing dataset jeopardy/default (download: 12.13 MiB, generated: 34.46 MiB, post-processed: Unknown size, total: 46.59 MiB) to /Users/mike/.cache/huggingface/datasets/jeopardy/default/0.1.0/25ee3e4a73755e637b8810f6493fd36e4523dea3ca8a540529d0a6e24c7f9810...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/builder.py\", line 608, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/builder.py\", line 675, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/Users/mike/.cache/huggingface/modules/datasets_modules/datasets/jeopardy/25ee3e4a73755e637b8810f6493fd36e4523dea3ca8a540529d0a6e24c7f9810/jeopardy.py\", line 72, in _split_generators\r\n    filepath = dl_manager.download_and_extract(_DATA_URL)\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 284, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 197, in download\r\n    download_func, url_or_urls, map_tuple=True, num_proc=download_config.num_proc, disable_tqdm=False\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 197, in map_nested\r\n    return function(data_struct)\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 217, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 305, in cached_path\r\n    use_auth_token=download_config.use_auth_token,\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 594, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach http://skeeto.s3.amazonaws.com/share/JEOPARDY_QUESTIONS1.json.gz\r\n```\r\n\r\n---\r\n\r\n```shell\r\n> curl http://skeeto.s3.amazonaws.com/share/JEOPARDY_QUESTIONS1.json.gz\r\n```\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>70Y9R36XNPEQXMGV</RequestId><HostId>G6F5AK4qo7JdaEdKGMtS0P6gdLPeFOdEfSEfvTOZEfk9km0/jAfp08QLfKSTFFj1oWIKoAoBehM=</HostId></Error>\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.14.0\r\n- Platform: macOS Catalina 10.15.7\r\n- Python version: 3.7.12\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3361/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3346", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3346/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3346/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3346/events", "html_url": "https://github.com/huggingface/datasets/issues/3346", "id": 1067632365, "node_id": "I_kwDODunzps4_osbt", "number": 3346, "title": "Failed to convert `string` with pyarrow for QED since 1.15.0", "user": {"login": "tianjianjiang", "id": 4812544, "node_id": "MDQ6VXNlcjQ4MTI1NDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/4812544?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tianjianjiang", "html_url": "https://github.com/tianjianjiang", "followers_url": "https://api.github.com/users/tianjianjiang/followers", "following_url": "https://api.github.com/users/tianjianjiang/following{/other_user}", "gists_url": "https://api.github.com/users/tianjianjiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/tianjianjiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tianjianjiang/subscriptions", "organizations_url": "https://api.github.com/users/tianjianjiang/orgs", "repos_url": "https://api.github.com/users/tianjianjiang/repos", "events_url": "https://api.github.com/users/tianjianjiang/events{/privacy}", "received_events_url": "https://api.github.com/users/tianjianjiang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-11-30T20:11:42Z", "updated_at": "2021-12-14T14:39:05Z", "closed_at": "2021-12-14T14:39:05Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading QED was fine until 1.15.0.\r\nrelated: bigscience-workshop/promptsource#659, bigscience-workshop/promptsource#670\r\n\r\nNot sure where the root cause is, but here are some candidates:\r\n- #3158\r\n- #3120\r\n- #3196\r\n- #2891\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset(\"qed\")\r\n```\r\n\r\n## Expected results\r\nLoading completed.\r\n\r\n## Actual results\r\n```shell\r\nArrowInvalid: Could not convert in with type str: tried to convert to boolean\r\nTraceback:\r\nFile \"/Users/s0s0cr3/Library/Python/3.9/lib/python/site-packages/streamlit/script_runner.py\", line 354, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/Users/s0s0cr3/Documents/GitHub/promptsource/promptsource/app.py\", line 260, in <module>\r\n    dataset = get_dataset(dataset_key, str(conf_option.name) if conf_option else None)\r\nFile \"/Users/s0s0cr3/Library/Python/3.9/lib/python/site-packages/streamlit/caching.py\", line 543, in wrapped_func\r\n    return get_or_create_cached_value()\r\nFile \"/Users/s0s0cr3/Library/Python/3.9/lib/python/site-packages/streamlit/caching.py\", line 527, in get_or_create_cached_value\r\n    return_value = func(*args, **kwargs)\r\nFile \"/Users/s0s0cr3/Documents/GitHub/promptsource/promptsource/utils.py\", line 49, in get_dataset\r\n    builder_instance.download_and_prepare()\r\nFile \"/Users/s0s0cr3/Library/Python/3.9/lib/python/site-packages/datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\nFile \"/Users/s0s0cr3/Library/Python/3.9/lib/python/site-packages/datasets/builder.py\", line 697, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\nFile \"/Users/s0s0cr3/Library/Python/3.9/lib/python/site-packages/datasets/builder.py\", line 1106, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\nFile \"/Users/s0s0cr3/Library/Python/3.9/lib/python/site-packages/datasets/arrow_writer.py\", line 456, in finalize\r\n    self.write_examples_on_file()\r\nFile \"/Users/s0s0cr3/Library/Python/3.9/lib/python/site-packages/datasets/arrow_writer.py\", line 325, in write_examples_on_file\r\n    pa_array = pa.array(typed_sequence)\r\nFile \"pyarrow/array.pxi\", line 222, in pyarrow.lib.array\r\nFile \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\nFile \"/Users/s0s0cr3/Library/Python/3.9/lib/python/site-packages/datasets/arrow_writer.py\", line 121, in __arrow_array__\r\n    out = pa.array(cast_to_python_objects(self.data, only_1d_for_numpy=True), type=type)\r\nFile \"pyarrow/array.pxi\", line 305, in pyarrow.lib.array\r\nFile \"pyarrow/array.pxi\", line 39, in pyarrow.lib._sequence_to_array\r\nFile \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\nFile \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.15.0, 1.16.1\r\n- Platform: macOS 1.15.7 or above\r\n- Python version: 3.7.12 and 3.9\r\n- PyArrow version: 3.0.0, 5.0.0, 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3346/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3345", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3345/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3345/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3345/events", "html_url": "https://github.com/huggingface/datasets/issues/3345", "id": 1067622951, "node_id": "I_kwDODunzps4_oqIn", "number": 3345, "title": "Failed to download species_800 from Google Drive zip file", "user": {"login": "tianjianjiang", "id": 4812544, "node_id": "MDQ6VXNlcjQ4MTI1NDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/4812544?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tianjianjiang", "html_url": "https://github.com/tianjianjiang", "followers_url": "https://api.github.com/users/tianjianjiang/followers", "following_url": "https://api.github.com/users/tianjianjiang/following{/other_user}", "gists_url": "https://api.github.com/users/tianjianjiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/tianjianjiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tianjianjiang/subscriptions", "organizations_url": "https://api.github.com/users/tianjianjiang/orgs", "repos_url": "https://api.github.com/users/tianjianjiang/repos", "events_url": "https://api.github.com/users/tianjianjiang/events{/privacy}", "received_events_url": "https://api.github.com/users/tianjianjiang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-11-30T20:00:28Z", "updated_at": "2021-12-01T17:53:15Z", "closed_at": "2021-12-01T17:53:15Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nOne can manually download the zip file on Google Drive, but `load_dataset()` cannot.\r\nrelated: #3248\r\n\r\n## Steps to reproduce the bug\r\n```shell\r\n> python\r\nPython 3.7.12 (default, Sep  5 2021, 08:34:29)\r\n[Clang 11.0.3 (clang-1103.0.32.62)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n```\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> s800 = load_dataset(\"species_800\")\r\n```\r\n\r\n## Expected results\r\nspecies_800 downloaded.\r\n\r\n## Actual results\r\n```shell\r\nDownloading: 5.68kB [00:00, 1.22MB/s]\r\nDownloading: 2.70kB [00:00, 691kB/s]\r\nDownloading and preparing dataset species800/species_800 (download: 17.36 MiB, generated: 3.53 MiB, post-processed: Unknown size, total: 20.89 MiB) to /Users/mike/.cache/huggingface/datasets/species800/species_800/1.0.0/532167f0bb8fbc0d77d6d03c4fd642c8c55527b9c5f2b1da77f3d00b0e559976...\r\n  0%|                                                                                             | 0/1 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/builder.py\", line 608, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/builder.py\", line 675, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/Users/mike/.cache/huggingface/modules/datasets_modules/datasets/species_800/532167f0bb8fbc0d77d6d03c4fd642c8c55527b9c5f2b1da77f3d00b0e559976/species_800.py\", line 104, in _split_generators\r\n    downloaded_files = dl_manager.download_and_extract(urls_to_download)\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 284, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 197, in download\r\n    download_func, url_or_urls, map_tuple=True, num_proc=download_config.num_proc, disable_tqdm=False\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 209, in map_nested\r\n    for obj in utils.tqdm(iterable, disable=disable_tqdm)\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 209, in <listcomp>\r\n    for obj in utils.tqdm(iterable, disable=disable_tqdm)\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 143, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 217, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 305, in cached_path\r\n    use_auth_token=download_config.use_auth_token,\r\n  File \"/Users/mike/Library/Caches/pypoetry/virtualenvs/promptsource-hsdAcWsQ-py3.7/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 594, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://drive.google.com/u/0/uc?id=1OletxmPYNkz2ltOr9pyT0b0iBtUWxslh&export=download/\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.14,0 1.15.0, 1.16.1\r\n- Platform: macOS Catalina 10.15.7\r\n- Python version: 3.7.12\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3345/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3345/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3337", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3337/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3337/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3337/events", "html_url": "https://github.com/huggingface/datasets/issues/3337", "id": 1066232936, "node_id": "I_kwDODunzps4_jWxo", "number": 3337, "title": "Typing of Dataset.__getitem__ could be improved.", "user": {"login": "Dref360", "id": 8976546, "node_id": "MDQ6VXNlcjg5NzY1NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/8976546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dref360", "html_url": "https://github.com/Dref360", "followers_url": "https://api.github.com/users/Dref360/followers", "following_url": "https://api.github.com/users/Dref360/following{/other_user}", "gists_url": "https://api.github.com/users/Dref360/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dref360/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dref360/subscriptions", "organizations_url": "https://api.github.com/users/Dref360/orgs", "repos_url": "https://api.github.com/users/Dref360/repos", "events_url": "https://api.github.com/users/Dref360/events{/privacy}", "received_events_url": "https://api.github.com/users/Dref360/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Dref360", "id": 8976546, "node_id": "MDQ6VXNlcjg5NzY1NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/8976546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dref360", "html_url": "https://github.com/Dref360", "followers_url": "https://api.github.com/users/Dref360/followers", "following_url": "https://api.github.com/users/Dref360/following{/other_user}", "gists_url": "https://api.github.com/users/Dref360/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dref360/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dref360/subscriptions", "organizations_url": "https://api.github.com/users/Dref360/orgs", "repos_url": "https://api.github.com/users/Dref360/repos", "events_url": "https://api.github.com/users/Dref360/events{/privacy}", "received_events_url": "https://api.github.com/users/Dref360/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Dref360", "id": 8976546, "node_id": "MDQ6VXNlcjg5NzY1NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/8976546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dref360", "html_url": "https://github.com/Dref360", "followers_url": "https://api.github.com/users/Dref360/followers", "following_url": "https://api.github.com/users/Dref360/following{/other_user}", "gists_url": "https://api.github.com/users/Dref360/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dref360/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dref360/subscriptions", "organizations_url": "https://api.github.com/users/Dref360/orgs", "repos_url": "https://api.github.com/users/Dref360/repos", "events_url": "https://api.github.com/users/Dref360/events{/privacy}", "received_events_url": "https://api.github.com/users/Dref360/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-11-29T16:20:11Z", "updated_at": "2021-12-14T10:28:54Z", "closed_at": "2021-12-14T10:28:54Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThe newly added typing for Dataset.__getitem__ is Union[Dict, List]. This makes tools like mypy a bit awkward to use as we need to check the type manually. We could use type overloading to make this easier. [Documentation](https://docs.python.org/3/library/typing.html#typing.overload)\r\n\r\n## Steps to reproduce the bug\r\nLet's have a file `test.py`\r\n\r\n```python\r\nfrom typing import List, Dict, Any\r\n\r\nfrom datasets import Dataset\r\n\r\nds = Dataset.from_dict({\r\n    'a': [1,2,3],\r\n    'b': [\"1\", \"2\", \"3\"]\r\n})\r\n\r\none_colum: List[str] = ds['a']\r\nsome_index: Dict[Any, Any] = ds[1]\r\n```\r\n\r\n## Expected results\r\n\r\nRunning `mypy test.py` should not give any error.\r\n\r\n\r\n## Actual results\r\n\r\n```\r\ntest.py:10: error: Incompatible types in assignment (expression has type \"Union[Dict[Any, Any], List[Any]]\", variable has type \"List[str]\")\r\ntest.py:11: error: Incompatible types in assignment (expression has type \"Union[Dict[Any, Any], List[Any]]\", variable has type \"Dict[Any, Any]\")\r\nFound 2 errors in 1 file (checked 1 source file)\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.13.3\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.8\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3337/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3337/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3331", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3331/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3331/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3331/events", "html_url": "https://github.com/huggingface/datasets/issues/3331", "id": 1065275896, "node_id": "I_kwDODunzps4_ftH4", "number": 3331, "title": "AttributeError: 'CommunityDatasetModuleFactoryWithoutScript' object has no attribute 'path'", "user": {"login": "luozhouyang", "id": 34032031, "node_id": "MDQ6VXNlcjM0MDMyMDMx", "avatar_url": "https://avatars.githubusercontent.com/u/34032031?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luozhouyang", "html_url": "https://github.com/luozhouyang", "followers_url": "https://api.github.com/users/luozhouyang/followers", "following_url": "https://api.github.com/users/luozhouyang/following{/other_user}", "gists_url": "https://api.github.com/users/luozhouyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/luozhouyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luozhouyang/subscriptions", "organizations_url": "https://api.github.com/users/luozhouyang/orgs", "repos_url": "https://api.github.com/users/luozhouyang/repos", "events_url": "https://api.github.com/users/luozhouyang/events{/privacy}", "received_events_url": "https://api.github.com/users/luozhouyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-11-28T08:54:05Z", "updated_at": "2021-11-29T13:49:44Z", "closed_at": "2021-11-29T13:34:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI add a new question answering dataset to huggingface datasets manually. Here is the link: [luozhouyang/question-answering-datasets](https://huggingface.co/datasets/luozhouyang/question-answering-datasets)\r\n\r\nBut when I load the dataset, an error raised: \r\n\r\n```bash\r\nAttributeError: 'CommunityDatasetModuleFactoryWithoutScript' object has no attribute 'path'\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"luozhouyang/question-answering-datasets\", data_files=[\"dureader_robust.train.json\"])\r\n```\r\n\r\n## Expected results\r\nLoad dataset successfully without any error.\r\n\r\n## Actual results\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/mnt/home/zhouyang.lzy/github/naivenlp/naivenlp/tests/question_answering_tests/dataset_test.py\", line 89, in test_load_dataset_with_hf\r\n    data_files=[\"dureader_robust.train.json\"],\r\n  File \"/mnt/home/zhouyang.lzy/.conda/envs/naivenlp/lib/python3.6/site-packages/datasets/load.py\", line 1616, in load_dataset\r\n    **config_kwargs,\r\n  File \"/mnt/home/zhouyang.lzy/.conda/envs/naivenlp/lib/python3.6/site-packages/datasets/load.py\", line 1443, in load_dataset_builder\r\n    path, revision=revision, download_config=download_config, download_mode=download_mode, data_files=data_files\r\n  File \"/mnt/home/zhouyang.lzy/.conda/envs/naivenlp/lib/python3.6/site-packages/datasets/load.py\", line 1157, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/mnt/home/zhouyang.lzy/.conda/envs/naivenlp/lib/python3.6/site-packages/datasets/load.py\", line 1144, in dataset_module_factory\r\n    download_mode=download_mode,\r\n  File \"/mnt/home/zhouyang.lzy/.conda/envs/naivenlp/lib/python3.6/site-packages/datasets/load.py\", line 798, in get_module\r\n    raise FileNotFoundError(f\"No data files or dataset script found in {self.path}\")\r\nAttributeError: 'CommunityDatasetModuleFactoryWithoutScript' object has no attribute 'path'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.15.1\r\n- Platform: linux\r\n- Python version: 3.6.13\r\n- PyArrow version: 6.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3331/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3329", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3329/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3329/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3329/events", "html_url": "https://github.com/huggingface/datasets/issues/3329", "id": 1065096971, "node_id": "I_kwDODunzps4_fBcL", "number": 3329, "title": "Map function: Type error on iter #999", "user": {"login": "josephkready666", "id": 52659318, "node_id": "MDQ6VXNlcjUyNjU5MzE4", "avatar_url": "https://avatars.githubusercontent.com/u/52659318?v=4", "gravatar_id": "", "url": "https://api.github.com/users/josephkready666", "html_url": "https://github.com/josephkready666", "followers_url": "https://api.github.com/users/josephkready666/followers", "following_url": "https://api.github.com/users/josephkready666/following{/other_user}", "gists_url": "https://api.github.com/users/josephkready666/gists{/gist_id}", "starred_url": "https://api.github.com/users/josephkready666/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/josephkready666/subscriptions", "organizations_url": "https://api.github.com/users/josephkready666/orgs", "repos_url": "https://api.github.com/users/josephkready666/repos", "events_url": "https://api.github.com/users/josephkready666/events{/privacy}", "received_events_url": "https://api.github.com/users/josephkready666/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-11-27T17:53:05Z", "updated_at": "2021-11-29T20:40:15Z", "closed_at": "2021-11-29T20:40:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nUsing the map function, it throws a type error on iter #999\r\n\r\nHere is the code I am calling:\r\n```\r\ndataset = datasets.load_dataset('squad')\r\ndataset['validation'].map(text_numbers_to_int, input_columns=['context'], fn_kwargs={'column': 'context'})\r\n``` \r\ntext_numbers_to_int returns the input text with numbers replaced in the format {'context': text}\r\n\r\nIt happens at \r\n`\r\nFile \"C:\\Users\\lonek\\anaconda3\\envs\\ai\\Lib\\site-packages\\datasets\\arrow_writer.py\", line 289, in <listcomp>\r\n    [row[0][col] for row in self.current_examples], type=col_type, try_type=col_try_type, col=col\r\n`\r\n\r\nThe issue is that the list comprehension expects self.current_examples to be type tuple(dict, str), but for some reason 26 out of 1000 of the sefl.current_examples are type tuple(str, str)\r\n\r\nHere is an example of what self.current_examples should be\r\n({'context': 'Super Bowl 50 was an...merals 50.'}, '')\r\n\r\nHere is an example of what self.current_examples are when it throws the error:\r\n('The Panthers used th... Marriott.', '')\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3329/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3327", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3327/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3327/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3327/events", "html_url": "https://github.com/huggingface/datasets/issues/3327", "id": 1064675888, "node_id": "I_kwDODunzps4_daow", "number": 3327, "title": "\"Shape of query is incorrect, it has to be either a 1D array or 2D (1, N)\"", "user": {"login": "eliasws", "id": 19492473, "node_id": "MDQ6VXNlcjE5NDkyNDcz", "avatar_url": "https://avatars.githubusercontent.com/u/19492473?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eliasws", "html_url": "https://github.com/eliasws", "followers_url": "https://api.github.com/users/eliasws/followers", "following_url": "https://api.github.com/users/eliasws/following{/other_user}", "gists_url": "https://api.github.com/users/eliasws/gists{/gist_id}", "starred_url": "https://api.github.com/users/eliasws/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eliasws/subscriptions", "organizations_url": "https://api.github.com/users/eliasws/orgs", "repos_url": "https://api.github.com/users/eliasws/repos", "events_url": "https://api.github.com/users/eliasws/events{/privacy}", "received_events_url": "https://api.github.com/users/eliasws/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-11-26T16:26:36Z", "updated_at": "2021-11-26T16:44:11Z", "closed_at": "2021-11-26T16:44:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nPassing a correctly shaped Numpy-Array to get_nearest_examples leads to the Exception\r\n\r\n\"Shape of query is incorrect, it has to be either a 1D array or 2D (1, N)\"\r\n\r\nProbably the reason for this is a wrongly converted assertion.\r\n\r\n1.15.1:\r\n\r\n`assert len(query.shape) == 1 or (len(query.shape) == 2 and query.shape[0] == 1)`\r\n\r\n1.16.1:\r\n\r\n```\r\n if len(query.shape) != 1 or (len(query.shape) == 2 and query.shape[0] != 1):\r\n            raise ValueError(\"Shape of query is incorrect, it has to be either a 1D array or 2D (1, N)\")\r\n```\r\n\r\n## Steps to reproduce the bug\r\n\r\nfollow the steps described here: https://huggingface.co/course/chapter5/6?fw=tf\r\n\r\n```python\r\n question_embedding.shape # (1, 768)\r\n\r\n scores, samples = embeddings_dataset.get_nearest_examples(\r\n    \"embeddings\", question_embedding, k=5 # Error\r\n)\r\n\r\n# \"Shape of query is incorrect, it has to be either a 1D array or 2D (1, N)\"\r\n```\r\n\r\n## Expected results\r\nShould work without exception\r\n\r\n## Actual results\r\nThrows exception\r\n\r\n## Environment info\r\n- `datasets` version: 1.15.1\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.12\r\n- PyArrow version: 6.0.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3327/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3320", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3320/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3320/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3320/events", "html_url": "https://github.com/huggingface/datasets/issues/3320", "id": 1063531992, "node_id": "I_kwDODunzps4_ZDXY", "number": 3320, "title": "Can't get tatoeba.rus dataset", "user": {"login": "mmg10", "id": 65535131, "node_id": "MDQ6VXNlcjY1NTM1MTMx", "avatar_url": "https://avatars.githubusercontent.com/u/65535131?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mmg10", "html_url": "https://github.com/mmg10", "followers_url": "https://api.github.com/users/mmg10/followers", "following_url": "https://api.github.com/users/mmg10/following{/other_user}", "gists_url": "https://api.github.com/users/mmg10/gists{/gist_id}", "starred_url": "https://api.github.com/users/mmg10/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mmg10/subscriptions", "organizations_url": "https://api.github.com/users/mmg10/orgs", "repos_url": "https://api.github.com/users/mmg10/repos", "events_url": "https://api.github.com/users/mmg10/events{/privacy}", "received_events_url": "https://api.github.com/users/mmg10/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-11-25T12:31:11Z", "updated_at": "2021-11-26T10:30:29Z", "closed_at": "2021-11-26T10:30:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nIt gives an error.\r\n\r\n> FileNotFoundError: Couldn't find file at https://github.com/facebookresearch/LASER/raw/master/data/tatoeba/v1/tatoeba.rus-eng.rus\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndata=load_dataset(\"xtreme\",\"tatoeba.rus\", split=\"validation\")\r\n```\r\n\r\n## Solution\r\nThe library tries to access the **master** branch. In the github repo of facebookresearch, it is in the **main** branch.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3320/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3313", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3313/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3313/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3313/events", "html_url": "https://github.com/huggingface/datasets/issues/3313", "id": 1060933392, "node_id": "I_kwDODunzps4_PI8Q", "number": 3313, "title": "TriviaQA License Mismatch", "user": {"login": "akhilkedia", "id": 16665267, "node_id": "MDQ6VXNlcjE2NjY1MjY3", "avatar_url": "https://avatars.githubusercontent.com/u/16665267?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akhilkedia", "html_url": "https://github.com/akhilkedia", "followers_url": "https://api.github.com/users/akhilkedia/followers", "following_url": "https://api.github.com/users/akhilkedia/following{/other_user}", "gists_url": "https://api.github.com/users/akhilkedia/gists{/gist_id}", "starred_url": "https://api.github.com/users/akhilkedia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akhilkedia/subscriptions", "organizations_url": "https://api.github.com/users/akhilkedia/orgs", "repos_url": "https://api.github.com/users/akhilkedia/repos", "events_url": "https://api.github.com/users/akhilkedia/events{/privacy}", "received_events_url": "https://api.github.com/users/akhilkedia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-11-23T08:00:15Z", "updated_at": "2021-11-29T11:24:21Z", "closed_at": "2021-11-29T11:24:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nTriviaQA Webpage at http://nlp.cs.washington.edu/triviaqa/ says they do not own the copyright to the data. However, Huggingface datasets at https://huggingface.co/datasets/trivia_qa mentions that the dataset is released under Apache License\r\n\r\nIs the License Information on HuggingFace correct?", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3313/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3313/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3310", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3310/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3310/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3310/events", "html_url": "https://github.com/huggingface/datasets/issues/3310", "id": 1060098104, "node_id": "I_kwDODunzps4_L9A4", "number": 3310, "title": "Fatal error condition occurred in aws-c-io", "user": {"login": "Crabzmatic", "id": 31850219, "node_id": "MDQ6VXNlcjMxODUwMjE5", "avatar_url": "https://avatars.githubusercontent.com/u/31850219?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Crabzmatic", "html_url": "https://github.com/Crabzmatic", "followers_url": "https://api.github.com/users/Crabzmatic/followers", "following_url": "https://api.github.com/users/Crabzmatic/following{/other_user}", "gists_url": "https://api.github.com/users/Crabzmatic/gists{/gist_id}", "starred_url": "https://api.github.com/users/Crabzmatic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Crabzmatic/subscriptions", "organizations_url": "https://api.github.com/users/Crabzmatic/orgs", "repos_url": "https://api.github.com/users/Crabzmatic/repos", "events_url": "https://api.github.com/users/Crabzmatic/events{/privacy}", "received_events_url": "https://api.github.com/users/Crabzmatic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 28, "created_at": "2021-11-22T12:27:54Z", "updated_at": "2023-02-08T10:31:05Z", "closed_at": "2021-11-29T22:22:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nFatal error when using the library\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('wikiann', 'en')\r\n```\r\n\r\n## Expected results\r\nNo fatal errors\r\n\r\n## Actual results\r\n```\r\nFatal error condition occurred in D:\\bld\\aws-c-io_1633633258269\\work\\source\\event_loop.c:74: aws_thread_launch(&cleanup_thread, s_event_loop_destroy_async_thread_fn, el_group, &thread_options) == AWS_OP_SUCCESS\r\nExiting Application\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.15.2.dev0\r\n- Platform: Windows-10-10.0.22504-SP0\r\n- Python version: 3.8.12\r\n- PyArrow version: 6.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3310/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3310/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3306", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3306/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3306/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3306/events", "html_url": "https://github.com/huggingface/datasets/issues/3306", "id": 1059185860, "node_id": "I_kwDODunzps4_IeTE", "number": 3306, "title": "nested sequence feature won't encode example if the first item of the outside sequence is an empty list", "user": {"login": "function2-llx", "id": 38486514, "node_id": "MDQ6VXNlcjM4NDg2NTE0", "avatar_url": "https://avatars.githubusercontent.com/u/38486514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/function2-llx", "html_url": "https://github.com/function2-llx", "followers_url": "https://api.github.com/users/function2-llx/followers", "following_url": "https://api.github.com/users/function2-llx/following{/other_user}", "gists_url": "https://api.github.com/users/function2-llx/gists{/gist_id}", "starred_url": "https://api.github.com/users/function2-llx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/function2-llx/subscriptions", "organizations_url": "https://api.github.com/users/function2-llx/orgs", "repos_url": "https://api.github.com/users/function2-llx/repos", "events_url": "https://api.github.com/users/function2-llx/events{/privacy}", "received_events_url": "https://api.github.com/users/function2-llx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-11-20T16:57:54Z", "updated_at": "2021-12-08T13:02:15Z", "closed_at": "2021-12-08T13:02:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nAs the title, nested sequence feature won't encode example if the first item of the outside sequence is an empty list.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import Features, Sequence, ClassLabel\r\nfeatures = Features({\r\n    'x': Sequence(Sequence(ClassLabel(names=['a', 'b']))),\r\n})\r\nprint(features.encode_batch({\r\n    'x': [\r\n        [['a'], ['b']],\r\n        [[], ['b']],\r\n    ]\r\n}))\r\n```\r\n\r\n## Expected results\r\nprint `{'x': [[[0], [1]], [[], ['1']]]}`\r\n\r\n## Actual results\r\nprint `{'x': [[[0], [1]], [[], ['b']]]}`\r\n\r\n## Environment info\r\n- `datasets` version: 1.15.1\r\n- Platform: Linux-5.13.0-21-generic-x86_64-with-glibc2.34\r\n- Python version: 3.9.7\r\n- PyArrow version: 6.0.0\r\n\r\n## Additional information\r\nI think the issue stems from [here](https://github.com/huggingface/datasets/blob/8555197a3fe826e98bd0206c2d031c4488c53c5c/src/datasets/features/features.py#L847-L848).\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3306/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3306/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3304", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3304/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3304/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3304/events", "html_url": "https://github.com/huggingface/datasets/issues/3304", "id": 1059130494, "node_id": "I_kwDODunzps4_IQx-", "number": 3304, "title": "Dataset object has no attribute `to_tf_dataset`", "user": {"login": "RajkumarGalaxy", "id": 59993678, "node_id": "MDQ6VXNlcjU5OTkzNjc4", "avatar_url": "https://avatars.githubusercontent.com/u/59993678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RajkumarGalaxy", "html_url": "https://github.com/RajkumarGalaxy", "followers_url": "https://api.github.com/users/RajkumarGalaxy/followers", "following_url": "https://api.github.com/users/RajkumarGalaxy/following{/other_user}", "gists_url": "https://api.github.com/users/RajkumarGalaxy/gists{/gist_id}", "starred_url": "https://api.github.com/users/RajkumarGalaxy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RajkumarGalaxy/subscriptions", "organizations_url": "https://api.github.com/users/RajkumarGalaxy/orgs", "repos_url": "https://api.github.com/users/RajkumarGalaxy/repos", "events_url": "https://api.github.com/users/RajkumarGalaxy/events{/privacy}", "received_events_url": "https://api.github.com/users/RajkumarGalaxy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-11-20T12:03:59Z", "updated_at": "2021-11-21T07:07:25Z", "closed_at": "2021-11-21T07:07:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am following HuggingFace Course. I am at Fine-tuning a model. \r\nLink: https://huggingface.co/course/chapter3/2?fw=tf\r\n\r\nI use tokenize_function and `map` as mentioned in the course to process data.\r\n\r\n`# define a tokenize function`\r\n`def Tokenize_function(example):`\r\n`    return tokenizer(example['sentence'], truncation=True)`\r\n\r\n`# tokenize entire data`\r\n`tokenized_data = raw_data.map(Tokenize_function, batched=True)`\r\n\r\nI get Dataset object at this point. When I try converting this to a TF dataset object as mentioned in the course, it throws the following error.\r\n\r\n`# convert to TF dataset`\r\n`train_data = tokenized_data[\"train\"].to_tf_dataset( `\r\n`   columns = ['attention_mask', 'input_ids', 'token_type_ids'], `\r\n`    label_cols = ['label'], `\r\n`    shuffle = True, `\r\n`    collate_fn = data_collator, `\r\n`    batch_size = 8 `\r\n`)`\r\n\r\nOutput:\r\n\r\n`---------------------------------------------------------------------------`\r\n`AttributeError                            Traceback (most recent call last)`\r\n`/tmp/ipykernel_42/103099799.py in <module>`\r\n`      1 # convert to TF dataset`\r\n`----> 2 train_data = tokenized_data[\"train\"].to_tf_dataset( \\`\r\n`      3     columns = ['attention_mask', 'input_ids', 'token_type_ids'], \\`\r\n`      4     label_cols = ['label'], \\`\r\n`      5     shuffle = True, \\`\r\n`AttributeError: 'Dataset' object has no attribute 'to_tf_dataset'`\r\n\r\nWhen I look for `dir(tokenized_data[\"train\"])`, there is no method or attribute in the name of `to_tf_dataset`.\r\n\r\nWhy do I get this error? And how to clear this?\r\n\r\nPlease help me.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3304/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3304/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3303", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3303/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3303/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3303/events", "html_url": "https://github.com/huggingface/datasets/issues/3303", "id": 1059129732, "node_id": "I_kwDODunzps4_IQmE", "number": 3303, "title": "DataCollatorWithPadding: TypeError", "user": {"login": "RajkumarGalaxy", "id": 59993678, "node_id": "MDQ6VXNlcjU5OTkzNjc4", "avatar_url": "https://avatars.githubusercontent.com/u/59993678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RajkumarGalaxy", "html_url": "https://github.com/RajkumarGalaxy", "followers_url": "https://api.github.com/users/RajkumarGalaxy/followers", "following_url": "https://api.github.com/users/RajkumarGalaxy/following{/other_user}", "gists_url": "https://api.github.com/users/RajkumarGalaxy/gists{/gist_id}", "starred_url": "https://api.github.com/users/RajkumarGalaxy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RajkumarGalaxy/subscriptions", "organizations_url": "https://api.github.com/users/RajkumarGalaxy/orgs", "repos_url": "https://api.github.com/users/RajkumarGalaxy/repos", "events_url": "https://api.github.com/users/RajkumarGalaxy/events{/privacy}", "received_events_url": "https://api.github.com/users/RajkumarGalaxy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-11-20T11:59:55Z", "updated_at": "2021-11-21T07:05:37Z", "closed_at": "2021-11-21T07:05:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI am following the HuggingFace course. I am now at Fine-tuning [https://huggingface.co/course/chapter3/3?fw=tf](https://huggingface.co/course/chapter3/3?fw=tf).  When I set up `DataCollatorWithPadding` as following I got an error while trying to reproduce the course code in Kaggle. This error occurs with either a CPU-only-device or a GPU-device.\r\n\r\nInput:\r\n```checkpoint = 'bert-base-uncased'\r\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\r\n```\r\n\r\nOutput:\r\n```---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/tmp/ipykernel_42/1563280798.py in <module>\r\n      1 checkpoint = 'bert-base-uncased'\r\n      2 tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n----> 3 data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\r\nTypeError: __init__() got an unexpected keyword argument 'return_tensors'\r\n```\r\n\r\nWhen I call `help` method, it too confirms that there is no argument `return_tensors`.\r\nInput:\r\n```\r\nhelp(DataCollatorWithPadding.__init__)\r\n```\r\nOutput:\r\n```\r\nHelp on function __init__ in module transformers.data.data_collator:\r\n\r\n__init__(self, tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = True, max_length: Union[int, NoneType] = None, pad_to_multiple_of: Union[int, NoneType] = None) -> None\r\n```\r\n\r\nBut, the source file *[Data Collator - docs](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorwithpadding)* says that there is such an argument. By default, it returns Pytorch tensors while I need TF tensors.\r\n\r\nWhere do I miss?\r\nPlease help me.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3303/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3295", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3295/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3295/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3295/events", "html_url": "https://github.com/huggingface/datasets/issues/3295", "id": 1057954892, "node_id": "I_kwDODunzps4_DxxM", "number": 3295, "title": "Temporary dataset_path for remote fs URIs not built properly in arrow_dataset.py::load_from_disk", "user": {"login": "francisco-perez-sorrosal", "id": 918006, "node_id": "MDQ6VXNlcjkxODAwNg==", "avatar_url": "https://avatars.githubusercontent.com/u/918006?v=4", "gravatar_id": "", "url": "https://api.github.com/users/francisco-perez-sorrosal", "html_url": "https://github.com/francisco-perez-sorrosal", "followers_url": "https://api.github.com/users/francisco-perez-sorrosal/followers", "following_url": "https://api.github.com/users/francisco-perez-sorrosal/following{/other_user}", "gists_url": "https://api.github.com/users/francisco-perez-sorrosal/gists{/gist_id}", "starred_url": "https://api.github.com/users/francisco-perez-sorrosal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/francisco-perez-sorrosal/subscriptions", "organizations_url": "https://api.github.com/users/francisco-perez-sorrosal/orgs", "repos_url": "https://api.github.com/users/francisco-perez-sorrosal/repos", "events_url": "https://api.github.com/users/francisco-perez-sorrosal/events{/privacy}", "received_events_url": "https://api.github.com/users/francisco-perez-sorrosal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-11-18T23:24:02Z", "updated_at": "2021-12-06T10:45:04Z", "closed_at": "2021-12-06T10:45:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen trying to build a temporary dataset path from a remote URI in this block of code:\r\n\r\nhttps://github.com/huggingface/datasets/blob/42f6b1d18a4a1b6009b6e62d115491be16dfca22/src/datasets/arrow_dataset.py#L1038-L1042\r\n\r\nthe result is not the expected when passing an absolute path in an URI like `hdfs:///absolute/path`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset_path = \"hdfs:///absolute/path\"\r\nsrc_dataset_path = extract_path_from_uri(dataset_path)\r\ntmp_dir = get_temporary_cache_files_directory()\r\ndataset_path = Path(tmp_dir, src_dataset_path)\r\nprint(dataset_path)\r\n```\r\n\r\n## Expected results\r\nWith the code above, we would expect a value in `dataset_path` similar to:\r\n`/tmp/tmpnwxyvao5/absolute/path`\r\n\r\n## Actual results\r\nHowever, we get a `dataset_path` value like:\r\n`/absolute/path`\r\n\r\nThis is because this line here: https://github.com/huggingface/datasets/blob/42f6b1d18a4a1b6009b6e62d115491be16dfca22/src/datasets/arrow_dataset.py#L1041\r\nreturns the last absolute path when two absolute paths (the one in `tmp_dir` and the one extracted from the URI in `src_dataset_path`) are passed as arguments.\r\n\r\n## Environment info\r\n- `datasets` version: 1.13.3\r\n- Platform: Linux-3.10.0-1160.15.2.el7.x86_64-x86_64-with-glibc2.33\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3295/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3295/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3292", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3292/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3292/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3292/events", "html_url": "https://github.com/huggingface/datasets/issues/3292", "id": 1056962554, "node_id": "I_kwDODunzps4-__f6", "number": 3292, "title": "Not able to load 'wikipedia' dataset", "user": {"login": "abhibisht89", "id": 13541524, "node_id": "MDQ6VXNlcjEzNTQxNTI0", "avatar_url": "https://avatars.githubusercontent.com/u/13541524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhibisht89", "html_url": "https://github.com/abhibisht89", "followers_url": "https://api.github.com/users/abhibisht89/followers", "following_url": "https://api.github.com/users/abhibisht89/following{/other_user}", "gists_url": "https://api.github.com/users/abhibisht89/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhibisht89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhibisht89/subscriptions", "organizations_url": "https://api.github.com/users/abhibisht89/orgs", "repos_url": "https://api.github.com/users/abhibisht89/repos", "events_url": "https://api.github.com/users/abhibisht89/events{/privacy}", "received_events_url": "https://api.github.com/users/abhibisht89/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-11-18T05:41:18Z", "updated_at": "2021-11-19T16:49:29Z", "closed_at": "2021-11-19T16:49:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am following the instruction for loading the wikipedia dataset using datasets. However getting the below error.\r\n\r\n## Steps to reproduce the bug\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"wikipedia\")\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/datasets/builder.py in _create_builder_config(self, name, custom_features, **config_kwargs)\r\n    339                         \"Config name is missing.\"\r\n    340                         \"\\nPlease pick one among the available configs: %s\" % list(self.builder_configs.keys())\r\n--> 341                         + \"\\nExample of usage:\\n\\t`{}`\".format(example_of_usage)\r\n    342                     )\r\n    343                 builder_config = self.BUILDER_CONFIGS[0]\r\n\r\nValueError: Config name is missing.\r\nPlease pick one among the available configs: ['20200501.aa', '20200501.ab', '20200501.ace', '20200501.ady', '20200501.af', '20200501.ak', '20200501.als', '20200501.am', '20200501.an', '20200501.ang', '20200501.ar', '20200501.arc', '20200501.arz', '20200501.as', '20200501.ast', '20200501.atj', '20200501.av', '20200501.ay', '20200501.az', '20200501.azb', '20200501.ba', '20200501.bar', '20200501.bat-smg', '20200501.bcl', '20200501.be', '20200501.be-x-old', '20200501.bg', '20200501.bh', '20200501.bi', '20200501.bjn', '20200501.bm', '20200501.bn', '20200501.bo', '20200501.bpy', '20200501.br', '20200501.bs', '20200501.bug', '20200501.bxr', '20200501.ca', '20200501.cbk-zam', '20200501.cdo', '20200501.ce', '20200501.ceb', '20200501.ch', '20200501.cho', '20200501.chr', '20200501.chy', '20200501.ckb', '20200501.co', '20200501.cr', '20200501.crh', '20200501.cs', '20200501.csb', '20200501.cu', '20200501.cv', '20200501.cy', '20200501.da', '20200501.de', '20200501.din', '20200501.diq', '20200501.dsb', '20200501.dty', '20200501.dv', '20200501.dz', '20200501.ee', '20200501.el', '20200501.eml', '20200501.en', '20200501.eo', '20200501.es', '20200501.et', '20200501.eu', '20200501.ext', '20200501.fa', '20200501.ff', '20200501.fi', '20200501.fiu-vro', '20200501.fj', '20200501.fo', '20200501.fr', '20200501.frp', '20200501.frr', '20200501.fur', '20200501.fy', '20200501.ga', '20200501.gag', '20200501.gan', '20200501.gd', '20200501.gl', '20200501.glk', '20200501.gn', '20200501.gom', '20200501.gor', '20200501.got', '20200501.gu', '20200501.gv', '20200501.ha', '20200501.hak', '20200501.haw', '20200501.he', '20200501.hi', '20200501.hif', '20200501.ho', '20200501.hr', '20200501.hsb', '20200501.ht', '20200501.hu', '20200501.hy', '20200501.ia', '20200501.id', '20200501.ie', '20200501.ig', '20200501.ii', '20200501.ik', '20200501.ilo', '20200501.inh', '20200501.io', '20200501.is', '20200501.it', '20200501.iu', '20200501.ja', '20200501.jam', '20200501.jbo', '20200501.jv', '20200501.ka', '20200501.kaa', '20200501.kab', '20200501.kbd', '20200501.kbp', '20200501.kg', '20200501.ki', '20200501.kj', '20200501.kk', '20200501.kl', '20200501.km', '20200501.kn', '20200501.ko', '20200501.koi', '20200501.krc', '20200501.ks', '20200501.ksh', '20200501.ku', '20200501.kv', '20200501.kw', '20200501.ky', '20200501.la', '20200501.lad', '20200501.lb', '20200501.lbe', '20200501.lez', '20200501.lfn', '20200501.lg', '20200501.li', '20200501.lij', '20200501.lmo', '20200501.ln', '20200501.lo', '20200501.lrc', '20200501.lt', '20200501.ltg', '20200501.lv', '20200501.mai', '20200501.map-bms', '20200501.mdf', '20200501.mg', '20200501.mh', '20200501.mhr', '20200501.mi', '20200501.min', '20200501.mk', '20200501.ml', '20200501.mn', '20200501.mr', '20200501.mrj', '20200501.ms', '20200501.mt', '20200501.mus', '20200501.mwl', '20200501.my', '20200501.myv', '20200501.mzn', '20200501.na', '20200501.nah', '20200501.nap', '20200501.nds', '20200501.nds-nl', '20200501.ne', '20200501.new', '20200501.ng', '20200501.nl', '20200501.nn', '20200501.no', '20200501.nov', '20200501.nrm', '20200501.nso', '20200501.nv', '20200501.ny', '20200501.oc', '20200501.olo', '20200501.om', '20200501.or', '20200501.os', '20200501.pa', '20200501.pag', '20200501.pam', '20200501.pap', '20200501.pcd', '20200501.pdc', '20200501.pfl', '20200501.pi', '20200501.pih', '20200501.pl', '20200501.pms', '20200501.pnb', '20200501.pnt', '20200501.ps', '20200501.pt', '20200501.qu', '20200501.rm', '20200501.rmy', '20200501.rn', '20200501.ro', '20200501.roa-rup', '20200501.roa-tara', '20200501.ru', '20200501.rue', '20200501.rw', '20200501.sa', '20200501.sah', '20200501.sat', '20200501.sc', '20200501.scn', '20200501.sco', '20200501.sd', '20200501.se', '20200501.sg', '20200501.sh', '20200501.si', '20200501.simple', '20200501.sk', '20200501.sl', '20200501.sm', '20200501.sn', '20200501.so', '20200501.sq', '20200501.sr', '20200501.srn', '20200501.ss', '20200501.st', '20200501.stq', '20200501.su', '20200501.sv', '20200501.sw', '20200501.szl', '20200501.ta', '20200501.tcy', '20200501.te', '20200501.tet', '20200501.tg', '20200501.th', '20200501.ti', '20200501.tk', '20200501.tl', '20200501.tn', '20200501.to', '20200501.tpi', '20200501.tr', '20200501.ts', '20200501.tt', '20200501.tum', '20200501.tw', '20200501.ty', '20200501.tyv', '20200501.udm', '20200501.ug', '20200501.uk', '20200501.ur', '20200501.uz', '20200501.ve', '20200501.vec', '20200501.vep', '20200501.vi', '20200501.vls', '20200501.vo', '20200501.wa', '20200501.war', '20200501.wo', '20200501.wuu', '20200501.xal', '20200501.xh', '20200501.xmf', '20200501.yi', '20200501.yo', '20200501.za', '20200501.zea', '20200501.zh', '20200501.zh-classical', '20200501.zh-min-nan', '20200501.zh-yue', '20200501.zu']\r\nExample of usage:\r\n\t`load_dataset('wikipedia', '20200501.aa')`\r\n\r\nI think the other parameter is missing in the load_dataset function that is not shown in the instruction.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3292/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3292/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3273", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3273/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3273/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3273/events", "html_url": "https://github.com/huggingface/datasets/issues/3273", "id": 1053554038, "node_id": "I_kwDODunzps4-y_V2", "number": 3273, "title": "Respect row ordering when concatenating datasets along axis=1", "user": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-11-15T11:27:14Z", "updated_at": "2021-11-17T15:41:11Z", "closed_at": "2021-11-17T15:41:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Currently, there is a bug when concatenating datasets along `axis=1` if more than one dataset has the `_indices` attribute defined. In that scenario, all indices mappings except the first one get ignored.\r\n\r\nA minimal reproducible example:\r\n```python\r\n>>> from datasets import Dataset, concatenate_datasets\r\n>>> a = Dataset.from_dict({\"a\": [30, 20, 10]})\r\n>>> b = Dataset.from_dict({\"b\": [2, 1, 3]})\r\n>>> d = concatenate_datasets([a.sort(\"a\"), b.sort(\"b\")], axis=1)\r\n>>> print(d[:3])              # expected: {'a': [10, 20, 30], 'b': [1, 2, 3]}\r\n{'a': [10, 20, 30], 'b': [3, 1, 2]}\r\n```\r\n\r\nI've noticed the bug while working on #3195.   ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3273/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3269", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3269/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3269/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3269/events", "html_url": "https://github.com/huggingface/datasets/issues/3269", "id": 1053218769, "node_id": "I_kwDODunzps4-xtfR", "number": 3269, "title": "coqa NonMatchingChecksumError", "user": {"login": "ZhaofengWu", "id": 11954789, "node_id": "MDQ6VXNlcjExOTU0Nzg5", "avatar_url": "https://avatars.githubusercontent.com/u/11954789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZhaofengWu", "html_url": "https://github.com/ZhaofengWu", "followers_url": "https://api.github.com/users/ZhaofengWu/followers", "following_url": "https://api.github.com/users/ZhaofengWu/following{/other_user}", "gists_url": "https://api.github.com/users/ZhaofengWu/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZhaofengWu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZhaofengWu/subscriptions", "organizations_url": "https://api.github.com/users/ZhaofengWu/orgs", "repos_url": "https://api.github.com/users/ZhaofengWu/repos", "events_url": "https://api.github.com/users/ZhaofengWu/events{/privacy}", "received_events_url": "https://api.github.com/users/ZhaofengWu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 18, "created_at": "2021-11-15T05:04:07Z", "updated_at": "2022-01-19T13:58:19Z", "closed_at": "2022-01-19T13:58:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "```\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"coqa\")\r\nDownloading: 3.82kB [00:00, 1.26MB/s]                                                                                                                                                                                                                       \r\nDownloading: 1.79kB [00:00, 733kB/s]                                                                                                                                                                                                                        \r\nUsing custom data configuration default\r\nDownloading and preparing dataset coqa/default (download: 55.40 MiB, generated: 18.35 MiB, post-processed: Unknown size, total: 73.75 MiB) to /Users/zhaofengw/.cache/huggingface/datasets/coqa/default/1.0.0/553ce70bfdcd15ff4b5f4abc4fc2f37137139cde1f58f4f60384a53a327716f0...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:00<00:00, 1.38MB/s]\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 222/222 [00:00<00:00, 1.32MB/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.91it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 1117.44it/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/zhaofengw/miniconda3/lib/python3.9/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Users/zhaofengw/miniconda3/lib/python3.9/site-packages/datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/zhaofengw/miniconda3/lib/python3.9/site-packages/datasets/builder.py\", line 679, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/Users/zhaofengw/miniconda3/lib/python3.9/site-packages/datasets/utils/info_utils.py\", line 40, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json', 'https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json']\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3269/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3265", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3265/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3265/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3265/events", "html_url": "https://github.com/huggingface/datasets/issues/3265", "id": 1052666558, "node_id": "I_kwDODunzps4-vmq-", "number": 3265, "title": "Checksum error for kilt_task_wow", "user": {"login": "slyviacassell", "id": 22296717, "node_id": "MDQ6VXNlcjIyMjk2NzE3", "avatar_url": "https://avatars.githubusercontent.com/u/22296717?v=4", "gravatar_id": "", "url": "https://api.github.com/users/slyviacassell", "html_url": "https://github.com/slyviacassell", "followers_url": "https://api.github.com/users/slyviacassell/followers", "following_url": "https://api.github.com/users/slyviacassell/following{/other_user}", "gists_url": "https://api.github.com/users/slyviacassell/gists{/gist_id}", "starred_url": "https://api.github.com/users/slyviacassell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/slyviacassell/subscriptions", "organizations_url": "https://api.github.com/users/slyviacassell/orgs", "repos_url": "https://api.github.com/users/slyviacassell/repos", "events_url": "https://api.github.com/users/slyviacassell/events{/privacy}", "received_events_url": "https://api.github.com/users/slyviacassell/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-11-13T12:04:17Z", "updated_at": "2021-11-16T11:23:53Z", "closed_at": "2021-11-16T11:21:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nChecksum failed when downloads kilt_tasks_wow. See error output for details.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\ndatasets.load_datasets('kilt_tasks','wow')\r\n```\r\n\r\n## Expected results\r\nDownload successful\r\n\r\n## Actual results\r\n```\r\nDownloading and preparing dataset kilt_tasks/wow (download: 72.07 MiB, generated: 61.82 MiB, post-processed: Unknown size, total: 133.89 MiB) to /root/.cache/huggingface/datasets/kilt_tasks/wow/1.0.0/57dc8b2431e76637e0c6ef79689ca4af61ed3a330e2e0cd62c8971465a35db3a...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 5121.25it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 1527.42it/s]\r\nTraceback (most recent call last):\r\n  File \"kilt_wow.py\", line 30, in <module>\r\n    main()\r\n  File \"kilt_wow.py\", line 27, in main\r\n    train, dev, test = dataset.generate_k_shot_data(k=32, seed=seed, path=\"../data/\")\r\n  File \"/workspace/projects/CrossFit/tasks/fewshot_gym_dataset.py\", line 79, in generate_k_shot_data\r\n    dataset = self.load_dataset()\r\n  File \"kilt_wow.py\", line 21, in load_dataset\r\n    return datasets.load_dataset('kilt_tasks','wow')\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 679, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/info_utils.py\", line 40, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['http://dl.fbaipublicfiles.com/KILT/wow-train-kilt.jsonl', 'http://dl.fbaipublicfiles.com/KILT/wow-dev-kilt.jsonl']\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.15.1\r\n- Platform: Linux-4.15.0-161-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.3\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3265/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3265/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3264", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3264/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3264/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3264/events", "html_url": "https://github.com/huggingface/datasets/issues/3264", "id": 1052663513, "node_id": "I_kwDODunzps4-vl7Z", "number": 3264, "title": "Downloading URL change for WikiAuto Manual, jeopardy and definite_pronoun_resolution", "user": {"login": "slyviacassell", "id": 22296717, "node_id": "MDQ6VXNlcjIyMjk2NzE3", "avatar_url": "https://avatars.githubusercontent.com/u/22296717?v=4", "gravatar_id": "", "url": "https://api.github.com/users/slyviacassell", "html_url": "https://github.com/slyviacassell", "followers_url": "https://api.github.com/users/slyviacassell/followers", "following_url": "https://api.github.com/users/slyviacassell/following{/other_user}", "gists_url": "https://api.github.com/users/slyviacassell/gists{/gist_id}", "starred_url": "https://api.github.com/users/slyviacassell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/slyviacassell/subscriptions", "organizations_url": "https://api.github.com/users/slyviacassell/orgs", "repos_url": "https://api.github.com/users/slyviacassell/repos", "events_url": "https://api.github.com/users/slyviacassell/events{/privacy}", "received_events_url": "https://api.github.com/users/slyviacassell/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-11-13T11:47:12Z", "updated_at": "2022-06-01T17:38:16Z", "closed_at": "2022-06-01T17:38:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n- WikiAuto Manual  \r\nThe original manual datasets with the following downloading URL in this [repository](https://github.com/chaojiang06/wiki-auto) was [deleted](https://github.com/chaojiang06/wiki-auto/commit/0af9b066f2b4e02726fb8a9be49283c0ad25367f) by the author.  \r\n```\r\nhttps://github.com/chaojiang06/wiki-auto/raw/master/wiki-manual/train.tsv\r\n```\r\n\r\n- jeopardy  \r\nThe downloading URL for jeopardy may move from \r\n```\r\nhttp://skeeto.s3.amazonaws.com/share/JEOPARDY_QUESTIONS1.json.gz\r\n```\r\n to \r\n```\r\nhttps://drive.google.com/file/d/0BwT5wj_P7BKXb2hfM3d2RHU1ckE/view?resourcekey=0-1abK4cJq-mqxFoSg86ieIg\r\n```\r\n\r\n- definite_pronoun_resolution\r\nThe following downloading URL for definite_pronoun_resolution cannot be reached for some reasons.\r\n```\r\nhttp://www.hlt.utdallas.edu/~vince/data/emnlp12/train.c.txt\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\ndatasets.load_datasets('wiki_auto','manual')\r\ndatasets.load_datasets('jeopardy')\r\ndatasets.load_datasets('definite_pronoun_resolution')\r\n```\r\n\r\n## Expected results\r\nDownload successfully \r\n\r\n## Actual results\r\n- WikiAuto Manual  \r\n```\r\nDownloading and preparing dataset wiki_auto/manual (download: 151.65 MiB, generated: 155.97 MiB, post-processed: Unknown size, total: 307.61 MiB) to /root/.cache/huggingface/datasets/wiki_auto/manual/1.0.0/5ffdd9fc62422d29bd02675fb9606f77c1251ee17169ac10b143ce07ef2f4db8...\r\n  0%|                                                                                                                                                                                                                      | 0/3 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"wiki_auto.py\", line 43, in <module>\r\n    main()\r\n  File \"wiki_auto.py\", line 40, in main\r\n    train, dev, test = dataset.generate_k_shot_data(k=16, seed=seed, path=\"../data/\")\r\n  File \"/workspace/projects/CrossFit/tasks/fewshot_gym_dataset.py\", line 24, in generate_k_shot_data\r\n    dataset = self.load_dataset()\r\n  File \"wiki_auto.py\", line 34, in load_dataset\r\n    return datasets.load_dataset('wiki_auto', 'manual')\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 675, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/wiki_auto/5ffdd9fc62422d29bd02675fb9606f77c1251ee17169ac10b143ce07ef2f4db8/wiki_auto.py\", line 193, in _split_generators\r\n    data_dir = dl_manager.download_and_extract(my_urls)\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 284, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 196, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 216, in map_nested\r\n    mapped = [\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 217, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True))\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 152, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 217, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 295, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 592, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://github.com/chaojiang06/wiki-auto/raw/master/wiki-manual/train.tsv\r\n```\r\n- jeopardy\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset jeopardy/default (download: 12.13 MiB, generated: 34.46 MiB, post-processed: Unknown size, total: 46.59 MiB) to /root/.cache/huggingface/datasets/jeopardy/default/0.1.0/25ee3e4a73755e637b8810f6493fd36e4523dea3ca8a540529d0a6e24c7f9810...\r\nTraceback (most recent call last):\r\n  File \"jeopardy.py\", line 45, in <module>\r\n    main()\r\n  File \"jeopardy.py\", line 42, in main\r\n    train, dev, test = dataset.generate_k_shot_data(k=32, seed=seed, path=\"../data/\")\r\n  File \"/workspace/projects/CrossFit/tasks/fewshot_gym_dataset.py\", line 79, in generate_k_shot_data\r\n    dataset = self.load_dataset()\r\n  File \"jeopardy.py\", line 36, in load_dataset\r\n    return datasets.load_dataset(\"jeopardy\")\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 675, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/jeopardy/25ee3e4a73755e637b8810f6493fd36e4523dea3ca8a540529d0a6e24c7f9810/jeopardy.py\", line 72, in _split_generators\r\n    filepath = dl_manager.download_and_extract(_DATA_URL)\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 284, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 196, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 206, in map_nested\r\n    return function(data_struct)\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 217, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 295, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 594, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach http://skeeto.s3.amazonaws.com/share/JEOPARDY_QUESTIONS1.json.gz\r\n```\r\n- definite_pronoun_resolution\r\n```\r\nDownloading and preparing dataset definite_pronoun_resolution/plain_text (download: 222.12 KiB, generated: 239.12 KiB, post-processed: Unknown size, total: 461.24 KiB) to /root/.cache/huggingface/datasets/definite_pronoun_resolution/plain_text/1.0.0/35a1dfd4fba4afb8ba226cbbb65ac7cef0dd3cf9302d8f803740f05d2f16ceff...\r\n  0%|                                                                                                                                                                                                                      | 0/2 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"definite_pronoun_resolution.py\", line 37, in <module>\r\n    main()\r\n  File \"definite_pronoun_resolution.py\", line 34, in main\r\n    train, dev, test = dataset.generate_k_shot_data(k=32, seed=seed, path=\"../data/\")\r\n  File \"/workspace/projects/CrossFit/tasks/fewshot_gym_dataset.py\", line 79, in generate_k_shot_data\r\n    dataset = self.load_dataset()\r\n  File \"definite_pronoun_resolution.py\", line 28, in load_dataset\r\n    return datasets.load_dataset('definite_pronoun_resolution')\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 675, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/definite_pronoun_resolution/35a1dfd4fba4afb8ba226cbbb65ac7cef0dd3cf9302d8f803740f05d2f16ceff/definite_pronoun_resolution.py\", line 76, in _split_generators\r\n    files = dl_manager.download_and_extract(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 284, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 196, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 216, in map_nested\r\n    mapped = [\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 217, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True))\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 152, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 217, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 295, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/opt/conda/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 594, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach http://www.hlt.utdallas.edu/~vince/data/emnlp12/train.c.txt\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.15.1\r\n- Platform: Linux-4.15.0-161-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.3\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3264/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3264/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3255", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3255/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3255/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3255/events", "html_url": "https://github.com/huggingface/datasets/issues/3255", "id": 1051783129, "node_id": "I_kwDODunzps4-sO_Z", "number": 3255, "title": "SciELO dataset ConnectionError", "user": {"login": "WojciechKusa", "id": 2575047, "node_id": "MDQ6VXNlcjI1NzUwNDc=", "avatar_url": "https://avatars.githubusercontent.com/u/2575047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WojciechKusa", "html_url": "https://github.com/WojciechKusa", "followers_url": "https://api.github.com/users/WojciechKusa/followers", "following_url": "https://api.github.com/users/WojciechKusa/following{/other_user}", "gists_url": "https://api.github.com/users/WojciechKusa/gists{/gist_id}", "starred_url": "https://api.github.com/users/WojciechKusa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WojciechKusa/subscriptions", "organizations_url": "https://api.github.com/users/WojciechKusa/orgs", "repos_url": "https://api.github.com/users/WojciechKusa/repos", "events_url": "https://api.github.com/users/WojciechKusa/events{/privacy}", "received_events_url": "https://api.github.com/users/WojciechKusa/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-11-12T09:57:14Z", "updated_at": "2021-11-16T17:55:22Z", "closed_at": "2021-11-16T17:55:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI get `ConnectionError` when I am trying to load the SciELO dataset. \r\n\r\n\r\nWhen I try the URL with `requests` I get:\r\n```\r\n>>> requests.head(\"https://ndownloader.figstatic.com/files/14019287\")\r\n<Response [302]>\r\n```\r\nAnd as far as I understand redirections in `datasets` are not supported for downloads. \r\nhttps://github.com/huggingface/datasets/blob/807341d0db0728073ab605c812c67f927d148f38/datasets/scielo/scielo.py#L45 \r\n\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"scielo\", \"en-es\")\r\n```\r\n\r\n## Expected results\r\nDownload SciELO dataset and load Dataset object\r\n\r\n\r\n## Actual results\r\n\r\n```\r\nDownloading and preparing dataset scielo/en-es (download: 21.90 MiB, generated: 68.45 MiB, post-processed: Unknown size, total: 90.35 MiB) to /Users/test/.cache/huggingface/datasets/scielo/en-es/1.0.0/7e05d55a20257efeb9925ff5de65bd4884fc6ddb6d765f1ea3e8860449d90e0e...\r\nTraceback (most recent call last):\r\n  File \"scielo.py\", line 3, in <module>\r\n    dataset = load_dataset(\"scielo\", \"en-es\")\r\n  File \"../lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"../lib/python3.8/site-packages/datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"../lib/python3.8/site-packages/datasets/builder.py\", line 675, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/Users/test/.cache/huggingface/modules/datasets_modules/datasets/scielo/7e05d55a20257efeb9925ff5de65bd4884fc6ddb6d765f1ea3e8860449d90e0e/scielo.py\", line 77, in _split_generators\r\n    data_dir = dl_manager.download_and_extract(_URLS[self.config.name])\r\n  File \"../lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 284, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"../lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 196, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"../lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 206, in map_nested\r\n    return function(data_struct)\r\n  File \"../lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 217, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"../lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 295, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"../lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 594, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://ndownloader.figstatic.com/files/14019287\r\n\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.15.1\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.12\r\n- PyArrow version: 6.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3255/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3253", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3253/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3253/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3253/events", "html_url": "https://github.com/huggingface/datasets/issues/3253", "id": 1051308972, "node_id": "I_kwDODunzps4-qbOs", "number": 3253, "title": "`GeneratorBasedBuilder` does not support `None` values", "user": {"login": "pavel-lexyr", "id": 69010336, "node_id": "MDQ6VXNlcjY5MDEwMzM2", "avatar_url": "https://avatars.githubusercontent.com/u/69010336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pavel-lexyr", "html_url": "https://github.com/pavel-lexyr", "followers_url": "https://api.github.com/users/pavel-lexyr/followers", "following_url": "https://api.github.com/users/pavel-lexyr/following{/other_user}", "gists_url": "https://api.github.com/users/pavel-lexyr/gists{/gist_id}", "starred_url": "https://api.github.com/users/pavel-lexyr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pavel-lexyr/subscriptions", "organizations_url": "https://api.github.com/users/pavel-lexyr/orgs", "repos_url": "https://api.github.com/users/pavel-lexyr/repos", "events_url": "https://api.github.com/users/pavel-lexyr/events{/privacy}", "received_events_url": "https://api.github.com/users/pavel-lexyr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-11-11T19:51:21Z", "updated_at": "2021-12-09T14:26:58Z", "closed_at": "2021-12-09T14:26:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n`GeneratorBasedBuilder` does not support `None` values.\r\n\r\n## Steps to reproduce the bug\r\nSee [this repository](https://github.com/pavel-lexyr/huggingface-datasets-bug-reproduction) for minimal reproduction.\r\n\r\n## Expected results\r\nDataset is initialized with a `None` value in the `value` column.\r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 3, in <module>\r\n    datasets.load_dataset(\"./bad-data\")\r\n  File \".../datasets/load.py\", line 1632, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \".../datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \".../datasets/builder.py\", line 697, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \".../datasets/builder.py\", line 1103, in _prepare_split\r\n    example = self.info.features.encode_example(record)\r\n  File \".../datasets/features/features.py\", line 1033, in encode_example\r\n    return encode_nested_example(self, example)\r\n  File \".../datasets/features/features.py\", line 808, in encode_nested_example\r\n    return {\r\n  File \".../datasets/features/features.py\", line 809, in <dictcomp>\r\n    k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)\r\n  File \".../datasets/features/features.py\", line 855, in encode_nested_example\r\n    return schema.encode_example(obj)\r\n  File \".../datasets/features/features.py\", line 299, in encode_example\r\n    return float(value)\r\nTypeError: float() argument must be a string or a number, not 'NoneType'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.15.1\r\n- Platform: Linux-5.4.0-81-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 6.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3253/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3253/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3247", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3247/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3247/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3247/events", "html_url": "https://github.com/huggingface/datasets/issues/3247", "id": 1049699088, "node_id": "I_kwDODunzps4-kSMQ", "number": 3247, "title": "Loading big json dataset raises pyarrow.lib.ArrowNotImplementedError", "user": {"login": "maxzirps", "id": 29249513, "node_id": "MDQ6VXNlcjI5MjQ5NTEz", "avatar_url": "https://avatars.githubusercontent.com/u/29249513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/maxzirps", "html_url": "https://github.com/maxzirps", "followers_url": "https://api.github.com/users/maxzirps/followers", "following_url": "https://api.github.com/users/maxzirps/following{/other_user}", "gists_url": "https://api.github.com/users/maxzirps/gists{/gist_id}", "starred_url": "https://api.github.com/users/maxzirps/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/maxzirps/subscriptions", "organizations_url": "https://api.github.com/users/maxzirps/orgs", "repos_url": "https://api.github.com/users/maxzirps/repos", "events_url": "https://api.github.com/users/maxzirps/events{/privacy}", "received_events_url": "https://api.github.com/users/maxzirps/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-11-10T11:17:59Z", "updated_at": "2022-04-10T14:05:57Z", "closed_at": "2022-04-10T14:05:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen trying to create a dataset from a json file with around 25MB, the following error is raised `pyarrow.lib.ArrowNotImplementedError: Unsupported cast from struct<b: int64, c: int64> to struct using function cast_struct`\r\n\r\nSplitting the big file into smaller ones and then loading it with the `load_dataset` method did also not work.\r\n\r\nCreating a pandas dataframe from it and then loading it with `Dataset.from_pandas` works\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset(\"json\", data_files=\"test.json\")\r\n```\r\n\r\ntest.json ~25MB\r\n```json\r\n{\"a\": {\"c\": 8, \"b\": 5}}\r\n{\"a\": {\"b\": 7, \"c\": 6}}\r\n{\"a\": {\"c\": 8, \"b\": 5}}\r\n{\"a\": {\"b\": 7, \"c\": 6}}\r\n{\"a\": {\"c\": 8, \"b\": 5}}\r\n...\r\n```\r\n\r\nworking.json ~160bytes\r\n```json\r\n{\"a\": {\"c\": 8, \"b\": 5}}\r\n{\"a\": {\"b\": 7, \"c\": 6}}\r\n{\"a\": {\"c\": 8, \"b\": 5}}\r\n{\"a\": {\"b\": 7, \"c\": 6}}\r\n{\"a\": {\"c\": 8, \"b\": 5}}\r\n```\r\n\r\n## Expected results\r\nIt should load the dataset from the json file without error.\r\n\r\n## Actual results\r\nIt raises Exception `pyarrow.lib.ArrowNotImplementedError: Unsupported cast from struct<b: int64, c: int64> to struct using function cast_struct`\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/m/workspace/xxx/project/main.py\", line 60, in <module>\r\n    dataset = load_dataset(\"json\", data_files=\"result.json\")\r\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/xxx/lib/python3.9/site-packages/datasets/load.py\", line 1627, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/xxx/lib/python3.9/site-packages/datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/xxx/lib/python3.9/site-packages/datasets/builder.py\", line 697, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/xxx/lib/python3.9/site-packages/datasets/builder.py\", line 1159, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/xxx/lib/python3.9/site-packages/datasets/arrow_writer.py\", line 428, in write_table\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"pyarrow/table.pxi\", line 1685, in pyarrow.lib.Table.from_arrays\r\n  File \"pyarrow/table.pxi\", line 630, in pyarrow.lib._sanitize_arrays\r\n  File \"pyarrow/array.pxi\", line 338, in pyarrow.lib.asarray\r\n  File \"pyarrow/table.pxi\", line 304, in pyarrow.lib.ChunkedArray.cast\r\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/xxx/lib/python3.9/site-packages/pyarrow/compute.py\", line 309, in cast\r\n    return call_function(\"cast\", [arr], options)\r\n  File \"pyarrow/_compute.pyx\", line 528, in pyarrow._compute.call_function\r\n  File \"pyarrow/_compute.pyx\", line 327, in pyarrow._compute.Function.call\r\n  File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 120, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowNotImplementedError: Unsupported cast from struct<b: int64, c: int64> to struct using function cast_struct\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.14.0\r\n- Platform: macOS-12.0.1-arm64-arm-64bit\r\n- Python version: 3.9.7\r\n- PyArrow version: 6.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3247/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3247/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3237", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3237/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3237/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3237/events", "html_url": "https://github.com/huggingface/datasets/issues/3237", "id": 1048165525, "node_id": "I_kwDODunzps4-ebyV", "number": 3237, "title": "wikitext description wrong", "user": {"login": "hongyuanmei", "id": 19693633, "node_id": "MDQ6VXNlcjE5NjkzNjMz", "avatar_url": "https://avatars.githubusercontent.com/u/19693633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hongyuanmei", "html_url": "https://github.com/hongyuanmei", "followers_url": "https://api.github.com/users/hongyuanmei/followers", "following_url": "https://api.github.com/users/hongyuanmei/following{/other_user}", "gists_url": "https://api.github.com/users/hongyuanmei/gists{/gist_id}", "starred_url": "https://api.github.com/users/hongyuanmei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hongyuanmei/subscriptions", "organizations_url": "https://api.github.com/users/hongyuanmei/orgs", "repos_url": "https://api.github.com/users/hongyuanmei/repos", "events_url": "https://api.github.com/users/hongyuanmei/events{/privacy}", "received_events_url": "https://api.github.com/users/hongyuanmei/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-11-09T04:06:52Z", "updated_at": "2022-02-14T15:45:11Z", "closed_at": "2021-11-09T13:49:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nDescriptions of the wikitext datasests are wrong. \r\n\r\n## Steps to reproduce the bug\r\nPlease see: https://github.com/huggingface/datasets/blob/f6dcafce996f39b6a4bbe3a9833287346f4a4b68/datasets/wikitext/wikitext.py#L50\r\n\r\n## Expected results\r\nThe descriptions for raw-v1 and v1 should be switched. ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3237/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3237/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3236", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3236/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3236/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3236/events", "html_url": "https://github.com/huggingface/datasets/issues/3236", "id": 1048026358, "node_id": "I_kwDODunzps4-d5z2", "number": 3236, "title": "Loading of datasets changed in #3110 returns no examples ", "user": {"login": "eladsegal", "id": 13485709, "node_id": "MDQ6VXNlcjEzNDg1NzA5", "avatar_url": "https://avatars.githubusercontent.com/u/13485709?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eladsegal", "html_url": "https://github.com/eladsegal", "followers_url": "https://api.github.com/users/eladsegal/followers", "following_url": "https://api.github.com/users/eladsegal/following{/other_user}", "gists_url": "https://api.github.com/users/eladsegal/gists{/gist_id}", "starred_url": "https://api.github.com/users/eladsegal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eladsegal/subscriptions", "organizations_url": "https://api.github.com/users/eladsegal/orgs", "repos_url": "https://api.github.com/users/eladsegal/repos", "events_url": "https://api.github.com/users/eladsegal/events{/privacy}", "received_events_url": "https://api.github.com/users/eladsegal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2021-11-08T23:29:46Z", "updated_at": "2021-11-09T16:46:05Z", "closed_at": "2021-11-09T16:45:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading of datasets changed in https://github.com/huggingface/datasets/pull/3110 returns no examples:\r\n```python\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'title', 'abstract', 'full_text', 'qas'],\r\n        num_rows: 0\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'title', 'abstract', 'full_text', 'qas'],\r\n        num_rows: 0\r\n    })\r\n})\r\n```\r\n\r\n## Steps to reproduce the bug\r\nLoad any of the datasets that were changed in https://github.com/huggingface/datasets/pull/3110:\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"qasper\")\r\n\r\n# The problem only started with the commit of #3110\r\nload_dataset(\"qasper\", revision=\"b6469baa22c174b3906c631802a7016fedea6780\")\r\n```\r\n\r\n## Expected results\r\n```python\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'title', 'abstract', 'full_text', 'qas'],\r\n        num_rows: 888\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'title', 'abstract', 'full_text', 'qas'],\r\n        num_rows: 281\r\n    })\r\n})\r\n```\r\nWhich can be received when specifying revision of the commit before https://github.com/huggingface/datasets/pull/3110:\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"qasper\", revision=\"acfe2abda1ca79f0ce5c1896aa83b4b78af76b7d\")\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.15.2.dev0 (master)\r\n- Python version: 3.8.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3236/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3232", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3232/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3232/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3232/events", "html_url": "https://github.com/huggingface/datasets/issues/3232", "id": 1047361573, "node_id": "I_kwDODunzps4-bXgl", "number": 3232, "title": "The Xsum datasets seems not able to download.", "user": {"login": "FYYFU", "id": 37999885, "node_id": "MDQ6VXNlcjM3OTk5ODg1", "avatar_url": "https://avatars.githubusercontent.com/u/37999885?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FYYFU", "html_url": "https://github.com/FYYFU", "followers_url": "https://api.github.com/users/FYYFU/followers", "following_url": "https://api.github.com/users/FYYFU/following{/other_user}", "gists_url": "https://api.github.com/users/FYYFU/gists{/gist_id}", "starred_url": "https://api.github.com/users/FYYFU/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FYYFU/subscriptions", "organizations_url": "https://api.github.com/users/FYYFU/orgs", "repos_url": "https://api.github.com/users/FYYFU/repos", "events_url": "https://api.github.com/users/FYYFU/events{/privacy}", "received_events_url": "https://api.github.com/users/FYYFU/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-11-08T11:58:54Z", "updated_at": "2021-11-09T15:07:16Z", "closed_at": "2021-11-09T15:07:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe download Link of the Xsum dataset provided in the repository is [Link](http://bollin.inf.ed.ac.uk/public/direct/XSUM-EMNLP18-Summary-Data-Original.tar.gz). It seems not able to download.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset('xsum')\r\n```\r\n\r\n\r\n## Actual results\r\n``` python\r\nraise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach http://bollin.inf.ed.ac.uk/public/direct/XSUM-EMNLP18-Summary-Data-Original.tar.gz\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3232/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3232/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3227", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3227/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3227/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3227/events", "html_url": "https://github.com/huggingface/datasets/issues/3227", "id": 1046667845, "node_id": "I_kwDODunzps4-YuJF", "number": 3227, "title": "Error in `Json(datasets.ArrowBasedBuilder)` class", "user": {"login": "JunShern", "id": 7796965, "node_id": "MDQ6VXNlcjc3OTY5NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/7796965?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JunShern", "html_url": "https://github.com/JunShern", "followers_url": "https://api.github.com/users/JunShern/followers", "following_url": "https://api.github.com/users/JunShern/following{/other_user}", "gists_url": "https://api.github.com/users/JunShern/gists{/gist_id}", "starred_url": "https://api.github.com/users/JunShern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JunShern/subscriptions", "organizations_url": "https://api.github.com/users/JunShern/orgs", "repos_url": "https://api.github.com/users/JunShern/repos", "events_url": "https://api.github.com/users/JunShern/events{/privacy}", "received_events_url": "https://api.github.com/users/JunShern/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-11-07T05:50:32Z", "updated_at": "2021-11-09T19:09:15Z", "closed_at": "2021-11-09T19:09:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen a json file contains a `text` field that is larger than the block_size, the JSON dataset builder fails.\r\n\r\n## Steps to reproduce the bug\r\nCreate a folder that contains the following:\r\n```\r\n.\r\n\u251c\u2500\u2500 testdata\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mydata.json\r\n\u2514\u2500\u2500 test.py\r\n```\r\n\r\nPlease download [this file](https://github.com/huggingface/datasets/files/7491797/mydata.txt) as `mydata.json`. (The error does not occur in JSON files with shorter text, but it is reproducible when the text is long as in the file I provide)\r\n:exclamation: :exclamation: GitHub doesn't allow me to upload JSON so this file is a TXT, and you should rename it to `.json`!\r\n\r\n`test.py` simply contains:\r\n```python\r\nfrom datasets import load_dataset\r\nmy_dataset = load_dataset(\"testdata\")\r\n```\r\n\r\nTo reproduce the error, simply run\r\n```\r\npython test.py\r\n```\r\n\r\n## Expected results\r\nThe data should load correctly without error.\r\n\r\n## Actual results\r\nThe dataset builder fails with:\r\n```\r\nUsing custom data configuration testdata-d490389b8ab4fd82\r\nDownloading and preparing dataset json/testdata to /home/junshern.chan/.cache/huggingface/datasets/json/testdata-d490389b8ab4fd82/0.0.0/3333a8af0db9764dfcff43a42ff26228f0f2e267f0d8a0a294452d188beadb34...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2264.74it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 447.01it/s]\r\nFailed to read file '/home/junshern.chan/hf-json-bug/testdata/mydata.json' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Missing a name for object member. in row 0\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 28, in <module>\r\n    my_dataset = load_dataset(\"testdata\")\r\n  File \"/home/junshern.chan/.casio/miniconda/envs/hf-json-bug/lib/python3.8/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/junshern.chan/.casio/miniconda/envs/hf-json-bug/lib/python3.8/site-packages/datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/junshern.chan/.casio/miniconda/envs/hf-json-bug/lib/python3.8/site-packages/datasets/builder.py\", line 697, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/junshern.chan/.casio/miniconda/envs/hf-json-bug/lib/python3.8/site-packages/datasets/builder.py\", line 1156, in _prepare_split\r\n    for key, table in utils.tqdm(\r\n  File \"/home/junshern.chan/.casio/miniconda/envs/hf-json-bug/lib/python3.8/site-packages/tqdm/std.py\", line 1168, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/junshern.chan/.casio/miniconda/envs/hf-json-bug/lib/python3.8/site-packages/datasets/packaged_modules/json/json.py\", line 146, in _generate_tables\r\n    raise ValueError(\r\nValueError: Not able to read records in the JSON file at /home/junshern.chan/hf-json-bug/testdata/mydata.json. You should probably indicate the field of the JSON file containing your records. This JSON file contain the following fields: ['text']. Select the correct one and provide it as `field='XXX'` to the dataset loading method. \r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.15.1\r\n- Platform: Linux-5.8.0-63-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.12\r\n- PyArrow version: 6.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3227/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3227/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3219", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3219/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3219/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3219/events", "html_url": "https://github.com/huggingface/datasets/issues/3219", "id": 1045095000, "node_id": "I_kwDODunzps4-SuJY", "number": 3219, "title": "Eventual Invalid Token Error at setup of private datasets", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-11-04T18:50:45Z", "updated_at": "2021-11-08T13:23:06Z", "closed_at": "2021-11-08T08:59:43Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nFrom time to time, there appear Invalid Token errors with private datasets:\r\n\r\n- https://app.circleci.com/pipelines/github/huggingface/datasets/8520/workflows/d44629f2-4749-40f8-a657-50931d0b3434/jobs/52534\r\n  ```\r\n  ____________ ERROR at setup of test_load_streaming_private_dataset _____________\r\n  ValueError: Invalid token passed!\r\n\r\n  ____ ERROR at setup of test_load_streaming_private_dataset_with_zipped_data ____\r\n  ValueError: Invalid token passed!\r\n  \r\n  =========================== short test summary info ============================\r\n  ERROR tests/test_load.py::test_load_streaming_private_dataset - ValueError: I...\r\n  ERROR tests/test_load.py::test_load_streaming_private_dataset_with_zipped_data\r\n  ```\r\n\r\n- https://app.circleci.com/pipelines/github/huggingface/datasets/8557/workflows/a8383181-ba6d-4487-9d0a-f750b6dcb936/jobs/52763\r\n  ```\r\n  ____ ERROR at setup of test_load_streaming_private_dataset_with_zipped_data ____\r\n  [gw1] linux -- Python 3.6.15 /home/circleci/.pyenv/versions/3.6.15/bin/python3.6\r\n\r\n  hf_api = <huggingface_hub.hf_api.HfApi object at 0x7f4899bab908>\r\n  hf_token = 'vgNbyuaLNEBuGbgCEtSBCOcPjZnngJufHkTaZvHwkXKGkHpjBPwmLQuJVXRxBuaRzNlGjlMpYRPbthfHPFWXaaEDTLiqTTecYENxukRYVAAdpeApIUPxcgsowadkTkPj'\r\n  zip_csv_path = PosixPath('/tmp/pytest-of-circleci/pytest-0/popen-gw1/data16/dataset.csv.zip')\r\n\r\n      @pytest.fixture(scope=\"session\")\r\n      def hf_private_dataset_repo_zipped_txt_data_(hf_api: HfApi, hf_token, zip_csv_path):\r\n          repo_name = \"repo_zipped_txt_data-{}\".format(int(time.time() * 10e3))\r\n          hf_api.create_repo(token=hf_token, name=repo_name, repo_type=\"dataset\", private=True)\r\n          repo_id = f\"{USER}/{repo_name}\"\r\n          hf_api.upload_file(\r\n              token=hf_token,\r\n              path_or_fileobj=str(zip_csv_path),\r\n              path_in_repo=\"data.zip\",\r\n              repo_id=repo_id,\r\n  >           repo_type=\"dataset\",\r\n          )\r\n\r\n  tests/hub_fixtures.py:68:\r\n\r\n  ...\r\n\r\n  ValueError: Invalid token passed!\r\n  =========================== short test summary info ============================\r\n  ERROR tests/test_load.py::test_load_streaming_private_dataset_with_zipped_data\r\n  ```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3219/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3217", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3217/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3217/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3217/events", "html_url": "https://github.com/huggingface/datasets/issues/3217", "id": 1045029710, "node_id": "I_kwDODunzps4-SeNO", "number": 3217, "title": "Fix code quality bug in riddle_sense dataset", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-11-04T17:40:32Z", "updated_at": "2021-11-04T17:50:02Z", "closed_at": "2021-11-04T17:50:02Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n```\r\ndatasets/riddle_sense/riddle_sense.py:36:21: W291 trailing whitespace\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3217/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3217/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3207", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3207/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3207/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3207/events", "html_url": "https://github.com/huggingface/datasets/issues/3207", "id": 1044496389, "node_id": "I_kwDODunzps4-QcAF", "number": 3207, "title": "CI error: Another metric with the same name already exists in Keras 2.7.0", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-11-04T09:04:11Z", "updated_at": "2021-11-04T09:30:54Z", "closed_at": "2021-11-04T09:30:54Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nRelease of TensorFlow 2.7.0 contains an incompatibility with Keras. See:\r\n- keras-team/keras#15579\r\n\r\nThis breaks our CI test suite: https://app.circleci.com/pipelines/github/huggingface/datasets/8493/workflows/055c7ae2-43bc-49b4-9f11-8fc71f35a25c/jobs/52363\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3207/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3207/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3204", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3204/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3204/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3204/events", "html_url": "https://github.com/huggingface/datasets/issues/3204", "id": 1043707307, "node_id": "I_kwDODunzps4-NbWr", "number": 3204, "title": "FileNotFoundError for TupleIE dataste", "user": {"login": "arda-vianai", "id": 75334917, "node_id": "MDQ6VXNlcjc1MzM0OTE3", "avatar_url": "https://avatars.githubusercontent.com/u/75334917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arda-vianai", "html_url": "https://github.com/arda-vianai", "followers_url": "https://api.github.com/users/arda-vianai/followers", "following_url": "https://api.github.com/users/arda-vianai/following{/other_user}", "gists_url": "https://api.github.com/users/arda-vianai/gists{/gist_id}", "starred_url": "https://api.github.com/users/arda-vianai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arda-vianai/subscriptions", "organizations_url": "https://api.github.com/users/arda-vianai/orgs", "repos_url": "https://api.github.com/users/arda-vianai/repos", "events_url": "https://api.github.com/users/arda-vianai/events{/privacy}", "received_events_url": "https://api.github.com/users/arda-vianai/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-11-03T14:56:55Z", "updated_at": "2021-11-05T15:51:15Z", "closed_at": "2021-11-05T14:16:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n`dataset = datasets.load_dataset('tuple_ie', 'all')`\r\n\r\nreturns a FileNotFound error. Is the data not available? \r\n\r\nMany thanks.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3204/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3190", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3190/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3190/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3190/events", "html_url": "https://github.com/huggingface/datasets/issues/3190", "id": 1041153631, "node_id": "I_kwDODunzps4-Dr5f", "number": 3190, "title": "combination of shuffle and filter results in a bug", "user": {"login": "rabeehk", "id": 6278280, "node_id": "MDQ6VXNlcjYyNzgyODA=", "avatar_url": "https://avatars.githubusercontent.com/u/6278280?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rabeehk", "html_url": "https://github.com/rabeehk", "followers_url": "https://api.github.com/users/rabeehk/followers", "following_url": "https://api.github.com/users/rabeehk/following{/other_user}", "gists_url": "https://api.github.com/users/rabeehk/gists{/gist_id}", "starred_url": "https://api.github.com/users/rabeehk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rabeehk/subscriptions", "organizations_url": "https://api.github.com/users/rabeehk/orgs", "repos_url": "https://api.github.com/users/rabeehk/repos", "events_url": "https://api.github.com/users/rabeehk/events{/privacy}", "received_events_url": "https://api.github.com/users/rabeehk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-11-01T13:07:29Z", "updated_at": "2021-11-02T10:50:49Z", "closed_at": "2021-11-02T10:50:49Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nHi,\r\nI would like to shuffle a dataset, then filter it based on each existing label. however, the combination of `filter`, `shuffle` seems to results in a bug. In the minimal example below, as you see in the filtered results, the filtered labels are not unique, meaning filter has not worked. Any suggestions as a temporary fix is appreciated @lhoestq.\r\n\r\nThanks.\r\n Best regards\r\nRabeeh \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport numpy as np\r\nimport datasets \r\n\r\ndatasets = datasets.load_dataset('super_glue', 'rte', script_version=\"master\")\r\nshuffled_data = datasets[\"train\"].shuffle(seed=42)\r\nfor label in range(2):\r\n    print(\"label \", label)\r\n    data = shuffled_data.filter(lambda example: int(example['label']) == label)\r\n    print(\"length \", len(data), np.unique(data['label']))\r\n```\r\n\r\n## Expected results\r\nFiltering per label, should only return the data with that specific label.\r\n\r\n## Actual results\r\nAs you can see, filtered data per label, has still two labels of [0, 1]\r\n```\r\nlabel  0\r\nlength  1249 [0 1]\r\nlabel  1\r\nlength  1241 [0 1]\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1 \r\n- Platform: linux \r\n- Python version: 3.7.11 \r\n- PyArrow version: 5.0.0 \r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3190/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3190/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3189", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3189/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3189/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3189/events", "html_url": "https://github.com/huggingface/datasets/issues/3189", "id": 1041044986, "node_id": "I_kwDODunzps4-DRX6", "number": 3189, "title": "conll2003 incorrect label explanation", "user": {"login": "BramVanroy", "id": 2779410, "node_id": "MDQ6VXNlcjI3Nzk0MTA=", "avatar_url": "https://avatars.githubusercontent.com/u/2779410?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BramVanroy", "html_url": "https://github.com/BramVanroy", "followers_url": "https://api.github.com/users/BramVanroy/followers", "following_url": "https://api.github.com/users/BramVanroy/following{/other_user}", "gists_url": "https://api.github.com/users/BramVanroy/gists{/gist_id}", "starred_url": "https://api.github.com/users/BramVanroy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BramVanroy/subscriptions", "organizations_url": "https://api.github.com/users/BramVanroy/orgs", "repos_url": "https://api.github.com/users/BramVanroy/repos", "events_url": "https://api.github.com/users/BramVanroy/events{/privacy}", "received_events_url": "https://api.github.com/users/BramVanroy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-11-01T11:03:30Z", "updated_at": "2021-11-09T10:40:58Z", "closed_at": "2021-11-09T10:40:58Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "In the [conll2003](https://huggingface.co/datasets/conll2003#data-fields) README, the labels are described as follows\r\n\r\n> - `id`: a `string` feature.\r\n> - `tokens`: a `list` of `string` features.\r\n> - `pos_tags`: a `list` of classification labels, with possible values including `\"` (0), `''` (1), `#` (2), `$` (3), `(` (4).\r\n> - `chunk_tags`: a `list` of classification labels, with possible values including `O` (0), `B-ADJP` (1), `I-ADJP` (2), `B-ADVP` (3), `I-ADVP` (4).\r\n> - `ner_tags`: a `list` of classification labels, with possible values including `O` (0), `B-PER` (1), `I-PER` (2), `B-ORG` (3), `I-ORG` (4) `B-LOC` (5), `I-LOC` (6) `B-MISC` (7), `I-MISC` (8).\r\n\r\nFirst of all, it would be great if we can get a list of ALL possible pos_tags.\r\n\r\nSecond, the chunk tags labels cannot be correct. The description says the values go from 0 to 4 whereas the data shows values from at least 11 to 21 and 0.\r\n\r\nEDIT: not really a bug, sorry for mistagging.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3189/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3189/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3181", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3181/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3181/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3181/events", "html_url": "https://github.com/huggingface/datasets/issues/3181", "id": 1039682097, "node_id": "I_kwDODunzps49-Eox", "number": 3181, "title": "`None` converted to `\"None\"` when loading a dataset", "user": {"login": "eladsegal", "id": 13485709, "node_id": "MDQ6VXNlcjEzNDg1NzA5", "avatar_url": "https://avatars.githubusercontent.com/u/13485709?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eladsegal", "html_url": "https://github.com/eladsegal", "followers_url": "https://api.github.com/users/eladsegal/followers", "following_url": "https://api.github.com/users/eladsegal/following{/other_user}", "gists_url": "https://api.github.com/users/eladsegal/gists{/gist_id}", "starred_url": "https://api.github.com/users/eladsegal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eladsegal/subscriptions", "organizations_url": "https://api.github.com/users/eladsegal/orgs", "repos_url": "https://api.github.com/users/eladsegal/repos", "events_url": "https://api.github.com/users/eladsegal/events{/privacy}", "received_events_url": "https://api.github.com/users/eladsegal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2021-10-29T15:23:53Z", "updated_at": "2021-12-11T01:16:40Z", "closed_at": "2021-12-09T14:26:57Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen loading a dataset `None` values of the type `NoneType` are converted to `'None'` of the type `str`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nqasper = load_dataset(\"qasper\", split=\"train\", download_mode=\"reuse_cache_if_exists\")\r\nprint(qasper[60][\"full_text\"][\"section_name\"])\r\n```\r\n\r\nWhen installing version 1.1.40, the output is\r\n`[None, 'Introduction', 'Benchmark Datasets', ...]`\r\n\r\nWhen installing from the master branch, the output is\r\n`['None', 'Introduction', 'Benchmark Datasets', ...]`\r\n\r\nNotice how the first element was changed from `NoneType` to `str`.\r\n\r\n## Expected results\r\n`None` should stay as is.\r\n\r\n## Actual results\r\n`None` is converted to a string.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: master\r\n- Platform: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3181/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3179", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3179/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3179/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3179/events", "html_url": "https://github.com/huggingface/datasets/issues/3179", "id": 1039571928, "node_id": "I_kwDODunzps499pvY", "number": 3179, "title": "Cannot load dataset when the config name is \"special\"", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 3470211881, "node_id": "LA_kwDODunzps7O1zsp", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset-viewer", "name": "dataset-viewer", "color": "E5583E", "default": false, "description": "Related to the dataset viewer on huggingface.co"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-10-29T13:30:47Z", "updated_at": "2021-10-29T13:35:21Z", "closed_at": "2021-10-29T13:35:21Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nAfter https://github.com/huggingface/datasets/pull/3159, we can get the config name of \"Check/region_1\", which is \"Check___region_1\".\r\n\r\nBut now we cannot load the dataset (not sure it's related to the above PR though). It's the case for all the similar datasets, listed in https://github.com/huggingface/datasets-preview-backend/issues/78\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> from datasets import get_dataset_config_names\r\n>>> get_dataset_config_names(\"Check/region_1\")\r\n['Check___region_1']\r\n>>> load_dataset(\"Check/region_1\")\r\nUsing custom data configuration Check___region_1-d2b3bc48f11c9be2\r\nDownloading and preparing dataset json/Check___region_1 to /home/slesage/.cache/huggingface/datasets/json/Check___region_1-d2b3bc48f11c9be2/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 4443.12it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 1277.19it/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/builder.py\", line 697, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/builder.py\", line 1159, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/arrow_writer.py\", line 442, in write_table\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.9/site-packages/datasets/arrow_writer.py\", line 442, in <listcomp>\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"pyarrow/table.pxi\", line 1249, in pyarrow.lib.Table.__getitem__\r\n  File \"pyarrow/table.pxi\", line 1825, in pyarrow.lib.Table.column\r\n  File \"pyarrow/table.pxi\", line 1800, in pyarrow.lib.Table._ensure_integer_index\r\nKeyError: 'Field \"builder_name\" does not exist in table schema'\r\n```\r\n\r\nLoading in streaming mode also returns something strange:\r\n\r\n```python\r\n>>> list(load_dataset(\"Check/region_1\", streaming=True, split=\"train\"))\r\nUsing custom data configuration Check___region_1-d2b3bc48f11c9be2\r\n[{'builder_name': None, 'citation': '', 'config_name': None, 'dataset_size': None, 'description': '', 'download_checksums': None, 'download_size': None, 'features': {'speech': {'feature': {'dtype': 'float64', 'id': None, '_type': 'Value'}, 'length': -1, 'id': None, '_type': 'Sequence'}, 'sampling_rate': {'dtype': 'int64', 'id': None, '_type': 'Value'}, 'label': {'dtype': 'string', 'id': None, '_type': 'Value'}}, 'homepage': '', 'license': '', 'post_processed': None, 'post_processing_size': None, 'size_in_bytes': None, 'splits': None, 'supervised_keys': None, 'task_templates': None, 'version': None}, {'_data_files': [{'filename': 'dataset.arrow'}], '_fingerprint': 'f1702bb5533c549c', '_format_columns': ['speech', 'sampling_rate', 'label'], '_format_kwargs': {}, '_format_type': None, '_indexes': {}, '_indices_data_files': None, '_output_all_columns': False, '_split': None}]\r\n```\r\n\r\n## Expected results\r\n\r\nThe dataset should be loaded\r\n\r\n## Actual results\r\n\r\nAn error occurs\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.14.1.dev0\r\n- Platform: Linux-5.11.0-1020-aws-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3179/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3179/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3178", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3178/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3178/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3178/events", "html_url": "https://github.com/huggingface/datasets/issues/3178", "id": 1039539076, "node_id": "I_kwDODunzps499huE", "number": 3178, "title": "\"Property couldn't be hashed properly\" even though fully picklable", "user": {"login": "BramVanroy", "id": 2779410, "node_id": "MDQ6VXNlcjI3Nzk0MTA=", "avatar_url": "https://avatars.githubusercontent.com/u/2779410?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BramVanroy", "html_url": "https://github.com/BramVanroy", "followers_url": "https://api.github.com/users/BramVanroy/followers", "following_url": "https://api.github.com/users/BramVanroy/following{/other_user}", "gists_url": "https://api.github.com/users/BramVanroy/gists{/gist_id}", "starred_url": "https://api.github.com/users/BramVanroy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BramVanroy/subscriptions", "organizations_url": "https://api.github.com/users/BramVanroy/orgs", "repos_url": "https://api.github.com/users/BramVanroy/repos", "events_url": "https://api.github.com/users/BramVanroy/events{/privacy}", "received_events_url": "https://api.github.com/users/BramVanroy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 23, "created_at": "2021-10-29T12:56:09Z", "updated_at": "2023-01-04T15:33:16Z", "closed_at": "2022-11-02T17:18:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to tokenize a dataset with spaCy. I found that no matter what I do, the spaCy language object (`nlp`) prevents `datasets` from pickling correctly - or so the warning says - even though manually pickling is no issue. It should not be an issue either, since spaCy objects are picklable.\r\n\r\n## Steps to reproduce the bug\r\n\r\nHere is a [colab](https://colab.research.google.com/drive/1gt75LCBIzsmBMvvipEOvWulvyZseBiA7?usp=sharing) but for some reason I cannot reproduce it there. That may have to do with logging/tqdm on Colab, or with running things in notebooks. I tried below code on Windows and Ubuntu as a Python script and getting the same issue (warning below).\r\n\r\n```python\r\nimport pickle\r\n\r\nfrom datasets import load_dataset\r\nimport spacy\r\n\r\n\r\nclass Processor:\r\n    def __init__(self):\r\n        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\r\n\r\n    @staticmethod\r\n    def collate(batch):\r\n        return [d[\"en\"] for d in batch]\r\n\r\n    def parse(self, batch):\r\n        batch = batch[\"translation\"]\r\n        return {\"translation_tok\": [{\"en_tok\": \" \".join([t.text for t in doc])} for doc in self.nlp.pipe(self.collate(batch))]}\r\n\r\n    def process(self):\r\n        ds = load_dataset(\"wmt16\", \"de-en\", split=\"train[:10%]\")\r\n        ds = ds.map(self.parse, batched=True, num_proc=6)\r\n\r\n\r\nif __name__ == '__main__':\r\n    pr = Processor()\r\n\r\n    # succeeds\r\n    with open(\"temp.pkl\", \"wb\") as f:\r\n        pickle.dump(pr, f)\r\n    print(\"Successfully pickled!\")\r\n\r\n    pr.process()\r\n\r\n```\r\n\r\n---\r\n\r\nHere is a small change that includes `Hasher.hash` that shows that the hasher cannot seem to successfully pickle parts form the NLP object.\r\n\r\n```python\r\n\r\nfrom datasets.fingerprint import Hasher\r\nimport pickle\r\n\r\nfrom datasets import load_dataset\r\nimport spacy\r\n\r\n\r\nclass Processor:\r\n    def __init__(self):\r\n        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\r\n\r\n    @staticmethod\r\n    def collate(batch):\r\n        return [d[\"en\"] for d in batch]\r\n\r\n    def parse(self, batch):\r\n        batch = batch[\"translation\"]\r\n        return {\"translation_tok\": [{\"en_tok\": \" \".join([t.text for t in doc])} for doc in self.nlp.pipe(self.collate(batch))]}\r\n\r\n    def process(self):\r\n        ds = load_dataset(\"wmt16\", \"de-en\", split=\"train[:10]\")\r\n        return ds.map(self.parse, batched=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n    pr = Processor()\r\n\r\n    # succeeds\r\n    with open(\"temp.pkl\", \"wb\") as f:\r\n        pickle.dump(pr, f)\r\n    print(\"Successfully pickled class instance!\")\r\n\r\n    # succeeds\r\n    with open(\"temp.pkl\", \"wb\") as f:\r\n        pickle.dump(pr.nlp, f)\r\n    print(\"Successfully pickled nlp!\")\r\n\r\n    # fails\r\n    print(Hasher.hash(pr.nlp))\r\n    pr.process()\r\n```\r\n\r\n## Expected results\r\nThis to be picklable, working (fingerprinted), and no warning.\r\n\r\n## Actual results\r\nIn the first snippet, I get this warning \r\nParameter 'function'=<function Processor.parse at 0x7f44982247a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\r\n\r\nIn the second, I get this traceback which directs to the `Hasher.hash` line.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 918, in save_global\r\n    obj2, parent = _getattribute(module, name)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 266, in _getattribute\r\n    .format(name, obj))\r\nAttributeError: Can't get local attribute 'add_codes.<locals>.ErrorsWithCodes' on <function add_codes at 0x00000296FF606EA0>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \" scratch_4.py\", line 40, in <module>\r\n    print(Hasher.hash(pr.nlp))\r\n  File \" \\lib\\site-packages\\datasets\\fingerprint.py\", line 191, in hash\r\n    return cls.hash_default(value)\r\n  File \" \\lib\\site-packages\\datasets\\fingerprint.py\", line 184, in hash_default\r\n    return cls.hash_bytes(dumps(value))\r\n  File \" \\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 345, in dumps\r\n    dump(obj, file)\r\n  File \" \\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 320, in dump\r\n    Pickler(file, recurse=True).dump(obj)\r\n  File \" \\lib\\site-packages\\dill\\_dill.py\", line 498, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 409, in dump\r\n    self.save(obj)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 521, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 634, in save_reduce\r\n    save(state)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\lib\\site-packages\\dill\\_dill.py\", line 990, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 781, in save_list\r\n    self._batch_appends(obj)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 805, in _batch_appends\r\n    save(x)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 736, in save_tuple\r\n    save(element)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 521, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 634, in save_reduce\r\n    save(state)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 736, in save_tuple\r\n    save(element)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\lib\\site-packages\\dill\\_dill.py\", line 990, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\lib\\site-packages\\dill\\_dill.py\", line 1176, in save_instancemethod0\r\n    pickler.save_reduce(MethodType, (obj.__func__, obj.__self__), obj=obj)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 610, in save_reduce\r\n    save(args)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 736, in save_tuple\r\n    save(element)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 523, in save_function\r\n    obj=obj,\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 610, in save_reduce\r\n    save(args)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 751, in save_tuple\r\n    save(element)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\lib\\site-packages\\dill\\_dill.py\", line 990, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 521, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 605, in save_reduce\r\n    save(cls)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \" \\lib\\site-packages\\dill\\_dill.py\", line 1439, in save_type\r\n    StockPickler.save_global(pickler, obj, name=name)\r\n  File \" \\Python\\Python36\\lib\\pickle.py\", line 922, in save_global\r\n    (obj, module_name, name))\r\n_pickle.PicklingError: Can't pickle <class 'spacy.errors.add_codes.<locals>.ErrorsWithCodes'>: it's not found as spacy.errors.add_codes.<locals>.ErrorsWithCodes\r\n```\r\n\r\n## Environment info\r\nTried on both Linux and Windows\r\n\r\n- `datasets` version: 1.14.0\r\n- Platform: Windows-10-10.0.19041-SP0 + Python 3.7.9; Linux-5.11.0-38-generic-x86_64-with-Ubuntu-20.04-focal + Python 3.7.12\r\n- PyArrow version: 6.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3178/reactions", "total_count": 4, "+1": 4, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3178/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3172", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3172/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3172/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3172/events", "html_url": "https://github.com/huggingface/datasets/issues/3172", "id": 1038351587, "node_id": "I_kwDODunzps494_zj", "number": 3172, "title": "`SystemError 15` thrown in `Dataset.__del__` when using `Dataset.map()` with `num_proc>1`", "user": {"login": "vlievin", "id": 9859840, "node_id": "MDQ6VXNlcjk4NTk4NDA=", "avatar_url": "https://avatars.githubusercontent.com/u/9859840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlievin", "html_url": "https://github.com/vlievin", "followers_url": "https://api.github.com/users/vlievin/followers", "following_url": "https://api.github.com/users/vlievin/following{/other_user}", "gists_url": "https://api.github.com/users/vlievin/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlievin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlievin/subscriptions", "organizations_url": "https://api.github.com/users/vlievin/orgs", "repos_url": "https://api.github.com/users/vlievin/repos", "events_url": "https://api.github.com/users/vlievin/events{/privacy}", "received_events_url": "https://api.github.com/users/vlievin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2021-10-28T10:29:00Z", "updated_at": "2023-01-26T07:07:54Z", "closed_at": "2021-11-03T11:26:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI use `datasets.map` to preprocess some data in my application. The error `SystemError 15` is thrown at the end of the execution of `Dataset.map()` (only with `num_proc>1`. Traceback included bellow. \r\n \r\nThe exception is raised only when the code runs within a specific context. Despite ~10h spent investigating this issue, I have failed to isolate the bug, so let me describe my setup. \r\n\r\nIn my project, `Dataset` is wrapped into a `LightningDataModule` and the data is preprocessed when calling `LightningDataModule.setup()`. Calling `.setup()` in an isolated script works fine (even when wrapped with `hydra.main()`). However, when calling `.setup()` within the experiment script (depends on `pytorch_lightning`), the script crashes and `SystemError 15`.\r\n\r\nI could avoid throwing this error by modifying ` Dataset.__del__()` (see bellow), but I believe this only moves the problem somewhere else. I am completely stuck with this issue, any hint would be welcome. \r\n\r\n```python\r\nclass Dataset()\r\n    ...\r\n    def __del__(self):\r\n        if hasattr(self, \"_data\"):\r\n            _ = self._data # <- ugly trick that allows avoiding the issue.\r\n            del self._data\r\n        if hasattr(self, \"_indices\"):\r\n            del self._indices\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Unfortunately I couldn't isolate the bug.\r\n```\r\n\r\n## Expected results\r\nCalling `Dataset.map()` without throwing an exception. Or at least raising a more detailed exception/traceback.\r\n\r\n## Actual results\r\n```\r\nException ignored in: <function Dataset.__del__ at 0x7f7cec179160>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:05<00:00,  1.17ba/s]\r\nTraceback (most recent call last):\r\n  File \".../python3.8/site-packages/datasets/arrow_dataset.py\", line 906, in __del__\r\n    del self._data\r\n  File \".../python3.8/site-packages/ray/worker.py\", line 1033, in sigterm_handler\r\n    sys.exit(signum)\r\nSystemExit: 15\r\n\r\n```\r\n\r\n## Environment info\r\n\r\nTested on 2 environments:\r\n\r\n**Environment 1.**\r\n- `datasets` version: 1.14.0\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.8\r\n- PyArrow version: 6.0.0\r\n\r\n**Environment 2.**\r\n- `datasets` version: 1.14.0\r\n- Platform: Linux-4.18.0-305.19.1.el8_4.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.9.7\r\n- PyArrow version: 6.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3172/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3168", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3168/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3168/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3168/events", "html_url": "https://github.com/huggingface/datasets/issues/3168", "id": 1036673263, "node_id": "I_kwDODunzps49ymDv", "number": 3168, "title": "OpenSLR/83 is empty", "user": {"login": "tyrius02", "id": 4561309, "node_id": "MDQ6VXNlcjQ1NjEzMDk=", "avatar_url": "https://avatars.githubusercontent.com/u/4561309?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tyrius02", "html_url": "https://github.com/tyrius02", "followers_url": "https://api.github.com/users/tyrius02/followers", "following_url": "https://api.github.com/users/tyrius02/following{/other_user}", "gists_url": "https://api.github.com/users/tyrius02/gists{/gist_id}", "starred_url": "https://api.github.com/users/tyrius02/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tyrius02/subscriptions", "organizations_url": "https://api.github.com/users/tyrius02/orgs", "repos_url": "https://api.github.com/users/tyrius02/repos", "events_url": "https://api.github.com/users/tyrius02/events{/privacy}", "received_events_url": "https://api.github.com/users/tyrius02/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "tyrius02", "id": 4561309, "node_id": "MDQ6VXNlcjQ1NjEzMDk=", "avatar_url": "https://avatars.githubusercontent.com/u/4561309?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tyrius02", "html_url": "https://github.com/tyrius02", "followers_url": "https://api.github.com/users/tyrius02/followers", "following_url": "https://api.github.com/users/tyrius02/following{/other_user}", "gists_url": "https://api.github.com/users/tyrius02/gists{/gist_id}", "starred_url": "https://api.github.com/users/tyrius02/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tyrius02/subscriptions", "organizations_url": "https://api.github.com/users/tyrius02/orgs", "repos_url": "https://api.github.com/users/tyrius02/repos", "events_url": "https://api.github.com/users/tyrius02/events{/privacy}", "received_events_url": "https://api.github.com/users/tyrius02/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tyrius02", "id": 4561309, "node_id": "MDQ6VXNlcjQ1NjEzMDk=", "avatar_url": "https://avatars.githubusercontent.com/u/4561309?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tyrius02", "html_url": "https://github.com/tyrius02", "followers_url": "https://api.github.com/users/tyrius02/followers", "following_url": "https://api.github.com/users/tyrius02/following{/other_user}", "gists_url": "https://api.github.com/users/tyrius02/gists{/gist_id}", "starred_url": "https://api.github.com/users/tyrius02/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tyrius02/subscriptions", "organizations_url": "https://api.github.com/users/tyrius02/orgs", "repos_url": "https://api.github.com/users/tyrius02/repos", "events_url": "https://api.github.com/users/tyrius02/events{/privacy}", "received_events_url": "https://api.github.com/users/tyrius02/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-10-26T19:42:21Z", "updated_at": "2021-10-29T10:04:09Z", "closed_at": "2021-10-29T10:04:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nAs the summary says, openslr / SLR83 / train is empty.\r\n\r\nThe dataset returned after loading indicates there are **zero** rows. The correct number should be **17877**.\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\n\r\n\r\ndatasets.load_dataset('openslr', 'SLR83')\r\n```\r\n\r\n## Expected results\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['path', 'audio', 'sentence'],\r\n        num_rows: 17877\r\n    })\r\n})\r\n```\r\n## Actual results\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['path', 'audio', 'sentence'],\r\n        num_rows: 0\r\n    })\r\n})\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.14.1.dev0 (master HEAD)\r\n- Platform: Ubuntu 20.04\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3168/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3168/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3167", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3167/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3167/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3167/events", "html_url": "https://github.com/huggingface/datasets/issues/3167", "id": 1036488992, "node_id": "I_kwDODunzps49x5Eg", "number": 3167, "title": "bookcorpusopen no longer works", "user": {"login": "lucadiliello", "id": 23355969, "node_id": "MDQ6VXNlcjIzMzU1OTY5", "avatar_url": "https://avatars.githubusercontent.com/u/23355969?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucadiliello", "html_url": "https://github.com/lucadiliello", "followers_url": "https://api.github.com/users/lucadiliello/followers", "following_url": "https://api.github.com/users/lucadiliello/following{/other_user}", "gists_url": "https://api.github.com/users/lucadiliello/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucadiliello/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucadiliello/subscriptions", "organizations_url": "https://api.github.com/users/lucadiliello/orgs", "repos_url": "https://api.github.com/users/lucadiliello/repos", "events_url": "https://api.github.com/users/lucadiliello/events{/privacy}", "received_events_url": "https://api.github.com/users/lucadiliello/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-10-26T16:06:15Z", "updated_at": "2021-11-17T15:53:46Z", "closed_at": "2021-11-17T15:53:46Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen using the latest version of datasets (1.14.0), I cannot use the `bookcorpusopen` dataset. The process blocks always around `9924 examples [00:06, 1439.61 examples/s]` when preparing the dataset. I also noticed that after half an hour the process is automatically killed because of the RAM usage (the machine has 1TB of RAM...).\r\n\r\nThis did not happen with 1.4.1.\r\nI tried also `rm -rf ~/.cache/huggingface` but did not help.\r\nChanging python version between 3.7, 3.8 and 3.9 did not help too.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nd = datasets.load_dataset('bookcorpusopen')\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.14.0\r\n- Platform: Linux-5.4.0-1054-aws-x86_64-with-glibc2.27\r\n- Python version: 3.9.7\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3167/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3167/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3155", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3155/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3155/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3155/events", "html_url": "https://github.com/huggingface/datasets/issues/3155", "id": 1034468757, "node_id": "I_kwDODunzps49qL2V", "number": 3155, "title": "Illegal instruction (core dumped) at datasets import", "user": {"login": "hacobe", "id": 91226467, "node_id": "MDQ6VXNlcjkxMjI2NDY3", "avatar_url": "https://avatars.githubusercontent.com/u/91226467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hacobe", "html_url": "https://github.com/hacobe", "followers_url": "https://api.github.com/users/hacobe/followers", "following_url": "https://api.github.com/users/hacobe/following{/other_user}", "gists_url": "https://api.github.com/users/hacobe/gists{/gist_id}", "starred_url": "https://api.github.com/users/hacobe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hacobe/subscriptions", "organizations_url": "https://api.github.com/users/hacobe/orgs", "repos_url": "https://api.github.com/users/hacobe/repos", "events_url": "https://api.github.com/users/hacobe/events{/privacy}", "received_events_url": "https://api.github.com/users/hacobe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-10-24T17:21:36Z", "updated_at": "2021-11-18T19:07:04Z", "closed_at": "2021-11-18T19:07:03Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI install datasets using conda and when I import datasets I get: \"Illegal instruction (core dumped)\"\r\n\r\n## Steps to reproduce the bug\r\n\r\n```\r\nconda create --prefix path/to/env\r\nconda activate path/to/env\r\nconda install -c huggingface -c conda-forge datasets\r\n# exits with output \"Illegal instruction (core dumped)\"\r\npython -m datasets\r\n```\r\n\r\n## Environment info\r\n\r\nWhen I run \"datasets-cli env\", I also get \"Illegal instruction (core dumped)\"\r\n\r\nIf I run the following commands:\r\n\r\n```\r\nconda create --prefix path/to/another/new/env\r\nconda activate path/to/another/new/env\r\nconda install -c huggingface transformers\r\ntransformers-cli env\r\n```\r\n\r\nThen I get:\r\n\r\n- `transformers` version: 4.11.3\r\n- Platform: Linux-5.4.0-67-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.12\r\n- PyTorch version (GPU?): not installed (NA)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n\r\nLet me know what additional information you need in order to debug this issue. Thanks in advance!", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3155/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3154", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3154/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3154/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3154/events", "html_url": "https://github.com/huggingface/datasets/issues/3154", "id": 1034361806, "node_id": "I_kwDODunzps49pxvO", "number": 3154, "title": "Sacrebleu unexpected behaviour/requirement for data format", "user": {"login": "BramVanroy", "id": 2779410, "node_id": "MDQ6VXNlcjI3Nzk0MTA=", "avatar_url": "https://avatars.githubusercontent.com/u/2779410?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BramVanroy", "html_url": "https://github.com/BramVanroy", "followers_url": "https://api.github.com/users/BramVanroy/followers", "following_url": "https://api.github.com/users/BramVanroy/following{/other_user}", "gists_url": "https://api.github.com/users/BramVanroy/gists{/gist_id}", "starred_url": "https://api.github.com/users/BramVanroy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BramVanroy/subscriptions", "organizations_url": "https://api.github.com/users/BramVanroy/orgs", "repos_url": "https://api.github.com/users/BramVanroy/repos", "events_url": "https://api.github.com/users/BramVanroy/events{/privacy}", "received_events_url": "https://api.github.com/users/BramVanroy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-10-24T08:55:33Z", "updated_at": "2021-10-31T09:08:32Z", "closed_at": "2021-10-31T09:08:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen comparing with the original `sacrebleu` implementation, the `datasets` implementation does some strange things that I do not quite understand. This issue was triggered when I was trying to implement TER and found the datasets implementation of BLEU [here](https://github.com/huggingface/datasets/pull/3153).\r\n\r\nIn the below snippet, the original sacrebleu snippet works just fine whereas the datasets implementation throws an error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport sacrebleu\r\nimport datasets\r\n\r\nrefs = [\r\n    ['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'],\r\n    ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.'],\r\n]\r\n\r\nhyps = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\r\n\r\nexpected_bleu = 48.530827\r\n\r\nds_bleu = datasets.load_metric(\"sacrebleu\")\r\n\r\nbleu_score_sb = sacrebleu.corpus_bleu(hyps, refs).score\r\nprint(bleu_score_sb, expected_bleu)\r\n# works: 48.5308...\r\nbleu_score_ds = ds_bleu.compute(predictions=hyps, references=refs)[\"score\"]\r\nprint(bleu_score_ds, expected_bleu)\r\n# ValueError: Predictions and/or references don't match the expected format.\r\n```\r\nThis seems to be related to how datasets forces the features format here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/87c71b9c29a40958973004910f97e4892559dfed/metrics/sacrebleu/sacrebleu.py#L94-L99\r\n\r\nand then manipulates the references during the compute stage here\r\n\r\nhttps://github.com/huggingface/datasets/blob/87c71b9c29a40958973004910f97e4892559dfed/metrics/sacrebleu/sacrebleu.py#L119-L122\r\n\r\nI do not quite understand why that is required since sacrebleu handles argument parsing quite well [by itself](https://github.com/mjpost/sacrebleu/blob/2787185dd0f8d224c72ee5a831d163c2ac711a47/sacrebleu/metrics/base.py#L229). \r\n## Actual results\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\bramv\\AppData\\Roaming\\JetBrains\\PyCharm2020.3\\scratches\\scratch_23.py\", line 23, in <module>\r\n    bleu_score_ds = ds_bleu.compute(predictions=hyps, references=refs)[\"score\"]\r\n  File \"C:\\dev\\python\\datasets\\src\\datasets\\metric.py\", line 392, in compute\r\n    self.add_batch(predictions=predictions, references=references)\r\n  File \"C:\\dev\\python\\datasets\\src\\datasets\\metric.py\", line 439, in add_batch\r\n    raise ValueError(\r\nValueError: Predictions and/or references don't match the expected format.\r\nExpected format: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')},\r\nInput predictions: ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.'],\r\nInput references: [['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'], ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.']]\r\n\r\n## Environment info\r\n- `datasets` version: 1.14.1.dev0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.9.2\r\n- PyArrow version: 4.0.1\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3154/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3148", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3148/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3148/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3148/events", "html_url": "https://github.com/huggingface/datasets/issues/3148", "id": 1033685208, "node_id": "I_kwDODunzps49nMjY", "number": 3148, "title": "Streaming with num_workers != 0", "user": {"login": "justheuristic", "id": 3491902, "node_id": "MDQ6VXNlcjM0OTE5MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3491902?v=4", "gravatar_id": "", "url": "https://api.github.com/users/justheuristic", "html_url": "https://github.com/justheuristic", "followers_url": "https://api.github.com/users/justheuristic/followers", "following_url": "https://api.github.com/users/justheuristic/following{/other_user}", "gists_url": "https://api.github.com/users/justheuristic/gists{/gist_id}", "starred_url": "https://api.github.com/users/justheuristic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/justheuristic/subscriptions", "organizations_url": "https://api.github.com/users/justheuristic/orgs", "repos_url": "https://api.github.com/users/justheuristic/repos", "events_url": "https://api.github.com/users/justheuristic/events{/privacy}", "received_events_url": "https://api.github.com/users/justheuristic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-10-22T15:07:17Z", "updated_at": "2022-07-04T12:14:58Z", "closed_at": "2022-07-04T12:14:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen using dataset streaming with pytorch DataLoader, the setting num_workers to anything other than 0 causes the code to freeze forever before yielding the first batch.\r\n\r\nThe code owner is likely @lhoestq \r\n\r\n## Steps to reproduce the bug\r\n\r\nFor your convenience, we've prepped a colab notebook that reproduces the bug\r\nhttps://colab.research.google.com/drive/1Mgl0oTZSNIE3UeGl_oX9wPCOIxRg19h1?usp=sharing\r\n```python\r\n!pip install datasets==1.14.0\r\n\r\nshould_freeze_forever = True\r\n# ^-- set this to True in order to freeze forever, set to False in order to work normally\r\n\r\nimport torch\r\nfrom datasets import load_dataset\r\n\r\ndata = load_dataset(\"oscar\", \"unshuffled_deduplicated_bn\", split=\"train\", streaming=True)\r\ndata = data.map(lambda x: {\"text\": x[\"text\"], \"orig\": f\"oscar[{x['id']}]\"}, batched=True)\r\ndata = data.shuffle(100, seed=1337)\r\n\r\ndata = data.with_format(\"torch\")\r\nloader = torch.utils.data.DataLoader(data, batch_size=2, num_workers=2 if should_freeze_forever else 0)\r\n\r\n# v-- the code should freeze forever at this line\r\nfor i, row in enumerate(loader):\r\n  print(row)\r\n  if i > 10: break\r\nprint(\"DONE!\")\r\n```\r\n\r\n## Expected results\r\nThe code should not freeze forever with num_workers=2\r\n\r\n## Actual results\r\nThe code freezes forever with num_workers=2\r\n\r\n## Environment info\r\n- `datasets` version: 1.14.0 (also found in previous versions)\r\n- Platform: google colab (also locally)\r\n- Python version: 3.7, (also 3.8)\r\n- PyArrow version: 3.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3148/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3148/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3146", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3146/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3146/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3146/events", "html_url": "https://github.com/huggingface/datasets/issues/3146", "id": 1033605947, "node_id": "I_kwDODunzps49m5M7", "number": 3146, "title": "CLI test command throws NonMatchingSplitsSizesError when saving infos", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-10-22T13:50:53Z", "updated_at": "2021-10-27T08:01:49Z", "closed_at": "2021-10-27T08:01:49Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "When trying to generate a datset JSON metadata, a `NonMatchingSplitsSizesError` is thrown:\r\n```\r\n$ datasets-cli test datasets/arabic_billion_words --save_infos --all_configs\r\nTesting builder 'Alittihad' (1/10)\r\nDownloading and preparing dataset arabic_billion_words/Alittihad (download: 332.13 MiB, generated: Unknown size, post-processed: Unknown size, total: 332.13 MiB) to .cache\\arabic_billion_words\\Alittihad\\1.1.0\\8175ff1c9714c6d5d15b1141b6042e5edf048276bb81a9c14e35e149a7a62ae4...\r\nTraceback (most recent call last):\r\n  File \"path\\huggingface\\datasets\\.venv\\Scripts\\datasets-cli-script.py\", line 33, in <module>\r\n    sys.exit(load_entry_point('datasets', 'console_scripts', 'datasets-cli')())\r\n  File \"path\\huggingface\\datasets\\src\\datasets\\commands\\datasets_cli.py\", line 33, in main\r\n    service.run()\r\n  File \"path\\huggingface\\datasets\\src\\datasets\\commands\\test.py\", line 144, in run\r\n    builder.download_and_prepare(\r\n  File \"path\\huggingface\\datasets\\src\\datasets\\builder.py\", line 607, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"path\\huggingface\\datasets\\src\\datasets\\builder.py\", line 709, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"path\\huggingface\\datasets\\src\\datasets\\utils\\info_utils.py\", line 74, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_splits))\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=0, num_examples=0, dataset_name='arabic_billion_words'), 'recorded': SplitInfo(name='train', num_bytes=1601790302, num_examples=349342, dataset_name='arabic_billion_words')}]\r\n```\r\n\r\nThis is due because a previous run generated a wrong `dataset_info.json`.\r\n\r\nThis error can be avoided by passing `--ignore_verifications`, but I think this should be assumed when passing `--save_infos`.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3146/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3146/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3134", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3134/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3134/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3134/events", "html_url": "https://github.com/huggingface/datasets/issues/3134", "id": 1033251755, "node_id": "I_kwDODunzps49liur", "number": 3134, "title": "Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/rouge/rouge.py", "user": {"login": "yananchen1989", "id": 26405281, "node_id": "MDQ6VXNlcjI2NDA1Mjgx", "avatar_url": "https://avatars.githubusercontent.com/u/26405281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yananchen1989", "html_url": "https://github.com/yananchen1989", "followers_url": "https://api.github.com/users/yananchen1989/followers", "following_url": "https://api.github.com/users/yananchen1989/following{/other_user}", "gists_url": "https://api.github.com/users/yananchen1989/gists{/gist_id}", "starred_url": "https://api.github.com/users/yananchen1989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yananchen1989/subscriptions", "organizations_url": "https://api.github.com/users/yananchen1989/orgs", "repos_url": "https://api.github.com/users/yananchen1989/repos", "events_url": "https://api.github.com/users/yananchen1989/events{/privacy}", "received_events_url": "https://api.github.com/users/yananchen1989/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-10-22T07:07:52Z", "updated_at": "2022-01-19T14:02:32Z", "closed_at": "2022-01-19T14:02:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "datasets version: 1.12.1\r\n\r\n`metric = datasets.load_metric('rouge')`\r\n\r\nThe error:\r\n\r\n> ConnectionError                           Traceback (most recent call last)\r\n> <ipython-input-3-dd10a0c5212f> in <module>\r\n> ----> 1 metric = datasets.load_metric('rouge')\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/datasets/load.py in load_metric(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, script_version, **metric_init_kwargs)\r\n>     613         download_config=download_config,\r\n>     614         download_mode=download_mode,\r\n> --> 615         dataset=False,\r\n>     616     )\r\n>     617     metric_cls = import_main_class(module_path, dataset=False)\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, **download_kwargs)\r\n>     328                 file_path = hf_github_url(path=path, name=name, dataset=dataset, version=script_version)\r\n>     329                 try:\r\n> --> 330                     local_path = cached_path(file_path, download_config=download_config)\r\n>     331                 except FileNotFoundError:\r\n>     332                     if script_version is not None:\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n>     296             use_etag=download_config.use_etag,\r\n>     297             max_retries=download_config.max_retries,\r\n> --> 298             use_auth_token=download_config.use_auth_token,\r\n>     299         )\r\n>     300     elif os.path.exists(url_or_filename):\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\r\n>     603             raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\n>     604         _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\r\n> --> 605         raise ConnectionError(\"Couldn't reach {}\".format(url))\r\n>     606\r\n>     607     # Try a second time\r\n> \r\n> ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/rouge/rouge.py\r\n\r\n\r\nIs there any remedy to solve the connection issue ?", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3134/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3134/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3126", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3126/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3126/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3126/events", "html_url": "https://github.com/huggingface/datasets/issues/3126", "id": 1032093055, "node_id": "I_kwDODunzps49hH1_", "number": 3126, "title": "\"arabic_billion_words\" dataset does not create the full dataset", "user": {"login": "vitalyshalumov", "id": 33824221, "node_id": "MDQ6VXNlcjMzODI0MjIx", "avatar_url": "https://avatars.githubusercontent.com/u/33824221?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vitalyshalumov", "html_url": "https://github.com/vitalyshalumov", "followers_url": "https://api.github.com/users/vitalyshalumov/followers", "following_url": "https://api.github.com/users/vitalyshalumov/following{/other_user}", "gists_url": "https://api.github.com/users/vitalyshalumov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vitalyshalumov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vitalyshalumov/subscriptions", "organizations_url": "https://api.github.com/users/vitalyshalumov/orgs", "repos_url": "https://api.github.com/users/vitalyshalumov/repos", "events_url": "https://api.github.com/users/vitalyshalumov/events{/privacy}", "received_events_url": "https://api.github.com/users/vitalyshalumov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-10-21T06:02:38Z", "updated_at": "2021-10-22T13:28:40Z", "closed_at": "2021-10-22T13:28:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen running: \r\nraw_dataset = load_dataset('arabic_billion_words','Alittihad')\r\nthe correct dataset file is pulled from the url.\r\nBut, the generated dataset includes just a small portion of the data included in the file.\r\nThis is true for all other portions of the \"arabic_billion_words\" dataset ('Almasryalyoum',.....)\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nraw_dataset = load_dataset('arabic_billion_words','Alittihad')\r\n\r\n#The screen message\r\nDownloading and preparing dataset arabic_billion_words/Alittihad (download: 332.13 MiB, generated: 20.62 MiB, post-processed: Unknown size, total: 352.74 MiB) \r\n\r\n## Expected results\r\nover 100K sentences\r\n\r\n## Actual results\r\nonly 11K sentences\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.14.0\r\n- Platform: Linux-5.8.0-63-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 4.0.1\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3126/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3126/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3123", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3123/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3123/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3123/events", "html_url": "https://github.com/huggingface/datasets/issues/3123", "id": 1031793207, "node_id": "I_kwDODunzps49f-o3", "number": 3123, "title": "Segmentation fault when loading datasets from file", "user": {"login": "TevenLeScao", "id": 26709476, "node_id": "MDQ6VXNlcjI2NzA5NDc2", "avatar_url": "https://avatars.githubusercontent.com/u/26709476?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TevenLeScao", "html_url": "https://github.com/TevenLeScao", "followers_url": "https://api.github.com/users/TevenLeScao/followers", "following_url": "https://api.github.com/users/TevenLeScao/following{/other_user}", "gists_url": "https://api.github.com/users/TevenLeScao/gists{/gist_id}", "starred_url": "https://api.github.com/users/TevenLeScao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TevenLeScao/subscriptions", "organizations_url": "https://api.github.com/users/TevenLeScao/orgs", "repos_url": "https://api.github.com/users/TevenLeScao/repos", "events_url": "https://api.github.com/users/TevenLeScao/events{/privacy}", "received_events_url": "https://api.github.com/users/TevenLeScao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-10-20T20:16:11Z", "updated_at": "2021-11-02T14:57:07Z", "closed_at": "2021-11-02T14:57:07Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nCustom dataset loading sometimes segfaults and kills the process if chunks contain a variety of features/\r\n\r\n## Steps to reproduce the bug\r\n\r\nDownload an example file:\r\n```\r\nwget https://gist.githubusercontent.com/TevenLeScao/11e2184394b3fa47d693de2550942c6b/raw/4232704d08fbfcaf93e5b51def9e5051507651ad/tiny_kelm.jsonl\r\n```\r\nThen in Python:\r\n```\r\nimport datasets\r\ntiny_kelm = datasets.load_dataset(\"json\", data_files=\"tiny_kelm.jsonl\", chunksize=100000)\r\n```\r\n\r\n## Expected results\r\na `tiny_kelm` functional dataset\r\n\r\n## Actual results\r\n\u2620\ufe0f `Segmentation fault (core dumped)` \u2620\ufe0f\r\n\r\n## Environment info\r\n- `datasets` version: 1.14.0\r\n- Platform: Linux-5.11.0-38-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3123/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3122", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3122/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3122/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3122/events", "html_url": "https://github.com/huggingface/datasets/issues/3122", "id": 1031787509, "node_id": "I_kwDODunzps49f9P1", "number": 3122, "title": "OSError with a custom dataset loading script", "user": {"login": "suzanab", "id": 38602977, "node_id": "MDQ6VXNlcjM4NjAyOTc3", "avatar_url": "https://avatars.githubusercontent.com/u/38602977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suzanab", "html_url": "https://github.com/suzanab", "followers_url": "https://api.github.com/users/suzanab/followers", "following_url": "https://api.github.com/users/suzanab/following{/other_user}", "gists_url": "https://api.github.com/users/suzanab/gists{/gist_id}", "starred_url": "https://api.github.com/users/suzanab/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suzanab/subscriptions", "organizations_url": "https://api.github.com/users/suzanab/orgs", "repos_url": "https://api.github.com/users/suzanab/repos", "events_url": "https://api.github.com/users/suzanab/events{/privacy}", "received_events_url": "https://api.github.com/users/suzanab/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2021-10-20T20:08:39Z", "updated_at": "2021-11-23T09:55:38Z", "closed_at": "2021-11-23T09:55:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am getting an OS error when trying to load the newly uploaded dataset classla/janes_tag. What puzzles me is that I have already uploaded a very similar dataset - classla/reldi_hr - with no issues. The loading scripts for the two datasets are almost identical and they have the same directory structure, yet I am only getting an error with janes_tag.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = datasets.load_dataset('classla/janes_tag', split='validation')\r\n```\r\n\r\n## Expected results\r\nDataset correctly loaded.\r\n\r\n## Actual results\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/mypath/test.py\", line 91, in <module>\r\n    load_and_print('janes_tag')\r\n  File \"C:/mypath/test.py\", line 32, in load_and_print\r\n    dataset = datasets.load_dataset('classla/{}'.format(ds_name), split='validation')\r\n  File \"C:\\mypath\\venv\\lib\\site-packages\\datasets\\load.py\", line 1632, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"C:\\mypath\\venv\\lib\\site-packages\\datasets\\builder.py\", line 608, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"C:\\mypath\\venv\\lib\\site-packages\\datasets\\builder.py\", line 704, in _download_and_prepare\r\n    ) from None\r\nOSError: Cannot find data file. \r\nOriginal error:\r\n[Errno 2] No such file or directory: 'C:\\\\mypath\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\2c9996e44bdc5af9c89bffb9e6d7a3e42fdb2f56bacab45de13b20f3032ea7ca\\\\data\\\\train_all.conllup'\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.14.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.7.5\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3122/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3117", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3117/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3117/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3117/events", "html_url": "https://github.com/huggingface/datasets/issues/3117", "id": 1031308083, "node_id": "I_kwDODunzps49eIMz", "number": 3117, "title": "CI error at each release commit", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-10-20T11:42:53Z", "updated_at": "2021-10-20T13:02:35Z", "closed_at": "2021-10-20T13:02:35Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "After 1.12.0, there is a recurrent CI error at each release commit: https://app.circleci.com/pipelines/github/huggingface/datasets/8289/workflows/665d954d-e409-4602-8202-e678594d2946/jobs/51110\r\n\r\n```\r\n____________________ LoadTest.test_load_dataset_canonical _____________________\r\n[gw0] win32 -- Python 3.6.8 C:\\tools\\miniconda3\\python.exe\r\n\r\nself = <tests.test_load.LoadTest testMethod=test_load_dataset_canonical>\r\n\r\n    def test_load_dataset_canonical(self):\r\n        scripts_version = os.getenv(\"HF_SCRIPTS_VERSION\", SCRIPTS_VERSION)\r\n        with self.assertRaises(FileNotFoundError) as context:\r\n            datasets.load_dataset(\"_dummy\")\r\n        self.assertIn(\r\n            f\"https://raw.githubusercontent.com/huggingface/datasets/{scripts_version}/datasets/_dummy/_dummy.py\",\r\n>           str(context.exception),\r\n        )\r\nE       AssertionError: 'https://raw.githubusercontent.com/huggingface/datasets/1.14.0/datasets/_dummy/_dummy.py' not found in \"Couldn't find a dataset script at C:\\\\Users\\\\circleci\\\\datasets\\\\_dummy\\\\_dummy.py or any data file in the same directory. Couldn't find '_dummy' on the Hugging Face Hub either: FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/_dummy/_dummy.py\"\r\n\r\ntests\\test_load.py:358: AssertionError\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3117/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3114", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3114/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3114/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3114/events", "html_url": "https://github.com/huggingface/datasets/issues/3114", "id": 1030693130, "node_id": "I_kwDODunzps49byEK", "number": 3114, "title": "load_from_disk in DatasetsDict/Dataset not working with PyArrowHDFS wrapper implementing fsspec.spec.AbstractFileSystem", "user": {"login": "francisco-perez-sorrosal", "id": 918006, "node_id": "MDQ6VXNlcjkxODAwNg==", "avatar_url": "https://avatars.githubusercontent.com/u/918006?v=4", "gravatar_id": "", "url": "https://api.github.com/users/francisco-perez-sorrosal", "html_url": "https://github.com/francisco-perez-sorrosal", "followers_url": "https://api.github.com/users/francisco-perez-sorrosal/followers", "following_url": "https://api.github.com/users/francisco-perez-sorrosal/following{/other_user}", "gists_url": "https://api.github.com/users/francisco-perez-sorrosal/gists{/gist_id}", "starred_url": "https://api.github.com/users/francisco-perez-sorrosal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/francisco-perez-sorrosal/subscriptions", "organizations_url": "https://api.github.com/users/francisco-perez-sorrosal/orgs", "repos_url": "https://api.github.com/users/francisco-perez-sorrosal/repos", "events_url": "https://api.github.com/users/francisco-perez-sorrosal/events{/privacy}", "received_events_url": "https://api.github.com/users/francisco-perez-sorrosal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-10-19T20:01:45Z", "updated_at": "2022-02-14T14:00:28Z", "closed_at": "2022-02-14T14:00:28Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nPassing a PyArrowHDFS implementation of fsspec.spec.AbstractFileSystem (in the `fs` param required by `load_from_disk` methods in `DatasetDict` (in datasets_dict.py) and `Dataset` (in arrow_dataset.py) results in an error when calling the download method in the `fs` parameter.\r\n\r\n\r\n## Steps to reproduce the bug\r\n\r\nThe documentation for the `fs` parameter states:\r\n\r\n```\r\nfs (:class:`~filesystems.S3FileSystem` or ``fsspec.spec.AbstractFileSystem``, optional, default ``None``):\r\n                Instance of the remote filesystem used to download the files from.\r\n```\r\n\r\n`PyArrowHDFS` from [fsspec](https://filesystem-spec.readthedocs.io/en/latest/_modules/fsspec/implementations/hdfs.html) implements `fsspec.spec.AbstractFileSystem`. However, when using it as shown below, I get an error.\r\n\r\n```python\r\nfrom fsspec.implementations.hdfs import PyArrowHDFS\r\n...\r\ntransformed_corpus_path = \"/user/my_user/clickbait/transformed_ds/\"\r\nfs = PyArrowHDFS(host, port, user, kerb_ticket=kerb_ticket)\r\ndss = DatasetDict.load_from_disk(transformed_corpus_path, fs, True)\r\n```\r\n\r\n## Expected results\r\n\r\nPrevious to load from disk, I have managed to successfully store in HDFS the data and meta-information of a DatasetDict by doing:\r\n```python\r\ntransformed_corpus_path = \"/user/my_user/clickbait/transformed_ds/\"\r\nfs = PyArrowHDFS(host, port, user, kerb_ticket=kerb_ticket)\r\nmy_datasets.save_to_disk(transformed_corpus_path, fs=fs)\r\n```\r\n\r\nAs I have 3 datasets in the DatasetDict named `my_datasets`, the previous Python code creates the following contents in HDFS:\r\n\r\n```sh\r\n$ hadoop fs -ls \"/user/my_user/clickbait/transformed_ds/\"\r\nFound 4 items\r\n-rw-------   3 my_user users         43 2021-10-19 03:08 /user/my_user/clickbait/transformed_ds/dataset_dict.json\r\ndrwx------   - my_user users          0 2021-10-19 03:08 /user/my_user/clickbait/transformed_ds/test\r\ndrwx------   - my_user users          0 2021-10-19 03:08 /user/my_user/clickbait/transformed_ds/train\r\ndrwx------   - my_user users          0 2021-10-19 03:08 /user/my_user/clickbait/transformed_ds/validation\r\n```\r\n\r\nI would expect to recover on `dss` the Arrow-backed datasets I previously saved in HDFS calling the `save_to_disk` method on the `DatasetDict` object when invoking `DatasetDict.load_from_disk(...)` as described above. \r\n\r\n## Actual results\r\n\r\nHowever, when trying to recover the saved datasets, I get this error:\r\n\r\n```\r\n...\r\n  File \"/home/fperez/dev/neuromancer/neuromancer/corpus.py\", line 186, in load_transformed_corpus_from_disk\r\n    dss = DatasetDict.load_from_disk(transformed_corpus_path, fs, True)\r\n  File \"/home/fperez/anaconda3/envs/neuromancer/lib/python3.9/site-packages/datasets/dataset_dict.py\", line 748, in load_from_disk\r\n    dataset_dict[k] = Dataset.load_from_disk(dataset_dict_split_path, fs, keep_in_memory=keep_in_memory)\r\n  File \"/home/fperez/anaconda3/envs/neuromancer/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 1048, in load_from_disk\r\n    fs.download(src_dataset_path, dataset_path.as_posix(), recursive=True)\r\n  File \"pyarrow/_hdfsio.pyx\", line 438, in pyarrow._hdfsio.HadoopFileSystem.download\r\nTypeError: download() got an unexpected keyword argument 'recursive'\r\n```\r\n\r\nExamining the [signature of the download method in pyarrow 5.0.0](https://github.com/apache/arrow/blob/54d2bd89c99df72fa091b025452f85dd5d88e3cf/python/pyarrow/_hdfsio.pyx#L438) we can see that there's no download parameter:\r\n\r\n```python\r\n    def download(self, path, stream, buffer_size=None):\r\n        with self.open(path, 'rb') as f:\r\n            f.download(stream, buffer_size=buffer_size)\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.13.3\r\n- Platform: Linux-3.10.0-1160.15.2.el7.x86_64-x86_64-with-glibc2.33\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3114/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3114/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3111", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3111/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3111/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3111/events", "html_url": "https://github.com/huggingface/datasets/issues/3111", "id": 1030598983, "node_id": "I_kwDODunzps49bbFH", "number": 3111, "title": "concatenate_datasets removes ClassLabel typing.", "user": {"login": "Dref360", "id": 8976546, "node_id": "MDQ6VXNlcjg5NzY1NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/8976546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dref360", "html_url": "https://github.com/Dref360", "followers_url": "https://api.github.com/users/Dref360/followers", "following_url": "https://api.github.com/users/Dref360/following{/other_user}", "gists_url": "https://api.github.com/users/Dref360/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dref360/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dref360/subscriptions", "organizations_url": "https://api.github.com/users/Dref360/orgs", "repos_url": "https://api.github.com/users/Dref360/repos", "events_url": "https://api.github.com/users/Dref360/events{/privacy}", "received_events_url": "https://api.github.com/users/Dref360/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-10-19T18:05:31Z", "updated_at": "2021-10-21T14:50:21Z", "closed_at": "2021-10-21T14:50:21Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen concatenating two datasets, we lose typing of ClassLabel columns.\r\n\r\nI can work on this if this is a legitimate bug,\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nfrom datasets import Dataset, ClassLabel, Value, concatenate_datasets\r\n\r\nDS_LEN = 100\r\nmy_dataset = Dataset.from_dict(\r\n    {\r\n        \"sentence\": [f\"{chr(i % 10)}\" for i in range(DS_LEN)],\r\n        \"label\": [i % 2 for i in range(DS_LEN)]\r\n    }\r\n)\r\nmy_predictions = Dataset.from_dict(\r\n    {\r\n        \"pred\": [(i + 1) % 2 for i in range(DS_LEN)]\r\n    }\r\n)\r\n\r\nmy_dataset = my_dataset.cast(datasets.Features({\"sentence\": Value(\"string\"), \"label\": ClassLabel(2, names=[\"POS\", \"NEG\"])}))\r\nprint(\"Original\")\r\nprint(my_dataset)\r\nprint(my_dataset.features)\r\n\r\n\r\nconcat_ds = concatenate_datasets([my_dataset, my_predictions], axis=1)\r\nprint(\"Concatenated\")\r\nprint(concat_ds)\r\nprint(concat_ds.features)\r\n\r\n\r\n```\r\n\r\n## Expected results\r\nThe features of `concat_ds` should contain ClassLabel.\r\n\r\n## Actual results\r\n\r\nOn master, I get: \r\n```\r\n{'sentence': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'pred': Value(dtype='int64', id=None)}\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.14.1.dev0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3111/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3104", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3104/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3104/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3104/events", "html_url": "https://github.com/huggingface/datasets/issues/3104", "id": 1029080412, "node_id": "I_kwDODunzps49VoVc", "number": 3104, "title": "Missing Zenodo 1.13.3 release", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-10-18T12:57:18Z", "updated_at": "2021-10-22T13:22:25Z", "closed_at": "2021-10-22T13:22:24Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "After `datasets` 1.13.3 release, this does not appear in Zenodo releases: https://zenodo.org/record/5570305\r\n\r\nTODO:\r\n- [x] Contact Zenodo support\r\n- [x] Check it is fixed", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3104/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3099", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3099/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3099/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3099/events", "html_url": "https://github.com/huggingface/datasets/issues/3099", "id": 1028338078, "node_id": "I_kwDODunzps49SzGe", "number": 3099, "title": "AttributeError: module 'huggingface_hub.hf_api' has no attribute 'DatasetInfo'", "user": {"login": "JTWang2000", "id": 49268567, "node_id": "MDQ6VXNlcjQ5MjY4NTY3", "avatar_url": "https://avatars.githubusercontent.com/u/49268567?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JTWang2000", "html_url": "https://github.com/JTWang2000", "followers_url": "https://api.github.com/users/JTWang2000/followers", "following_url": "https://api.github.com/users/JTWang2000/following{/other_user}", "gists_url": "https://api.github.com/users/JTWang2000/gists{/gist_id}", "starred_url": "https://api.github.com/users/JTWang2000/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JTWang2000/subscriptions", "organizations_url": "https://api.github.com/users/JTWang2000/orgs", "repos_url": "https://api.github.com/users/JTWang2000/repos", "events_url": "https://api.github.com/users/JTWang2000/events{/privacy}", "received_events_url": "https://api.github.com/users/JTWang2000/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2021-10-17T14:17:47Z", "updated_at": "2021-11-09T16:42:29Z", "closed_at": "2021-11-09T16:42:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen using `pip install datasets`\r\nor use `conda install -c huggingface -c conda-forge datasets`\r\ncannot install datasets\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"sst\", \"default\")\r\n```\r\n\r\n## Actual results\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-5-fbe7981e6e21> in <module>\r\n      1 import torch\r\n      2 import transformers\r\n----> 3 from datasets import load_dataset\r\n      4 \r\n      5 dataset = load_dataset(\"sst\", \"default\")\r\n\r\n~/miniforge3/envs/actor/lib/python3.8/site-packages/datasets/__init__.py in <module>\r\n     35 from .arrow_reader import ArrowReader, ReadInstruction\r\n     36 from .arrow_writer import ArrowWriter\r\n---> 37 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n     38 from .combine import interleave_datasets\r\n     39 from .dataset_dict import DatasetDict, IterableDatasetDict\r\n\r\n~/miniforge3/envs/actor/lib/python3.8/site-packages/datasets/builder.py in <module>\r\n     42 )\r\n     43 from .arrow_writer import ArrowWriter, BeamWriter\r\n---> 44 from .data_files import DataFilesDict, _sanitize_patterns\r\n     45 from .dataset_dict import DatasetDict, IterableDatasetDict\r\n     46 from .fingerprint import Hasher\r\n\r\n~/miniforge3/envs/actor/lib/python3.8/site-packages/datasets/data_files.py in <module>\r\n    118 \r\n    119 def _exec_patterns_in_dataset_repository(\r\n--> 120     dataset_info: huggingface_hub.hf_api.DatasetInfo,\r\n    121     patterns: List[str],\r\n    122     allowed_extensions: Optional[list] = None,\r\n\r\nAttributeError: module 'huggingface_hub.hf_api' has no attribute 'DatasetInfo'\r\n\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.13.3\r\n- Platform: macOS-11.3.1-arm64-arm-64bit\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3099/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3097", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3097/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3097/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3097/events", "html_url": "https://github.com/huggingface/datasets/issues/3097", "id": 1027750811, "node_id": "I_kwDODunzps49Qjub", "number": 3097, "title": "`ModuleNotFoundError: No module named 'fsspec.exceptions'`", "user": {"login": "VictorSanh", "id": 16107619, "node_id": "MDQ6VXNlcjE2MTA3NjE5", "avatar_url": "https://avatars.githubusercontent.com/u/16107619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VictorSanh", "html_url": "https://github.com/VictorSanh", "followers_url": "https://api.github.com/users/VictorSanh/followers", "following_url": "https://api.github.com/users/VictorSanh/following{/other_user}", "gists_url": "https://api.github.com/users/VictorSanh/gists{/gist_id}", "starred_url": "https://api.github.com/users/VictorSanh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VictorSanh/subscriptions", "organizations_url": "https://api.github.com/users/VictorSanh/orgs", "repos_url": "https://api.github.com/users/VictorSanh/repos", "events_url": "https://api.github.com/users/VictorSanh/events{/privacy}", "received_events_url": "https://api.github.com/users/VictorSanh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-10-15T19:34:38Z", "updated_at": "2021-10-18T07:51:54Z", "closed_at": "2021-10-18T07:51:54Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nI keep runnig into a fsspec ModuleNotFound error\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> from datasets import get_dataset_infos\r\n2021-10-15 15:25:37.863206: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-10-15 15:25:37.863252: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/__init__.py\", line 37, in <module>\r\n    from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 56, in <module>\r\n    from .utils.streaming_download_manager import StreamingDownloadManager\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/streaming_download_manager.py\", line 11, in <module>\r\n    from fsspec.exceptions import FSTimeoutError\r\nModuleNotFoundError: No module named 'fsspec.exceptions'\r\n```\r\n\r\nYet, I do have `fsspec`:\r\n```bash\r\nhf@victor-scale:~/dev/promptsource$ pip show fsspec\r\nName: fsspec\r\nVersion: 2021.5.0\r\nSummary: File-system specification\r\nHome-page: http://github.com/intake/filesystem_spec\r\nAuthor: None\r\nAuthor-email: None\r\nLicense: BSD\r\nLocation: /home/hf/dev/promptsource/.venv/lib/python3.7/site-packages\r\nRequires: \r\nRequired-by: datasets\r\n```\r\n\r\nWith the same version of fsspec and `datasets==1.9.0`, I don't see this problem....\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\nI can't even run `datasets-cli env` actually.., but here's my env:\r\n- `datasets` version: 1.13.3\r\n- Platform: Ubuntu 18.04\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3097/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3097/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3095", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3095/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3095/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3095/events", "html_url": "https://github.com/huggingface/datasets/issues/3095", "id": 1027453146, "node_id": "I_kwDODunzps49PbDa", "number": 3095, "title": "`cast_column` makes audio decoding fail", "user": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-10-15T13:36:58Z", "updated_at": "2023-04-07T09:43:20Z", "closed_at": "2021-10-15T15:38:30Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nAfter changing the sampling rate automatic decoding fails.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nimport datasets\r\n\r\nds = load_dataset(\"common_voice\", \"ab\", split=\"train\")\r\n\r\nds = ds.cast_column(\"audio\", datasets.features.Audio(sampling_rate=16_000))\r\n\r\nprint(ds[0][\"audio\"])  # <- this fails currently\r\n```\r\n\r\nyields:\r\n\r\n```\r\nTypeError: forward() takes 2 positional arguments but 4 were given\r\n```\r\n\r\n## Expected results\r\nno failure\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 1.13.2 (master)\r\n- Platform: Linux-5.11.0-1019-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3095/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3095/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3093", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3093/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3093/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3093/events", "html_url": "https://github.com/huggingface/datasets/issues/3093", "id": 1027262124, "node_id": "I_kwDODunzps49Osas", "number": 3093, "title": "Error loading json dataset with multiple splits if keys in nested dicts have a different order", "user": {"login": "dthulke", "id": 8331189, "node_id": "MDQ6VXNlcjgzMzExODk=", "avatar_url": "https://avatars.githubusercontent.com/u/8331189?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dthulke", "html_url": "https://github.com/dthulke", "followers_url": "https://api.github.com/users/dthulke/followers", "following_url": "https://api.github.com/users/dthulke/following{/other_user}", "gists_url": "https://api.github.com/users/dthulke/gists{/gist_id}", "starred_url": "https://api.github.com/users/dthulke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dthulke/subscriptions", "organizations_url": "https://api.github.com/users/dthulke/orgs", "repos_url": "https://api.github.com/users/dthulke/repos", "events_url": "https://api.github.com/users/dthulke/events{/privacy}", "received_events_url": "https://api.github.com/users/dthulke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-10-15T09:33:25Z", "updated_at": "2022-04-10T14:06:29Z", "closed_at": "2022-04-10T14:06:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading a json dataset with multiple splits that have nested dicts with keys in different order results in the error below.\r\n\r\nIf the keys in the nested dicts always have the same order or even if you just load a single split in which the nested dicts don't have the same order, everything works fine.\r\n\r\n## Steps to reproduce the bug\r\nCreate two json files:\r\n\r\ntrain.json\r\n```\r\n{\"a\": {\"c\": 8, \"b\": 5}}\r\n{\"a\": {\"b\": 7, \"c\": 6}}\r\n```\r\n\r\ntest.json\r\n```\r\n{\"a\": {\"b\": 1, \"c\": 2}}\r\n{\"a\": {\"b\": 3, \"c\": 4}}\r\n```\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n# Loading the files individually works (even though the keys in train.json don't have the same order)\r\nload_dataset('json', data_files={\"test\": \"test.json\"})\r\nload_dataset('json', data_files={\"train\": \"train.json\"})\r\n# Loading both splits fails\r\nload_dataset('json', data_files={\"train\": \"train.json\", \"test\": \"test.json\"})\r\n```\r\n\r\n## Expected results\r\nLoading both splits should not give an error whether the nested dicts are have the same order or not.\r\n\r\n## Actual results\r\n```\r\n>>> load_dataset('json', data_files={\"train\": \"train.json\", \"test\": \"test.json\"})\r\nUsing custom data configuration default-f1bc76fd07398c4c\r\nDownloading and preparing dataset json/default to /home/dthulke/.cache/huggingface/datasets/json/default-f1bc76fd07398c4c/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 8839.42it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 477.82it/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/dthulke/venvs/venv_torch_transformers/lib/python3.6/site-packages/datasets/load.py\", line 1632, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/dthulke/venvs/venv_torch_transformers/lib/python3.6/site-packages/datasets/builder.py\", line 608, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/dthulke/venvs/venv_torch_transformers/lib/python3.6/site-packages/datasets/builder.py\", line 697, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/dthulke/venvs/venv_torch_transformers/lib/python3.6/site-packages/datasets/builder.py\", line 1159, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/home/dthulke/venvs/venv_torch_transformers/lib/python3.6/site-packages/datasets/arrow_writer.py\", line 428, in write_table\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"pyarrow/table.pxi\", line 1596, in pyarrow.lib.Table.from_arrays\r\n  File \"pyarrow/table.pxi\", line 592, in pyarrow.lib._sanitize_arrays\r\n  File \"pyarrow/array.pxi\", line 329, in pyarrow.lib.asarray\r\n  File \"pyarrow/table.pxi\", line 277, in pyarrow.lib.ChunkedArray.cast\r\n  File \"/home/dthulke/venvs/venv_torch_transformers/lib/python3.6/site-packages/pyarrow/compute.py\", line 297, in cast\r\n    return call_function(\"cast\", [arr], options)\r\n  File \"pyarrow/_compute.pyx\", line 527, in pyarrow._compute.call_function\r\n  File \"pyarrow/_compute.pyx\", line 337, in pyarrow._compute.Function.call\r\n  File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 120, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowNotImplementedError: Unsupported cast from struct<b: int64, c: int64> to struct using function cast_struct\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.13.2\r\n- Platform: Linux-4.15.0-147-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3093/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3093/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3091", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3091/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3091/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3091/events", "html_url": "https://github.com/huggingface/datasets/issues/3091", "id": 1027251530, "node_id": "I_kwDODunzps49Op1K", "number": 3091, "title": "`blog_authorship_corpus` is broken", "user": {"login": "fdtomasi", "id": 12514317, "node_id": "MDQ6VXNlcjEyNTE0MzE3", "avatar_url": "https://avatars.githubusercontent.com/u/12514317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fdtomasi", "html_url": "https://github.com/fdtomasi", "followers_url": "https://api.github.com/users/fdtomasi/followers", "following_url": "https://api.github.com/users/fdtomasi/following{/other_user}", "gists_url": "https://api.github.com/users/fdtomasi/gists{/gist_id}", "starred_url": "https://api.github.com/users/fdtomasi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fdtomasi/subscriptions", "organizations_url": "https://api.github.com/users/fdtomasi/orgs", "repos_url": "https://api.github.com/users/fdtomasi/repos", "events_url": "https://api.github.com/users/fdtomasi/events{/privacy}", "received_events_url": "https://api.github.com/users/fdtomasi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-10-15T09:20:40Z", "updated_at": "2021-10-19T13:06:10Z", "closed_at": "2021-10-19T12:50:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe dataset `blog_authorship_corpus` is broken.\r\nBy bypassing the checksum checks, the loading does not return any error but the resulting dataset is empty.\r\nI suspect it is because the data download url is broken (http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"blog_authorship_corpus\", split=\"train\", download_mode='force_redownload')\r\n```\r\n\r\n## Expected results\r\nNo error.\r\n\r\n## Actual results\r\n```\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n/tmp/ipykernel_5237/1729238701.py in <module>\r\n      2 ds = load_dataset(\r\n      3     \"blog_authorship_corpus\", split=\"train\",\r\n----> 4     download_mode='force_redownload'\r\n      5 )\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\r\n   1115         ignore_verifications=ignore_verifications,\r\n   1116         try_from_hf_gcs=try_from_hf_gcs,\r\n-> 1117         use_auth_token=use_auth_token,\r\n   1118     )\r\n   1119 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    635                     if not downloaded_from_gcs:\r\n    636                         self._download_and_prepare(\r\n--> 637                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    638                         )\r\n    639                     # Sync info\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    707         if verify_infos:\r\n    708             verify_checksums(\r\n--> 709                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n    710             )\r\n    711 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.13.2\r\n- Platform: Linux-4.19.0-18-cloud-amd64-x86_64-with-debian-10.11\r\n- Python version: 3.7.10\r\n- PyArrow version: 5.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3091/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3089", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3089/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3089/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3089/events", "html_url": "https://github.com/huggingface/datasets/issues/3089", "id": 1026973360, "node_id": "I_kwDODunzps49Nl6w", "number": 3089, "title": "JNLPBA Dataset", "user": {"login": "sciarrilli", "id": 10460111, "node_id": "MDQ6VXNlcjEwNDYwMTEx", "avatar_url": "https://avatars.githubusercontent.com/u/10460111?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sciarrilli", "html_url": "https://github.com/sciarrilli", "followers_url": "https://api.github.com/users/sciarrilli/followers", "following_url": "https://api.github.com/users/sciarrilli/following{/other_user}", "gists_url": "https://api.github.com/users/sciarrilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/sciarrilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sciarrilli/subscriptions", "organizations_url": "https://api.github.com/users/sciarrilli/orgs", "repos_url": "https://api.github.com/users/sciarrilli/repos", "events_url": "https://api.github.com/users/sciarrilli/events{/privacy}", "received_events_url": "https://api.github.com/users/sciarrilli/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-10-15T01:16:02Z", "updated_at": "2021-10-22T08:23:57Z", "closed_at": "2021-10-22T08:23:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nThe dataset loading script for this dataset is incorrect. This is a biomedical dataset used for named entity recognition. The entities in the [script](https://github.com/huggingface/datasets/blob/master/datasets/jnlpba/jnlpba.py#L81-L83) are: O, B, and I.  The correct entities from the original data file are: \r\n\r\n['O',\r\n 'B-DNA',\r\n 'I-DNA',\r\n 'B-RNA',\r\n 'I-RNA',\r\n 'B-cell_line',\r\n 'I-cell_line',\r\n 'B-cell_type',\r\n 'I-cell_type',\r\n 'B-protein',\r\n 'I-protein']\r\n\r\n## Actual results\r\nThe dataset loader script needs to include the following NER names:\r\n\r\n['O',\r\n 'B-DNA',\r\n 'I-DNA',\r\n 'B-RNA',\r\n 'I-RNA',\r\n 'B-cell_line',\r\n 'I-cell_line',\r\n 'B-cell_type',\r\n 'I-cell_type',\r\n 'B-protein',\r\n 'I-protein']\r\n\r\nAnd the [data](https://github.com/huggingface/datasets/blob/master/datasets/jnlpba/jnlpba.py#L46) that is being pulled has been modified from the original dataset and does not include the original NER tags.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3089/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3087", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3087/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3087/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3087/events", "html_url": "https://github.com/huggingface/datasets/issues/3087", "id": 1026780469, "node_id": "I_kwDODunzps49M201", "number": 3087, "title": "Removing label column in a text classification dataset yields to errors", "user": {"login": "sgugger", "id": 35901082, "node_id": "MDQ6VXNlcjM1OTAxMDgy", "avatar_url": "https://avatars.githubusercontent.com/u/35901082?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sgugger", "html_url": "https://github.com/sgugger", "followers_url": "https://api.github.com/users/sgugger/followers", "following_url": "https://api.github.com/users/sgugger/following{/other_user}", "gists_url": "https://api.github.com/users/sgugger/gists{/gist_id}", "starred_url": "https://api.github.com/users/sgugger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sgugger/subscriptions", "organizations_url": "https://api.github.com/users/sgugger/orgs", "repos_url": "https://api.github.com/users/sgugger/repos", "events_url": "https://api.github.com/users/sgugger/events{/privacy}", "received_events_url": "https://api.github.com/users/sgugger/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-10-14T20:12:50Z", "updated_at": "2021-10-15T10:11:04Z", "closed_at": "2021-10-15T10:11:04Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThis looks like #3059 but it's not linked to the cache this time. Removing the `label` column from a text classification dataset and then performing any processing will result in an error.\r\n\r\nTo reproduce:\r\n```py\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\n\r\nraw_datasets = load_dataset(\"imdb\")\r\nraw_datasets = raw_datasets.remove_columns(\"label\")\r\n\r\nmodel_checkpoint = \"distilbert-base-cased\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\r\ncontext_length = 128\r\n\r\ndef tokenize_pad_and_truncate(texts):\r\n    return tokenizer(texts[\"text\"], truncation=True, padding=\"max_length\", max_length=context_length)\r\n\r\ntokenized_datasets = raw_datasets.map(tokenize_pad_and_truncate, batched=True)\r\n```\r\n\r\n\r\nTraceback:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-1-ba61bb32f786> in <module>\r\n     12     return tokenizer(texts[\"text\"], truncation=True, padding=\"max_length\", max_length=context_length)\r\n     13 \r\n---> 14 tokenized_datasets = raw_datasets.map(tokenize_pad_and_truncate, batched=True)\r\n\r\n~/git/datasets/src/datasets/dataset_dict.py in map(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\r\n    500                     desc=desc,\r\n    501                 )\r\n--> 502                 for k, dataset in self.items()\r\n    503             }\r\n    504         )\r\n\r\n~/git/datasets/src/datasets/dataset_dict.py in <dictcomp>(.0)\r\n    500                     desc=desc,\r\n    501                 )\r\n--> 502                 for k, dataset in self.items()\r\n    503             }\r\n    504         )\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   2051                 new_fingerprint=new_fingerprint,\r\n   2052                 disable_tqdm=disable_tqdm,\r\n-> 2053                 desc=desc,\r\n   2054             )\r\n   2055         else:\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    501             self: \"Dataset\" = kwargs.pop(\"self\")\r\n    502         # apply actual function\r\n--> 503         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    504         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    505         for dataset in datasets:\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    468         }\r\n    469         # apply actual function\r\n--> 470         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    471         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    472         # re-apply format to the output\r\n\r\n~/git/datasets/src/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    404             # Call actual function\r\n    405 \r\n--> 406             out = func(self, *args, **kwargs)\r\n    407 \r\n    408             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\r\n   2243             if os.path.exists(cache_file_name) and load_from_cache_file:\r\n   2244                 logger.warning(\"Loading cached processed dataset at %s\", cache_file_name)\r\n-> 2245                 info = self.info.copy()\r\n   2246                 info.features = features\r\n   2247                 info.task_templates = None\r\n\r\n~/git/datasets/src/datasets/info.py in copy(self)\r\n    278 \r\n    279     def copy(self) -> \"DatasetInfo\":\r\n--> 280         return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})\r\n    281 \r\n    282 \r\n\r\n~/git/datasets/src/datasets/info.py in __init__(self, description, citation, homepage, license, features, post_processed, supervised_keys, task_templates, builder_name, config_name, version, splits, download_checksums, download_size, post_processing_size, dataset_size, size_in_bytes)\r\n\r\n~/git/datasets/src/datasets/info.py in __post_init__(self)\r\n    177                 for idx, template in enumerate(self.task_templates):\r\n    178                     if isinstance(template, TextClassification):\r\n--> 179                         labels = self.features[template.label_column].names\r\n    180                         self.task_templates[idx] = TextClassification(\r\n    181                             text_column=template.text_column, label_column=template.label_column, labels=labels\r\n\r\nKeyError: 'label'\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3087/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3087/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3084", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3084/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3084/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3084/events", "html_url": "https://github.com/huggingface/datasets/issues/3084", "id": 1026428992, "node_id": "I_kwDODunzps49LhBA", "number": 3084, "title": "VisibleDeprecationWarning when using `set_format(\"numpy\")`", "user": {"login": "Rocketknight1", "id": 12866554, "node_id": "MDQ6VXNlcjEyODY2NTU0", "avatar_url": "https://avatars.githubusercontent.com/u/12866554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rocketknight1", "html_url": "https://github.com/Rocketknight1", "followers_url": "https://api.github.com/users/Rocketknight1/followers", "following_url": "https://api.github.com/users/Rocketknight1/following{/other_user}", "gists_url": "https://api.github.com/users/Rocketknight1/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rocketknight1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rocketknight1/subscriptions", "organizations_url": "https://api.github.com/users/Rocketknight1/orgs", "repos_url": "https://api.github.com/users/Rocketknight1/repos", "events_url": "https://api.github.com/users/Rocketknight1/events{/privacy}", "received_events_url": "https://api.github.com/users/Rocketknight1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Rocketknight1", "id": 12866554, "node_id": "MDQ6VXNlcjEyODY2NTU0", "avatar_url": "https://avatars.githubusercontent.com/u/12866554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rocketknight1", "html_url": "https://github.com/Rocketknight1", "followers_url": "https://api.github.com/users/Rocketknight1/followers", "following_url": "https://api.github.com/users/Rocketknight1/following{/other_user}", "gists_url": "https://api.github.com/users/Rocketknight1/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rocketknight1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rocketknight1/subscriptions", "organizations_url": "https://api.github.com/users/Rocketknight1/orgs", "repos_url": "https://api.github.com/users/Rocketknight1/repos", "events_url": "https://api.github.com/users/Rocketknight1/events{/privacy}", "received_events_url": "https://api.github.com/users/Rocketknight1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Rocketknight1", "id": 12866554, "node_id": "MDQ6VXNlcjEyODY2NTU0", "avatar_url": "https://avatars.githubusercontent.com/u/12866554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rocketknight1", "html_url": "https://github.com/Rocketknight1", "followers_url": "https://api.github.com/users/Rocketknight1/followers", "following_url": "https://api.github.com/users/Rocketknight1/following{/other_user}", "gists_url": "https://api.github.com/users/Rocketknight1/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rocketknight1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rocketknight1/subscriptions", "organizations_url": "https://api.github.com/users/Rocketknight1/orgs", "repos_url": "https://api.github.com/users/Rocketknight1/repos", "events_url": "https://api.github.com/users/Rocketknight1/events{/privacy}", "received_events_url": "https://api.github.com/users/Rocketknight1/received_events", "type": "User", "site_admin": false}, {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-10-14T13:53:01Z", "updated_at": "2021-10-22T16:04:14Z", "closed_at": "2021-10-22T16:04:14Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Code to reproduce:\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"glue\", \"mnli\")\r\n\r\nfrom transformers import AutoTokenizer\r\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\r\n\r\ndef tokenize_function(dataset):\r\n    return tokenizer(dataset['premise'])\r\n\r\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].features)\r\n\r\ntokenized_datasets.set_format(\"numpy\")\r\n\r\ntokenized_datasets['train'][5:8]\r\n```\r\nOutputs:\r\n\r\n```\r\npython3.9/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\r\n  return np.array(array, copy=False, **self.np_array_kwargs)\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3084/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3084/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3083", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3083/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3083/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3083/events", "html_url": "https://github.com/huggingface/datasets/issues/3083", "id": 1026397062, "node_id": "I_kwDODunzps49LZOG", "number": 3083, "title": "Datasets with Audio feature raise error when loaded from cache due to _resampler parameter", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-10-14T13:23:53Z", "updated_at": "2021-10-14T15:13:40Z", "closed_at": "2021-10-14T15:13:40Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAs reported by @patrickvonplaten, when loaded from the cache, datasets containing the Audio feature raise TypeError.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# load first time works\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\") \r\n\r\n# load from cache breaks\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\") \r\n```\r\n\r\n## Actual results\r\n```\r\nTypeError: __init__() got an unexpected keyword argument '_resampler'\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3083/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3080", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3080/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3080/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3080/events", "html_url": "https://github.com/huggingface/datasets/issues/3080", "id": 1026380626, "node_id": "I_kwDODunzps49LVNS", "number": 3080, "title": "Error related to timeout keyword argument", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-10-14T13:10:58Z", "updated_at": "2021-10-14T14:39:51Z", "closed_at": "2021-10-14T14:39:51Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAs reported by @patrickvonplaten, a TypeError is raised when trying to load a dataset.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\") \r\n```\r\n\r\n## Actual results\r\n```\r\nTypeError: dataset_info() got an unexpected keyword argument 'timeout'\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3080/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3080/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3076", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3076/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3076/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3076/events", "html_url": "https://github.com/huggingface/datasets/issues/3076", "id": 1026113484, "node_id": "I_kwDODunzps49KT_M", "number": 3076, "title": "Error when loading a metric", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-10-14T08:29:27Z", "updated_at": "2021-10-14T09:14:55Z", "closed_at": "2021-10-14T09:14:55Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAs reported by @sgugger, after last release, exception is thrown when loading a metric.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_metric\r\n\r\nmetric = load_metric(\"squad_v2\")\r\n```\r\n\r\n## Actual results\r\n```\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-1-e612a8cab787> in <module>\r\n      1 from datasets import load_metric\r\n----> 2 metric = load_metric(\"squad_v2\")\r\n\r\nd:\\projects\\huggingface\\datasets\\src\\datasets\\load.py in load_metric(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, script_version, **metric_init_kwargs)\r\n   1336         )\r\n   1337         revision = script_version\r\n-> 1338     metric_module = metric_module_factory(\r\n   1339         path, revision=revision, download_config=download_config, download_mode=download_mode\r\n   1340     ).module_path\r\n\r\nd:\\projects\\huggingface\\datasets\\src\\datasets\\load.py in metric_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\r\n   1237                 if not isinstance(e1, FileNotFoundError):\r\n   1238                     raise e1 from None\r\n-> 1239                 raise FileNotFoundError(\r\n   1240                     f\"Couldn't find a metric script at {relative_to_absolute_path(combined_path)}. \"\r\n   1241                     f\"Metric '{path}' doesn't exist on the Hugging Face Hub either.\"\r\n\r\nFileNotFoundError: Couldn't find a metric script at D:\\projects\\huggingface\\datasets\\squad_v2\\squad_v2.py. Metric 'squad_v2' doesn't exist on the Hugging Face Hub either.\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3076/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3073", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3073/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3073/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3073/events", "html_url": "https://github.com/huggingface/datasets/issues/3073", "id": 1025718469, "node_id": "I_kwDODunzps49IzjF", "number": 3073, "title": "Import error installing with ppc64le", "user": {"login": "gcervantes8", "id": 21228908, "node_id": "MDQ6VXNlcjIxMjI4OTA4", "avatar_url": "https://avatars.githubusercontent.com/u/21228908?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gcervantes8", "html_url": "https://github.com/gcervantes8", "followers_url": "https://api.github.com/users/gcervantes8/followers", "following_url": "https://api.github.com/users/gcervantes8/following{/other_user}", "gists_url": "https://api.github.com/users/gcervantes8/gists{/gist_id}", "starred_url": "https://api.github.com/users/gcervantes8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gcervantes8/subscriptions", "organizations_url": "https://api.github.com/users/gcervantes8/orgs", "repos_url": "https://api.github.com/users/gcervantes8/repos", "events_url": "https://api.github.com/users/gcervantes8/events{/privacy}", "received_events_url": "https://api.github.com/users/gcervantes8/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-10-13T21:37:23Z", "updated_at": "2021-10-14T16:35:46Z", "closed_at": "2021-10-14T16:33:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nInstalling the datasets library with a computer running with ppc64le seems to cause an issue when importing the datasets library.\r\n\r\n\r\n```\r\npython\r\nPython 3.6.13 | packaged by conda-forge | (default, Sep 23 2021, 07:37:44) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import datasets\r\nIllegal instruction (core dumped)\r\n```\r\n\r\nError when importing\r\n`Illegal instruction (core dumped)`\r\n\r\n## Steps to reproduce the bug\r\nI get this error when installing the library by using conda.  I can't install with pip I believe because pyarrow only has the ppc64le library on conda forge\r\n```\r\nconda create --name transformers_py36_v2 python=3.6 \r\nconda activate transformers_py36_v2 \r\nconda install datasets \r\n```\r\n\r\n## Tracebacks\r\nconda create --name transformers_py36_v2 python=3.6\r\n\r\n\r\n```\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: done\r\n\r\n\r\n==> WARNING: A newer version of conda exists. <==\r\n  current version: 4.9.2\r\n  latest version: 4.10.3\r\n\r\nPlease update conda by running\r\n\r\n    $ conda update -n base -c defaults conda\r\n\r\n\r\n\r\n## Package Plan ##\r\n\r\n  environment location: /p/home/gerryc/.conda/envs/transformers_py36_v2\r\n\r\n  added / updated specs:\r\n    - python=3.6\r\n\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  _libgcc_mutex      conda-forge/linux-ppc64le::_libgcc_mutex-0.1-conda_forge\r\n  _openmp_mutex      conda-forge/linux-ppc64le::_openmp_mutex-4.5-1_gnu\r\n  ca-certificates    conda-forge/linux-ppc64le::ca-certificates-2021.10.8-h1084571_0\r\n  certifi            pkgs/main/linux-ppc64le::certifi-2020.12.5-py36h6ffa863_0\r\n  ld_impl_linux-ppc~ conda-forge/linux-ppc64le::ld_impl_linux-ppc64le-2.36.1-ha35d02b_2\r\n  libffi             conda-forge/linux-ppc64le::libffi-3.4.2-h3b9df90_4\r\n  libgcc-ng          conda-forge/linux-ppc64le::libgcc-ng-11.2.0-h7698a5e_11\r\n  libgomp            conda-forge/linux-ppc64le::libgomp-11.2.0-h7698a5e_11\r\n  libstdcxx-ng       conda-forge/linux-ppc64le::libstdcxx-ng-11.2.0-habdf983_11\r\n  libzlib            conda-forge/linux-ppc64le::libzlib-1.2.11-h339bb43_1013\r\n  ncurses            conda-forge/linux-ppc64le::ncurses-6.2-hea85c5d_4\r\n  openssl            conda-forge/linux-ppc64le::openssl-1.1.1l-h4e0d66e_0\r\n  pip                conda-forge/noarch::pip-21.3-pyhd8ed1ab_0\r\n  python             conda-forge/linux-ppc64le::python-3.6.13-h57873ef_2_cpython\r\n  readline           conda-forge/linux-ppc64le::readline-8.1-h5c45dff_0\r\n  setuptools         pkgs/main/linux-ppc64le::setuptools-58.0.4-py36h6ffa863_0\r\n  sqlite             conda-forge/linux-ppc64le::sqlite-3.36.0-h4e2196e_2\r\n  tk                 conda-forge/linux-ppc64le::tk-8.6.11-h41c6715_1\r\n  wheel              conda-forge/noarch::wheel-0.37.0-pyhd8ed1ab_1\r\n  xz                 conda-forge/linux-ppc64le::xz-5.2.5-h6eb9509_1\r\n  zlib               conda-forge/linux-ppc64le::zlib-1.2.11-h339bb43_1013\r\n\r\n\r\nProceed ([y]/n)? y\r\n\r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n#\r\n# To activate this environment, use\r\n#\r\n#     $ conda activate transformers_py36_v2\r\n#\r\n# To deactivate an active environment, use\r\n#\r\n#     $ conda deactivate\r\n```\r\n\r\n\r\nconda activate transformers_py36_v2\r\nconda install datasets\r\n```\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: done\r\n\r\n\r\n==> WARNING: A newer version of conda exists. <==\r\n  current version: 4.9.2\r\n  latest version: 4.10.3\r\n\r\nPlease update conda by running\r\n\r\n    $ conda update -n base -c defaults conda\r\n\r\n\r\n\r\n## Package Plan ##\r\n\r\n  environment location: /p/home/gerryc/.conda/envs/transformers_py36_v2\r\n\r\n  added / updated specs:\r\n    - datasets\r\n\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  abseil-cpp         conda-forge/linux-ppc64le::abseil-cpp-20210324.2-h3b9df90_0\r\n  aiohttp            conda-forge/linux-ppc64le::aiohttp-3.7.4.post0-py36hc33305d_0\r\n  arrow-cpp          conda-forge/linux-ppc64le::arrow-cpp-5.0.0-py36hf9cf308_8_cpu\r\n  async-timeout      conda-forge/noarch::async-timeout-3.0.1-py_1000\r\n  attrs              conda-forge/noarch::attrs-21.2.0-pyhd8ed1ab_0\r\n  aws-c-cal          conda-forge/linux-ppc64le::aws-c-cal-0.5.11-hb3fac3d_0\r\n  aws-c-common       conda-forge/linux-ppc64le::aws-c-common-0.6.2-h4e0d66e_0\r\n  aws-c-event-stream conda-forge/linux-ppc64le::aws-c-event-stream-0.2.7-h76da5f2_13\r\n  aws-c-io           conda-forge/linux-ppc64le::aws-c-io-0.10.5-hf6a6c7c_0\r\n  aws-checksums      conda-forge/linux-ppc64le::aws-checksums-0.1.11-hfe76d68_7\r\n  aws-sdk-cpp        conda-forge/linux-ppc64le::aws-sdk-cpp-1.8.186-h90855e8_3\r\n  brotlipy           conda-forge/linux-ppc64le::brotlipy-0.7.0-py36hc33305d_1001\r\n  bzip2              conda-forge/linux-ppc64le::bzip2-1.0.8-h4e0d66e_4\r\n  c-ares             conda-forge/linux-ppc64le::c-ares-1.17.2-h4e0d66e_0\r\n  cffi               conda-forge/linux-ppc64le::cffi-1.14.6-py36h021ab3c_1\r\n  chardet            conda-forge/linux-ppc64le::chardet-4.0.0-py36h270354c_1\r\n  colorama           conda-forge/noarch::colorama-0.4.4-pyh9f0ad1d_0\r\n  cryptography       conda-forge/linux-ppc64le::cryptography-3.4.7-py36hc71b123_0\r\n  dataclasses        conda-forge/noarch::dataclasses-0.8-pyh787bdff_2\r\n  datasets           conda-forge/noarch::datasets-1.12.1-pyhd8ed1ab_1\r\n  dill               conda-forge/noarch::dill-0.3.4-pyhd8ed1ab_0\r\n  filelock           conda-forge/noarch::filelock-3.3.0-pyhd8ed1ab_0\r\n  fsspec             conda-forge/noarch::fsspec-2021.10.0-pyhd8ed1ab_0\r\n  gflags             conda-forge/linux-ppc64le::gflags-2.2.2-hb209c28_1004\r\n  glog               conda-forge/linux-ppc64le::glog-0.5.0-h4040248_0\r\n  grpc-cpp           conda-forge/linux-ppc64le::grpc-cpp-1.40.0-h2bf711c_2\r\n  huggingface_hub    conda-forge/noarch::huggingface_hub-0.0.19-pyhd8ed1ab_0\r\n  idna               conda-forge/noarch::idna-2.10-pyh9f0ad1d_0\r\n  idna_ssl           conda-forge/noarch::idna_ssl-1.0.0-0\r\n  importlib-metadata conda-forge/linux-ppc64le::importlib-metadata-4.8.1-py36h270354c_0\r\n  importlib_metadata conda-forge/noarch::importlib_metadata-4.8.1-hd8ed1ab_0\r\n  krb5               conda-forge/linux-ppc64le::krb5-1.19.2-haf43566_2\r\n  libblas            conda-forge/linux-ppc64le::libblas-3.9.0-11_linuxppc64le_openblas\r\n  libbrotlicommon    conda-forge/linux-ppc64le::libbrotlicommon-1.0.9-h4e0d66e_5\r\n  libbrotlidec       conda-forge/linux-ppc64le::libbrotlidec-1.0.9-h4e0d66e_5\r\n  libbrotlienc       conda-forge/linux-ppc64le::libbrotlienc-1.0.9-h4e0d66e_5\r\n  libcblas           conda-forge/linux-ppc64le::libcblas-3.9.0-11_linuxppc64le_openblas\r\n  libcurl            conda-forge/linux-ppc64le::libcurl-7.79.1-he415e40_1\r\n  libedit            conda-forge/linux-ppc64le::libedit-3.1.20191231-h41a240f_2\r\n  libev              conda-forge/linux-ppc64le::libev-4.33-h6eb9509_1\r\n  libevent           conda-forge/linux-ppc64le::libevent-2.1.10-h97db324_4\r\n  libgfortran-ng     conda-forge/linux-ppc64le::libgfortran-ng-11.2.0-hfdc3801_11\r\n  libgfortran5       conda-forge/linux-ppc64le::libgfortran5-11.2.0-he58fbb4_11\r\n  liblapack          conda-forge/linux-ppc64le::liblapack-3.9.0-11_linuxppc64le_openblas\r\n  libnghttp2         conda-forge/linux-ppc64le::libnghttp2-1.43.0-h42039ad_1\r\n  libopenblas        conda-forge/linux-ppc64le::libopenblas-0.3.17-pthreads_h486567c_1\r\n  libprotobuf        conda-forge/linux-ppc64le::libprotobuf-3.18.1-h690f14c_0\r\n  libssh2            conda-forge/linux-ppc64le::libssh2-1.10.0-ha5a9321_2\r\n  libthrift          conda-forge/linux-ppc64le::libthrift-0.15.0-h54f692e_1\r\n  libutf8proc        conda-forge/linux-ppc64le::libutf8proc-2.6.1-h4e0d66e_0\r\n  lz4-c              conda-forge/linux-ppc64le::lz4-c-1.9.3-h3b9df90_1\r\n  multidict          conda-forge/linux-ppc64le::multidict-5.2.0-py36hc33305d_0\r\n  multiprocess       conda-forge/linux-ppc64le::multiprocess-0.70.12.2-py36hc33305d_0\r\n  numpy              conda-forge/linux-ppc64le::numpy-1.19.5-py36h86665d4_1\r\n  orc                conda-forge/linux-ppc64le::orc-1.7.0-hae6b4bd_0\r\n  packaging          conda-forge/noarch::packaging-21.0-pyhd8ed1ab_0\r\n  pandas             conda-forge/linux-ppc64le::pandas-1.1.5-py36hab1a6e6_0\r\n  parquet-cpp        conda-forge/noarch::parquet-cpp-1.5.1-2\r\n  pyarrow            conda-forge/linux-ppc64le::pyarrow-5.0.0-py36h7a46c7e_8_cpu\r\n  pycparser          conda-forge/noarch::pycparser-2.20-pyh9f0ad1d_2\r\n  pyopenssl          conda-forge/noarch::pyopenssl-21.0.0-pyhd8ed1ab_0\r\n  pyparsing          conda-forge/noarch::pyparsing-2.4.7-pyh9f0ad1d_0\r\n  pysocks            conda-forge/linux-ppc64le::pysocks-1.7.1-py36h270354c_3\r\n  python-dateutil    conda-forge/noarch::python-dateutil-2.8.2-pyhd8ed1ab_0\r\n  python-xxhash      conda-forge/linux-ppc64le::python-xxhash-2.0.2-py36hc33305d_0\r\n  python_abi         conda-forge/linux-ppc64le::python_abi-3.6-2_cp36m\r\n  pytz               conda-forge/noarch::pytz-2021.3-pyhd8ed1ab_0\r\n  pyyaml             conda-forge/linux-ppc64le::pyyaml-5.4.1-py36hc33305d_1\r\n  re2                conda-forge/linux-ppc64le::re2-2021.09.01-h3b9df90_0\r\n  requests           conda-forge/noarch::requests-2.25.1-pyhd3deb0d_0\r\n  s2n                conda-forge/linux-ppc64le::s2n-1.0.10-h97db324_0\r\n  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0\r\n  snappy             conda-forge/linux-ppc64le::snappy-1.1.8-hb209c28_3\r\n  tqdm               conda-forge/noarch::tqdm-4.62.3-pyhd8ed1ab_0\r\n  typing-extensions  conda-forge/noarch::typing-extensions-3.10.0.2-hd8ed1ab_0\r\n  typing_extensions  conda-forge/noarch::typing_extensions-3.10.0.2-pyha770c72_0\r\n  urllib3            conda-forge/noarch::urllib3-1.26.7-pyhd8ed1ab_0\r\n  xxhash             conda-forge/linux-ppc64le::xxhash-0.8.0-h4e0d66e_3\r\n  yaml               conda-forge/linux-ppc64le::yaml-0.2.5-h6eb9509_0\r\n  yarl               conda-forge/linux-ppc64le::yarl-1.6.3-py36hc33305d_2\r\n  zipp               conda-forge/noarch::zipp-3.6.0-pyhd8ed1ab_0\r\n  zstd               conda-forge/linux-ppc64le::zstd-1.5.0-h65c4b1a_0\r\n\r\nThe following packages will be UPDATED:\r\n\r\n  certifi            pkgs/main::certifi-2020.12.5-py36h6ff~ --> conda-forge::certifi-2021.5.30-py36h270354c_0\r\n\r\n\r\nProceed ([y]/n)? y\r\n\r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n```\r\n\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Red Hat Enterprise Linux 8.2 (Ootpa)\r\n- Python version: 3.6\r\n- PyArrow version: pyarrow - 5.0.0 - py36h7a46c7e_8_cpu - conda-forge\r\n\r\n\r\nAny help would be appreciated! I've been struggling on installing datasets on this machine.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3073/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3069", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3069/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3069/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3069/events", "html_url": "https://github.com/huggingface/datasets/issues/3069", "id": 1024818680, "node_id": "I_kwDODunzps49FX34", "number": 3069, "title": "CI fails on Windows with FileNotFoundError when stting up s3_base fixture", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-10-13T05:52:26Z", "updated_at": "2021-10-13T08:05:49Z", "closed_at": "2021-10-13T06:49:48Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAfter commit 9353fc863d0c99ab0427f83cc5a4f04fcf52f1df, the CI fails on Windows  with FileNotFoundError when stting up s3_base fixture. See: https://app.circleci.com/pipelines/github/huggingface/datasets/8151/workflows/5db8d154-badd-4d3d-b202-ca7a318997a2/jobs/50321\r\n\r\nError summary:\r\n```\r\nERROR tests/test_arrow_dataset.py::test_dummy_dataset_serialize_s3 - FileNotF...\r\nERROR tests/test_dataset_dict.py::test_dummy_dataset_serialize_s3 - FileNotFo...\r\n```\r\n\r\nStack trace:\r\n```\r\n______________ ERROR at setup of test_dummy_dataset_serialize_s3 ______________\r\n[gw0] win32 -- Python 3.6.8 C:\\tools\\miniconda3\\python.exe\r\n\r\n    @pytest.fixture()\r\n    def s3_base():\r\n        # writable local S3 system\r\n        import shlex\r\n        import subprocess\r\n    \r\n        # Mocked AWS Credentials for moto.\r\n        old_environ = os.environ.copy()\r\n        os.environ.update(S3_FAKE_ENV_VARS)\r\n    \r\n>       proc = subprocess.Popen(shlex.split(\"moto_server s3 -p %s\" % s3_port))\r\n\r\ntests\\s3_fixtures.py:32: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nC:\\tools\\miniconda3\\lib\\subprocess.py:729: in __init__\r\n    restore_signals, start_new_session)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <subprocess.Popen object at 0x0000012BB8A4B908>\r\nargs = 'moto_server s3 -p 5555', executable = None, preexec_fn = None\r\nclose_fds = True, pass_fds = (), cwd = None, env = None\r\nstartupinfo = <subprocess.STARTUPINFO object at 0x0000012BB8177630>\r\ncreationflags = 0, shell = False, p2cread = -1, p2cwrite = -1, c2pread = -1\r\nc2pwrite = -1, errread = -1, errwrite = -1, unused_restore_signals = True\r\nunused_start_new_session = False\r\n\r\n    def _execute_child(self, args, executable, preexec_fn, close_fds,\r\n                       pass_fds, cwd, env,\r\n                       startupinfo, creationflags, shell,\r\n                       p2cread, p2cwrite,\r\n                       c2pread, c2pwrite,\r\n                       errread, errwrite,\r\n                       unused_restore_signals, unused_start_new_session):\r\n        \"\"\"Execute program (MS Windows version)\"\"\"\r\n    \r\n        assert not pass_fds, \"pass_fds not supported on Windows.\"\r\n    \r\n        if not isinstance(args, str):\r\n            args = list2cmdline(args)\r\n    \r\n        # Process startup details\r\n        if startupinfo is None:\r\n            startupinfo = STARTUPINFO()\r\n        if -1 not in (p2cread, c2pwrite, errwrite):\r\n            startupinfo.dwFlags |= _winapi.STARTF_USESTDHANDLES\r\n            startupinfo.hStdInput = p2cread\r\n            startupinfo.hStdOutput = c2pwrite\r\n            startupinfo.hStdError = errwrite\r\n    \r\n        if shell:\r\n            startupinfo.dwFlags |= _winapi.STARTF_USESHOWWINDOW\r\n            startupinfo.wShowWindow = _winapi.SW_HIDE\r\n            comspec = os.environ.get(\"COMSPEC\", \"cmd.exe\")\r\n            args = '{} /c \"{}\"'.format (comspec, args)\r\n    \r\n        # Start the process\r\n        try:\r\n            hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\r\n                                     # no special security\r\n                                     None, None,\r\n                                     int(not close_fds),\r\n                                     creationflags,\r\n                                     env,\r\n                                     os.fspath(cwd) if cwd is not None else None,\r\n>                                    startupinfo)\r\nE                                    FileNotFoundError: [WinError 2] The system cannot find the file specified\r\n\r\nC:\\tools\\miniconda3\\lib\\subprocess.py:1017: FileNotFoundError\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3069/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3060", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3060/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3060/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3060/events", "html_url": "https://github.com/huggingface/datasets/issues/3060", "id": 1022936396, "node_id": "I_kwDODunzps48-MVM", "number": 3060, "title": "load_dataset('openwebtext') yields \"Compressed file ended before the end-of-stream marker was reached\"", "user": {"login": "RylanSchaeffer", "id": 8942987, "node_id": "MDQ6VXNlcjg5NDI5ODc=", "avatar_url": "https://avatars.githubusercontent.com/u/8942987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RylanSchaeffer", "html_url": "https://github.com/RylanSchaeffer", "followers_url": "https://api.github.com/users/RylanSchaeffer/followers", "following_url": "https://api.github.com/users/RylanSchaeffer/following{/other_user}", "gists_url": "https://api.github.com/users/RylanSchaeffer/gists{/gist_id}", "starred_url": "https://api.github.com/users/RylanSchaeffer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RylanSchaeffer/subscriptions", "organizations_url": "https://api.github.com/users/RylanSchaeffer/orgs", "repos_url": "https://api.github.com/users/RylanSchaeffer/repos", "events_url": "https://api.github.com/users/RylanSchaeffer/events{/privacy}", "received_events_url": "https://api.github.com/users/RylanSchaeffer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-10-11T17:05:27Z", "updated_at": "2021-10-28T05:52:21Z", "closed_at": "2021-10-28T05:52:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen I try `load_dataset('openwebtext')`, I receive a \"EOFError: Compressed file ended before the end-of-stream marker was reached\" error.\r\n\r\n## Steps to reproduce the bug\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('openwebtext')\r\n```\r\n\r\n## Expected results\r\nI expect the `dataset` variable to be properly constructed.\r\n\r\n## Actual results\r\n\r\n```\r\nFile \"/home/rschaef/CoCoSci-Language-Distillation/distillation_v2/ratchet_learning/tasks/base.py\", line 37, in create_dataset\r\n    dataset_str,\r\n  File \"/home/rschaef/CoCoSci-Language-Distillation/cocosci/lib/python3.6/site-packages/datasets/load.py\", line 1117, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/rschaef/CoCoSci-Language-Distillation/cocosci/lib/python3.6/site-packages/datasets/builder.py\", line 637, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/rschaef/CoCoSci-Language-Distillation/cocosci/lib/python3.6/site-packages/datasets/builder.py\", line 704, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/rschaef/.cache/huggingface/modules/datasets_modules/datasets/openwebtext/85b3ae7051d2d72e7c5fdf6dfb462603aaa26e9ed506202bf3a24d261c6c40a1/openwebtext.py\", line 61, in _split_generators\r\n    dl_dir = dl_manager.download_and_extract(_URL)\r\n  File \"/home/rschaef/CoCoSci-Language-Distillation/cocosci/lib/python3.6/site-packages/datasets/utils/download_manager.py\", line 284, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/home/rschaef/CoCoSci-Language-Distillation/cocosci/lib/python3.6/site-packages/datasets/utils/download_manager.py\", line 261, in extract\r\n    partial(cached_path, download_config=download_config), path_or_paths, num_proc=num_proc, disable_tqdm=False\r\n  File \"/home/rschaef/CoCoSci-Language-Distillation/cocosci/lib/python3.6/site-packages/datasets/utils/py_utils.py\", line 197, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/rschaef/CoCoSci-Language-Distillation/cocosci/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 316, in cached_path\r\n    output_path, force_extract=download_config.force_extract\r\n  File \"/home/rschaef/CoCoSci-Language-Distillation/cocosci/lib/python3.6/site-packages/datasets/utils/extract.py\", line 40, in extract\r\n    self.extractor.extract(input_path, output_path, extractor=extractor)\r\n  File \"/home/rschaef/CoCoSci-Language-Distillation/cocosci/lib/python3.6/site-packages/datasets/utils/extract.py\", line 179, in extract\r\n    return extractor.extract(input_path, output_path)\r\n  File \"/home/rschaef/CoCoSci-Language-Distillation/cocosci/lib/python3.6/site-packages/datasets/utils/extract.py\", line 53, in extract\r\n    tar_file.extractall(output_path)\r\n  File \"/usr/lib/python3.6/tarfile.py\", line 2010, in extractall\r\n    numeric_owner=numeric_owner)\r\n  File \"/usr/lib/python3.6/tarfile.py\", line 2052, in extract\r\n    numeric_owner=numeric_owner)\r\n  File \"/usr/lib/python3.6/tarfile.py\", line 2122, in _extract_member\r\n    self.makefile(tarinfo, targetpath)\r\n  File \"/usr/lib/python3.6/tarfile.py\", line 2171, in makefile\r\n    copyfileobj(source, target, tarinfo.size, ReadError, bufsize)\r\n  File \"/usr/lib/python3.6/tarfile.py\", line 249, in copyfileobj\r\n    buf = src.read(bufsize)\r\n  File \"/usr/lib/python3.6/lzma.py\", line 200, in read\r\n    return self._buffer.read(size)\r\n  File \"/usr/lib/python3.6/_compression.py\", line 68, in readinto\r\n    data = self.read(len(byte_view))\r\n  File \"/usr/lib/python3.6/_compression.py\", line 99, in read\r\n    raise EOFError(\"Compressed file ended before the \"\r\npython-BaseException\r\nEOFError: Compressed file ended before the end-of-stream marker was reached\r\n```\r\n\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-4.4.0-173-generic-x86_64-with-Ubuntu-16.04-xenial\r\n- Python version: 3.6.10\r\n- PyArrow version: 5.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3060/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3058", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3058/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3058/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3058/events", "html_url": "https://github.com/huggingface/datasets/issues/3058", "id": 1022612664, "node_id": "I_kwDODunzps4889S4", "number": 3058, "title": "Dataset wikipedia and Bookcorpusopen cannot be fetched from dataloader.", "user": {"login": "hobbitlzy", "id": 35392624, "node_id": "MDQ6VXNlcjM1MzkyNjI0", "avatar_url": "https://avatars.githubusercontent.com/u/35392624?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hobbitlzy", "html_url": "https://github.com/hobbitlzy", "followers_url": "https://api.github.com/users/hobbitlzy/followers", "following_url": "https://api.github.com/users/hobbitlzy/following{/other_user}", "gists_url": "https://api.github.com/users/hobbitlzy/gists{/gist_id}", "starred_url": "https://api.github.com/users/hobbitlzy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hobbitlzy/subscriptions", "organizations_url": "https://api.github.com/users/hobbitlzy/orgs", "repos_url": "https://api.github.com/users/hobbitlzy/repos", "events_url": "https://api.github.com/users/hobbitlzy/events{/privacy}", "received_events_url": "https://api.github.com/users/hobbitlzy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-10-11T11:54:59Z", "updated_at": "2022-01-19T14:03:49Z", "closed_at": "2022-01-19T14:03:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI have used the previous version of `transformers` and `datasets`. The dataset `wikipedia` can be successfully used. Recently, I upgrade them to the newest version and find it raises errors. I also tried other datasets. The `wikitext` works and the `bookcorpusopen` raises the same errors as `wikipedia`.\r\n\r\n## Steps to reproduce the bug\r\nRun the `run_mlm_no_trainer.py` and the given script on this [link](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling). Change the dataset from wikitext to wikipedia or bookcorpusopen. BTW, the library transformers is of version 4.11.3.\r\n\r\n## Expected results\r\nThe data batchs are fetched from the data loader and train.\r\n\r\n## Actual results\r\nThe first time to fetch data batch occurs error.\r\n`Traceback (most recent call last):\r\n  File \"/home/zyli/anaconda3/envs/LatestStacking/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 705, in convert_to_tensors\r\n    tensor = as_tensor(value)\r\nValueError: too many dimensions 'str'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"src/original_run_mlm_no_trainer.py\", line 528, in <module>\r\n    main()\r\n  File \"src/original_run_mlm_no_trainer.py\", line 488, in main\r\n    for step, batch in enumerate(train_dataloader):\r\n  File \"/home/zyli/anaconda3/envs/LatestStacking/lib/python3.7/site-packages/accelerate/data_loader.py\", line 303, in __iter__\r\n    for batch in super().__iter__():\r\n  File \"/home/zyli/anaconda3/envs/LatestStacking/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 517, in __next__\r\n    data = self._next_data()\r\n  File \"/home/zyli/anaconda3/envs/LatestStacking/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 557, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"/home/zyli/anaconda3/envs/LatestStacking/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\r\n    return self.collate_fn(data)\r\n  File \"/home/zyli/anaconda3/envs/LatestStacking/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 41, in __call__\r\n    return self.torch_call(features)\r\n  File \"/home/zyli/anaconda3/envs/LatestStacking/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 671, in torch_call\r\n    batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\r\n  File \"/home/zyli/anaconda3/envs/LatestStacking/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2774, in pad\r\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\r\n  File \"/home/zyli/anaconda3/envs/LatestStacking/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\r\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\r\n  File \"/home/zyli/anaconda3/envs/LatestStacking/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 722, in convert_to_tensors\r\n    \"Unable to create tensor, you should probably activate truncation and/or padding \"\r\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\r\n`\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.8.0-59-generic-x86_64-with-debian-bullseye-sid\r\n- Python version: 3.7.6\r\n- PyArrow version: 5.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3058/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3058/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3057", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3057/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3057/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3057/events", "html_url": "https://github.com/huggingface/datasets/issues/3057", "id": 1022508315, "node_id": "I_kwDODunzps488j0b", "number": 3057, "title": "Error in per class precision computation ", "user": {"login": "tidhamecha2", "id": 38906722, "node_id": "MDQ6VXNlcjM4OTA2NzIy", "avatar_url": "https://avatars.githubusercontent.com/u/38906722?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tidhamecha2", "html_url": "https://github.com/tidhamecha2", "followers_url": "https://api.github.com/users/tidhamecha2/followers", "following_url": "https://api.github.com/users/tidhamecha2/following{/other_user}", "gists_url": "https://api.github.com/users/tidhamecha2/gists{/gist_id}", "starred_url": "https://api.github.com/users/tidhamecha2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tidhamecha2/subscriptions", "organizations_url": "https://api.github.com/users/tidhamecha2/orgs", "repos_url": "https://api.github.com/users/tidhamecha2/repos", "events_url": "https://api.github.com/users/tidhamecha2/events{/privacy}", "received_events_url": "https://api.github.com/users/tidhamecha2/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-10-11T10:05:19Z", "updated_at": "2021-10-11T10:17:44Z", "closed_at": "2021-10-11T10:16:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen trying to get the per class precision values by providing `average=None`, following error is thrown `ValueError: can only convert an array of size 1 to a Python scalar`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\nprecision_metric = load_metric(\"precision\")\r\npredictions = [0, 2, 1, 0, 0, 1]\r\nreferences = [0, 1, 2, 0, 1, 2]\r\nresults = precision_metric.compute(predictions=predictions, references=references, average=None)\r\n```\r\n\r\n## Expected results\r\n`  {'precision': array([0.66666667, 0.        , 0.        ])}`\r\nas per https://github.com/huggingface/datasets/blob/master/metrics/precision/precision.py\r\n\r\n## Actual results\r\n```\r\n   output = self._compute(predictions=predictions, references=references, **kwargs)\r\n  File \"~/.cache/huggingface/modules/datasets_modules/metrics/precision/94709a71c6fe37171ef49d3466fec24dee9a79846c9f176dff66a649e9811690/precision.py\", line 110, in _compute\r\n    sample_weight=sample_weight,\r\nValueError: can only convert an array of size 1 to a Python scalar\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: linux\r\n- Python version: 3.6.9\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3057/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3055", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3055/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3055/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3055/events", "html_url": "https://github.com/huggingface/datasets/issues/3055", "id": 1022319238, "node_id": "I_kwDODunzps4871qG", "number": 3055, "title": "CI test suite fails after meteor metric update", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-10-11T06:37:12Z", "updated_at": "2021-10-11T07:30:31Z", "closed_at": "2021-10-11T07:30:31Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nCI test suite fails: https://app.circleci.com/pipelines/github/huggingface/datasets/8110/workflows/f059ba43-9154-4632-bebb-82318447ddc9/jobs/50010\r\n\r\nStack trace:\r\n```\r\n___________________ LocalMetricTest.test_load_metric_meteor ____________________\r\n[gw1] linux -- Python 3.6.15 /home/circleci/.pyenv/versions/3.6.15/bin/python3.6\r\n\r\nself = <tests.test_metric_common.LocalMetricTest testMethod=test_load_metric_meteor>\r\nmetric_name = 'meteor'\r\n\r\n    def test_load_metric(self, metric_name):\r\n        doctest.ELLIPSIS_MARKER = \"[...]\"\r\n        metric_module = importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0])\r\n        metric = datasets.load.import_main_class(metric_module.__name__, dataset=False)\r\n        # check parameters\r\n        parameters = inspect.signature(metric._compute).parameters\r\n        self.assertTrue(\"predictions\" in parameters)\r\n        self.assertTrue(\"references\" in parameters)\r\n        self.assertTrue(all([p.kind != p.VAR_KEYWORD for p in parameters.values()]))  # no **kwargs\r\n        # run doctest\r\n        with self.patch_intensive_calls(metric_name, metric_module.__name__):\r\n            with self.use_local_metrics():\r\n>               results = doctest.testmod(metric_module, verbose=True, raise_on_error=True)\r\n\r\ntests/test_metric_common.py:75: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../.pyenv/versions/3.6.15/lib/python3.6/doctest.py:1951: in testmod\r\n    runner.run(test)\r\n../.pyenv/versions/3.6.15/lib/python3.6/doctest.py:1839: in run\r\n    r = DocTestRunner.run(self, test, compileflags, out, False)\r\n../.pyenv/versions/3.6.15/lib/python3.6/doctest.py:1476: in run\r\n    return self.__run(test, compileflags, out)\r\n../.pyenv/versions/3.6.15/lib/python3.6/doctest.py:1382: in __run\r\n    exception)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <doctest.DebugRunner object at 0x7f4c26bd3da0>\r\nout = <built-in method write of _io.TextIOWrapper object at 0x7f51a21852d0>\r\ntest = <DocTest datasets_modules.datasets.meteor.6201bb45d2c144ea7963680949d20f523d74a741fa0f8a806f836e6caa5245d7.meteor.Mete...ets_modules/datasets/meteor/6201bb45d2c144ea7963680949d20f523d74a741fa0f8a806f836e6caa5245d7/meteor.py:87 (5 examples)>\r\nexample = <doctest.Example object at 0x7f4c26bd3eb8>\r\nexc_info = (<class 'TypeError'>, TypeError('\"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): It is a guide to action which ensures that the military always obeys the commands of the party',), <traceback object at 0x7f4cd01afec8>)\r\n\r\n    def report_unexpected_exception(self, out, test, example, exc_info):\r\n>       raise UnexpectedException(test, example, exc_info)\r\nE       doctest.UnexpectedException: <DocTest datasets_modules.datasets.meteor.6201bb45d2c144ea7963680949d20f523d74a741fa0f8a806f836e6caa5245d7.meteor.Meteor from /tmp/pytest-of-circleci/pytest-0/popen-gw1/cache/modules/datasets_modules/datasets/meteor/6201bb45d2c144ea7963680949d20f523d74a741fa0f8a806f836e6caa5245d7/meteor.py:87 (5 examples)>\r\n\r\n../.pyenv/versions/3.6.15/lib/python3.6/doctest.py:1845: UnexpectedException\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3055/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3053", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3053/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3053/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3053/events", "html_url": "https://github.com/huggingface/datasets/issues/3053", "id": 1022076905, "node_id": "I_kwDODunzps4866fp", "number": 3053, "title": "load_dataset('the_pile_openwebtext2') produces ArrowInvalid, value too large to fit in C integer type", "user": {"login": "davidbau", "id": 3458792, "node_id": "MDQ6VXNlcjM0NTg3OTI=", "avatar_url": "https://avatars.githubusercontent.com/u/3458792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidbau", "html_url": "https://github.com/davidbau", "followers_url": "https://api.github.com/users/davidbau/followers", "following_url": "https://api.github.com/users/davidbau/following{/other_user}", "gists_url": "https://api.github.com/users/davidbau/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidbau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidbau/subscriptions", "organizations_url": "https://api.github.com/users/davidbau/orgs", "repos_url": "https://api.github.com/users/davidbau/repos", "events_url": "https://api.github.com/users/davidbau/events{/privacy}", "received_events_url": "https://api.github.com/users/davidbau/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-10-10T19:55:21Z", "updated_at": "2023-02-24T14:02:20Z", "closed_at": "2023-02-24T14:02:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen loading `the_pile_openwebtext2`, we get the error `pyarrow.lib.ArrowInvalid: Value 2111 too large to fit in C integer type`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nds = datasets.load_dataset('the_pile_openwebtext2')\r\n```\r\n\r\n## Expected results\r\nShould download the dataset, convert it to an arrow file, and return a working Dataset object.\r\n\r\n## Actual results\r\nThe download works, but conversion to the arrow file fails as follows:\r\n\r\n```\r\n>>> ds = datasets.load_dataset('the_pile_openwebtext2')\r\nDownloading and preparing dataset openwebtext2/plain_text (download: 27.33 GiB, generated: 63.86 GiB\r\n, post-processed: Unknown size, total: 91.19 GiB) to /home/davidbau/.cache/huggingface/datasets/open\r\nwebtext2/plain_text/1.0.0/c48ec73ba3483bac673463f48f67e9a4fd8cb49a9d6ec4fb957f0b424b97cf25...\r\nTraceback (most recent call last):\r\n  File \"/home/davidbau/.conda/envs/tenv/lib/python3.9/site-packages/datasets/builder.py\", line 1133,\r\n in _prepare_split\r\n    writer.write(example, key)\r\n  File \"/home/davidbau/.conda/envs/tenv/lib/python3.9/site-packages/datasets/arrow_writer.py\", line\r\n366, in write\r\n    self.write_examples_on_file()\r\n  File \"/home/davidbau/.conda/envs/tenv/lib/python3.9/site-packages/datasets/arrow_writer.py\", line\r\n311, in write_examples_on_file\r\n    pa_array = pa.array(typed_sequence)\r\n  File \"pyarrow/array.pxi\", line 222, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/home/davidbau/.conda/envs/tenv/lib/python3.9/site-packages/datasets/arrow_writer.py\", line\r\n115, in __arrow_array__\r\n    out = pa.array(cast_to_python_objects(self.data, only_1d_for_numpy=True), type=type)\r\n  File \"pyarrow/array.pxi\", line 305, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 39, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Value 2111 too large to fit in C integer type\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n```\r\n\r\n- Platform: Ubuntu 20.04\r\n- Python version: python 3.9\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3053/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3053/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3052", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3052/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3052/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3052/events", "html_url": "https://github.com/huggingface/datasets/issues/3052", "id": 1021944435, "node_id": "I_kwDODunzps486aJz", "number": 3052, "title": "load_dataset cannot download the data and hangs on forever if cache dir specified", "user": {"login": "BenoitDalFerro", "id": 69694610, "node_id": "MDQ6VXNlcjY5Njk0NjEw", "avatar_url": "https://avatars.githubusercontent.com/u/69694610?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BenoitDalFerro", "html_url": "https://github.com/BenoitDalFerro", "followers_url": "https://api.github.com/users/BenoitDalFerro/followers", "following_url": "https://api.github.com/users/BenoitDalFerro/following{/other_user}", "gists_url": "https://api.github.com/users/BenoitDalFerro/gists{/gist_id}", "starred_url": "https://api.github.com/users/BenoitDalFerro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BenoitDalFerro/subscriptions", "organizations_url": "https://api.github.com/users/BenoitDalFerro/orgs", "repos_url": "https://api.github.com/users/BenoitDalFerro/repos", "events_url": "https://api.github.com/users/BenoitDalFerro/events{/privacy}", "received_events_url": "https://api.github.com/users/BenoitDalFerro/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-10-10T10:31:36Z", "updated_at": "2021-10-11T10:57:09Z", "closed_at": "2021-10-11T10:56:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nAfter updating datasets, a code that ran just fine for ages began to fail. Specifying _datasets.load_dataset_'s _cache_dir_ optional argument on Windows 10 machine results in data download to hang on forever. Same call without cache_dir works just fine. Surprisingly exact same code just runs perfectly fine on Linux docker instance running in cloud.\r\n\r\nUnfortunately I updated Windows also at the same time and I can't remember which version of datasets was running in my conda environment prior to the update otherwise I would have tried both to check this out.  :(\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\n```\r\ncache_dir = 'c:/data/datasets'\r\ndataset = load_dataset('wikipedia', '20200501.en', split='train',cache_dir=cache_dir) \r\n```\r\nNote that exact same code without specifying _cache_dir_ argument works perfectly fine.\r\n```\r\ncache_dir = 'c:/data/datasets'\r\ndataset = load_dataset('wikipedia', '20200501.en', split='train') \r\n```\r\n\r\n## Expected results\r\nDownloads the dataset and cache is handled in the _cache_dir_ directory\r\n## Actual results\r\nData download keeps hanging on forever, **NO TRACEBACK**!\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Windows-10-10.0.19042-SP0\r\n- Python version: 3.8.11\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3052/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3051", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3051/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3051/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3051/events", "html_url": "https://github.com/huggingface/datasets/issues/3051", "id": 1021852234, "node_id": "I_kwDODunzps486DpK", "number": 3051, "title": "Non-Matching Checksum Error with crd3 dataset", "user": {"login": "RylanSchaeffer", "id": 8942987, "node_id": "MDQ6VXNlcjg5NDI5ODc=", "avatar_url": "https://avatars.githubusercontent.com/u/8942987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RylanSchaeffer", "html_url": "https://github.com/RylanSchaeffer", "followers_url": "https://api.github.com/users/RylanSchaeffer/followers", "following_url": "https://api.github.com/users/RylanSchaeffer/following{/other_user}", "gists_url": "https://api.github.com/users/RylanSchaeffer/gists{/gist_id}", "starred_url": "https://api.github.com/users/RylanSchaeffer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RylanSchaeffer/subscriptions", "organizations_url": "https://api.github.com/users/RylanSchaeffer/orgs", "repos_url": "https://api.github.com/users/RylanSchaeffer/repos", "events_url": "https://api.github.com/users/RylanSchaeffer/events{/privacy}", "received_events_url": "https://api.github.com/users/RylanSchaeffer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-10-10T01:32:43Z", "updated_at": "2022-03-15T15:54:26Z", "closed_at": "2022-03-15T15:54:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen I try loading the crd3 dataset (https://huggingface.co/datasets/crd3), an error is thrown.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = load_dataset('crd3', split='train')\r\n```\r\n\r\n## Expected results\r\nI expect no error to be thrown.\r\n\r\n\r\n## Actual results\r\nA non-matching checksum error is thrown.\r\n\r\n```\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://github.com/RevanthRameshkumar/CRD3/archive/master.zip']\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-4.4.0-173-generic-x86_64-with-Ubuntu-16.04-xenial\r\n- Python version: 3.6.10\r\n- PyArrow version: 5.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3051/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3049", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3049/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3049/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3049/events", "html_url": "https://github.com/huggingface/datasets/issues/3049", "id": 1021770008, "node_id": "I_kwDODunzps485vkY", "number": 3049, "title": "TimeoutError during streaming", "user": {"login": "borisdayma", "id": 715491, "node_id": "MDQ6VXNlcjcxNTQ5MQ==", "avatar_url": "https://avatars.githubusercontent.com/u/715491?v=4", "gravatar_id": "", "url": "https://api.github.com/users/borisdayma", "html_url": "https://github.com/borisdayma", "followers_url": "https://api.github.com/users/borisdayma/followers", "following_url": "https://api.github.com/users/borisdayma/following{/other_user}", "gists_url": "https://api.github.com/users/borisdayma/gists{/gist_id}", "starred_url": "https://api.github.com/users/borisdayma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/borisdayma/subscriptions", "organizations_url": "https://api.github.com/users/borisdayma/orgs", "repos_url": "https://api.github.com/users/borisdayma/repos", "events_url": "https://api.github.com/users/borisdayma/events{/privacy}", "received_events_url": "https://api.github.com/users/borisdayma/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-10-09T18:06:51Z", "updated_at": "2021-10-11T09:35:38Z", "closed_at": "2021-10-11T09:35:38Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI got a TimeoutError after streaming for about 10h.\r\n\r\n## Steps to reproduce the bug\r\nVery long code but we could do a test of streaming indefinitely data, though error may take a while to appear.\r\n\r\n## Expected results\r\nThis error was not expected in the code which considers only `ClientError` but not `TimeoutError`.\r\nSee [this line](https://github.com/huggingface/datasets/blob/2814fbd0e18150be409f10804670e98d9ecb87d4/src/datasets/utils/streaming_download_manager.py#L129).\r\nBased on the traceback, it looks like the `TimeoutError` was not captured.\r\n\r\n## Actual results\r\n```\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/fsspec/asyn.py\", line 25, in _runner\r\n    result[0] = await coro\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/fsspec/implementations/http.py\", line 614, in async_fetch_range\r\n    out = await r.read()\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/aiohttp/client_reqrep.py\", line 1032, in read\r\n    self._body = await self.content.read()\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/aiohttp/streams.py\", line 370, in read\r\n    block = await self.readany()\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/aiohttp/streams.py\", line 392, in readany\r\n    await self._wait(\"readany\")\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/aiohttp/streams.py\", line 306, in _wait\r\n    await waiter\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/aiohttp/helpers.py\", line 656, in __exit__\r\n    raise asyncio.TimeoutError from None\r\nasyncio.exceptions.TimeoutError\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"/home/koush/dalle-mini/dev/seq2seq/run_seq2seq_flax.py\", line 1027, in <module>\r\n    main()\r\n  File \"/home/koush/dalle-mini/dev/seq2seq/run_seq2seq_flax.py\", line 991, in main\r\n    for batch in tqdm(\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/tqdm/std.py\", line 1180, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/koush/dalle-mini/dev/seq2seq/run_seq2seq_flax.py\", line 376, in data_loader_streaming\r\n    for item in dataset:\r\n  File \"/home/koush/datasets/src/datasets/iterable_dataset.py\", line 341, in __iter__\r\n    for key, example in self._iter():\r\n  File \"/home/koush/datasets/src/datasets/iterable_dataset.py\", line 338, in _iter\r\n    yield from ex_iterable\r\n  File \"/home/koush/datasets/src/datasets/iterable_dataset.py\", line 179, in __iter__\r\n    key_examples_list = [(key, example)] + [\r\n  File \"/home/koush/datasets/src/datasets/iterable_dataset.py\", line 179, in <listcomp>\r\n    key_examples_list = [(key, example)] + [\r\n  File \"/home/koush/datasets/src/datasets/iterable_dataset.py\", line 176, in __iter__\r\n    for key, example in iterator:\r\n  File \"/home/koush/datasets/src/datasets/iterable_dataset.py\", line 225, in __iter__\r\n    for x in self.ex_iterable:\r\n  File \"/home/koush/datasets/src/datasets/iterable_dataset.py\", line 99, in __iter__\r\n    for key, example in self.generate_examples_fn(**kwargs_with_shuffled_shards):\r\n  File \"/home/koush/datasets/src/datasets/iterable_dataset.py\", line 287, in wrapper\r\n    for key, table in generate_tables_fn(**kwargs):\r\n  File \"/home/koush/datasets/src/datasets/packaged_modules/json/json.py\", line 107, in _generate_tables\r\n    batch = f.read(self.config.chunksize)\r\n  File \"/home/koush/datasets/src/datasets/utils/streaming_download_manager.py\", line 126, in read_with_retries\r\n    out = read(*args, **kwargs)\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/fsspec/implementations/http.py\", line 572, in read\r\n    return super().read(length)\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/fsspec/spec.py\", line 1533, in read\r\n    out = self.cache._fetch(self.loc, self.loc + length)\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/fsspec/caching.py\", line 390, in _fetch\r\n    self.cache = self.fetcher(start, bend)\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/fsspec/asyn.py\", line 91, in wrapper\r\n    return sync(self.loop, func, *args, **kwargs)\r\n  File \"/home/koush/.pyenv/versions/dev/lib/python3.9/site-packages/fsspec/asyn.py\", line 69, in sync\r\n    raise FSTimeoutError from return_result\r\nfsspec.exceptions.FSTimeoutError\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.2.dev0\r\n- Platform: Linux-5.4.0-1043-gcp-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3049/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3049/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3047", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3047/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3047/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3047/events", "html_url": "https://github.com/huggingface/datasets/issues/3047", "id": 1021360616, "node_id": "I_kwDODunzps484Lno", "number": 3047, "title": "Loading from cache a dataset for LM built from a text classification dataset sometimes errors", "user": {"login": "sgugger", "id": 35901082, "node_id": "MDQ6VXNlcjM1OTAxMDgy", "avatar_url": "https://avatars.githubusercontent.com/u/35901082?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sgugger", "html_url": "https://github.com/sgugger", "followers_url": "https://api.github.com/users/sgugger/followers", "following_url": "https://api.github.com/users/sgugger/following{/other_user}", "gists_url": "https://api.github.com/users/sgugger/gists{/gist_id}", "starred_url": "https://api.github.com/users/sgugger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sgugger/subscriptions", "organizations_url": "https://api.github.com/users/sgugger/orgs", "repos_url": "https://api.github.com/users/sgugger/repos", "events_url": "https://api.github.com/users/sgugger/events{/privacy}", "received_events_url": "https://api.github.com/users/sgugger/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-10-08T18:23:11Z", "updated_at": "2021-11-03T17:13:08Z", "closed_at": "2021-11-03T17:13:08Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nYes, I know, that description sucks. So the problem is arising in the course when we build a masked language modeling dataset using the IMDB dataset. To reproduce (or try since it's a bit fickle).\r\n\r\nCreate a dataset for masled-language modeling from the IMDB dataset.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom transformers import Autotokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased)\r\nimdb_dataset = load_dataset(\"imdb\", split=\"train\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"])\r\n\r\ntokenized_dataset = imdb_dataset.map(\r\n    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\r\n)\r\n\r\nchunk_size = 128\r\n\r\ndef group_texts(examples):\r\n    # Concatenate all texts.\r\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\r\n    # Compute length of concatenated texts\r\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\r\n    # We drop the last chunk if it's smaller than chunk_size\r\n    total_length = (total_length // chunk_size) * chunk_size\r\n    # Split by chunks of max_len.\r\n    result = {\r\n        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\r\n        for k, t in concatenated_examples.items()\r\n    }\r\n    # Create a new labels column\r\n    result[\"labels\"] = result[\"input_ids\"].copy()\r\n    return result\r\n\r\nlm_dataset = tokenized_dataset.map(group_texts, batched=True)\r\n```\r\n\r\nUntil now, all is well. The problem comes when you re-execute that code, more specifically:\r\n\r\n```python\r\ntokenized_dataset = imdb_dataset.map(\r\n    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\r\n)\r\nlm_dataset = tokenized_dataset.map(group_texts, batched=True)\r\n```\r\n\r\nTry several times if the bug doesn't appear instantly, or just each line at a time, ideally in a notebook/Colab and you should get at some point:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-40-357a56ee3d53> in <module>\r\n----> 1 lm_dataset = tokenized_dataset.map(group_texts, batched=True)\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   1947                 new_fingerprint=new_fingerprint,\r\n   1948                 disable_tqdm=disable_tqdm,\r\n-> 1949                 desc=desc,\r\n   1950             )\r\n   1951         else:\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    424         }\r\n    425         # apply actual function\r\n--> 426         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    427         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    428         # re-apply format to the output\r\n\r\n~/git/datasets/src/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    404             # Call actual function\r\n    405 \r\n--> 406             out = func(self, *args, **kwargs)\r\n    407 \r\n    408             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n~/git/datasets/src/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\r\n   2138             if os.path.exists(cache_file_name) and load_from_cache_file:\r\n   2139                 logger.warning(\"Loading cached processed dataset at %s\", cache_file_name)\r\n-> 2140                 info = self.info.copy()\r\n   2141                 info.features = features\r\n   2142                 return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n\r\n~/git/datasets/src/datasets/info.py in copy(self)\r\n    278 \r\n    279     def copy(self) -> \"DatasetInfo\":\r\n--> 280         return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})\r\n    281 \r\n    282 \r\n\r\n~/git/datasets/src/datasets/info.py in __init__(self, description, citation, homepage, license, features, post_processed, supervised_keys, task_templates, builder_name, config_name, version, splits, download_checksums, download_size, post_processing_size, dataset_size, size_in_bytes)\r\n\r\n~/git/datasets/src/datasets/info.py in __post_init__(self)\r\n    177                 for idx, template in enumerate(self.task_templates):\r\n    178                     if isinstance(template, TextClassification):\r\n--> 179                         labels = self.features[template.label_column].names\r\n    180                         self.task_templates[idx] = TextClassification(\r\n    181                             text_column=template.text_column, label_column=template.label_column, labels=labels\r\n\r\nKeyError: 'label'\r\n```\r\n\r\nIt seems that when loading the cache, the dataset tries to access some kind of text classification template (which I imagine comes from the original dataset) and to look at a key that has since been removed.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3047/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3040", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3040/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3040/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3040/events", "html_url": "https://github.com/huggingface/datasets/issues/3040", "id": 1018782475, "node_id": "I_kwDODunzps48uWML", "number": 3040, "title": "[save_to_disk] Using `select()` followed by `save_to_disk` saves complete dataset making it hard to create dummy dataset", "user": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2021-10-06T17:08:47Z", "updated_at": "2021-11-02T15:41:08Z", "closed_at": "2021-11-02T15:41:08Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen only keeping a dummy size of a dataset (say the first 100 samples), and then saving it  to disk to upload it in the following to the hub for easy demo/use - not just the small dataset is saved but the whole dataset with an indices file. The problem with this is that the dataset is still very big.\r\n\r\n## Steps to reproduce the bug\r\n\r\nE.g. run the following:\r\n\r\n```python\r\nfrom datasets import load_dataset, save_to_disk\r\n\r\nnlp = load_dataset(\"glue\", \"mnli\", split=\"train\")\r\nnlp.save_to_disk(\"full\")\r\n\r\nnlp = nlp.select(range(100))\r\nnlp.save_to_disk(\"dummy\")\r\n```\r\n\r\nNow one can see that both `\"dummy\"` and `\"full\"` have the same size. This shouldn't be the case IMO.\r\n\r\n## Expected results\r\n\r\nIMO `\"dummy\"` should be much smaller so that one can easily play around with the dataset on the hub.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n\r\n- `datasets` version: 1.12.2.dev0\r\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3040/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3032", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3032/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3032/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3032/events", "html_url": "https://github.com/huggingface/datasets/issues/3032", "id": 1016488475, "node_id": "I_kwDODunzps48lmIb", "number": 3032, "title": "Error when loading private dataset with \"data_files\" arg", "user": {"login": "borisdayma", "id": 715491, "node_id": "MDQ6VXNlcjcxNTQ5MQ==", "avatar_url": "https://avatars.githubusercontent.com/u/715491?v=4", "gravatar_id": "", "url": "https://api.github.com/users/borisdayma", "html_url": "https://github.com/borisdayma", "followers_url": "https://api.github.com/users/borisdayma/followers", "following_url": "https://api.github.com/users/borisdayma/following{/other_user}", "gists_url": "https://api.github.com/users/borisdayma/gists{/gist_id}", "starred_url": "https://api.github.com/users/borisdayma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/borisdayma/subscriptions", "organizations_url": "https://api.github.com/users/borisdayma/orgs", "repos_url": "https://api.github.com/users/borisdayma/repos", "events_url": "https://api.github.com/users/borisdayma/events{/privacy}", "received_events_url": "https://api.github.com/users/borisdayma/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-10-05T15:46:27Z", "updated_at": "2021-10-12T15:26:22Z", "closed_at": "2021-10-12T15:25:46Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\nPrivate datasets with no loading script can't be loaded using `data_files` parameter.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndata_files = {\"train\": \"**/train/*/*.jsonl\", \"valid\": \"**/valid/*/*.jsonl\"}\r\ndataset = load_dataset('dalle-mini/encoded', data_files=data_files, use_auth_token=True, streaming=True)\r\n```\r\n\r\nSame error happens in non-streaming mode.\r\n\r\n## Expected results\r\nFiles should be loaded (whether in streaming or not).\r\n\r\n## Actual results\r\nError:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, return_associated_base_path, data_files, **download_kwargs)\r\n    539                 try:\r\n--> 540                     local_path = cached_path(file_path, download_config=download_config)\r\n    541                 except FileNotFoundError:\r\n\r\n8 frames\r\nFileNotFoundError: Couldn't find file at https://huggingface.co/datasets/dalle-mini/encoded/resolve/main/encoded.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nHTTPError                                 Traceback (most recent call last)\r\nHTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/datasets/dalle-mini/encoded?full=true\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFileNotFoundError                         Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, return_associated_base_path, data_files, **download_kwargs)\r\n    547                     except Exception:\r\n    548                         raise FileNotFoundError(\r\n--> 549                             f\"Couldn't find a directory or a {resource_type} named '{path}'. \"\r\n    550                             f\"It doesn't exist locally at {expected_dir_for_combined_path_abs} or remotely on {hf_api.endpoint}/datasets\"\r\n    551                         )\r\n\r\nFileNotFoundError: Couldn't find a directory or a dataset named 'dalle-mini/encoded'. It doesn't exist locally at /content/dalle-mini/encoded or remotely on https://huggingface.co/datasets\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.12\r\n- PyArrow version: 3.0.0\r\n\r\n@lhoestq ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3032/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3032/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3024", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3024/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3024/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3024/events", "html_url": "https://github.com/huggingface/datasets/issues/3024", "id": 1016052911, "node_id": "I_kwDODunzps48j7yv", "number": 3024, "title": "Windows test suite fails", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-10-05T08:46:46Z", "updated_at": "2021-10-05T09:58:27Z", "closed_at": "2021-10-05T09:58:27Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThere is an error during installation of tests dependencies for Windows: https://app.circleci.com/pipelines/github/huggingface/datasets/7981/workflows/9b6a0114-2b8e-4069-94e5-e844dbbdba4e/jobs/49206\r\n\r\n```\r\nERROR: Cannot uninstall 'ruamel-yaml'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3024/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3010", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3010/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3010/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3010/events", "html_url": "https://github.com/huggingface/datasets/issues/3010", "id": 1014918470, "node_id": "I_kwDODunzps48fm1G", "number": 3010, "title": "Chain filtering is leaking", "user": {"login": "DrMatters", "id": 22641583, "node_id": "MDQ6VXNlcjIyNjQxNTgz", "avatar_url": "https://avatars.githubusercontent.com/u/22641583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DrMatters", "html_url": "https://github.com/DrMatters", "followers_url": "https://api.github.com/users/DrMatters/followers", "following_url": "https://api.github.com/users/DrMatters/following{/other_user}", "gists_url": "https://api.github.com/users/DrMatters/gists{/gist_id}", "starred_url": "https://api.github.com/users/DrMatters/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DrMatters/subscriptions", "organizations_url": "https://api.github.com/users/DrMatters/orgs", "repos_url": "https://api.github.com/users/DrMatters/repos", "events_url": "https://api.github.com/users/DrMatters/events{/privacy}", "received_events_url": "https://api.github.com/users/DrMatters/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2021-10-04T09:04:55Z", "updated_at": "2022-06-01T17:36:44Z", "closed_at": "2022-06-01T17:36:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nAs there's no support for lists within dataset fields, I convert my lists to json-string format. However, the bug described is occurring even when the data format is 'string'.\r\nThese samples show that filtering behavior diverges from what's expected when chaining filterings.\r\nOn sample 2 the second filtering leads to \"leaking\" of data that should've been filtered on the first filtering into the results.\r\n\r\n## Steps to reproduce the bug\r\nSample 1:\r\n```python\r\nimport datasets\r\nimport json\r\n\r\nitems = [[1, 2], [3], [4]]\r\njsoned_items = map(json.dumps, [[1, 2], [3], [4]])\r\nds = datasets.Dataset.from_dict({'a': jsoned_items})\r\nprint(list(ds))\r\n# > Prints: [{'a': '[1, 2]'}, {'a': '[3]'}, {'a': '[4]'}] as expected\r\n\r\nfiltered = ds\r\n\r\n# get all lists that are shorter than 2\r\nfiltered = filtered.filter(lambda x: len(json.loads(x['a'])) < 2, load_from_cache_file=False)\r\nprint(list(filtered))\r\n# > Prints: [{'a': '[3]'}, {'a': '[4]'}] as expected\r\n\r\n# get all lists, which have a value bigger than 3 on its zero index\r\nfiltered = filtered.filter(lambda x: json.loads(x['a'])[0] > 3, load_from_cache_file=False)\r\nprint(list(filtered))\r\n# > Should be: [{'a': [4]}]\r\n# > Prints: [{'a': [3]}]\r\n```\r\nSample 2:\r\n```python\r\nimport datasets\r\nimport json\r\n\r\nitems = [[1, 2], [3], [4]]\r\njsoned_items = map(json.dumps, [[1, 2], [3], [4]])\r\nds = datasets.Dataset.from_dict({'a': jsoned_items})\r\nprint(list(ds))\r\n# > Prints: [{'a': '[1, 2]'}, {'a': '[3]'}, {'a': '[4]'}]\r\n\r\nfiltered = ds\r\n\r\n# get all lists, which have a value bigger than 3 on its zero index\r\nfiltered = filtered.filter(lambda x: json.loads(x['a'])[0] > 3, load_from_cache_file=False)\r\nprint(list(filtered))\r\n# > Prints: [{'a': '[4]'}] as expected\r\n\r\n# get all lists that are shorter than 2\r\nfiltered = filtered.filter(lambda x: len(json.loads(x['a'])) < 2, load_from_cache_file=False)\r\nprint(list(filtered))\r\n# > Prints: [{'a': '[1, 2]'}]\r\n# > Should be: [{'a': '[4]'}] (remain intact)\r\n```\r\n\r\n## Expected results\r\nExpected and actual results are attached to the code snippets.\r\n\r\n## Actual results\r\nExpected and actual results are attached to the code snippets.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Windows-10-10.0.19042-SP0\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3010/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3010/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/3005", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/3005/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/3005/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/3005/events", "html_url": "https://github.com/huggingface/datasets/issues/3005", "id": 1014615420, "node_id": "I_kwDODunzps48ec18", "number": 3005, "title": "DatasetDict.filter and Dataset.filter crashes with any \"fn_kwargs\" argument", "user": {"login": "DrMatters", "id": 22641583, "node_id": "MDQ6VXNlcjIyNjQxNTgz", "avatar_url": "https://avatars.githubusercontent.com/u/22641583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DrMatters", "html_url": "https://github.com/DrMatters", "followers_url": "https://api.github.com/users/DrMatters/followers", "following_url": "https://api.github.com/users/DrMatters/following{/other_user}", "gists_url": "https://api.github.com/users/DrMatters/gists{/gist_id}", "starred_url": "https://api.github.com/users/DrMatters/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DrMatters/subscriptions", "organizations_url": "https://api.github.com/users/DrMatters/orgs", "repos_url": "https://api.github.com/users/DrMatters/repos", "events_url": "https://api.github.com/users/DrMatters/events{/privacy}", "received_events_url": "https://api.github.com/users/DrMatters/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-10-04T00:49:29Z", "updated_at": "2021-10-11T10:18:01Z", "closed_at": "2021-10-04T08:46:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe \".filter\" method of DatasetDict or Dataset objects fails when passing any \"fn_kwargs\" argument\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\n\r\nexample_dataset = datasets.Dataset.from_dict({\"a\": {1, 2, 3, 4}})\r\n\r\ndef filter_value(example, value):\r\n    return example['a'] == value\r\n\r\nfiltered = example_dataset.filter(filter_value, fn_kwargs={'value': 3})\r\n```\r\n\r\n## Expected results\r\n`filtered` is a dataset containing {\"a\": {3}}\r\n\r\n## Actual results\r\n\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\qsemi\\Documents\\git\\nlp_experiments\\gpt_celebrity\\src\\test_faulty_filter.py\", line 8, in <module>\r\n>     filtered = example_dataset.filter(filter_value, fn_kwargs={'value': 3})\r\n>   File \"C:\\Users\\qsemi\\miniconda3\\envs\\main\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 185, in wrapper\r\n>     out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n>   File \"C:\\Users\\qsemi\\miniconda3\\envs\\main\\lib\\site-packages\\datasets\\fingerprint.py\", line 398, in wrapper\r\n>     out = func(self, *args, **kwargs)\r\n>   File \"C:\\Users\\qsemi\\miniconda3\\envs\\main\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 2169, in filter\r\n>     indices = self.map(\r\n>   File \"C:\\Users\\qsemi\\miniconda3\\envs\\main\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 1686, in map\r\n>     return self._map_single(\r\n>   File \"C:\\Users\\qsemi\\miniconda3\\envs\\main\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 185, in wrapper\r\n>     out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n>   File \"C:\\Users\\qsemi\\miniconda3\\envs\\main\\lib\\site-packages\\datasets\\fingerprint.py\", line 398, in wrapper\r\n>     out = func(self, *args, **kwargs)\r\n>   File \"C:\\Users\\qsemi\\miniconda3\\envs\\main\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 2048, in _map_single\r\n>     batch = apply_function_on_filtered_inputs(\r\n>   File \"C:\\Users\\qsemi\\miniconda3\\envs\\main\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 1939, in apply_function_on_filtered_inputs\r\n>     function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n> TypeError: get_indices_from_mask_function() got an unexpected keyword argument 'value'\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Windows-10-10.0.19042-SP0\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/3005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/3005/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2993", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2993/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2993/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2993/events", "html_url": "https://github.com/huggingface/datasets/issues/2993", "id": 1012702665, "node_id": "I_kwDODunzps48XJ3J", "number": 2993, "title": "Can't download `trivia_qa/unfiltered`", "user": {"login": "VictorSanh", "id": 16107619, "node_id": "MDQ6VXNlcjE2MTA3NjE5", "avatar_url": "https://avatars.githubusercontent.com/u/16107619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VictorSanh", "html_url": "https://github.com/VictorSanh", "followers_url": "https://api.github.com/users/VictorSanh/followers", "following_url": "https://api.github.com/users/VictorSanh/following{/other_user}", "gists_url": "https://api.github.com/users/VictorSanh/gists{/gist_id}", "starred_url": "https://api.github.com/users/VictorSanh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VictorSanh/subscriptions", "organizations_url": "https://api.github.com/users/VictorSanh/orgs", "repos_url": "https://api.github.com/users/VictorSanh/repos", "events_url": "https://api.github.com/users/VictorSanh/events{/privacy}", "received_events_url": "https://api.github.com/users/VictorSanh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-09-30T23:00:18Z", "updated_at": "2021-10-01T19:07:23Z", "closed_at": "2021-10-01T19:07:22Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nFor some reason, I can't download `trivia_qa/unfilted`. A file seems to be missing... I am able to see it fine though the viewer tough...\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> load_dataset(\"trivia_qa\", \"unfiltered\")\r\nDownloading and preparing dataset trivia_qa/unfiltered (download: 3.07 GiB, generated: 27.23 GiB, post-processed: Unknown size, total: 30.30 GiB) to /gpfsscratch/rech/six/commun/datasets/trivia_qa/unfiltered/1.1.0/9977a5d6f72acfd92f587de052403e8138b43bb0d1ce595016c3baf7e14deba6...\r\nTraceback (most recent call last):\r\n  File \"/gpfswork/rech/six/commun/modules/datasets_modules/datasets/trivia_qa/9977a5d6f72acfd92f587de052403e8138b43bb0d1ce595016c3baf7e14deba6/trivia_qa.py\", line 251, in _add_context\r\n    with open(os.path.join(file_dir, fname), encoding=\"utf-8\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/gpfsscratch/rech/six/commun/datasets/downloads/extracted/9fcb7eddc6afd46fd074af3c5128931dfe4b548f933c925a23847faf4c1995ad/evidence/wikipedia/Peanuts.txt'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/gpfswork/rech/six/commun/conda/victor/lib/python3.7/site-packages/datasets/load.py\", line 852, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/gpfswork/rech/six/commun/conda/victor/lib/python3.7/site-packages/datasets/builder.py\", line 616, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/gpfswork/rech/six/commun/conda/victor/lib/python3.7/site-packages/datasets/builder.py\", line 693, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/gpfswork/rech/six/commun/conda/victor/lib/python3.7/site-packages/datasets/builder.py\", line 1107, in _prepare_split\r\n    disable=bool(logging.get_verbosity() == logging.NOTSET),\r\n  File \"/gpfswork/rech/six/commun/conda/victor/lib/python3.7/site-packages/tqdm/std.py\", line 1133, in __iter__\r\n    for obj in iterable:\r\n  File \"/gpfswork/rech/six/commun/modules/datasets_modules/datasets/trivia_qa/9977a5d6f72acfd92f587de052403e8138b43bb0d1ce595016c3baf7e14deba6/trivia_qa.py\", line 303, in _generate_examples\r\n    example = parse_example(article)\r\n  File \"/gpfswork/rech/six/commun/modules/datasets_modules/datasets/trivia_qa/9977a5d6f72acfd92f587de052403e8138b43bb0d1ce595016c3baf7e14deba6/trivia_qa.py\", line 274, in parse_example\r\n    _add_context(article.get(\"EntityPages\", []), \"WikiContext\", wiki_dir),\r\n  File \"/gpfswork/rech/six/commun/modules/datasets_modules/datasets/trivia_qa/9977a5d6f72acfd92f587de052403e8138b43bb0d1ce595016c3baf7e14deba6/trivia_qa.py\", line 253, in _add_context\r\n    except (IOError, datasets.Value(\"errors\").NotFoundError):\r\n  File \"<string>\", line 5, in __init__\r\n  File \"/gpfswork/rech/six/commun/conda/victor/lib/python3.7/site-packages/datasets/features.py\", line 265, in __post_init__\r\n    self.pa_type = string_to_arrow(self.dtype)\r\n  File \"/gpfswork/rech/six/commun/conda/victor/lib/python3.7/site-packages/datasets/features.py\", line 134, in string_to_arrow\r\n    f\"Neither {datasets_dtype} nor {datasets_dtype + '_'} seems to be a pyarrow data type. \"\r\nValueError: Neither errors nor errors_ seems to be a pyarrow data type. Please make sure to use a correct data type, see: https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions\r\n```\r\n\r\n## Expected results\r\nI am able to load another subset (`rc`), but unable to load.\r\nI am not sure why the try/except doesn't catch it...\r\nhttps://github.com/huggingface/datasets/blob/9675a5a1e7b99a86f9c250f6ea5fa5d1e6d5cc7d/datasets/trivia_qa/trivia_qa.py#L253\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: Linux-4.18.0-147.51.2.el8_1.x86_64-x86_64-with-redhat-8.1-Ootpa\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2993/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2993/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2988", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2988/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2988/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2988/events", "html_url": "https://github.com/huggingface/datasets/issues/2988", "id": 1011148017, "node_id": "I_kwDODunzps48ROTx", "number": 2988, "title": "IndexError: Invalid key: 14 is out of bounds for size 0", "user": {"login": "dorost1234", "id": 79165106, "node_id": "MDQ6VXNlcjc5MTY1MTA2", "avatar_url": "https://avatars.githubusercontent.com/u/79165106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dorost1234", "html_url": "https://github.com/dorost1234", "followers_url": "https://api.github.com/users/dorost1234/followers", "following_url": "https://api.github.com/users/dorost1234/following{/other_user}", "gists_url": "https://api.github.com/users/dorost1234/gists{/gist_id}", "starred_url": "https://api.github.com/users/dorost1234/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dorost1234/subscriptions", "organizations_url": "https://api.github.com/users/dorost1234/orgs", "repos_url": "https://api.github.com/users/dorost1234/repos", "events_url": "https://api.github.com/users/dorost1234/events{/privacy}", "received_events_url": "https://api.github.com/users/dorost1234/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2021-09-29T16:04:24Z", "updated_at": "2022-04-10T14:49:49Z", "closed_at": "2022-04-10T14:49:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\nHi. I am trying to implement stochastic weighted averaging optimizer with transformer library as described here https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/ , for this I am using a run_clm.py codes which is working fine before adding SWA optimizer, the moment I modify the model with `swa_model = AveragedModel(model)` in this script, I am getting the below error,  since I am NOT touching the dataloader part, I am confused why this is occurring, I very much appreciate your opinion on this @lhoestq \r\n\r\n## Steps to reproduce the bug\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_clm.py\", line 723, in <module>\r\n    main()\r\n  File \"run_clm.py\", line 669, in main\r\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\r\n  File \"/user/dara/libs/anaconda3/envs/success/lib/python3.7/site-packages/transformers/trainer.py\", line 1258, in train\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"/user/dara/libs/anaconda3/envs/success/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\r\n    data = self._next_data()\r\n  File \"/user/dara/libs/anaconda3/envs/success/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 475, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"/user/dara/libs/anaconda3/envs/success/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/user/dara/libs/anaconda3/envs/success/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/user/dara/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1530, in __getitem__\r\n    format_kwargs=self._format_kwargs,\r\n  File \"/user/dara/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1517, in _getitem\r\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n  File \"/user/dara/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets/formatting/formatting.py\", line 368, in query_table\r\n    _check_valid_index_key(key, size)\r\n  File \"/user/dara/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets/formatting/formatting.py\", line 311, in _check_valid_index_key\r\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\r\nIndexError: Invalid key: 14 is out of bounds for size 0\r\n```\r\n\r\n\r\n\r\n## Expected results\r\nnot getting the index error\r\n\r\n## Actual results\r\nPlease see the above\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: datasets                  1.12.1 \r\n- Platform: linux\r\n- Python version: 3.7.11 \r\n- PyArrow version:  5.0.0 \r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2988/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2988/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2987", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2987/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2987/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2987/events", "html_url": "https://github.com/huggingface/datasets/issues/2987", "id": 1011026141, "node_id": "I_kwDODunzps48Qwjd", "number": 2987, "title": "ArrowInvalid: Can only convert 1-dimensional array values", "user": {"login": "NielsRogge", "id": 48327001, "node_id": "MDQ6VXNlcjQ4MzI3MDAx", "avatar_url": "https://avatars.githubusercontent.com/u/48327001?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NielsRogge", "html_url": "https://github.com/NielsRogge", "followers_url": "https://api.github.com/users/NielsRogge/followers", "following_url": "https://api.github.com/users/NielsRogge/following{/other_user}", "gists_url": "https://api.github.com/users/NielsRogge/gists{/gist_id}", "starred_url": "https://api.github.com/users/NielsRogge/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NielsRogge/subscriptions", "organizations_url": "https://api.github.com/users/NielsRogge/orgs", "repos_url": "https://api.github.com/users/NielsRogge/repos", "events_url": "https://api.github.com/users/NielsRogge/events{/privacy}", "received_events_url": "https://api.github.com/users/NielsRogge/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-09-29T14:18:52Z", "updated_at": "2021-10-01T13:57:45Z", "closed_at": "2021-10-01T13:57:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nFor the ViT and LayoutLMv2 demo notebooks in my [Transformers-Tutorials repo](https://github.com/NielsRogge/Transformers-Tutorials), people reported an ArrowInvalid issue after applying the following function to a Dataset:\r\n\r\n```\r\ndef preprocess_data(examples):\r\n  images = [Image.open(path).convert(\"RGB\") for path in examples['image_path']]\r\n  words = examples['words']\r\n  boxes = examples['bboxes']\r\n  word_labels = examples['ner_tags']\r\n  \r\n  encoded_inputs = processor(images, words, boxes=boxes, word_labels=word_labels,\r\n                             padding=\"max_length\", truncation=True)\r\n  \r\n  return encoded_inputs \r\n```\r\n\r\n```\r\nFull trace:\r\n\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n<ipython-input-8-0fc3efc6f0c2> in <module>()\r\n     27 \r\n     28 train_dataset = datasets['train'].map(preprocess_data, batched=True, remove_columns=datasets['train'].column_names,\r\n---> 29                                       features=features)\r\n     30 test_dataset = datasets['test'].map(preprocess_data, batched=True, remove_columns=datasets['test'].column_names,\r\n     31                                       features=features)\r\n\r\n13 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   1701                 new_fingerprint=new_fingerprint,\r\n   1702                 disable_tqdm=disable_tqdm,\r\n-> 1703                 desc=desc,\r\n   1704             )\r\n   1705         else:\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    183         }\r\n    184         # apply actual function\r\n--> 185         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    186         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    187         # re-apply format to the output\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    396             # Call actual function\r\n    397 \r\n--> 398             out = func(self, *args, **kwargs)\r\n    399 \r\n    400             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\r\n   2063                                 writer.write_table(batch)\r\n   2064                             else:\r\n-> 2065                                 writer.write_batch(batch)\r\n   2066                 if update_data and writer is not None:\r\n   2067                     writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)\r\n    409             typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\r\n    410             typed_sequence_examples[col] = typed_sequence\r\n--> 411         pa_table = pa.Table.from_pydict(typed_sequence_examples)\r\n    412         self.write_table(pa_table, writer_batch_size)\r\n    413 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_pydict()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi in pyarrow.lib.asarray()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in __arrow_array__(self, type)\r\n    106                     storage = numpy_to_pyarrow_listarray(self.data, type=type.value_type)\r\n    107                 else:\r\n--> 108                     storage = pa.array(self.data, type.storage_dtype)\r\n    109                 out = pa.ExtensionArray.from_storage(type, storage)\r\n    110             elif isinstance(self.data, np.ndarray):\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Can only convert 1-dimensional array values\r\n```\r\nIt can be fixed by adding the following line:\r\n\r\n```diff\r\ndef preprocess_data(examples):\r\n  images = [Image.open(path).convert(\"RGB\") for path in examples['image_path']]\r\n  words = examples['words']\r\n  boxes = examples['bboxes']\r\n  word_labels = examples['ner_tags']\r\n  \r\n  encoded_inputs = processor(images, words, boxes=boxes, word_labels=word_labels,\r\n                             padding=\"max_length\", truncation=True)\r\n+ encoded_inputs[\"image\"] = np.array(encoded_inputs[\"image\"])\r\n  \r\n  return encoded_inputs \r\n```\r\n\r\nHowever, would be great if this can be fixed within Datasets itself.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2987/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2984", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2984/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2984/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2984/events", "html_url": "https://github.com/huggingface/datasets/issues/2984", "id": 1010484326, "node_id": "I_kwDODunzps48OsRm", "number": 2984, "title": "Exceeded maximum rows when reading large files", "user": {"login": "zijwang", "id": 25057983, "node_id": "MDQ6VXNlcjI1MDU3OTgz", "avatar_url": "https://avatars.githubusercontent.com/u/25057983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zijwang", "html_url": "https://github.com/zijwang", "followers_url": "https://api.github.com/users/zijwang/followers", "following_url": "https://api.github.com/users/zijwang/following{/other_user}", "gists_url": "https://api.github.com/users/zijwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/zijwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zijwang/subscriptions", "organizations_url": "https://api.github.com/users/zijwang/orgs", "repos_url": "https://api.github.com/users/zijwang/repos", "events_url": "https://api.github.com/users/zijwang/events{/privacy}", "received_events_url": "https://api.github.com/users/zijwang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-09-29T04:49:22Z", "updated_at": "2021-10-12T06:05:42Z", "closed_at": "2021-10-12T06:05:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\nWhen using `load_dataset` with json files, if the files are too large, there will be \"Exceeded maximum rows\" error.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = load_dataset('json', data_files=data_files) # data files have 3M rows in a single file\r\n```\r\n\r\n## Expected results\r\nNo error\r\n\r\n## Actual results\r\n```\r\n~/anaconda3/envs/python/lib/python3.9/site-packages/datasets/packaged_modules/json/json.py in _generate_tables(self, files)\r\n    134                                 with open(file, encoding=\"utf-8\") as f:\r\n--> 135                                     dataset = json.load(f)\r\n    136                             except json.JSONDecodeError:\r\n\r\n~/anaconda3/envs/python/lib/python3.9/json/__init__.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\r\n    292     \"\"\"\r\n--> 293     return loads(fp.read(),\r\n    294         cls=cls, object_hook=object_hook,\r\n\r\n~/anaconda3/envs/python/lib/python3.9/json/__init__.py in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\r\n    345             parse_constant is None and object_pairs_hook is None and not kw):\r\n--> 346         return _default_decoder.decode(s)\r\n    347     if cls is None:\r\n\r\n~/anaconda3/envs/python/lib/python3.9/json/decoder.py in decode(self, s, _w)\r\n    339         if end != len(s):\r\n--> 340             raise JSONDecodeError(\"Extra data\", s, end)\r\n    341         return obj\r\n\r\nJSONDecodeError: Extra data: line 2 column 1 (char 20321)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nArrowInvalid                              Traceback (most recent call last)\r\n<ipython-input-20-ab3718a6482f> in <module>\r\n----> 1 dataset = load_dataset('json', data_files=data_files)\r\n\r\n~/anaconda3/envs/python/lib/python3.9/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\r\n    841 \r\n    842     # Download and prepare data\r\n--> 843     builder_instance.download_and_prepare(\r\n    844         download_config=download_config,\r\n    845         download_mode=download_mode,\r\n\r\n~/anaconda3/envs/python/lib/python3.9/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    606                             logger.warning(\"HF google storage unreachable. Downloading and preparing it from source\")\r\n    607                     if not downloaded_from_gcs:\r\n--> 608                         self._download_and_prepare(\r\n    609                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    610                         )\r\n\r\n~/anaconda3/envs/python/lib/python3.9/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    684             try:\r\n    685                 # Prepare split will record examples associated to the split\r\n--> 686                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    687             except OSError as e:\r\n    688                 raise OSError(\r\n\r\n~/anaconda3/envs/python/lib/python3.9/site-packages/datasets/builder.py in _prepare_split(self, split_generator)\r\n   1153         generator = self._generate_tables(**split_generator.gen_kwargs)\r\n   1154         with ArrowWriter(features=self.info.features, path=fpath) as writer:\r\n-> 1155             for key, table in utils.tqdm(\r\n   1156                 generator, unit=\" tables\", leave=False, disable=bool(logging.get_verbosity() == logging.NOTSET)\r\n   1157             ):\r\n\r\n\r\n~/anaconda3/envs/python/lib/python3.9/site-packages/datasets/packaged_modules/json/json.py in _generate_tables(self, files)\r\n    135                                     dataset = json.load(f)\r\n    136                             except json.JSONDecodeError:\r\n--> 137                                 raise e\r\n    138                             raise ValueError(\r\n    139                                 f\"Not able to read records in the JSON file at {file}. \"\r\n\r\n~/anaconda3/envs/python/lib/python3.9/site-packages/datasets/packaged_modules/json/json.py in _generate_tables(self, files)\r\n    114                             while True:\r\n    115                                 try:\r\n--> 116                                     pa_table = paj.read_json(\r\n    117                                         BytesIO(batch), read_options=paj.ReadOptions(block_size=block_size)\r\n    118                                     )\r\n\r\n~/anaconda3/envs/python/lib/python3.9/site-packages/pyarrow/_json.pyx in pyarrow._json.read_json()\r\n\r\n~/anaconda3/envs/python/lib/python3.9/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/anaconda3/envs/python/lib/python3.9/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Exceeded maximum rows\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform: Linux\r\n- Python version: 3.9\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2984/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2979", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2979/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2979/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2979/events", "html_url": "https://github.com/huggingface/datasets/issues/2979", "id": 1009634147, "node_id": "I_kwDODunzps48Lctj", "number": 2979, "title": "ValueError when computing f1 metric with average None", "user": {"login": "asofiaoliveira", "id": 74454835, "node_id": "MDQ6VXNlcjc0NDU0ODM1", "avatar_url": "https://avatars.githubusercontent.com/u/74454835?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asofiaoliveira", "html_url": "https://github.com/asofiaoliveira", "followers_url": "https://api.github.com/users/asofiaoliveira/followers", "following_url": "https://api.github.com/users/asofiaoliveira/following{/other_user}", "gists_url": "https://api.github.com/users/asofiaoliveira/gists{/gist_id}", "starred_url": "https://api.github.com/users/asofiaoliveira/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asofiaoliveira/subscriptions", "organizations_url": "https://api.github.com/users/asofiaoliveira/orgs", "repos_url": "https://api.github.com/users/asofiaoliveira/repos", "events_url": "https://api.github.com/users/asofiaoliveira/events{/privacy}", "received_events_url": "https://api.github.com/users/asofiaoliveira/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-09-28T11:34:53Z", "updated_at": "2021-10-01T14:17:38Z", "closed_at": "2021-10-01T14:17:38Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen I try to compute the f1 score for each class in a multiclass classification problem, I get a ValueError. The same happens with recall and precision. I traced the error to the `.item()` in these scripts, which is probably there for the other averages. E.g. from f1.py:\r\n```python \r\nreturn {\r\n            \"f1\": f1_score(\r\n                references,\r\n                predictions,\r\n                labels=labels,\r\n                pos_label=pos_label,\r\n                average=average,\r\n                sample_weight=sample_weight,\r\n            ).item(),\r\n        }\r\n```\r\nSince the result is an array with more than one item, the `.item()` throws the error. I didn't submit a PR because this might be needed for the other averages, I'm not very familiar with the library\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_metric\r\nmetric = load_metric(\"f1\")\r\nmetric.add_batch(predictions=[2,34,1,34,1,2,3], references=[23,52,1,3,523,5,8])\r\nmetric.compute(average=None)\r\n```\r\n\r\n## Expected results\r\n`array([0.66666667, 0.        , 0.        , 0.        , 0.        ,\r\n       0.        , 0.        , 0.        , 0.        ])`\r\n\r\n## Actual results\r\nValueError: can only convert an array of size 1 to a Python scalar\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.9.5\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2979/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2977", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2977/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2977/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2977/events", "html_url": "https://github.com/huggingface/datasets/issues/2977", "id": 1009378692, "node_id": "I_kwDODunzps48KeWE", "number": 2977, "title": "Impossible to load compressed csv", "user": {"login": "Valahaar", "id": 19476123, "node_id": "MDQ6VXNlcjE5NDc2MTIz", "avatar_url": "https://avatars.githubusercontent.com/u/19476123?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Valahaar", "html_url": "https://github.com/Valahaar", "followers_url": "https://api.github.com/users/Valahaar/followers", "following_url": "https://api.github.com/users/Valahaar/following{/other_user}", "gists_url": "https://api.github.com/users/Valahaar/gists{/gist_id}", "starred_url": "https://api.github.com/users/Valahaar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Valahaar/subscriptions", "organizations_url": "https://api.github.com/users/Valahaar/orgs", "repos_url": "https://api.github.com/users/Valahaar/repos", "events_url": "https://api.github.com/users/Valahaar/events{/privacy}", "received_events_url": "https://api.github.com/users/Valahaar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-09-28T07:18:54Z", "updated_at": "2021-10-01T15:53:16Z", "closed_at": "2021-10-01T15:53:15Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nIt is not possible to load from a compressed csv anymore.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset('csv', data_files=['/path/to/csv.bz2'])\r\n```\r\n\r\n## Problem and possible solution\r\nThis used to work, but the commit that broke it is [this one](https://github.com/huggingface/datasets/commit/ad489d4597381fc2d12c77841642cbeaecf7a2e0#diff-6f60f8d0552b75be8b3bfd09994480fd60dcd4e7eb08d02f721218c3acdd2782).\r\n\r\n`pandas` usually gets the compression information from the filename itself (which was previously directly passed). Now, since it gets a file descriptor, it might be good to auto-infer the compression or let the user pass the `compression` kwarg to `load_dataset` (or maybe warn the user if the file ends with a commonly known compression scheme?).\r\n\r\n## Environment info\r\n- `datasets` version: 1.10.0 (and over)\r\n- Platform: Linux-5.8.0-45-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2977/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2977/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2976", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2976/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2976/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2976/events", "html_url": "https://github.com/huggingface/datasets/issues/2976", "id": 1008647889, "node_id": "I_kwDODunzps48Hr7R", "number": 2976, "title": "Can't load dataset", "user": {"login": "mskovalova", "id": 77006774, "node_id": "MDQ6VXNlcjc3MDA2Nzc0", "avatar_url": "https://avatars.githubusercontent.com/u/77006774?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mskovalova", "html_url": "https://github.com/mskovalova", "followers_url": "https://api.github.com/users/mskovalova/followers", "following_url": "https://api.github.com/users/mskovalova/following{/other_user}", "gists_url": "https://api.github.com/users/mskovalova/gists{/gist_id}", "starred_url": "https://api.github.com/users/mskovalova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mskovalova/subscriptions", "organizations_url": "https://api.github.com/users/mskovalova/orgs", "repos_url": "https://api.github.com/users/mskovalova/repos", "events_url": "https://api.github.com/users/mskovalova/events{/privacy}", "received_events_url": "https://api.github.com/users/mskovalova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-09-27T21:38:14Z", "updated_at": "2022-12-01T09:12:29Z", "closed_at": "2021-09-28T06:53:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to load a wikitext dataset\r\n\r\n```\r\nfrom datasets import load_dataset\r\nraw_datasets = load_dataset(\"wikitext\")\r\n```\r\n\r\nValueError: Config name is missing.\r\nPlease pick one among the available configs: ['wikitext-103-raw-v1', 'wikitext-2-raw-v1', 'wikitext-103-v1', 'wikitext-2-v1']\r\nExample of usage:\r\n\t`load_dataset('wikitext', 'wikitext-103-raw-v1')`.\r\n\r\nIf I try\r\n```\r\nfrom datasets import load_dataset\r\nraw_datasets = load_dataset(\"wikitext-2-v1\")\r\n```\r\n\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/wikitext-2-v1/wikitext-2-v1.py\r\n\r\n#### Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic (colab)\r\n- Python version: 3.7.12\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2976/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2976/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2972", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2972/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2972/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2972/events", "html_url": "https://github.com/huggingface/datasets/issues/2972", "id": 1007808714, "node_id": "I_kwDODunzps48EfDK", "number": 2972, "title": "OSError: Not enough disk space.", "user": {"login": "qqaatw", "id": 24835382, "node_id": "MDQ6VXNlcjI0ODM1Mzgy", "avatar_url": "https://avatars.githubusercontent.com/u/24835382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qqaatw", "html_url": "https://github.com/qqaatw", "followers_url": "https://api.github.com/users/qqaatw/followers", "following_url": "https://api.github.com/users/qqaatw/following{/other_user}", "gists_url": "https://api.github.com/users/qqaatw/gists{/gist_id}", "starred_url": "https://api.github.com/users/qqaatw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qqaatw/subscriptions", "organizations_url": "https://api.github.com/users/qqaatw/orgs", "repos_url": "https://api.github.com/users/qqaatw/repos", "events_url": "https://api.github.com/users/qqaatw/events{/privacy}", "received_events_url": "https://api.github.com/users/qqaatw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2021-09-27T07:41:22Z", "updated_at": "2022-08-29T23:21:36Z", "closed_at": "2021-09-28T06:43:15Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI'm trying to download `natural_questions` dataset from the Internet, and I've specified the cache_dir which locates in a mounted disk and has enough disk space. However, even though the space is enough, the disk space checking function still reports the space of root `/` disk having no enough space. \r\n\r\nThe file system structure is like below. The root `/` has `115G` disk space available, and the `sda1` is mounted to `/mnt`, which has `1.2T` disk space available:\r\n```\r\n/\r\n/mnt/sda1/path/to/args.dataset_cache_dir \r\n```\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset_config = DownloadConfig(\r\n    cache_dir=os.path.abspath(args.dataset_cache_dir),\r\n    resume_download=True,\r\n)\r\ndataset = load_dataset(\"natural_questions\", download_config=dataset_config)\r\n```\r\n\r\n## Expected results\r\n\r\nCan download the dataset without an error.\r\n\r\n## Actual results\r\n\r\nThe following error raised:\r\n```\r\nOSError: Not enough disk space. Needed: 134.92 GiB (download: 41.97 GiB, generated: 92.95 GiB, post-processed: Unknown size)\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Ubuntu 18.04\r\n- Python version: 3.8.10\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2972/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2971", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2971/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2971/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2971/events", "html_url": "https://github.com/huggingface/datasets/issues/2971", "id": 1007696522, "node_id": "I_kwDODunzps48EDqK", "number": 2971, "title": "masakhaner dataset load problem", "user": {"login": "ontocord", "id": 8900094, "node_id": "MDQ6VXNlcjg5MDAwOTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/8900094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ontocord", "html_url": "https://github.com/ontocord", "followers_url": "https://api.github.com/users/ontocord/followers", "following_url": "https://api.github.com/users/ontocord/following{/other_user}", "gists_url": "https://api.github.com/users/ontocord/gists{/gist_id}", "starred_url": "https://api.github.com/users/ontocord/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ontocord/subscriptions", "organizations_url": "https://api.github.com/users/ontocord/orgs", "repos_url": "https://api.github.com/users/ontocord/repos", "events_url": "https://api.github.com/users/ontocord/events{/privacy}", "received_events_url": "https://api.github.com/users/ontocord/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-09-27T04:59:07Z", "updated_at": "2021-09-27T12:59:59Z", "closed_at": "2021-09-27T12:59:59Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nMasakhaner dataset is not loading\r\n\r\n## Steps to reproduce the bug\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"masakhaner\",'amh')\r\n```\r\n\r\n## Expected results\r\nExpected the return of a dataset\r\n\r\n## Actual results\r\n\r\n```\r\n\r\nNonMatchingSplitsSizesError               Traceback (most recent call last)\r\n<ipython-input-3-a6abc1161d4c> in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 dataset = load_dataset(\"masakhaner\",'amh')\r\n\r\n\r\n\r\n3 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/info_utils.py\r\n in verify_splits(expected_splits, recorded_splits)\r\n     72     ]\r\n     73     if len(bad_splits) > 0:\r\n---> 74         raise NonMatchingSplitsSizesError(str(bad_splits))\r\n     75     logger.info(\"All the splits matched successfully.\")\r\n     76 \r\n\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=639927, num_examples=1751, dataset_name='masakhaner'), 'recorded': SplitInfo(name='train', num_bytes=639911, num_examples=1750, dataset_name='masakhaner')}, {'expected': SplitInfo(name='validation', num_bytes=92768, num_examples=251, dataset_name='masakhaner'), 'recorded': SplitInfo(name='validation', num_bytes=92753, num_examples=250, dataset_name='masakhaner')}, {'expected': SplitInfo(name='test', num_bytes=184286, num_examples=501, dataset_name='masakhaner'), 'recorded': SplitInfo(name='test', num_bytes=184271, num_examples=500, dataset_name='masakhaner')}]\r\n```\r\n\r\n## Environment info\r\nGoogle Colab\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2971/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2969", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2969/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2969/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2969/events", "html_url": "https://github.com/huggingface/datasets/issues/2969", "id": 1007217867, "node_id": "I_kwDODunzps48COzL", "number": 2969, "title": "medical-dialog error", "user": {"login": "smeyerhot", "id": 43877130, "node_id": "MDQ6VXNlcjQzODc3MTMw", "avatar_url": "https://avatars.githubusercontent.com/u/43877130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smeyerhot", "html_url": "https://github.com/smeyerhot", "followers_url": "https://api.github.com/users/smeyerhot/followers", "following_url": "https://api.github.com/users/smeyerhot/following{/other_user}", "gists_url": "https://api.github.com/users/smeyerhot/gists{/gist_id}", "starred_url": "https://api.github.com/users/smeyerhot/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smeyerhot/subscriptions", "organizations_url": "https://api.github.com/users/smeyerhot/orgs", "repos_url": "https://api.github.com/users/smeyerhot/repos", "events_url": "https://api.github.com/users/smeyerhot/events{/privacy}", "received_events_url": "https://api.github.com/users/smeyerhot/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-09-25T23:08:44Z", "updated_at": "2021-10-11T07:46:42Z", "closed_at": "2021-10-11T07:46:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\nWhen I attempt to download the huggingface datatset medical_dialog it errors out midway through\r\n## Steps to reproduce the bug\r\n```python\r\nraw_datasets = load_dataset(\"medical_dialog\", \"en\", split=\"train\", download_mode=\"force_redownload\", data_dir=\"./Medical-Dialogue-Dataset-English\")\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\nNo error\r\n## Actual results\r\n\r\n```\r\n3 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/info_utils.py in verify_splits(expected_splits, recorded_splits)\r\n     72     ]\r\n     73     if len(bad_splits) > 0:\r\n---> 74         raise NonMatchingSplitsSizesError(str(bad_splits))\r\n     75     logger.info(\"All the splits matched successfully.\")\r\n     76 \r\n\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=0, num_examples=0, dataset_name='medical_dialog'), 'recorded': SplitInfo(name='train', num_bytes=295097913, num_examples=229674, dataset_name='medical_dialog')}]\r\n```\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.21.1\r\n- Platform: colab\r\n- Python version: colab 3.7\r\n- PyArrow version: N/A\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2969/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2968", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2968/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2968/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2968/events", "html_url": "https://github.com/huggingface/datasets/issues/2968", "id": 1007209488, "node_id": "I_kwDODunzps48CMwQ", "number": 2968, "title": "`DatasetDict` cannot be exported to parquet if the splits have different features", "user": {"login": "LysandreJik", "id": 30755778, "node_id": "MDQ6VXNlcjMwNzU1Nzc4", "avatar_url": "https://avatars.githubusercontent.com/u/30755778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LysandreJik", "html_url": "https://github.com/LysandreJik", "followers_url": "https://api.github.com/users/LysandreJik/followers", "following_url": "https://api.github.com/users/LysandreJik/following{/other_user}", "gists_url": "https://api.github.com/users/LysandreJik/gists{/gist_id}", "starred_url": "https://api.github.com/users/LysandreJik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LysandreJik/subscriptions", "organizations_url": "https://api.github.com/users/LysandreJik/orgs", "repos_url": "https://api.github.com/users/LysandreJik/repos", "events_url": "https://api.github.com/users/LysandreJik/events{/privacy}", "received_events_url": "https://api.github.com/users/LysandreJik/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2021-09-25T22:18:39Z", "updated_at": "2021-10-07T22:47:42Z", "closed_at": "2021-10-07T22:47:26Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nI'm trying to use parquet as a means of serialization for both `Dataset` and `DatasetDict` objects. Using `to_parquet` alongside `from_parquet` or `load_dataset` for a `Dataset` works perfectly. \r\n\r\nFor `DatasetDict`, I use `to_parquet` on each split to save the parquet files in individual folders representing individual splits. This works too, as long as the splits have identical features. If a split has different features to neighboring splits, then loading the dataset will fail: a single schema is used to load both splits, resulting in a failure to load the second parquet file.\r\n\r\n## Steps to reproduce the bug\r\n\r\nThe following works as expected:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"lhoestq/custom_squad\")\r\n\r\nds['train'].to_parquet(\"./ds/train/split.parquet\")\r\nds['validation'].to_parquet(\"./ds/validation/split.parquet\")\r\n\r\nbrand_new_dataset = load_dataset(\"ds\")\r\n```\r\n\r\nModifying a single split to add a new feature ends up in a crash:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"lhoestq/custom_squad\")\r\n\r\n\r\ndef identical_answers(e):\r\n    e['identical_answers'] = len(set(e['answers']['text'])) == 1\r\n    return e\r\n\r\n\r\nds['validation'] = ds['validation'].map(identical_answers)\r\nds['train'].to_parquet(\"./ds/train/split.parquet\")\r\nds['validation'].to_parquet(\"./ds/validation/split.parquet\")\r\n\r\nbrand_new_dataset = load_dataset(\"ds\")\r\n```\r\n```\r\n  File \"/home/lysandre/.config/JetBrains/PyCharm2021.2/scratches/datasets/upload_dataset.py\", line 26, in <module>\r\n    brand_new_dataset = load_dataset(\"ds\")\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/load.py\", line 1151, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/builder.py\", line 642, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/builder.py\", line 732, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/builder.py\", line 1194, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/arrow_writer.py\", line 428, in write_table\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/arrow_writer.py\", line 428, in <listcomp>\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"pyarrow/table.pxi\", line 1257, in pyarrow.lib.Table.__getitem__\r\n  File \"pyarrow/table.pxi\", line 1833, in pyarrow.lib.Table.column\r\n  File \"pyarrow/table.pxi\", line 1808, in pyarrow.lib.Table._ensure_integer_index\r\nKeyError: 'Field \"identical_answers\" does not exist in table schema'\r\n```\r\n\r\nIt does work, however, to use the `save_to_disk` and `load_from_disk` methods:\r\n\r\n```py\r\nfrom datasets import load_from_disk\r\n\r\nds = load_dataset(\"lhoestq/custom_squad\")\r\n\r\n\r\ndef identical_answers(e):\r\n    e['identical_answers'] = len(set(e['answers']['text'])) == 1\r\n    return e\r\n\r\n\r\nds['validation'] = ds['validation'].map(identical_answers)\r\n\r\nds.save_to_disk(\"local_path\")\r\nbrand_new_dataset = load_from_disk(\"local_path\")\r\n```\r\n\r\n## Expected results\r\n\r\nThe saving works correctly - but the loading fails. I would expect either an error when saving or an error-less instantiation of the dataset through the parquet files.\r\n\r\nIf it's helpful, I've traced a possible patch to the `write_table` method here: \r\n\r\nhttps://github.com/huggingface/datasets/blob/26ff41aa3a642e46489db9e95be1e9a8c4e64bea/src/datasets/arrow_writer.py#L424-L425\r\n\r\nThe writer is built only if the parquet writer is `None`, but I expect we would want to build a new writer as the table schema has changed. Furthermore, it relies on having the property `update_features` set to `True` in order to update the features:\r\n\r\nhttps://github.com/huggingface/datasets/blob/26ff41aa3a642e46489db9e95be1e9a8c4e64bea/src/datasets/arrow_writer.py#L254-L255\r\n\r\nbut the `ArrowWriter` is instantiated without that option in the `_prepare_split` method of the `ArrowBasedBuilder`:\r\n\r\nhttps://github.com/huggingface/datasets/blob/26ff41aa3a642e46489db9e95be1e9a8c4e64bea/src/datasets/builder.py#L1190\r\n\r\nUpdating these two parts to recreate a schema on each split results in an error that is, unfortunately, out of my expertise:\r\n\r\n```\r\n  File \"/home/lysandre/.config/JetBrains/PyCharm2021.2/scratches/datasets/upload_dataset.py\", line 27, in <module>\r\n    brand_new_dataset = load_dataset(\"ds\")\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/load.py\", line 1163, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/builder.py\", line 819, in as_dataset\r\n    datasets = utils.map_nested(\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/utils/py_utils.py\", line 207, in map_nested\r\n    mapped = [\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/utils/py_utils.py\", line 208, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True))\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/utils/py_utils.py\", line 143, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/builder.py\", line 850, in _build_single_dataset\r\n    ds = self._as_dataset(\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/builder.py\", line 920, in _as_dataset\r\n    dataset_kwargs = ArrowReader(self._cache_dir, self.info).read(\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/arrow_reader.py\", line 217, in read\r\n    return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/arrow_reader.py\", line 238, in read_files\r\n    pa_table = self._read_files(files, in_memory=in_memory)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/arrow_reader.py\", line 173, in _read_files\r\n    pa_table: Table = self._get_table_from_filename(f_dict, in_memory=in_memory)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/arrow_reader.py\", line 308, in _get_table_from_filename\r\n    table = ArrowReader.read_table(filename, in_memory=in_memory)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/arrow_reader.py\", line 327, in read_table\r\n    return table_cls.from_file(filename)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/table.py\", line 458, in from_file\r\n    table = _memory_mapped_arrow_table_from_file(filename)\r\n  File \"/home/lysandre/Workspaces/Python/datasets/src/datasets/table.py\", line 45, in _memory_mapped_arrow_table_from_file\r\n    pa_table = opened_stream.read_all()\r\n  File \"pyarrow/ipc.pxi\", line 563, in pyarrow.lib.RecordBatchReader.read_all\r\n  File \"pyarrow/error.pxi\", line 114, in pyarrow.lib.check_status\r\nOSError: Header-type of flatbuffer-encoded Message is not RecordBatch.\r\n```\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.2.dev0\r\n- Platform: Linux-5.14.7-arch1-1-x86_64-with-glibc2.33\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2968/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2965", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2965/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2965/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2965/events", "html_url": "https://github.com/huggingface/datasets/issues/2965", "id": 1007084153, "node_id": "I_kwDODunzps48BuJ5", "number": 2965, "title": "Invalid download URL of WMT17 `zh-en` data ", "user": {"login": "Ririkoo", "id": 3339950, "node_id": "MDQ6VXNlcjMzMzk5NTA=", "avatar_url": "https://avatars.githubusercontent.com/u/3339950?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ririkoo", "html_url": "https://github.com/Ririkoo", "followers_url": "https://api.github.com/users/Ririkoo/followers", "following_url": "https://api.github.com/users/Ririkoo/following{/other_user}", "gists_url": "https://api.github.com/users/Ririkoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ririkoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ririkoo/subscriptions", "organizations_url": "https://api.github.com/users/Ririkoo/orgs", "repos_url": "https://api.github.com/users/Ririkoo/repos", "events_url": "https://api.github.com/users/Ririkoo/events{/privacy}", "received_events_url": "https://api.github.com/users/Ririkoo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2067388877, "node_id": "MDU6TGFiZWwyMDY3Mzg4ODc3", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset%20bug", "name": "dataset bug", "color": "2edb81", "default": false, "description": "A bug in a dataset script provided in the library"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-09-25T13:17:32Z", "updated_at": "2022-08-31T06:47:11Z", "closed_at": "2022-08-31T06:47:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nPartial data (wmt17 zh-en) cannot be downloaded due to an invalid URL.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n dataset = load_dataset('wmt17','zh-en')\r\n```\r\n\r\n## Expected results\r\nConnectionError: Couldn't reach ftp://cwmt-wmt:cwmt-wmt@datasets.nju.edu.cn/parallel/casia2015.zip", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2965/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 1, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2965/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2964", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2964/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2964/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2964/events", "html_url": "https://github.com/huggingface/datasets/issues/2964", "id": 1006605904, "node_id": "I_kwDODunzps47_5ZQ", "number": 2964, "title": "Error when calculating Matthews Correlation Coefficient loaded with `load_metric`", "user": {"login": "alvarobartt", "id": 36760800, "node_id": "MDQ6VXNlcjM2NzYwODAw", "avatar_url": "https://avatars.githubusercontent.com/u/36760800?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alvarobartt", "html_url": "https://github.com/alvarobartt", "followers_url": "https://api.github.com/users/alvarobartt/followers", "following_url": "https://api.github.com/users/alvarobartt/following{/other_user}", "gists_url": "https://api.github.com/users/alvarobartt/gists{/gist_id}", "starred_url": "https://api.github.com/users/alvarobartt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alvarobartt/subscriptions", "organizations_url": "https://api.github.com/users/alvarobartt/orgs", "repos_url": "https://api.github.com/users/alvarobartt/repos", "events_url": "https://api.github.com/users/alvarobartt/events{/privacy}", "received_events_url": "https://api.github.com/users/alvarobartt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-09-24T15:55:21Z", "updated_at": "2021-09-25T08:06:07Z", "closed_at": "2021-09-25T08:06:07Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nAfter loading the metric named \"[Matthews Correlation Coefficient](https://huggingface.co/metrics/matthews_correlation)\" from `\ud83e\udd17datasets`, the `.compute` method fails with the following exception `AttributeError: 'float' object has no attribute 'item'` (complete stack trace can be provided if required).\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\nimport torch\r\npredictions = torch.ones((10,))\r\nreferences = torch.zeros((10,))\r\n\r\nfrom datasets import load_metric\r\n\r\nMETRIC = load_metric(\"matthews_correlation\")\r\n\r\nresult = METRIC.compute(predictions=predictions, references=references)\r\n```\r\n\r\n## Expected results\r\n\r\nWe should expect a Python `dict` as it follows:\r\n\r\n```\r\n{\r\n    \"matthews_correlation\": float()\r\n}\r\n```\r\n\r\nas defined in https://github.com/huggingface/datasets/blob/master/metrics/matthews_correlation/matthews_correlation.py, so the fix will imply removing `.item()`, since the value returned by the `scikit-learn` function is not a `torch.Tensor` but a `float`, which means that the `.item()` will fail.\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/alvaro.bartolome/XXX/xxx/cli.py\", line 59, in main\r\n    app()\r\n  File \"/home/alvaro.bartolome/miniconda3/envs/xxx/lib/python3.9/site-packages/typer/main.py\", line 214, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n  File \"/home/alvaro.bartolome/miniconda3/envs/xxx/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/alvaro.bartolome/miniconda3/envs/xxx/lib/python3.9/site-packages/click/core.py\", line 1062, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/alvaro.bartolome/miniconda3/envs/xxx/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/alvaro.bartolome/miniconda3/envs/xxx/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/alvaro.bartolome/miniconda3/envs/xxx/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/alvaro.bartolome/miniconda3/envs/xxx/lib/python3.9/site-packages/typer/main.py\", line 500, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n  File \"/home/alvaro.bartolome/XXX/xxx/cli.py\", line 43, in train\r\n    metrics = trainer.evaluate()\r\n  File \"/home/alvaro.bartolome/miniconda3/envs/xxx/lib/python3.9/site-packages/transformers/trainer.py\", line 2051, in evaluate\r\n    output = eval_loop(\r\n  File \"/home/alvaro.bartolome/miniconda3/envs/xxx/lib/python3.9/site-packages/transformers/trainer.py\", line 2292, in evaluation_loop\r\n    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\r\n  File \"/home/alvaro.bartolome/XXX/xxx/metrics.py\", line 20, in compute_metrics\r\n    res = METRIC.compute(predictions=predictions, references=eval_preds.label_ids)\r\n  File \"/home/alvaro.bartolome/miniconda3/envs/lang/lib/python3.9/site-packages/datasets/metric.py\", line 402, in compute\r\n    output = self._compute(predictions=predictions, references=references, **kwargs)\r\n  File \"/home/alvaro.bartolome/.cache/huggingface/modules/datasets_modules/metrics/matthews_correlation/0275f1e9a4d318e3ea8cdd87547ee0d58d894966616052e3d18444ac8ddd2357/matthews_correlation.py\", line 88, in _compute\r\n    \"matthews_correlation\": matthews_corrcoef(references, predictions, sample_weight=sample_weight).item(),\r\nAttributeError: 'float' object has no attribute 'item'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-4.15.0-1113-azure-x86_64-with-glibc2.23\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2964/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2964/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2963", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2963/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2963/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2963/events", "html_url": "https://github.com/huggingface/datasets/issues/2963", "id": 1006588605, "node_id": "I_kwDODunzps47_1K9", "number": 2963, "title": "    raise TypeError( TypeError: Provided `function` which is applied to all elements of table returns a variable of type <class 'list'>. Make sure provided `function` returns a variable of type `dict` to update the dataset or `None` if you are only interested in side effects.", "user": {"login": "keloemma", "id": 40454218, "node_id": "MDQ6VXNlcjQwNDU0MjE4", "avatar_url": "https://avatars.githubusercontent.com/u/40454218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keloemma", "html_url": "https://github.com/keloemma", "followers_url": "https://api.github.com/users/keloemma/followers", "following_url": "https://api.github.com/users/keloemma/following{/other_user}", "gists_url": "https://api.github.com/users/keloemma/gists{/gist_id}", "starred_url": "https://api.github.com/users/keloemma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keloemma/subscriptions", "organizations_url": "https://api.github.com/users/keloemma/orgs", "repos_url": "https://api.github.com/users/keloemma/repos", "events_url": "https://api.github.com/users/keloemma/events{/privacy}", "received_events_url": "https://api.github.com/users/keloemma/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-09-24T15:35:11Z", "updated_at": "2021-09-24T15:38:24Z", "closed_at": "2021-09-24T15:38:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\nI am trying to use Dataset to load my file in order to use Bert embeddings model baut when I finished loading using dataset and I want to pass to the tokenizer using the function map; I get the following error : raise TypeError(\r\nTypeError: Provided `function` which is applied to all elements of table returns a variable of type <class 'list'>. Make sure provided `function` returns a variable of type `dict` to update the dataset or `None` if you are only interested in side effects.\r\n\r\nI was able to load my file using dataset before but since this morning , I keep getting this erreor.\r\n## Steps to reproduce the bug\r\n```python\r\n# Xtrain, ytrain, filename, len_labels = read_file_2(fic)\r\n\t# Xtrain, lge_size = get_flaubert_layer(Xtrain, path_to_model_lge)\r\n\r\n\tdata_preprocessed = make_new_traindata(Xtrain)\r\n\t\r\n\tmy_dict = {\"verbatim\": data_preprocessed[1], \"label\": ytrain} # lemme avec conjonction\r\n\tdataset = Dataset.from_dict(my_dict)\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2963/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2963/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2957", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2957/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2957/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2957/events", "html_url": "https://github.com/huggingface/datasets/issues/2957", "id": 1004868337, "node_id": "I_kwDODunzps475RLx", "number": 2957, "title": "MultiWOZ Dataset NonMatchingChecksumError", "user": {"login": "bradyneal", "id": 8754873, "node_id": "MDQ6VXNlcjg3NTQ4NzM=", "avatar_url": "https://avatars.githubusercontent.com/u/8754873?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bradyneal", "html_url": "https://github.com/bradyneal", "followers_url": "https://api.github.com/users/bradyneal/followers", "following_url": "https://api.github.com/users/bradyneal/following{/other_user}", "gists_url": "https://api.github.com/users/bradyneal/gists{/gist_id}", "starred_url": "https://api.github.com/users/bradyneal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bradyneal/subscriptions", "organizations_url": "https://api.github.com/users/bradyneal/orgs", "repos_url": "https://api.github.com/users/bradyneal/repos", "events_url": "https://api.github.com/users/bradyneal/events{/privacy}", "received_events_url": "https://api.github.com/users/bradyneal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-09-22T23:45:00Z", "updated_at": "2022-03-15T16:07:02Z", "closed_at": "2022-03-15T16:07:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe checksums for the downloaded MultiWOZ dataset and source MultiWOZ dataset aren't matching.\r\n\r\n## Steps to reproduce the bug\r\nBoth of the below dataset versions yield the checksum error:\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('multi_woz_v22', 'v2.2')\r\ndataset = load_dataset('multi_woz_v22', 'v2.2_active_only')\r\n```\r\n\r\n## Expected results\r\nFor the above calls to `load_dataset` to work.\r\n\r\n## Actual results\r\nNonMatchingChecksumError. Traceback:\r\n> Traceback (most recent call last):\r\n  File \"/Users/brady/anaconda3/envs/elysium/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-15-4e91280e112e>\", line 1, in <module>\r\n    dataset = load_dataset('multi_woz_v22', 'v2.2')\r\n  File \"/Users/brady/anaconda3/envs/elysium/lib/python3.8/site-packages/datasets/load.py\", line 847, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Users/brady/anaconda3/envs/elysium/lib/python3.8/site-packages/datasets/builder.py\", line 615, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/brady/anaconda3/envs/elysium/lib/python3.8/site-packages/datasets/builder.py\", line 675, in _download_and_prepare\r\n    verify_checksums(\r\n  File \"/Users/brady/anaconda3/envs/elysium/lib/python3.8/site-packages/datasets/utils/info_utils.py\", line 40, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/dialog_acts.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/test/dialogues_001.json']\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2957/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2943", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2943/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2943/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2943/events", "html_url": "https://github.com/huggingface/datasets/issues/2943", "id": 1000355115, "node_id": "I_kwDODunzps47oDUr", "number": 2943, "title": "Backwards compatibility broken for cached datasets that use `.filter()`", "user": {"login": "anton-l", "id": 26864830, "node_id": "MDQ6VXNlcjI2ODY0ODMw", "avatar_url": "https://avatars.githubusercontent.com/u/26864830?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anton-l", "html_url": "https://github.com/anton-l", "followers_url": "https://api.github.com/users/anton-l/followers", "following_url": "https://api.github.com/users/anton-l/following{/other_user}", "gists_url": "https://api.github.com/users/anton-l/gists{/gist_id}", "starred_url": "https://api.github.com/users/anton-l/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anton-l/subscriptions", "organizations_url": "https://api.github.com/users/anton-l/orgs", "repos_url": "https://api.github.com/users/anton-l/repos", "events_url": "https://api.github.com/users/anton-l/events{/privacy}", "received_events_url": "https://api.github.com/users/anton-l/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2021-09-19T16:16:37Z", "updated_at": "2021-09-20T16:25:43Z", "closed_at": "2021-09-20T16:25:42Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \r\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\r\n\r\nRelated feature: https://github.com/huggingface/datasets/pull/2836\r\n\r\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \r\n\r\n## Workaround\r\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\r\n\r\n## Steps to reproduce the bug\r\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\r\n\r\n2. `pip install datasets==1.11.0` and run the following snippet:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nids = [\"1272-141231-0000\"]\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\nds = ds.filter(lambda x: x[\"id\"] in ids)\r\n```\r\n3. `pip install datasets==1.12.1` and re-run the code again\r\n\r\n## Expected results\r\nSame result as with the previous `datasets` version.\r\n\r\n## Actual results\r\n```bash\r\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\r\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\r\nTraceback (most recent call last):\r\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\r\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\r\n    indices = self.map(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\r\n    return self._map_single(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\r\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\r\n    return cls(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\r\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\r\n    return Features(recursive_reorder(self, other))\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\r\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\r\nValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2943/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2943/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2937", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2937/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2937/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2937/events", "html_url": "https://github.com/huggingface/datasets/issues/2937", "id": 999548277, "node_id": "I_kwDODunzps47k-V1", "number": 2937, "title": "load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied", "user": {"login": "daqieq", "id": 40532020, "node_id": "MDQ6VXNlcjQwNTMyMDIw", "avatar_url": "https://avatars.githubusercontent.com/u/40532020?v=4", "gravatar_id": "", "url": "https://api.github.com/users/daqieq", "html_url": "https://github.com/daqieq", "followers_url": "https://api.github.com/users/daqieq/followers", "following_url": "https://api.github.com/users/daqieq/following{/other_user}", "gists_url": "https://api.github.com/users/daqieq/gists{/gist_id}", "starred_url": "https://api.github.com/users/daqieq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/daqieq/subscriptions", "organizations_url": "https://api.github.com/users/daqieq/orgs", "repos_url": "https://api.github.com/users/daqieq/repos", "events_url": "https://api.github.com/users/daqieq/events{/privacy}", "received_events_url": "https://api.github.com/users/daqieq/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-09-17T16:52:10Z", "updated_at": "2022-08-24T13:09:08Z", "closed_at": "2022-08-24T13:09:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('wiki_bio')\r\n```\r\n\r\n## Expected results\r\nIt is expected that the dataset downloads without any errors.\r\n\r\n## Actual results\r\nPermissionError see trace below:\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\builder.py\", line 644, in download_and_prepare\r\n    self._save_info()\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\contextlib.py\", line 120, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\builder.py\", line 598, in incomplete_dir\r\n    os.rename(tmp_dir, dirname)\r\nPermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\username\\\\.cache\\\\huggingface\\\\datasets\\\\wiki_bio\\\\default\\\\1.1.0\\\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9.incomplete' -> 'C:\\\\Users\\\\username\\\\.cache\\\\huggingface\\\\datasets\\\\wiki_bio\\\\default\\\\1.1.0\\\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9'\r\n```\r\nBy commenting out the os.rename() [L604](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L604) and the shutil.rmtree() [L607](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L607) lines, in my virtual environment, I was able to get the load process to complete, rename the directory manually and then rerun the `load_dataset('wiki_bio')` to get what I needed.\r\n\r\nIt seems that os.rename() in the `incomplete_dir` content manager is the culprit. Here's another project [Conan](https://github.com/conan-io/conan/issues/6560) with similar issue with os.rename() if it helps debug this issue.\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Windows-10-10.0.22449-SP0\r\n- Python version: 3.8.12\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2937/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2934", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2934/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2934/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2934/events", "html_url": "https://github.com/huggingface/datasets/issues/2934", "id": 999477413, "node_id": "I_kwDODunzps47ktCl", "number": 2934, "title": "to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows", "user": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-09-17T15:26:53Z", "updated_at": "2021-10-13T09:03:23Z", "closed_at": "2021-10-13T09:03:23Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "To reproduce:\r\n```python\r\nimport datasets as ds\r\nimport weakref\r\nimport gc\r\n\r\nd = ds.load_dataset(\"mnist\", split=\"train\")\r\nref = weakref.ref(d._data.table)\r\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\r\ndel tfd, d\r\ngc.collect()\r\nassert ref() is None, \"Error: there is at least one reference left\"\r\n```\r\n\r\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\r\n\r\nMoreover the CI test of the `to_tf_dataset` method isn't able to clean up the temporary arrow files because of this.\r\n\r\ncc @Rocketknight1 ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2934/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2934/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2932", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2932/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2932/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2932/events", "html_url": "https://github.com/huggingface/datasets/issues/2932", "id": 999317750, "node_id": "I_kwDODunzps47kGD2", "number": 2932, "title": "Conda build fails", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-09-17T12:49:22Z", "updated_at": "2021-09-21T15:31:10Z", "closed_at": "2021-09-21T15:31:10Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nCurrent `datasets` version in conda is 1.9 instead of 1.12.\r\n\r\nThe build of the conda package fails.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2932/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2932/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2930", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2930/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2930/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2930/events", "html_url": "https://github.com/huggingface/datasets/issues/2930", "id": 998154311, "node_id": "I_kwDODunzps47fqBH", "number": 2930, "title": "Mutable columns argument breaks set_format", "user": {"login": "Rocketknight1", "id": 12866554, "node_id": "MDQ6VXNlcjEyODY2NTU0", "avatar_url": "https://avatars.githubusercontent.com/u/12866554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rocketknight1", "html_url": "https://github.com/Rocketknight1", "followers_url": "https://api.github.com/users/Rocketknight1/followers", "following_url": "https://api.github.com/users/Rocketknight1/following{/other_user}", "gists_url": "https://api.github.com/users/Rocketknight1/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rocketknight1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rocketknight1/subscriptions", "organizations_url": "https://api.github.com/users/Rocketknight1/orgs", "repos_url": "https://api.github.com/users/Rocketknight1/repos", "events_url": "https://api.github.com/users/Rocketknight1/events{/privacy}", "received_events_url": "https://api.github.com/users/Rocketknight1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "Rocketknight1", "id": 12866554, "node_id": "MDQ6VXNlcjEyODY2NTU0", "avatar_url": "https://avatars.githubusercontent.com/u/12866554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rocketknight1", "html_url": "https://github.com/Rocketknight1", "followers_url": "https://api.github.com/users/Rocketknight1/followers", "following_url": "https://api.github.com/users/Rocketknight1/following{/other_user}", "gists_url": "https://api.github.com/users/Rocketknight1/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rocketknight1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rocketknight1/subscriptions", "organizations_url": "https://api.github.com/users/Rocketknight1/orgs", "repos_url": "https://api.github.com/users/Rocketknight1/repos", "events_url": "https://api.github.com/users/Rocketknight1/events{/privacy}", "received_events_url": "https://api.github.com/users/Rocketknight1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Rocketknight1", "id": 12866554, "node_id": "MDQ6VXNlcjEyODY2NTU0", "avatar_url": "https://avatars.githubusercontent.com/u/12866554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rocketknight1", "html_url": "https://github.com/Rocketknight1", "followers_url": "https://api.github.com/users/Rocketknight1/followers", "following_url": "https://api.github.com/users/Rocketknight1/following{/other_user}", "gists_url": "https://api.github.com/users/Rocketknight1/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rocketknight1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rocketknight1/subscriptions", "organizations_url": "https://api.github.com/users/Rocketknight1/orgs", "repos_url": "https://api.github.com/users/Rocketknight1/repos", "events_url": "https://api.github.com/users/Rocketknight1/events{/privacy}", "received_events_url": "https://api.github.com/users/Rocketknight1/received_events", "type": "User", "site_admin": false}, {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-09-16T12:27:22Z", "updated_at": "2021-09-16T13:50:53Z", "closed_at": "2021-09-16T13:50:53Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nIf you pass a mutable list to the `columns` argument of `set_format` and then change the list afterwards, the returned columns also change.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"glue\", \"cola\")\r\n\r\ncolumn_list = [\"idx\", \"label\"]\r\ndataset.set_format(\"python\", columns=column_list)\r\ncolumn_list[1] = \"foo\"  # Change the list after we call `set_format`\r\ndataset['train'][:4].keys()\r\n```\r\n\r\n## Expected results\r\n```python\r\ndict_keys(['idx', 'label'])\r\n```\r\n\r\n## Actual results\r\n```python\r\ndict_keys(['idx'])\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2930/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2927", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2927/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2927/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2927/events", "html_url": "https://github.com/huggingface/datasets/issues/2927", "id": 997654680, "node_id": "I_kwDODunzps47dwCY", "number": 2927, "title": "Datasets 1.12 dataset.filter TypeError: get_indices_from_mask_function() got an unexpected keyword argument", "user": {"login": "timothyjlaurent", "id": 2000204, "node_id": "MDQ6VXNlcjIwMDAyMDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/2000204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/timothyjlaurent", "html_url": "https://github.com/timothyjlaurent", "followers_url": "https://api.github.com/users/timothyjlaurent/followers", "following_url": "https://api.github.com/users/timothyjlaurent/following{/other_user}", "gists_url": "https://api.github.com/users/timothyjlaurent/gists{/gist_id}", "starred_url": "https://api.github.com/users/timothyjlaurent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/timothyjlaurent/subscriptions", "organizations_url": "https://api.github.com/users/timothyjlaurent/orgs", "repos_url": "https://api.github.com/users/timothyjlaurent/repos", "events_url": "https://api.github.com/users/timothyjlaurent/events{/privacy}", "received_events_url": "https://api.github.com/users/timothyjlaurent/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-09-16T01:14:02Z", "updated_at": "2021-09-20T16:23:22Z", "closed_at": "2021-09-20T16:23:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nUpgrading to 1.12 caused `dataset.filter` call to fail with \r\n\r\n> get_indices_from_mask_function() got an unexpected keyword argument valid_rel_labels\r\n\r\n\r\n## Steps to reproduce the bug\r\n```pythondef \r\n\r\nfilter_good_rows(\r\n    ex: Dict,\r\n    valid_rel_labels: Set[str],\r\n    valid_ner_labels: Set[str],\r\n    tokenizer: PreTrainedTokenizerFast,\r\n) -> bool:\r\n    \"\"\"Get the good rows\"\"\"\r\n    encoding = get_encoding_for_text(text=ex[\"text\"], tokenizer=tokenizer)\r\n    ex[\"encoding\"] = encoding\r\n    for relation in ex[\"relations\"]:\r\n        if not is_valid_relation(relation, valid_rel_labels):\r\n            return False\r\n    for span in ex[\"spans\"]:\r\n        if not is_valid_span(span, valid_ner_labels, encoding):\r\n            return False\r\n    return True\r\n    \r\ndef get_dataset():    \r\n    loader_path = str(Path(__file__).parent / \"prodigy_dataset_builder.py\")\r\n    ds = load_dataset(\r\n        loader_path,\r\n        name=\"prodigy-dataset\",\r\n        data_files=sorted(file_paths),\r\n        cache_dir=cache_dir,\r\n    )[\"train\"]\r\n\r\n    valid_ner_labels = set(vocab.ner_category)\r\n    valid_relations = set(vocab.relation_types.keys())\r\n    ds = ds.filter(\r\n        filter_good_rows,\r\n        fn_kwargs=dict(\r\n            valid_rel_labels=valid_relations,\r\n            valid_ner_labels=valid_ner_labels,\r\n            tokenizer=vocab.tokenizer,\r\n        ),\r\n        keep_in_memory=True,\r\n        num_proc=num_proc,\r\n    )\r\n\r\n```\r\n\r\n`ds` is a `DatasetDict` produced by a jsonl dataset.\r\nThis runs fine on 1.11 but fails on 1.12\r\n\r\n**Stack Trace**\r\n\r\n\r\n\r\n## Expected results\r\n\r\nI expect 1.12 datasets filter to filter the dataset without raising as it does on 1.11\r\n\r\n## Actual results\r\n```\r\ntf_ner_rel_lib/dataset.py:695: in load_prodigy_arrow_datasets_from_jsonl\r\n    ds = ds.filter(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2169: in filter\r\n    indices = self.map(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1686: in map\r\n    return self._map_single(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2048: in _map_single\r\n    batch = apply_function_on_filtered_inputs(\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ninputs = {'_input_hash': [2108817714, 1477695082, -1021597032, 2130671338, -1260483858, -1203431639, ...], '_task_hash': [18070...ons', 'relations', 'relations', ...], 'answer': ['accept', 'accept', 'accept', 'accept', 'accept', 'accept', ...], ...}\r\nindices = [0, 1, 2, 3, 4, 5, ...], check_same_num_examples = False, offset = 0\r\n\r\n    def apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples=False, offset=0):\r\n        \"\"\"Utility to apply the function on a selection of columns.\"\"\"\r\n        nonlocal update_data\r\n        fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]\r\n        if offset == 0:\r\n            effective_indices = indices\r\n        else:\r\n            effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset\r\n        processed_inputs = (\r\n>           function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n        )\r\nE       TypeError: get_indices_from_mask_function() got an unexpected keyword argument 'valid_rel_labels'\r\n\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1939: TypeError\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Mac\r\n- Python version: 3.8.9\r\n- PyArrow version: pyarrow==5.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2927/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2924", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2924/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2924/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2924/events", "html_url": "https://github.com/huggingface/datasets/issues/2924", "id": 997378113, "node_id": "I_kwDODunzps47cshB", "number": 2924, "title": "\"File name too long\" error for file locks", "user": {"login": "gar1t", "id": 184949, "node_id": "MDQ6VXNlcjE4NDk0OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/184949?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gar1t", "html_url": "https://github.com/gar1t", "followers_url": "https://api.github.com/users/gar1t/followers", "following_url": "https://api.github.com/users/gar1t/following{/other_user}", "gists_url": "https://api.github.com/users/gar1t/gists{/gist_id}", "starred_url": "https://api.github.com/users/gar1t/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gar1t/subscriptions", "organizations_url": "https://api.github.com/users/gar1t/orgs", "repos_url": "https://api.github.com/users/gar1t/repos", "events_url": "https://api.github.com/users/gar1t/events{/privacy}", "received_events_url": "https://api.github.com/users/gar1t/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2021-09-15T18:16:50Z", "updated_at": "2023-03-28T06:50:18Z", "closed_at": "2021-10-29T09:42:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nGetting the following error when calling `load_dataset(\"gar1t/test\")`:\r\n\r\n```\r\nOSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'\r\n```\r\n\r\n## Steps to reproduce the bug\r\n\r\nWhere the user cache dir (e.g. `~/.cache`) is on a file system that limits filenames to 255 chars (e.g. ext4):\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"gar1t/test\")\r\n```\r\n\r\n## Expected results\r\n\r\nExpect the function to return without an error.\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/builder.py\", line 644, in download_and_prepare\r\n    self._save_info()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/builder.py\", line 765, in _save_info\r\n    with FileLock(lock_path):\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 323, in __enter__\r\n    self.acquire()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 272, in acquire\r\n    self._acquire()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 403, in _acquire\r\n    fd = os.open(self._lock_file, open_mode)\r\nOSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2924/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 1, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2924/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2923", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2923/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2923/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2923/events", "html_url": "https://github.com/huggingface/datasets/issues/2923", "id": 997351590, "node_id": "I_kwDODunzps47cmCm", "number": 2923, "title": "Loading an autonlp dataset raises in normal mode but not in streaming mode", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 3470211881, "node_id": "LA_kwDODunzps7O1zsp", "url": "https://api.github.com/repos/huggingface/datasets/labels/dataset-viewer", "name": "dataset-viewer", "color": "E5583E", "default": false, "description": "Related to the dataset viewer on huggingface.co"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-09-15T17:44:38Z", "updated_at": "2022-04-12T10:09:40Z", "closed_at": "2022-04-12T10:09:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThe same dataset (from autonlp) raises an error in normal mode, but does not raise in streaming mode\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"severo/autonlp-data-sentiment_detection-3c8bcd36\", split=\"train\", streaming=False)\r\n## raises an error\r\n\r\nload_dataset(\"severo/autonlp-data-sentiment_detection-3c8bcd36\", split=\"train\", streaming=True)\r\n## does not raise an error\r\n```\r\n\r\n## Expected results\r\n\r\nBoth calls should raise the same error\r\n\r\n## Actual results\r\n\r\nCall with streaming=False:\r\n\r\n```\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 5825.42it/s]\r\nUsing custom data configuration autonlp-data-sentiment_detection-3c8bcd36-fe30267462d1d42b\r\nDownloading and preparing dataset json/autonlp-data-sentiment_detection-3c8bcd36 to /home/slesage/.cache/huggingface/datasets/json/autonlp-data-sentiment_detection-3c8bcd36-fe30267462d1d42b/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 15923.71it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 3346.88it/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 636, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 726, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 1187, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 418, in write_table\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 418, in <listcomp>\r\n    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\r\n  File \"pyarrow/table.pxi\", line 1249, in pyarrow.lib.Table.__getitem__\r\n  File \"pyarrow/table.pxi\", line 1825, in pyarrow.lib.Table.column\r\n  File \"pyarrow/table.pxi\", line 1800, in pyarrow.lib.Table._ensure_integer_index\r\nKeyError: 'Field \"splits\" does not exist in table schema'\r\n```\r\n\r\nCall with `streaming=False`:\r\n\r\n```\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6000.43it/s]\r\nUsing custom data configuration autonlp-data-sentiment_detection-3c8bcd36-fe30267462d1d42b\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 46916.15it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 148734.18it/s]\r\n```\r\n\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1.dev0\r\n- Platform: Linux-5.11.0-1017-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2923/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2923/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2918", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2918/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2918/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2918/events", "html_url": "https://github.com/huggingface/datasets/issues/2918", "id": 997063347, "node_id": "I_kwDODunzps47bfqz", "number": 2918, "title": "`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming", "user": {"login": "SBrandeis", "id": 33657802, "node_id": "MDQ6VXNlcjMzNjU3ODAy", "avatar_url": "https://avatars.githubusercontent.com/u/33657802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SBrandeis", "html_url": "https://github.com/SBrandeis", "followers_url": "https://api.github.com/users/SBrandeis/followers", "following_url": "https://api.github.com/users/SBrandeis/following{/other_user}", "gists_url": "https://api.github.com/users/SBrandeis/gists{/gist_id}", "starred_url": "https://api.github.com/users/SBrandeis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SBrandeis/subscriptions", "organizations_url": "https://api.github.com/users/SBrandeis/orgs", "repos_url": "https://api.github.com/users/SBrandeis/repos", "events_url": "https://api.github.com/users/SBrandeis/events{/privacy}", "received_events_url": "https://api.github.com/users/SBrandeis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 3287858981, "node_id": "MDU6TGFiZWwzMjg3ODU4OTgx", "url": "https://api.github.com/repos/huggingface/datasets/labels/streaming", "name": "streaming", "color": "fef2c0", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-09-15T13:06:07Z", "updated_at": "2021-12-01T08:15:00Z", "closed_at": "2021-12-01T08:15:00Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nTrying to load the `\"FullText\"` config of the `\"scitldr\"` dataset with `streaming=True` raises an error from `aiohttp`:\r\n```python\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\ncc @lhoestq \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\niter_dset = iter(\r\n    load_dataset(\"scitldr\", name=\"FullText\", split=\"test\", streaming=True)\r\n)\r\n\r\nnext(iter_dset)\r\n```\r\n\r\n## Expected results\r\nReturns the first sample of the dataset\r\n\r\n## Actual results\r\nCalling `__next__` crashes with the following Traceback:\r\n\r\n```python\r\n----> 1 next(dset_iter)\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n    339\r\n    340     def __iter__(self):\r\n--> 341         for key, example in self._iter():\r\n    342             if self.features:\r\n    343                 # we encode the example for ClassLabel feature types for example\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in _iter(self)\r\n    336         else:\r\n    337             ex_iterable = self._ex_iterable\r\n--> 338         yield from ex_iterable\r\n    339\r\n    340     def __iter__(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n     76\r\n     77     def __iter__(self):\r\n---> 78         for key, example in self.generate_examples_fn(**self.kwargs):\r\n     79             yield key, example\r\n     80\r\n\r\n~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\scitldr\\72d6e2195786c57e1d343066fb2cc4f93ea39c5e381e53e6ae7c44bbfd1f05ef\\scitldr.py in _generate_examples(self, filepath, split)\r\n    162\r\n    163         with open(filepath, encoding=\"utf-8\") as f:\r\n--> 164             for id_, row in enumerate(f):\r\n    165                 data = json.loads(row)\r\n    166                 if self.config.name == \"AIC\":\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in read(self, length)\r\n    496         else:\r\n    497             length = min(self.size - self.loc, length)\r\n--> 498         return super().read(length)\r\n    499\r\n    500     async def async_fetch_all(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\spec.py in read(self, length)\r\n   1481             # don't even bother calling fetch\r\n   1482             return b\"\"\r\n-> 1483         out = self.cache._fetch(self.loc, self.loc + length)\r\n   1484         self.loc += len(out)\r\n   1485         return out\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\caching.py in _fetch(self, start, end)\r\n    378         elif start < self.start:\r\n    379             if self.end - end > self.blocksize:\r\n--> 380                 self.cache = self.fetcher(start, bend)\r\n    381                 self.start = start\r\n    382             else:\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in wrapper(*args, **kwargs)\r\n     86     def wrapper(*args, **kwargs):\r\n     87         self = obj or args[0]\r\n---> 88         return sync(self.loop, func, *args, **kwargs)\r\n     89\r\n     90     return wrapper\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in sync(loop, func, timeout, *args, **kwargs)\r\n     67         raise FSTimeoutError\r\n     68     if isinstance(result[0], BaseException):\r\n---> 69         raise result[0]\r\n     70     return result[0]\r\n     71\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in _runner(event, coro, result, timeout)\r\n     23         coro = asyncio.wait_for(coro, timeout=timeout)\r\n     24     try:\r\n---> 25         result[0] = await coro\r\n     26     except Exception as ex:\r\n     27         result[0] = ex\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in async_fetch_range(self, start, end)\r\n    538             if r.status == 206:\r\n    539                 # partial content, as expected\r\n--> 540                 out = await r.read()\r\n    541             elif \"Content-Length\" in r.headers:\r\n    542                 cl = int(r.headers[\"Content-Length\"])\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\client_reqrep.py in read(self)\r\n   1030         if self._body is None:\r\n   1031             try:\r\n-> 1032                 self._body = await self.content.read()\r\n   1033                 for trace in self._traces:\r\n   1034                     await trace.send_response_chunk_received(\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\streams.py in read(self, n)\r\n    342     async def read(self, n: int = -1) -> bytes:\r\n    343         if self._exception is not None:\r\n--> 344             raise self._exception\r\n    345\r\n    346         # migration problem; with DataQueue you have to catch\r\n\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.8.5\r\n- PyArrow version: 2.0.0\r\n- aiohttp version: 3.7.4.post0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2918/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2918/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2917", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2917/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2917/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2917/events", "html_url": "https://github.com/huggingface/datasets/issues/2917", "id": 997041658, "node_id": "I_kwDODunzps47baX6", "number": 2917, "title": "windows download abnormal", "user": {"login": "wei1826676931", "id": 52347799, "node_id": "MDQ6VXNlcjUyMzQ3Nzk5", "avatar_url": "https://avatars.githubusercontent.com/u/52347799?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wei1826676931", "html_url": "https://github.com/wei1826676931", "followers_url": "https://api.github.com/users/wei1826676931/followers", "following_url": "https://api.github.com/users/wei1826676931/following{/other_user}", "gists_url": "https://api.github.com/users/wei1826676931/gists{/gist_id}", "starred_url": "https://api.github.com/users/wei1826676931/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wei1826676931/subscriptions", "organizations_url": "https://api.github.com/users/wei1826676931/orgs", "repos_url": "https://api.github.com/users/wei1826676931/repos", "events_url": "https://api.github.com/users/wei1826676931/events{/privacy}", "received_events_url": "https://api.github.com/users/wei1826676931/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-09-15T12:45:35Z", "updated_at": "2021-09-16T17:17:48Z", "closed_at": "2021-09-16T17:17:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe script clearly exists (accessible from the browser), but the script download fails on windows. Then I tried it again and it can be downloaded normally on linux. why??\r\n## Steps to reproduce the bug\r\n```python3.7 + windows\r\n![image](https://user-images.githubusercontent.com/52347799/133436174-4303f847-55d5-434f-a749-08da3bb9b654.png)\r\n\r\n\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nIt can be downloaded normally.\r\n\r\n## Actual results\r\nit cann't\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:1.11.0\r\n- Platform:windows\r\n- Python version:3.7\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2917/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2914", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2914/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2914/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2914/events", "html_url": "https://github.com/huggingface/datasets/issues/2914", "id": 996770168, "node_id": "I_kwDODunzps47aYF4", "number": 2914, "title": "Having a dependency defining fsspec entrypoint raises an AttributeError when importing datasets", "user": {"login": "pierre-godard", "id": 3969168, "node_id": "MDQ6VXNlcjM5NjkxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/3969168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pierre-godard", "html_url": "https://github.com/pierre-godard", "followers_url": "https://api.github.com/users/pierre-godard/followers", "following_url": "https://api.github.com/users/pierre-godard/following{/other_user}", "gists_url": "https://api.github.com/users/pierre-godard/gists{/gist_id}", "starred_url": "https://api.github.com/users/pierre-godard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pierre-godard/subscriptions", "organizations_url": "https://api.github.com/users/pierre-godard/orgs", "repos_url": "https://api.github.com/users/pierre-godard/repos", "events_url": "https://api.github.com/users/pierre-godard/events{/privacy}", "received_events_url": "https://api.github.com/users/pierre-godard/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-09-15T07:54:06Z", "updated_at": "2021-09-15T16:49:17Z", "closed_at": "2021-09-15T16:49:16Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nIn one of my project, I defined a custom fsspec filesystem with an entrypoint.\r\nMy guess is that by doing so, a variable named `spec` is created in the module `fsspec` (created by entering a for loop as there are entrypoints defined, see the loop in question [here](https://github.com/intake/filesystem_spec/blob/0589358d8a029ed6b60d031018f52be2eb721291/fsspec/__init__.py#L55)).\r\nSo that `fsspec.spec`, that was previously referring to the `spec` submodule, is now referring to that `spec` variable.\r\nThis make the import of datasets failing as it is using that `fsspec.spec`.\r\n\r\n## Steps to reproduce the bug\r\nI could reproduce the bug with a dummy poetry project.\r\n\r\nHere is the pyproject.toml:\r\n```toml\r\n[tool.poetry]\r\nname = \"debug-datasets\"\r\nversion = \"0.1.0\"\r\ndescription = \"\"\r\nauthors = [\"Pierre Godard\"]\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.8\"\r\ndatasets = \"^1.11.0\"\r\n\r\n[tool.poetry.dev-dependencies]\r\n\r\n[build-system]\r\nrequires = [\"poetry-core>=1.0.0\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n\r\n[tool.poetry.plugins.\"fsspec.specs\"]\r\n\"file2\" = \"fsspec.implementations.local.LocalFileSystem\"\r\n```\r\n\r\nThe only other file being a `debug_datasets/__init__.py` empty file.\r\n\r\nThe overall structure of the project is as follows:\r\n```\r\n.\r\n\u251c\u2500\u2500 pyproject.toml\r\n\u2514\u2500\u2500 debug_datasets\r\n    \u2514\u2500\u2500 __init__.py\r\n```\r\n\r\nThen, within the project folder run:\r\n\r\n```\r\npoetry install\r\npoetry run python\r\n```\r\n\r\nAnd in the python interpreter, try to import `datasets`:\r\n\r\n```\r\nimport datasets\r\n```\r\n\r\n## Expected results\r\nThe import should run successfully.\r\n\r\n## Actual results\r\n\r\nHere is the trace of the error I get:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/__init__.py\", line 33, in <module>\r\n    from .arrow_dataset import Dataset, concatenate_datasets\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 48, in <module>\r\n    from .filesystems import extract_path_from_uri, is_remote_filesystem\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/filesystems/__init__.py\", line 30, in <module>\r\n    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:\r\nAttributeError: 'EntryPoint' object has no attribute 'AbstractFileSystem'\r\n```\r\n\r\n## Suggested fix\r\n\r\n`datasets/filesystems/__init__.py`, line 30, replace:\r\n```\r\n    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:\r\n```\r\nby:\r\n```\r\n    def is_remote_filesystem(fs: fsspec.AbstractFileSystem) -> bool:\r\n```\r\n\r\nI will come up with a PR soon if this effectively solves the issue.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: WSL2 (Ubuntu 20.04.1 LTS)\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n- `fsspec` version: 2021.8.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2914/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2914/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2913", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2913/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2913/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2913/events", "html_url": "https://github.com/huggingface/datasets/issues/2913", "id": 996436368, "node_id": "I_kwDODunzps47ZGmQ", "number": 2913, "title": "timit_asr dataset only includes one text phrase", "user": {"login": "margotwagner", "id": 39107794, "node_id": "MDQ6VXNlcjM5MTA3Nzk0", "avatar_url": "https://avatars.githubusercontent.com/u/39107794?v=4", "gravatar_id": "", "url": "https://api.github.com/users/margotwagner", "html_url": "https://github.com/margotwagner", "followers_url": "https://api.github.com/users/margotwagner/followers", "following_url": "https://api.github.com/users/margotwagner/following{/other_user}", "gists_url": "https://api.github.com/users/margotwagner/gists{/gist_id}", "starred_url": "https://api.github.com/users/margotwagner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/margotwagner/subscriptions", "organizations_url": "https://api.github.com/users/margotwagner/orgs", "repos_url": "https://api.github.com/users/margotwagner/repos", "events_url": "https://api.github.com/users/margotwagner/events{/privacy}", "received_events_url": "https://api.github.com/users/margotwagner/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-09-14T21:06:07Z", "updated_at": "2021-09-15T08:05:19Z", "closed_at": "2021-09-15T08:05:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe dataset 'timit_asr' only includes one text phrase. It only includes the transcription \"Would such an act of refusal be useful?\" multiple times rather than different phrases.\r\n\r\n## Steps to reproduce the bug\r\nNote: I am following the tutorial https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\n1. Install the dataset and other packages\r\n```python\r\n!pip install datasets>=1.5.0\r\n!pip install transformers==4.4.0\r\n!pip install soundfile\r\n!pip install jiwer\r\n```\r\n2. Load the dataset\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\n\r\ntimit = load_dataset(\"timit_asr\")\r\n```\r\n3. Remove columns that we don't want\r\n```python\r\ntimit = timit.remove_columns([\"phonetic_detail\", \"word_detail\", \"dialect_region\", \"id\", \"sentence_type\", \"speaker_id\"])\r\n```\r\n4. Write a short function to display some random samples of the dataset.\r\n```python\r\nfrom datasets import ClassLabel\r\nimport random\r\nimport pandas as pd\r\nfrom IPython.display import display, HTML\r\n\r\ndef show_random_elements(dataset, num_examples=10):\r\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\r\n    picks = []\r\n    for _ in range(num_examples):\r\n        pick = random.randint(0, len(dataset)-1)\r\n        while pick in picks:\r\n            pick = random.randint(0, len(dataset)-1)\r\n        picks.append(pick)\r\n    \r\n    df = pd.DataFrame(dataset[picks])\r\n    display(HTML(df.to_html()))\r\n\r\nshow_random_elements(timit[\"train\"].remove_columns([\"file\"]))\r\n```\r\n\r\n## Expected results\r\n10 random different transcription phrases.\r\n\r\n## Actual results\r\n10 of the same transcription phrase \"Would such an act of refusal be useful?\"\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.4.1\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: not listed\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2913/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2913/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2901", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2901/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2901/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2901/events", "html_url": "https://github.com/huggingface/datasets/issues/2901", "id": 995232844, "node_id": "MDU6SXNzdWU5OTUyMzI4NDQ=", "number": 2901, "title": "Incompatibility with pytest", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-09-13T19:12:17Z", "updated_at": "2021-09-14T08:40:47Z", "closed_at": "2021-09-14T08:40:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\npytest complains about xpathopen / path.open(\"w\")\r\n\r\n## Steps to reproduce the bug\r\n\r\nCreate a test file, `test.py`:\r\n\r\n```python\r\nimport datasets as ds\r\ndef load_dataset():\r\n    ds.load_dataset(\"counter\", split=\"train\", streaming=True)\r\n```\r\n\r\nAnd launch it with pytest:\r\n\r\n```bash\r\npython -m pytest test.py\r\n```\r\n\r\n## Expected results\r\n\r\nIt should give something like:\r\n\r\n```\r\ncollected 1 item\r\n\r\ntest.py .                                                                                                                                                                                                                                             [100%]\r\n\r\n======= 1 passed in 3.15s =======\r\n```\r\n\r\n## Actual results\r\n\r\n```\r\n============================================================================================================================= test session starts ==============================================================================================================================\r\nplatform linux -- Python 3.8.11, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\r\nrootdir: /home/slesage/hf/datasets-preview-backend, configfile: pyproject.toml\r\nplugins: anyio-3.3.1\r\ncollected 1 item\r\n\r\ntests/queries/test_rows.py .                                                                                                                                                                                                                                             [100%]Traceback (most recent call last):\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pytest/__main__.py\", line 5, in <module>\r\n    raise SystemExit(pytest.console_main())\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py\", line 185, in console_main\r\n    code = main()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py\", line 162, in main\r\n    ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py\", line 265, in __call__\r\n    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py\", line 80, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 60, in _multicall\r\n    return outcome.get_result()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py\", line 60, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 39, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py\", line 316, in pytest_cmdline_main\r\n    return wrap_session(config, _main)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py\", line 304, in wrap_session\r\n    config.hook.pytest_sessionfinish(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py\", line 265, in __call__\r\n    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py\", line 80, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 55, in _multicall\r\n    gen.send(outcome)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/terminal.py\", line 803, in pytest_sessionfinish\r\n    outcome.get_result()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py\", line 60, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 39, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py\", line 428, in pytest_sessionfinish\r\n    config.cache.set(\"cache/nodeids\", sorted(self.cached_nodeids))\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py\", line 188, in set\r\n    f = path.open(\"w\")\r\nTypeError: xpathopen() takes 1 positional argument but 2 were given\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.0\r\n- Platform: Linux-5.11.0-1017-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2901/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2892", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2892/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2892/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2892/events", "html_url": "https://github.com/huggingface/datasets/issues/2892", "id": 993274572, "node_id": "MDU6SXNzdWU5OTMyNzQ1NzI=", "number": 2892, "title": "Error when encoding a dataset with None objects with a Sequence feature", "user": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-09-10T14:11:43Z", "updated_at": "2021-09-13T14:18:13Z", "closed_at": "2021-09-13T14:17:42Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "There is an error when encoding a dataset with None objects with a Sequence feature\r\n\r\nTo reproduce:\r\n```python\r\nfrom datasets import Dataset, Features, Value, Sequence\r\ndata = {\"a\": [[0], None]}\r\nfeatures = Features({\"a\": Sequence(Value(\"int32\"))})\r\ndataset = Dataset.from_dict(data, features=features)\r\n```\r\nraises\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-24-40add67f8751> in <module>\r\n      2 data = {\"a\": [[0], None]}\r\n      3 features = Features({\"a\": Sequence(Value(\"int32\"))})\r\n----> 4 dataset = Dataset.from_dict(data, features=features)\r\n[...]\r\n~/datasets/features.py in encode_nested_example(schema, obj)\r\n    888         if isinstance(obj, str):  # don't interpret a string as a list\r\n    889             raise ValueError(\"Got a string but expected a list instead: '{}'\".format(obj))\r\n--> 890         return [encode_nested_example(schema.feature, o) for o in obj]\r\n    891     # Object with special encoding:\r\n    892     # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\r\n\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\n\r\nInstead, if should run without error, as if the `features` were not passed", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2892/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2892/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2882", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2882/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2882/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2882/events", "html_url": "https://github.com/huggingface/datasets/issues/2882", "id": 991800141, "node_id": "MDU6SXNzdWU5OTE4MDAxNDE=", "number": 2882, "title": "`load_dataset('docred')` results in a `NonMatchingChecksumError` ", "user": {"login": "tmpr", "id": 51313597, "node_id": "MDQ6VXNlcjUxMzEzNTk3", "avatar_url": "https://avatars.githubusercontent.com/u/51313597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tmpr", "html_url": "https://github.com/tmpr", "followers_url": "https://api.github.com/users/tmpr/followers", "following_url": "https://api.github.com/users/tmpr/following{/other_user}", "gists_url": "https://api.github.com/users/tmpr/gists{/gist_id}", "starred_url": "https://api.github.com/users/tmpr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tmpr/subscriptions", "organizations_url": "https://api.github.com/users/tmpr/orgs", "repos_url": "https://api.github.com/users/tmpr/repos", "events_url": "https://api.github.com/users/tmpr/events{/privacy}", "received_events_url": "https://api.github.com/users/tmpr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-09-09T05:55:02Z", "updated_at": "2021-09-13T11:24:30Z", "closed_at": "2021-09-13T11:24:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI get consistent `NonMatchingChecksumError: Checksums didn't match for dataset source files` errors when trying to execute `datasets.load_dataset('docred')`.\r\n\r\n## Steps to reproduce the bug\r\nIt is quasi only this code:\r\n```python\r\nimport datasets\r\ndata = datasets.load_dataset('docred')\r\n```\r\n\r\n## Expected results\r\nThe DocRED dataset should be loaded without any problems.\r\n\r\n## Actual results\r\n```\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-4-b1b83f25a16c> in <module>\r\n----> 1 d = datasets.load_dataset('docred')\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\r\n    845 \r\n    846     # Download and prepare data\r\n--> 847     builder_instance.download_and_prepare(\r\n    848         download_config=download_config,\r\n    849         download_mode=download_mode,\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    613                             logger.warning(\"HF google storage unreachable. Downloading and preparing it from source\")\r\n    614                     if not downloaded_from_gcs:\r\n--> 615                         self._download_and_prepare(\r\n    616                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    617                         )\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    673         # Checksums verification\r\n    674         if verify_infos:\r\n--> 675             verify_checksums(\r\n    676                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n    677             )\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=1fDmfUUo5G7gfaoqWWvK81u08m71TK2g7']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Linux-5.11.0-7633-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n\r\nThis error also happened on my Windows-partition, after freshly installing python 3.9 and `datasets`.\r\n\r\n## Remarks\r\n\r\n- I have already called `rm -rf /home/<user>/.cache/huggingface`, i.e., I have tried clearing the cache.\r\n- The problem does not exist for other datasets, i.e., it seems to be DocRED-specific.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2882/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2879", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2879/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2879/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2879/events", "html_url": "https://github.com/huggingface/datasets/issues/2879", "id": 990257404, "node_id": "MDU6SXNzdWU5OTAyNTc0MDQ=", "number": 2879, "title": "In v1.4.1, all TIMIT train transcripts are \"Would such an act of refusal be useful?\"", "user": {"login": "rcgale", "id": 2279700, "node_id": "MDQ6VXNlcjIyNzk3MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/2279700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rcgale", "html_url": "https://github.com/rcgale", "followers_url": "https://api.github.com/users/rcgale/followers", "following_url": "https://api.github.com/users/rcgale/following{/other_user}", "gists_url": "https://api.github.com/users/rcgale/gists{/gist_id}", "starred_url": "https://api.github.com/users/rcgale/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rcgale/subscriptions", "organizations_url": "https://api.github.com/users/rcgale/orgs", "repos_url": "https://api.github.com/users/rcgale/repos", "events_url": "https://api.github.com/users/rcgale/events{/privacy}", "received_events_url": "https://api.github.com/users/rcgale/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-09-07T18:53:45Z", "updated_at": "2021-09-08T16:55:19Z", "closed_at": "2021-09-08T09:12:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nUsing version 1.4.1 of `datasets`, TIMIT transcripts are all the same.\r\n\r\n## Steps to reproduce the bug\r\nI was following this tutorial\r\n- https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\nBut here's a distilled repro:\r\n```python\r\n!pip install datasets==1.4.1\r\nfrom datasets import load_dataset\r\ntimit = load_dataset(\"timit_asr\", cache_dir=\"./temp\")\r\nunique_transcripts = set(timit[\"train\"][\"text\"])\r\nprint(unique_transcripts)\r\nassert len(unique_transcripts) > 1\r\n```\r\n## Expected results\r\nExpected the correct TIMIT data. Or an error saying that this version of `datasets` can't produce it.\r\n\r\n## Actual results\r\nEvery train transcript was \"Would such an act of refusal be useful?\" Every test transcript was \"The bungalow was pleasantly situated near the shore.\"\r\n\r\n## Environment info\r\n- `datasets` version: 1.4.1\r\n- Platform: Darwin-18.7.0-x86_64-i386-64bit\r\n- Python version: 3.7.9\r\n- PyTorch version (GPU?): 1.9.0 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: tried both\r\n- Using distributed or parallel set-up in script?: no\r\n- \r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2879/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2879/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2871", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2871/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2871/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2871/events", "html_url": "https://github.com/huggingface/datasets/issues/2871", "id": 989436088, "node_id": "MDU6SXNzdWU5ODk0MzYwODg=", "number": 2871, "title": "datasets.config.PYARROW_VERSION has no attribute 'major'", "user": {"login": "bwang482", "id": 6764450, "node_id": "MDQ6VXNlcjY3NjQ0NTA=", "avatar_url": "https://avatars.githubusercontent.com/u/6764450?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bwang482", "html_url": "https://github.com/bwang482", "followers_url": "https://api.github.com/users/bwang482/followers", "following_url": "https://api.github.com/users/bwang482/following{/other_user}", "gists_url": "https://api.github.com/users/bwang482/gists{/gist_id}", "starred_url": "https://api.github.com/users/bwang482/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bwang482/subscriptions", "organizations_url": "https://api.github.com/users/bwang482/orgs", "repos_url": "https://api.github.com/users/bwang482/repos", "events_url": "https://api.github.com/users/bwang482/events{/privacy}", "received_events_url": "https://api.github.com/users/bwang482/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-09-06T21:06:57Z", "updated_at": "2021-09-08T08:51:52Z", "closed_at": "2021-09-08T08:51:52Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "In the test_dataset_common.py script, line 288-289\r\n\r\n```\r\nif datasets.config.PYARROW_VERSION.major < 3:\r\n   packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]\r\n```\r\n\r\nwhich throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.\r\n\r\n```\r\nimport datasets\r\ndatasets.config.PYARROW_VERSION.major\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>\r\n      1 import datasets\r\n----> 2 datasets.config.PYARROW_VERSION.major\r\n\r\nAttributeError: 'str' object has no attribute 'major'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2871/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2871/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2869", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2869/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2869/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2869/events", "html_url": "https://github.com/huggingface/datasets/issues/2869", "id": 987676420, "node_id": "MDU6SXNzdWU5ODc2NzY0MjA=", "number": 2869, "title": "TypeError: 'NoneType' object is not callable", "user": {"login": "Chenfei-Kang", "id": 40911446, "node_id": "MDQ6VXNlcjQwOTExNDQ2", "avatar_url": "https://avatars.githubusercontent.com/u/40911446?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Chenfei-Kang", "html_url": "https://github.com/Chenfei-Kang", "followers_url": "https://api.github.com/users/Chenfei-Kang/followers", "following_url": "https://api.github.com/users/Chenfei-Kang/following{/other_user}", "gists_url": "https://api.github.com/users/Chenfei-Kang/gists{/gist_id}", "starred_url": "https://api.github.com/users/Chenfei-Kang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Chenfei-Kang/subscriptions", "organizations_url": "https://api.github.com/users/Chenfei-Kang/orgs", "repos_url": "https://api.github.com/users/Chenfei-Kang/repos", "events_url": "https://api.github.com/users/Chenfei-Kang/events{/privacy}", "received_events_url": "https://api.github.com/users/Chenfei-Kang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2021-09-03T11:27:39Z", "updated_at": "2022-03-30T05:30:38Z", "closed_at": "2021-09-08T09:24:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nTypeError: 'NoneType' object is not callable\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\ndataset = datasets.load_dataset(\"glue\", 'cola')\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform:\r\n- Python version: 3.7\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2869/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2869/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2866", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2866/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2866/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2866/events", "html_url": "https://github.com/huggingface/datasets/issues/2866", "id": 986706676, "node_id": "MDU6SXNzdWU5ODY3MDY2NzY=", "number": 2866, "title": "\"counter\" dataset raises an error in normal mode, but not in streaming mode", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2021-09-02T13:10:53Z", "updated_at": "2021-10-14T09:24:09Z", "closed_at": "2021-10-14T09:24:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\n`counter` dataset raises an error on `load_dataset()`, but simply returns an empty iterator in streaming mode.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> a = ds.load_dataset('counter', split=\"train\", streaming=False)\r\nUsing custom data configuration default\r\nDownloading and preparing dataset counter/default (download: 1.29 MiB, generated: 2.48 MiB, post-processed: Unknown size, total: 3.77 MiB) to /home/slesage/.cache/huggingface/datasets/counter/default/1.0.0/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9...\r\nTraceback (most recent call last):\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 726, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 1124, in _prepare_split\r\n    for key, record in utils.tqdm(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/tqdm/std.py\", line 1185, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/slesage/.cache/huggingface/modules/datasets_modules/datasets/counter/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9/counter.py\", line 161, in _generate_examples\r\n    with derived_file.open(encoding=\"utf-8\") as f:\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py\", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py\", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 636, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 728, in _download_and_prepare\r\n    raise OSError(\r\nOSError: Cannot find data file.\r\nOriginal error:\r\n[Errno 2] No such file or directory: '/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml'\r\n```\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> b = ds.load_dataset('counter', split=\"train\", streaming=True)\r\nUsing custom data configuration default\r\n>>> list(b)\r\n[]\r\n```\r\n\r\n## Expected results\r\n\r\nAn exception should be raised in streaming mode\r\n\r\n## Actual results\r\n\r\nNo exception is raised in streaming mode: there is no way to tell if something has broken or if the dataset is simply empty.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.11.1.dev0\r\n- Platform: Linux-5.11.0-1016-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2866/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2866/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2860", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2860/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2860/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2860/events", "html_url": "https://github.com/huggingface/datasets/issues/2860", "id": 985013339, "node_id": "MDU6SXNzdWU5ODUwMTMzMzk=", "number": 2860, "title": "Cannot download TOTTO dataset", "user": {"login": "mrm8488", "id": 3653789, "node_id": "MDQ6VXNlcjM2NTM3ODk=", "avatar_url": "https://avatars.githubusercontent.com/u/3653789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrm8488", "html_url": "https://github.com/mrm8488", "followers_url": "https://api.github.com/users/mrm8488/followers", "following_url": "https://api.github.com/users/mrm8488/following{/other_user}", "gists_url": "https://api.github.com/users/mrm8488/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrm8488/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrm8488/subscriptions", "organizations_url": "https://api.github.com/users/mrm8488/orgs", "repos_url": "https://api.github.com/users/mrm8488/repos", "events_url": "https://api.github.com/users/mrm8488/events{/privacy}", "received_events_url": "https://api.github.com/users/mrm8488/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-09-01T11:04:10Z", "updated_at": "2021-09-02T06:47:40Z", "closed_at": "2021-09-02T06:47:40Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Error: Couldn't find file at https://storage.googleapis.com/totto/totto_data.zip\r\n\r\n`datasets version: 1.11.0`\r\n# How to reproduce:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('totto')\r\n```\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2860/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2860/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2846", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2846/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2846/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2846/events", "html_url": "https://github.com/huggingface/datasets/issues/2846", "id": 981587590, "node_id": "MDU6SXNzdWU5ODE1ODc1OTA=", "number": 2846, "title": "Negative timezone", "user": {"login": "jadermcs", "id": 7156771, "node_id": "MDQ6VXNlcjcxNTY3NzE=", "avatar_url": "https://avatars.githubusercontent.com/u/7156771?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jadermcs", "html_url": "https://github.com/jadermcs", "followers_url": "https://api.github.com/users/jadermcs/followers", "following_url": "https://api.github.com/users/jadermcs/following{/other_user}", "gists_url": "https://api.github.com/users/jadermcs/gists{/gist_id}", "starred_url": "https://api.github.com/users/jadermcs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jadermcs/subscriptions", "organizations_url": "https://api.github.com/users/jadermcs/orgs", "repos_url": "https://api.github.com/users/jadermcs/repos", "events_url": "https://api.github.com/users/jadermcs/events{/privacy}", "received_events_url": "https://api.github.com/users/jadermcs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-08-27T20:50:33Z", "updated_at": "2021-09-10T11:51:07Z", "closed_at": "2021-09-10T11:51:07Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nThe load_dataset method do not accept a parquet file with a negative timezone, as it has the following regex:\r\n```\r\n\"^(s|ms|us|ns),\\s*tz=([a-zA-Z0-9/_+:]*)$\"\r\n```\r\nSo a valid timestap ```timestamp[us, tz=-03:00]``` returns an error when loading parquet files.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Where the timestamp column has a tz of -03:00\r\ndatasets = load_dataset('parquet', data_files={'train': train_files, 'validation': validation_files,\r\n                                        'test': test_files}, cache_dir=\"./cache_teste/\")\r\n```\r\n\r\n## Expected results\r\nThe -03:00 is a valid tz so the regex should accept this without raising an error.\r\n\r\n## Actual results\r\nAs this regex disaproves a valid tz it raises the following error:\r\n```python\r\nraise ValueError(\r\n                f\"{datasets_dtype} is not a validly formatted string representation of a pyarrow timestamp.\"\r\n                f\"Examples include timestamp[us] or timestamp[us, tz=America/New_York]\"\r\n                f\"See: https://arrow.apache.org/docs/python/generated/pyarrow.timestamp.html#pyarrow.timestamp\"\r\n            )\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: Ubuntu 20.04\r\n- Python version: 3.8\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2846/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2839", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2839/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2839/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2839/events", "html_url": "https://github.com/huggingface/datasets/issues/2839", "id": 980271715, "node_id": "MDU6SXNzdWU5ODAyNzE3MTU=", "number": 2839, "title": "OpenWebText: NonMatchingSplitsSizesError", "user": {"login": "thomasw21", "id": 24695242, "node_id": "MDQ6VXNlcjI0Njk1MjQy", "avatar_url": "https://avatars.githubusercontent.com/u/24695242?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomasw21", "html_url": "https://github.com/thomasw21", "followers_url": "https://api.github.com/users/thomasw21/followers", "following_url": "https://api.github.com/users/thomasw21/following{/other_user}", "gists_url": "https://api.github.com/users/thomasw21/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomasw21/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomasw21/subscriptions", "organizations_url": "https://api.github.com/users/thomasw21/orgs", "repos_url": "https://api.github.com/users/thomasw21/repos", "events_url": "https://api.github.com/users/thomasw21/events{/privacy}", "received_events_url": "https://api.github.com/users/thomasw21/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2021-08-26T13:50:26Z", "updated_at": "2021-09-21T14:12:40Z", "closed_at": "2021-09-21T14:09:43Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen downloading `openwebtext`, I'm getting:\r\n```\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=39769494896, num_examples=8013769, dataset_name='openwebtext'), 'recorded': SplitInfo(name='train', num_bytes=39611023912, num_examples=7982430, dataset_name='openwebtext')}]\r\n```\r\n\r\nI suspect that the file we download from has changed since the size doesn't look like to match with documentation\r\n\r\n`Downloading:   0%|          | 0.00/12.9G [00:00<?, ?B/s]` This suggest the total size is 12.9GB, whereas the one documented mentions `Size of downloaded dataset files: 12283.35 MB`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"openwebtext\", download_mode=\"force_redownload\")\r\n```\r\n\r\n## Expected results\r\n\r\nLoading is successful\r\n\r\n## Actual results\r\n\r\nLoading throws above error.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.10.2\r\n- Platform: linux (Redhat version 8.1)\r\n- Python version: 3.8\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2839/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2837", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2837/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2837/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2837/events", "html_url": "https://github.com/huggingface/datasets/issues/2837", "id": 979298297, "node_id": "MDU6SXNzdWU5NzkyOTgyOTc=", "number": 2837, "title": "prepare_module issue when loading from read-only fs", "user": {"login": "Dref360", "id": 8976546, "node_id": "MDQ6VXNlcjg5NzY1NDY=", "avatar_url": "https://avatars.githubusercontent.com/u/8976546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dref360", "html_url": "https://github.com/Dref360", "followers_url": "https://api.github.com/users/Dref360/followers", "following_url": "https://api.github.com/users/Dref360/following{/other_user}", "gists_url": "https://api.github.com/users/Dref360/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dref360/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dref360/subscriptions", "organizations_url": "https://api.github.com/users/Dref360/orgs", "repos_url": "https://api.github.com/users/Dref360/repos", "events_url": "https://api.github.com/users/Dref360/events{/privacy}", "received_events_url": "https://api.github.com/users/Dref360/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-08-25T15:21:26Z", "updated_at": "2021-10-05T17:58:22Z", "closed_at": "2021-10-05T17:58:22Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen we use prepare_module from a readonly file system, we create a FileLock using the `local_path`.\r\nThis path is not necessarily writable.\r\n\r\n`lock_path = local_path + \".lock\"`\r\n\r\n\r\n## Steps to reproduce the bug\r\n\r\nRun `load_dataset` on a readonly python loader file.\r\n```python\r\nds = load_dataset(\r\n        python_loader, data_files={\"train\": train_path, \"test\": test_path}\r\n    )\r\n```\r\n\r\nwhere `python_loader` is a path to a file located in a readonly folder.\r\n\r\n## Expected results\r\nThis should work I think?\r\n\r\n## Actual results\r\n\r\n```python\r\n    return load_dataset(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/load.py\", line 711, in load_dataset\r\n    module_path, hash, resolved_file_path = prepare_module(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/load.py\", line 465, in prepare_module\r\n    with FileLock(lock_path):\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/filelock.py\", line 314, in __enter__\r\n    self.acquire()\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/filelock.py\", line 263, in acquire\r\n    self._acquire()\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/filelock.py\", line 378, in _acquire\r\n    fd = os.open(self._lock_file, open_mode)\r\nOSError: [Errno 30] Read-only file system: 'YOUR_FILE.py.lock'\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.7.0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.8\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2837/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2825", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2825/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2825/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2825/events", "html_url": "https://github.com/huggingface/datasets/issues/2825", "id": 976584926, "node_id": "MDU6SXNzdWU5NzY1ODQ5MjY=", "number": 2825, "title": "The datasets.map function does not load cached dataset after moving python script", "user": {"login": "hobbitlzy", "id": 35392624, "node_id": "MDQ6VXNlcjM1MzkyNjI0", "avatar_url": "https://avatars.githubusercontent.com/u/35392624?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hobbitlzy", "html_url": "https://github.com/hobbitlzy", "followers_url": "https://api.github.com/users/hobbitlzy/followers", "following_url": "https://api.github.com/users/hobbitlzy/following{/other_user}", "gists_url": "https://api.github.com/users/hobbitlzy/gists{/gist_id}", "starred_url": "https://api.github.com/users/hobbitlzy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hobbitlzy/subscriptions", "organizations_url": "https://api.github.com/users/hobbitlzy/orgs", "repos_url": "https://api.github.com/users/hobbitlzy/repos", "events_url": "https://api.github.com/users/hobbitlzy/events{/privacy}", "received_events_url": "https://api.github.com/users/hobbitlzy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-08-23T03:23:37Z", "updated_at": "2021-08-31T13:14:41Z", "closed_at": "2021-08-31T13:13:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe datasets.map function caches the processed data to a certain directory. When the map function is called another time with totally the same parameters, the cached data are supposed to be reloaded instead of re-processing. However, it doesn't reuse cached data sometimes. I use the common data processing in different tasks, the datasets are processed again, the only difference is that I run them in different files.\r\n\r\n## Steps to reproduce the bug\r\nJust run the following codes in different .py files.\r\n```python\r\nif __name__ == '__main__':\r\n    from datasets import load_dataset\r\n    from transformers import AutoTokenizer\r\n    raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\r\n\r\n\r\n    def tokenize_function(examples):\r\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\n\r\n    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\r\n```\r\n\r\n## Expected results\r\nThe map function should reload data in the second or any later runs.\r\n\r\n## Actual results\r\nThe processing happens in each run.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: linux\r\n- Python version: 3.7.6\r\n- PyArrow version: 3.0.0\r\n\r\nThis is the first time I report a bug. If there is any problem or confusing description, please let me know \ud83d\ude04.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2825/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2825/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2821", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2821/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2821/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2821/events", "html_url": "https://github.com/huggingface/datasets/issues/2821", "id": 975556032, "node_id": "MDU6SXNzdWU5NzU1NTYwMzI=", "number": 2821, "title": "Cannot load linnaeus dataset", "user": {"login": "NielsRogge", "id": 48327001, "node_id": "MDQ6VXNlcjQ4MzI3MDAx", "avatar_url": "https://avatars.githubusercontent.com/u/48327001?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NielsRogge", "html_url": "https://github.com/NielsRogge", "followers_url": "https://api.github.com/users/NielsRogge/followers", "following_url": "https://api.github.com/users/NielsRogge/following{/other_user}", "gists_url": "https://api.github.com/users/NielsRogge/gists{/gist_id}", "starred_url": "https://api.github.com/users/NielsRogge/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NielsRogge/subscriptions", "organizations_url": "https://api.github.com/users/NielsRogge/orgs", "repos_url": "https://api.github.com/users/NielsRogge/repos", "events_url": "https://api.github.com/users/NielsRogge/events{/privacy}", "received_events_url": "https://api.github.com/users/NielsRogge/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-08-20T12:15:15Z", "updated_at": "2021-08-31T13:13:02Z", "closed_at": "2021-08-31T13:12:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nThe [linnaeus](https://huggingface.co/datasets/linnaeus) dataset cannot be loaded. To reproduce:\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndatasets = load_dataset(\"linnaeus\")\r\n```\r\nThis results in:\r\n```\r\nDownloading and preparing dataset linnaeus/linnaeus (download: 17.36 MiB, generated: 8.74 MiB, post-processed: Unknown size, total: 26.10 MiB) to /root/.cache/huggingface/datasets/linnaeus/linnaeus/1.0.0/2ff05dbc256108233262f596e09e322dbc3db067202de14286913607cd9cb704...\r\n---------------------------------------------------------------------------\r\nConnectionError                           Traceback (most recent call last)\r\n<ipython-input-4-7ef3a88f6276> in <module>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 datasets = load_dataset(\"linnaeus\")\r\n\r\n11 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\r\n    603             raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\n    604         _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\r\n--> 605         raise ConnectionError(\"Couldn't reach {}\".format(url))\r\n    606 \r\n    607     # Try a second time\r\n\r\nConnectionError: Couldn't reach https://drive.google.com/u/0/uc?id=1OletxmPYNkz2ltOr9pyT0b0iBtUWxslh&export=download/\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2821/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2821/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2820", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2820/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2820/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2820/events", "html_url": "https://github.com/huggingface/datasets/issues/2820", "id": 975210712, "node_id": "MDU6SXNzdWU5NzUyMTA3MTI=", "number": 2820, "title": "Downloading \u201creddit\u201d dataset keeps timing out.", "user": {"login": "smeyerhot", "id": 43877130, "node_id": "MDQ6VXNlcjQzODc3MTMw", "avatar_url": "https://avatars.githubusercontent.com/u/43877130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smeyerhot", "html_url": "https://github.com/smeyerhot", "followers_url": "https://api.github.com/users/smeyerhot/followers", "following_url": "https://api.github.com/users/smeyerhot/following{/other_user}", "gists_url": "https://api.github.com/users/smeyerhot/gists{/gist_id}", "starred_url": "https://api.github.com/users/smeyerhot/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smeyerhot/subscriptions", "organizations_url": "https://api.github.com/users/smeyerhot/orgs", "repos_url": "https://api.github.com/users/smeyerhot/repos", "events_url": "https://api.github.com/users/smeyerhot/events{/privacy}", "received_events_url": "https://api.github.com/users/smeyerhot/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2021-08-20T02:52:36Z", "updated_at": "2021-09-08T14:52:02Z", "closed_at": "2021-09-08T14:52:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\nEverytime I try and download the reddit dataset it times out before finishing and I have to try again.\r\n\r\nThere is some timeout error that I will post once it happens again.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"reddit\", ignore_verifications=True, cache_dir=\"/Volumes/My Passport for Mac/og-chat-data\")\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\nI would expect the download to finish, or at least provide a parameter to extend the read timeout window.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\nShown below in error message.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:  1.11.0\r\n- Platform: macOS \r\n- Python version: 3.9.6 (conda env)\r\n- PyArrow version: N/A\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2820/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2820/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2799", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2799/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2799/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2799/events", "html_url": "https://github.com/huggingface/datasets/issues/2799", "id": 970507351, "node_id": "MDU6SXNzdWU5NzA1MDczNTE=", "number": 2799, "title": "Loading JSON throws ArrowNotImplementedError", "user": {"login": "lewtun", "id": 26859204, "node_id": "MDQ6VXNlcjI2ODU5MjA0", "avatar_url": "https://avatars.githubusercontent.com/u/26859204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lewtun", "html_url": "https://github.com/lewtun", "followers_url": "https://api.github.com/users/lewtun/followers", "following_url": "https://api.github.com/users/lewtun/following{/other_user}", "gists_url": "https://api.github.com/users/lewtun/gists{/gist_id}", "starred_url": "https://api.github.com/users/lewtun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lewtun/subscriptions", "organizations_url": "https://api.github.com/users/lewtun/orgs", "repos_url": "https://api.github.com/users/lewtun/repos", "events_url": "https://api.github.com/users/lewtun/events{/privacy}", "received_events_url": "https://api.github.com/users/lewtun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2021-08-13T15:31:48Z", "updated_at": "2022-01-10T18:59:32Z", "closed_at": "2022-01-10T18:59:32Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nI have created a [dataset](https://huggingface.co/datasets/lewtun/github-issues-test) of GitHub issues in line-separated JSON format and am finding that I cannot load it with the `json` loading script (see stack trace below).\r\n\r\nCuriously, there is no problem loading the dataset with `pandas` which suggests some incorrect type inference is being made on the `datasets` side. For example, the stack trace indicates that some URL fields are being parsed as timestamps.\r\n\r\nYou can find a Colab notebook which reproduces the error [here](https://colab.research.google.com/drive/1YUCM0j1vx5ZrouQbYSzal6RwB4-Aoh4o?usp=sharing).\r\n\r\n**Edit:** If one repeatedly tries to load the dataset, it _eventually_ works but I think it would still be good to understand why it fails in the first place :)\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfrom huggingface_hub import hf_hub_url\r\nimport pandas as pd\r\n\r\n# returns https://huggingface.co/datasets/lewtun/github-issues-test/resolve/main/issues-datasets.jsonl\r\ndata_files = hf_hub_url(repo_id=\"lewtun/github-issues-test\", filename=\"issues-datasets.jsonl\", repo_type=\"dataset\")\r\n# throws ArrowNotImplementedError\r\ndset = load_dataset(\"json\", data_files=data_files, split=\"test\")\r\n# no problem with pandas ...\r\ndf = pd.read_json(data_files, orient=\"records\", lines=True)\r\ndf.head()\r\n```\r\n\r\n## Expected results\r\nI can load any line-separated JSON file, similar to `pandas`.\r\n\r\n## Actual results\r\n```\r\n---------------------------------------------------------------------------\r\nArrowNotImplementedError                  Traceback (most recent call last)\r\n<ipython-input-7-5b8e82b6c3a2> in <module>()\r\n----> 1 dset = load_dataset(\"json\", data_files=data_files, split=\"test\")\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowNotImplementedError: JSON conversion to struct<url: timestamp[s], html_url: timestamp[s], labels_url: timestamp[s], id: int64, node_id: timestamp[s], number: int64, title: timestamp[s], description: timestamp[s], creator: struct<login: timestamp[s], id: int64, node_id: timestamp[s], avatar_url: timestamp[s], gravatar_id: timestamp[s], url: timestamp[s], html_url: timestamp[s], followers_url: timestamp[s], following_url: timestamp[s], gists_url: timestamp[s], starred_url: timestamp[s], subscriptions_url: timestamp[s], organizations_url: timestamp[s], repos_url: timestamp[s], events_url: timestamp[s], received_events_url: timestamp[s], type: timestamp[s], site_admin: bool>, open_issues: int64, closed_issues: int64, state: timestamp[s], created_at: timestamp[s], updated_at: timestamp[s], due_on: timestamp[s], closed_at: timestamp[s]> is not supported\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.11\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2799/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2787", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2787/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2787/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2787/events", "html_url": "https://github.com/huggingface/datasets/issues/2787", "id": 967018406, "node_id": "MDU6SXNzdWU5NjcwMTg0MDY=", "number": 2787, "title": "ConnectionError: Couldn't reach https://raw.githubusercontent.com", "user": {"login": "jinec", "id": 39627475, "node_id": "MDQ6VXNlcjM5NjI3NDc1", "avatar_url": "https://avatars.githubusercontent.com/u/39627475?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jinec", "html_url": "https://github.com/jinec", "followers_url": "https://api.github.com/users/jinec/followers", "following_url": "https://api.github.com/users/jinec/following{/other_user}", "gists_url": "https://api.github.com/users/jinec/gists{/gist_id}", "starred_url": "https://api.github.com/users/jinec/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jinec/subscriptions", "organizations_url": "https://api.github.com/users/jinec/orgs", "repos_url": "https://api.github.com/users/jinec/repos", "events_url": "https://api.github.com/users/jinec/events{/privacy}", "received_events_url": "https://api.github.com/users/jinec/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2021-08-11T16:19:01Z", "updated_at": "2021-11-24T06:25:38Z", "closed_at": "2021-08-18T15:09:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI am trying to run run_glue.py and it gives me this error -\r\n\r\nTraceback (most recent call last):\r\n  File \"E:/BERT/pytorch_hugging/transformers/examples/pytorch/text-classification/run_glue.py\", line 546, in <module>\r\n    main()\r\n  File \"E:/BERT/pytorch_hugging/transformers/examples/pytorch/text-classification/run_glue.py\", line 250, in main\r\n    datasets = load_dataset(\"glue\", data_args.task_name, cache_dir=model_args.cache_dir)\r\n  File \"C:\\install\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\datasets\\load.py\", line 718, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"C:\\install\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\datasets\\load.py\", line 320, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"C:\\install\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\datasets\\utils\\file_utils.py\", line 291, in cached_path\r\n    use_auth_token=download_config.use_auth_token,\r\n  File \"C:\\install\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\datasets\\utils\\file_utils.py\", line 623, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py\r\n\r\nTrying to do python run_glue.py  --model_name_or_path\r\nbert-base-cased\r\n--task_name\r\nmrpc\r\n--do_train\r\n--do_eval\r\n--max_seq_length\r\n128\r\n--per_device_train_batch_size\r\n32\r\n--learning_rate\r\n2e-5\r\n--num_train_epochs\r\n3\r\n--output_dir\r\n./tmp/mrpc/\r\n\r\nIs this something on my end? From what I can tell, this was re-fixeded by @fullyz a few months ago.\r\nThank you!\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2787/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2787/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2781", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2781/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2781/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2781/events", "html_url": "https://github.com/huggingface/datasets/issues/2781", "id": 964805351, "node_id": "MDU6SXNzdWU5NjQ4MDUzNTE=", "number": 2781, "title": "Latest v2.0.0 release of sacrebleu has broken some metrics", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-08-10T09:59:41Z", "updated_at": "2021-08-10T11:16:07Z", "closed_at": "2021-08-10T11:16:07Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nAfter `sacrebleu` v2.0.0 release (see changes here: https://github.com/mjpost/sacrebleu/pull/152/files#diff-2553a315bb1f7e68c9c1b00d56eaeb74f5205aeb3a189bc3e527b122c6078795L17-R15), some of `datasets` metrics are broken:\r\n- Default tokenizer `sacrebleu.DEFAULT_TOKENIZER` no longer exists:\r\n  - #2739\r\n  - #2778\r\n- Bleu tokenizers are no longer accessible with `sacrebleu.TOKENIZERS`:\r\n  - #2779\r\n- `corpus_bleu` args have been renamed from `(sys_stream, ref_streams)` to `(hipotheses, references)`: \r\n  - #2782 ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2781/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2768", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2768/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2768/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2768/events", "html_url": "https://github.com/huggingface/datasets/issues/2768", "id": 963229173, "node_id": "MDU6SXNzdWU5NjMyMjkxNzM=", "number": 2768, "title": "`ArrowInvalid: Added column's length must match table's length.` after using `select`", "user": {"login": "lvwerra", "id": 8264887, "node_id": "MDQ6VXNlcjgyNjQ4ODc=", "avatar_url": "https://avatars.githubusercontent.com/u/8264887?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lvwerra", "html_url": "https://github.com/lvwerra", "followers_url": "https://api.github.com/users/lvwerra/followers", "following_url": "https://api.github.com/users/lvwerra/following{/other_user}", "gists_url": "https://api.github.com/users/lvwerra/gists{/gist_id}", "starred_url": "https://api.github.com/users/lvwerra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lvwerra/subscriptions", "organizations_url": "https://api.github.com/users/lvwerra/orgs", "repos_url": "https://api.github.com/users/lvwerra/repos", "events_url": "https://api.github.com/users/lvwerra/events{/privacy}", "received_events_url": "https://api.github.com/users/lvwerra/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-08-07T13:17:29Z", "updated_at": "2021-08-09T11:26:43Z", "closed_at": "2021-08-09T11:26:43Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nI would like to add a column to a downsampled dataset. However I get an error message saying the length don't match with the length of the unsampled dataset indicated. I suspect that the dataset size is not updated when calling `select`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"tweets_hate_speech_detection\")['train'].select(range(128))\r\nds = ds.add_column('ones', [1]*128)\r\n```\r\n\r\n## Expected results\r\nI would expect a new column named `ones` filled with `1`. When I check the length of `ds` it says `128`. Interestingly, it works when calling `ds = ds.map(lambda x: x)` before adding the column.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n```python\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n/var/folders/l4/2905jygx4tx5jv8_kn03vxsw0000gn/T/ipykernel_6301/868709636.py in <module>\r\n      1 from datasets import load_dataset\r\n      2 ds = load_dataset(\"tweets_hate_speech_detection\")['train'].select(range(128))\r\n----> 3 ds = ds.add_column('ones', [0]*128)\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    183         }\r\n    184         # apply actual function\r\n--> 185         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    186         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    187         # re-apply format to the output\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    395             # Call actual function\r\n    396 \r\n--> 397             out = func(self, *args, **kwargs)\r\n    398 \r\n    399             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/arrow_dataset.py in add_column(self, name, column, new_fingerprint)\r\n   2965         column_table = InMemoryTable.from_pydict({name: column})\r\n   2966         # Concatenate tables horizontally\r\n-> 2967         table = ConcatenationTable.from_tables([self._data, column_table], axis=1)\r\n   2968         # Update features\r\n   2969         info = self.info.copy()\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in from_tables(cls, tables, axis)\r\n    715             table_blocks = to_blocks(table)\r\n    716             blocks = _extend_blocks(blocks, table_blocks, axis=axis)\r\n--> 717         return cls.from_blocks(blocks)\r\n    718 \r\n    719     @property\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in from_blocks(cls, blocks)\r\n    663             return cls(table, blocks)\r\n    664         else:\r\n--> 665             table = cls._concat_blocks_horizontally_and_vertically(blocks)\r\n    666             return cls(table, blocks)\r\n    667 \r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in _concat_blocks_horizontally_and_vertically(cls, blocks)\r\n    623             if not tables:\r\n    624                 continue\r\n--> 625             pa_table_horizontally_concatenated = cls._concat_blocks(tables, axis=1)\r\n    626             pa_tables_to_concat_vertically.append(pa_table_horizontally_concatenated)\r\n    627         return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in _concat_blocks(blocks, axis)\r\n    612                 else:\r\n    613                     for name, col in zip(table.column_names, table.columns):\r\n--> 614                         pa_table = pa_table.append_column(name, col)\r\n    615             return pa_table\r\n    616         else:\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.append_column()\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.add_column()\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Added column's length must match table's length. Expected length 31962 but got length 128\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2768/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2768/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2767", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2767/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2767/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2767/events", "html_url": "https://github.com/huggingface/datasets/issues/2767", "id": 963002120, "node_id": "MDU6SXNzdWU5NjMwMDIxMjA=", "number": 2767, "title": "equal operation to perform unbatch for huggingface datasets ", "user": {"login": "dorooddorood606", "id": 79288051, "node_id": "MDQ6VXNlcjc5Mjg4MDUx", "avatar_url": "https://avatars.githubusercontent.com/u/79288051?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dorooddorood606", "html_url": "https://github.com/dorooddorood606", "followers_url": "https://api.github.com/users/dorooddorood606/followers", "following_url": "https://api.github.com/users/dorooddorood606/following{/other_user}", "gists_url": "https://api.github.com/users/dorooddorood606/gists{/gist_id}", "starred_url": "https://api.github.com/users/dorooddorood606/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dorooddorood606/subscriptions", "organizations_url": "https://api.github.com/users/dorooddorood606/orgs", "repos_url": "https://api.github.com/users/dorooddorood606/repos", "events_url": "https://api.github.com/users/dorooddorood606/events{/privacy}", "received_events_url": "https://api.github.com/users/dorooddorood606/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-08-06T19:45:52Z", "updated_at": "2022-03-07T13:58:00Z", "closed_at": "2022-03-07T13:58:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi\r\nI need to use \"unbatch\" operation in tensorflow on a huggingface dataset, I could not find this operation, could you kindly direct me how I can do it, here is the problem I am trying to solve:\r\n\r\nI am considering \"record\" dataset in SuperGlue and I need to replicate each entery of the dataset for each answer, to make it similar to what T5 originally did:\r\n\r\nhttps://github.com/google-research/text-to-text-transfer-transformer/blob/3c58859b8fe72c2dbca6a43bc775aa510ba7e706/t5/data/preprocessors.py#L925\r\n\r\nHere please find an example:\r\n\r\n  For example, a typical example from ReCoRD might look like\r\n  {\r\n      'passsage': 'This is the passage.',\r\n      'query': 'A @placeholder is a bird.',\r\n      'entities': ['penguin', 'potato', 'pigeon'],\r\n      'answers': ['penguin', 'pigeon'],\r\n  }\r\n  and I need a prosessor which would turn this example into the following two examples:\r\n  {\r\n      'inputs': 'record query: A @placeholder is a bird. entities: penguin, '\r\n                'potato, pigeon passage: This is the passage.',\r\n      'targets': 'penguin',\r\n  }\r\n  and\r\n  {\r\n      'inputs': 'record query: A @placeholder is a bird. entities: penguin, '\r\n                'potato, pigeon passage: This is the passage.',\r\n      'targets': 'pigeon',\r\n  }\r\n\r\n\r\nFor doing this, one need unbatch, as each entry can map to multiple samples depending on the number of answers, I am not sure how to perform this operation with  huggingface datasets library and greatly appreciate your help\r\n\r\n@lhoestq \r\n\r\nThank you very much.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2767/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2767/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2765", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2765/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2765/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2765/events", "html_url": "https://github.com/huggingface/datasets/issues/2765", "id": 962861395, "node_id": "MDU6SXNzdWU5NjI4NjEzOTU=", "number": 2765, "title": "BERTScore Error", "user": {"login": "gagan3012", "id": 49101362, "node_id": "MDQ6VXNlcjQ5MTAxMzYy", "avatar_url": "https://avatars.githubusercontent.com/u/49101362?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gagan3012", "html_url": "https://github.com/gagan3012", "followers_url": "https://api.github.com/users/gagan3012/followers", "following_url": "https://api.github.com/users/gagan3012/following{/other_user}", "gists_url": "https://api.github.com/users/gagan3012/gists{/gist_id}", "starred_url": "https://api.github.com/users/gagan3012/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gagan3012/subscriptions", "organizations_url": "https://api.github.com/users/gagan3012/orgs", "repos_url": "https://api.github.com/users/gagan3012/repos", "events_url": "https://api.github.com/users/gagan3012/events{/privacy}", "received_events_url": "https://api.github.com/users/gagan3012/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-08-06T15:58:57Z", "updated_at": "2021-08-09T11:16:25Z", "closed_at": "2021-08-09T11:16:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\npredictions = [\"hello there\", \"general kenobi\"]\r\nreferences = [\"hello there\", \"general kenobi\"]\r\nbert = load_metric('bertscore')\r\nbert.compute(predictions=predictions, references=references,lang='en')\r\n```\r\n\r\n# Bug\r\n`TypeError: get_hash() missing 1 required positional argument: 'use_fast_tokenizer'`\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:\r\n- Platform: Colab \r\n- Python version:\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2765/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2761", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2761/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2761/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2761/events", "html_url": "https://github.com/huggingface/datasets/issues/2761", "id": 961568287, "node_id": "MDU6SXNzdWU5NjE1NjgyODc=", "number": 2761, "title": "Error loading C4 realnewslike dataset", "user": {"login": "danshirron", "id": 32061512, "node_id": "MDQ6VXNlcjMyMDYxNTEy", "avatar_url": "https://avatars.githubusercontent.com/u/32061512?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danshirron", "html_url": "https://github.com/danshirron", "followers_url": "https://api.github.com/users/danshirron/followers", "following_url": "https://api.github.com/users/danshirron/following{/other_user}", "gists_url": "https://api.github.com/users/danshirron/gists{/gist_id}", "starred_url": "https://api.github.com/users/danshirron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danshirron/subscriptions", "organizations_url": "https://api.github.com/users/danshirron/orgs", "repos_url": "https://api.github.com/users/danshirron/repos", "events_url": "https://api.github.com/users/danshirron/events{/privacy}", "received_events_url": "https://api.github.com/users/danshirron/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-08-05T08:16:58Z", "updated_at": "2021-08-08T19:44:34Z", "closed_at": "2021-08-08T19:44:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nError loading C4 realnewslike dataset. Validation part mismatch\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n raw_datasets = load_dataset('c4', 'realnewslike', cache_dir=model_args.cache_dir)\r\n## Expected results\r\nsuccess on data loading\r\n## Actual results\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.3M/15.3M [00:00<00:00, 28.1MB/s]Traceback (most recent call last):                                                                                                                                                                                                                                             \r\n  File \"run_mlm_tf.py\", line 794, in <module>                                                                                                                                                                                                                                  \r\n    main()                                                                                                                                                                                                                                                                     \r\n  File \"run_mlm_tf.py\", line 425, in main                                                                                                                                                                                                                                      \r\n    raw_datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)                                                                                                                                                           File \"/home/dshirron/.local/lib/python3.8/site-packages/datasets/load.py\", line 843, in load_dataset                                                                                                                                                                         \r\n    builder_instance.download_and_prepare(                                                                                                                                                                                                                                     \r\n  File \"/home/dshirron/.local/lib/python3.8/site-packages/datasets/builder.py\", line 608, in download_and_prepare                                                                                                                                                              \r\n    self._download_and_prepare(                                                                                                                                                                                                                                                \r\n  File \"/home/dshirron/.local/lib/python3.8/site-packages/datasets/builder.py\", line 698, in _download_and_prepare                                                                                                                                                                 verify_splits(self.info.splits, split_dict)                                                                                                                                                                                                                                  File \"/home/dshirron/.local/lib/python3.8/site-packages/datasets/utils/info_utils.py\", line 74, in verify_splits                                                                                                                                                             \r\n    raise NonMatchingSplitsSizesError(str(bad_splits))                                                                                                                                                                                                                         \r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='validation', num_bytes=38165657946, num_examples=13799838, dataset_name='c4'), 'recorded': SplitInfo(name='validation', num_bytes=37875873, num_examples=13863, dataset_name='c4')}] \r\n\r\n## Environment info\r\n- `datasets` version: 1.10.2\r\n- Platform: Linux-5.4.0-58-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 4.0.1", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2761/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2761/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2757", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2757/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2757/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2757/events", "html_url": "https://github.com/huggingface/datasets/issues/2757", "id": 959984081, "node_id": "MDU6SXNzdWU5NTk5ODQwODE=", "number": 2757, "title": "Unexpected type after `concatenate_datasets`", "user": {"login": "JulesBelveze", "id": 32683010, "node_id": "MDQ6VXNlcjMyNjgzMDEw", "avatar_url": "https://avatars.githubusercontent.com/u/32683010?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JulesBelveze", "html_url": "https://github.com/JulesBelveze", "followers_url": "https://api.github.com/users/JulesBelveze/followers", "following_url": "https://api.github.com/users/JulesBelveze/following{/other_user}", "gists_url": "https://api.github.com/users/JulesBelveze/gists{/gist_id}", "starred_url": "https://api.github.com/users/JulesBelveze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JulesBelveze/subscriptions", "organizations_url": "https://api.github.com/users/JulesBelveze/orgs", "repos_url": "https://api.github.com/users/JulesBelveze/repos", "events_url": "https://api.github.com/users/JulesBelveze/events{/privacy}", "received_events_url": "https://api.github.com/users/JulesBelveze/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-08-04T07:10:39Z", "updated_at": "2021-08-04T16:01:24Z", "closed_at": "2021-08-04T16:01:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to concatenate two `Dataset` using `concatenate_datasets` but it turns out that after concatenation the features are casted from `torch.Tensor` to `list`. \r\nIt then leads to a weird tensors when trying to convert it to a `DataLoader`. However, if I use each `Dataset` separately everything behave as expected.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> featurized_teacher\r\nDataset({\r\n    features: ['t_labels', 't_input_ids', 't_token_type_ids', 't_attention_mask'],\r\n    num_rows: 502\r\n})\r\n>>> for f in featurized_teacher.features:\r\n     print(featurized_teacher[f].shape)\r\ntorch.Size([502])\r\ntorch.Size([502, 300])\r\ntorch.Size([502, 300])\r\ntorch.Size([502, 300])\r\n\r\n>>> featurized_student\r\nDataset({\r\n    features: ['s_features', 's_labels'],\r\n    num_rows: 502\r\n})\r\n>>> for f in featurized_student.features:\r\n     print(featurized_student[f].shape)\r\ntorch.Size([502, 64])\r\ntorch.Size([502])\r\n```\r\nThe shapes seem alright to me. Then the results after concatenation are as follow:\r\n```python\r\n>>> concat_dataset = datasets.concatenate_datasets([featurized_student, featurized_teacher], axis=1)\r\n>>> type(concat_dataset[\"t_labels\"])\r\n<class 'list'>\r\n```\r\nOne would expect to obtain the same type as the one before concatenation.\r\n\r\nAm I doing something wrong here? Any idea on how to fix this unexpected behavior?\r\n\r\n## Environment info\r\n- `datasets` version: 1.9.0\r\n- Platform: macOS-10.14.6-x86_64-i386-64bit\r\n- Python version: 3.9.5\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2757/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2757/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2750", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2750/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2750/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2750/events", "html_url": "https://github.com/huggingface/datasets/issues/2750", "id": 958984730, "node_id": "MDU6SXNzdWU5NTg5ODQ3MzA=", "number": 2750, "title": "Second concatenation of datasets produces errors", "user": {"login": "Aktsvigun", "id": 36672861, "node_id": "MDQ6VXNlcjM2NjcyODYx", "avatar_url": "https://avatars.githubusercontent.com/u/36672861?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aktsvigun", "html_url": "https://github.com/Aktsvigun", "followers_url": "https://api.github.com/users/Aktsvigun/followers", "following_url": "https://api.github.com/users/Aktsvigun/following{/other_user}", "gists_url": "https://api.github.com/users/Aktsvigun/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aktsvigun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aktsvigun/subscriptions", "organizations_url": "https://api.github.com/users/Aktsvigun/orgs", "repos_url": "https://api.github.com/users/Aktsvigun/repos", "events_url": "https://api.github.com/users/Aktsvigun/events{/privacy}", "received_events_url": "https://api.github.com/users/Aktsvigun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2021-08-03T10:47:04Z", "updated_at": "2022-01-19T14:23:43Z", "closed_at": "2022-01-19T14:19:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI am need to concatenate my dataset with others several times, and after I concatenate it for the second time, the features of features (e.g. tags names) are collapsed. This hinders, for instance, the usage of tokenize function with `data.map`.\r\n\r\n```\r\nfrom datasets import load_dataset, concatenate_datasets\r\n\r\ndata = load_dataset('trec')['train']\r\nconcatenated = concatenate_datasets([data, data])\r\nconcatenated_2 = concatenate_datasets([concatenated, concatenated])\r\nprint('True features of features:', concatenated.features)\r\nprint('\\nProduced features of features:', concatenated_2.features)\r\n```\r\noutputs \r\n\r\n```\r\nTrue features of features: {'label-coarse': ClassLabel(num_classes=6, names=['DESC', 'ENTY', 'ABBR', 'HUM', 'NUM', 'LOC'], names_file=None, id=None), 'label-fine': ClassLabel(num_classes=47, names=['manner', 'cremat', 'animal', 'exp', 'ind', 'gr', 'title', 'def', 'date', 'reason', 'event', 'state', 'desc', 'count', 'other', 'letter', 'religion', 'food', 'country', 'color', 'termeq', 'city', 'body', 'dismed', 'mount', 'money', 'product', 'period', 'substance', 'sport', 'plant', 'techmeth', 'volsize', 'instru', 'abb', 'speed', 'word', 'lang', 'perc', 'code', 'dist', 'temp', 'symbol', 'ord', 'veh', 'weight', 'currency'], names_file=None, id=None), 'text': Value(dtype='string', id=None)}\r\n\r\nProduced features of features: {'label-coarse': Value(dtype='int64', id=None), 'label-fine': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}\r\n```\r\n\r\nI am using `datasets` v.1.11.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2750/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2750/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2749", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2749/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2749/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2749/events", "html_url": "https://github.com/huggingface/datasets/issues/2749", "id": 958968748, "node_id": "MDU6SXNzdWU5NTg5Njg3NDg=", "number": 2749, "title": "Raise a proper exception when trying to stream a dataset that requires to manually download files", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-08-03T10:26:27Z", "updated_at": "2021-08-09T08:53:35Z", "closed_at": "2021-08-04T11:36:30Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nAt least for 'reclor', 'telugu_books', 'turkish_movie_sentiment', 'ubuntu_dialogs_corpus', 'wikihow', trying to `load_dataset` in streaming mode raises a `TypeError` without any detail about why it fails.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"reclor\", streaming=True)\r\n```\r\n\r\n## Expected results\r\n\r\nIdeally: raise a specific exception, something like `ManualDownloadError`.\r\n\r\nOr at least give the reason in the message, as when we load in normal mode:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"reclor\")\r\n```\r\n\r\n```\r\nAssertionError: The dataset reclor with config default requires manual data.\r\n Please follow the manual download instructions:   to use ReClor you need to download it manually. Please go to its homepage (http://whyu.me/reclor/) fill the google\r\n  form and you will receive a download link and a password to extract it.Please extract all files in one folder and use the path folder in datasets.load_dataset('reclor', data_dir='path/to/folder/folder_name')\r\n  .\r\n Manual data can be loaded with `datasets.load_dataset(reclor, data_dir='<path/to/manual/data>')\r\n```\r\n\r\n## Actual results\r\n\r\n```\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.11.0\r\n- Platform: macOS-11.5-x86_64-i386-64bit\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2749/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2746", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2746/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2746/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2746/events", "html_url": "https://github.com/huggingface/datasets/issues/2746", "id": 958551619, "node_id": "MDU6SXNzdWU5NTg1NTE2MTk=", "number": 2746, "title": "Cannot load `few-nerd` dataset", "user": {"login": "Mehrad0711", "id": 28717374, "node_id": "MDQ6VXNlcjI4NzE3Mzc0", "avatar_url": "https://avatars.githubusercontent.com/u/28717374?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mehrad0711", "html_url": "https://github.com/Mehrad0711", "followers_url": "https://api.github.com/users/Mehrad0711/followers", "following_url": "https://api.github.com/users/Mehrad0711/following{/other_user}", "gists_url": "https://api.github.com/users/Mehrad0711/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mehrad0711/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mehrad0711/subscriptions", "organizations_url": "https://api.github.com/users/Mehrad0711/orgs", "repos_url": "https://api.github.com/users/Mehrad0711/repos", "events_url": "https://api.github.com/users/Mehrad0711/events{/privacy}", "received_events_url": "https://api.github.com/users/Mehrad0711/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-08-02T22:18:57Z", "updated_at": "2021-11-16T08:51:34Z", "closed_at": "2021-08-03T19:45:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nCannot load `few-nerd` dataset.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset('few-nerd', 'supervised')\r\n```\r\n\r\n## Actual results\r\n\r\nExecuting above code will give the following error:\r\n\r\n```\r\nUsing the latest cached version of the module from /Users/Mehrad/.cache/huggingface/modules/datasets_modules/datasets/few-nerd/62464ace912a40a0f33a11a8310f9041c9dc3590ff2b3c77c14d83ca53cfec53 (last modified on Wed Jun  2 11:34:25 2021) since it couldn't be found locally at /Users/Mehrad/Documents/GitHub/genienlp/few-nerd/few-nerd.py, or remotely (FileNotFoundError).\r\nDownloading and preparing dataset few_nerd/supervised (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/Mehrad/.cache/huggingface/datasets/few_nerd/supervised/0.0.0/62464ace912a40a0f33a11a8310f9041c9dc3590ff2b3c77c14d83ca53cfec53...\r\nTraceback (most recent call last):\r\n  File \"/Users/Mehrad/opt/anaconda3/lib/python3.7/site-packages/datasets/builder.py\", line 693, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/Users/Mehrad/opt/anaconda3/lib/python3.7/site-packages/datasets/builder.py\", line 1107, in _prepare_split\r\n    disable=bool(logging.get_verbosity() == logging.NOTSET),\r\n  File \"/Users/Mehrad/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py\", line 1133, in __iter__\r\n    for obj in iterable:\r\n  File \"/Users/Mehrad/.cache/huggingface/modules/datasets_modules/datasets/few-nerd/62464ace912a40a0f33a11a8310f9041c9dc3590ff2b3c77c14d83ca53cfec53/few-nerd.py\", line 196, in _generate_examples\r\n    with open(filepath, encoding=\"utf-8\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/Mehrad/.cache/huggingface/datasets/downloads/supervised/train.json'\r\n```\r\nThe bug is probably in identifying and downloading the dataset. If I download the json splits directly from [link](https://github.com/nbroad1881/few-nerd/tree/main/uncompressed) and put them under the downloads directory, they will be processed into arrow format correctly. \r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Python version: 3.8\r\n- PyArrow version: 1.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2746/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2743", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2743/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2743/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2743/events", "html_url": "https://github.com/huggingface/datasets/issues/2743", "id": 958119251, "node_id": "MDU6SXNzdWU5NTgxMTkyNTE=", "number": 2743, "title": "Dataset JSON is incorrect", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-08-02T13:01:26Z", "updated_at": "2021-08-03T10:06:57Z", "closed_at": "2021-08-03T09:25:33Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nThe JSON file generated for https://github.com/huggingface/datasets/blob/573f3d35081cee239d1b962878206e9abe6cde91/datasets/journalists_questions/journalists_questions.py is https://github.com/huggingface/datasets/blob/573f3d35081cee239d1b962878206e9abe6cde91/datasets/journalists_questions/dataset_infos.json.\r\n\r\nThe only config should be `plain_text`, but the first key in the JSON is `journalists_questions` (the dataset id) instead.\r\n\r\n```json\r\n{\r\n  \"journalists_questions\": {\r\n    \"description\": \"The journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\\ntweets manually labeled for question identification over Arabic tweets posted by journalists.\\n\",\r\n    ...\r\n```\r\n\r\n## Steps to reproduce the bug\r\n\r\nLook at the files.\r\n\r\n## Expected results\r\n\r\nThe first key should be `plain_text`:\r\n\r\n```json\r\n{\r\n  \"plain_text\": {\r\n    \"description\": \"The journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\\ntweets manually labeled for question identification over Arabic tweets posted by journalists.\\n\",\r\n    ...\r\n```\r\n\r\n## Actual results\r\n\r\n```json\r\n{\r\n  \"journalists_questions\": {\r\n    \"description\": \"The journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\\ntweets manually labeled for question identification over Arabic tweets posted by journalists.\\n\",\r\n    ...\r\n```\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2743/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2737", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2737/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2737/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2737/events", "html_url": "https://github.com/huggingface/datasets/issues/2737", "id": 957124881, "node_id": "MDU6SXNzdWU5NTcxMjQ4ODE=", "number": 2737, "title": "SacreBLEU update", "user": {"login": "devrimcavusoglu", "id": 46989091, "node_id": "MDQ6VXNlcjQ2OTg5MDkx", "avatar_url": "https://avatars.githubusercontent.com/u/46989091?v=4", "gravatar_id": "", "url": "https://api.github.com/users/devrimcavusoglu", "html_url": "https://github.com/devrimcavusoglu", "followers_url": "https://api.github.com/users/devrimcavusoglu/followers", "following_url": "https://api.github.com/users/devrimcavusoglu/following{/other_user}", "gists_url": "https://api.github.com/users/devrimcavusoglu/gists{/gist_id}", "starred_url": "https://api.github.com/users/devrimcavusoglu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/devrimcavusoglu/subscriptions", "organizations_url": "https://api.github.com/users/devrimcavusoglu/orgs", "repos_url": "https://api.github.com/users/devrimcavusoglu/repos", "events_url": "https://api.github.com/users/devrimcavusoglu/events{/privacy}", "received_events_url": "https://api.github.com/users/devrimcavusoglu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-07-30T23:53:08Z", "updated_at": "2021-09-22T10:47:41Z", "closed_at": "2021-08-03T04:23:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "With the latest release of [sacrebleu](https://github.com/mjpost/sacrebleu), `datasets.metrics.sacrebleu` is broken, and getting error.\r\n\r\n    AttributeError: module 'sacrebleu' has no attribute 'DEFAULT_TOKENIZER'\r\n\r\nthis happens since in new version of sacrebleu there is no `DEFAULT_TOKENIZER`, but sacrebleu.py tries to import it anyways. This can be fixed currently with fixing `sacrebleu==1.5.0`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nsacrebleu= datasets.load_metric('sacrebleu')\r\npredictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\"]\r\nreferences = [\"It is a guide to action that ensures that the military will forever heed Party commands\"]\r\nresults = sacrebleu.compute(predictions=predictions, references=references)\r\nprint(results)\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: Python 3.8.0\r\n- PyArrow version: 5.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2737/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2729", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2729/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2729/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2729/events", "html_url": "https://github.com/huggingface/datasets/pull/2729", "id": 955920489, "node_id": "MDExOlB1bGxSZXF1ZXN0Njk5NTk5MjA4", "number": 2729, "title": "Fix IndexError while loading Arabic Billion Words dataset", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-07-29T14:47:02Z", "updated_at": "2021-07-30T13:03:55Z", "closed_at": "2021-07-30T13:03:55Z", "author_association": "MEMBER", "active_lock_reason": null, "draft": false, "pull_request": {"url": "https://api.github.com/repos/huggingface/datasets/pulls/2729", "html_url": "https://github.com/huggingface/datasets/pull/2729", "diff_url": "https://github.com/huggingface/datasets/pull/2729.diff", "patch_url": "https://github.com/huggingface/datasets/pull/2729.patch", "merged_at": "2021-07-30T13:03:55Z"}, "body": "Catch `IndexError` and ignore that record.\r\n\r\nClose #2727.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2729/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2729/timeline", "performed_via_github_app": null, "state_reason": null}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2727", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2727/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2727/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2727/events", "html_url": "https://github.com/huggingface/datasets/issues/2727", "id": 955812149, "node_id": "MDU6SXNzdWU5NTU4MTIxNDk=", "number": 2727, "title": "Error in loading the Arabic Billion Words Corpus", "user": {"login": "M-Salti", "id": 9285264, "node_id": "MDQ6VXNlcjkyODUyNjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/9285264?v=4", "gravatar_id": "", "url": "https://api.github.com/users/M-Salti", "html_url": "https://github.com/M-Salti", "followers_url": "https://api.github.com/users/M-Salti/followers", "following_url": "https://api.github.com/users/M-Salti/following{/other_user}", "gists_url": "https://api.github.com/users/M-Salti/gists{/gist_id}", "starred_url": "https://api.github.com/users/M-Salti/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/M-Salti/subscriptions", "organizations_url": "https://api.github.com/users/M-Salti/orgs", "repos_url": "https://api.github.com/users/M-Salti/repos", "events_url": "https://api.github.com/users/M-Salti/events{/privacy}", "received_events_url": "https://api.github.com/users/M-Salti/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-07-29T12:53:09Z", "updated_at": "2021-07-30T13:03:55Z", "closed_at": "2021-07-30T13:03:55Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI get `IndexError: list index out of range` when trying to load the `Techreen` and `Almustaqbal` configs of the dataset.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nload_dataset(\"arabic_billion_words\", \"Techreen\")\r\nload_dataset(\"arabic_billion_words\", \"Almustaqbal\")\r\n```\r\n\r\n## Expected results\r\nThe datasets load succefully.\r\n\r\n## Actual results\r\n```python\r\n_extract_tags(self, sample, tag)\r\n    139             if len(out) > 0:\r\n    140                 break\r\n--> 141         return out[0]\r\n    142 \r\n    143     def _clean_text(self, text):\r\n\r\nIndexError: list index out of range\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.10.2\r\n- Platform: Ubuntu 18.04.5 LTS\r\n- Python version: 3.7.11\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2727/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2724", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2724/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2724/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2724/events", "html_url": "https://github.com/huggingface/datasets/issues/2724", "id": 954919607, "node_id": "MDU6SXNzdWU5NTQ5MTk2MDc=", "number": 2724, "title": "404 Error when loading remote data files from private repo", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-07-28T14:24:23Z", "updated_at": "2021-07-29T04:58:49Z", "closed_at": "2021-07-28T16:38:01Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen loading remote data files from a private repo, a 404 error is raised.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nurl = hf_hub_url(\"lewtun/asr-preds-test\", \"preds.jsonl\", repo_type=\"dataset\")\r\ndset = load_dataset(\"json\", data_files=url, use_auth_token=True)\r\n# HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/datasets/lewtun/asr-preds-test/resolve/main/preds.jsonl\r\n```\r\n\r\n## Expected results\r\nLoad dataset.\r\n\r\n## Actual results\r\n404 Error.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2724/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2722", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2722/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2722/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2722/events", "html_url": "https://github.com/huggingface/datasets/issues/2722", "id": 954446053, "node_id": "MDU6SXNzdWU5NTQ0NDYwNTM=", "number": 2722, "title": "Missing cache file", "user": {"login": "PosoSAgapo", "id": 33200481, "node_id": "MDQ6VXNlcjMzMjAwNDgx", "avatar_url": "https://avatars.githubusercontent.com/u/33200481?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PosoSAgapo", "html_url": "https://github.com/PosoSAgapo", "followers_url": "https://api.github.com/users/PosoSAgapo/followers", "following_url": "https://api.github.com/users/PosoSAgapo/following{/other_user}", "gists_url": "https://api.github.com/users/PosoSAgapo/gists{/gist_id}", "starred_url": "https://api.github.com/users/PosoSAgapo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PosoSAgapo/subscriptions", "organizations_url": "https://api.github.com/users/PosoSAgapo/orgs", "repos_url": "https://api.github.com/users/PosoSAgapo/repos", "events_url": "https://api.github.com/users/PosoSAgapo/events{/privacy}", "received_events_url": "https://api.github.com/users/PosoSAgapo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-28T03:52:07Z", "updated_at": "2022-03-21T08:27:51Z", "closed_at": "2022-03-21T08:27:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Strangely missing cache file after I  restart my program again.\r\n\r\n`glue_dataset = datasets.load_dataset('glue', 'sst2')`\r\n\r\n`FileNotFoundError: [Errno 2] No such file or directory: /Users/chris/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96d6053ad/dataset_info.json'`\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2722/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2716", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2716/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2716/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2716/events", "html_url": "https://github.com/huggingface/datasets/issues/2716", "id": 952902778, "node_id": "MDU6SXNzdWU5NTI5MDI3Nzg=", "number": 2716, "title": "Calling shuffle on IterableDataset will disable batching in case any functions were mapped", "user": {"login": "amankhandelia", "id": 7098967, "node_id": "MDQ6VXNlcjcwOTg5Njc=", "avatar_url": "https://avatars.githubusercontent.com/u/7098967?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amankhandelia", "html_url": "https://github.com/amankhandelia", "followers_url": "https://api.github.com/users/amankhandelia/followers", "following_url": "https://api.github.com/users/amankhandelia/following{/other_user}", "gists_url": "https://api.github.com/users/amankhandelia/gists{/gist_id}", "starred_url": "https://api.github.com/users/amankhandelia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amankhandelia/subscriptions", "organizations_url": "https://api.github.com/users/amankhandelia/orgs", "repos_url": "https://api.github.com/users/amankhandelia/repos", "events_url": "https://api.github.com/users/amankhandelia/events{/privacy}", "received_events_url": "https://api.github.com/users/amankhandelia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-07-26T13:24:59Z", "updated_at": "2021-07-26T18:04:43Z", "closed_at": "2021-07-26T18:04:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When using dataset in streaming mode, if one applies `shuffle` method on the dataset and `map` method for which `batched=True` than the batching operation will not happen, instead `batched` will be set to `False`\r\n\r\nI did RCA on the dataset codebase, the problem is emerging from [this line of code](https://github.com/huggingface/datasets/blob/d25a0bf94d9f9a9aa6cabdf5b450b9c327d19729/src/datasets/iterable_dataset.py#L197) here as it is\r\n`self.ex_iterable.shuffle_data_sources(seed), function=self.function, batch_size=self.batch_size`, as one can see it is missing batched argument, which means that the iterator fallsback to default constructor value, which in this case is `False`.\r\nTo remedy the problem we can change this line to\r\n`self.ex_iterable.shuffle_data_sources(seed), function=self.function, batched=self.batched, batch_size=self.batch_size`\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2716/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2716/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2709", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2709/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2709/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2709/events", "html_url": "https://github.com/huggingface/datasets/issues/2709", "id": 951534757, "node_id": "MDU6SXNzdWU5NTE1MzQ3NTc=", "number": 2709, "title": "Missing documentation for wnut_17 (ner_tags)", "user": {"login": "maxpel", "id": 31095360, "node_id": "MDQ6VXNlcjMxMDk1MzYw", "avatar_url": "https://avatars.githubusercontent.com/u/31095360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/maxpel", "html_url": "https://github.com/maxpel", "followers_url": "https://api.github.com/users/maxpel/followers", "following_url": "https://api.github.com/users/maxpel/following{/other_user}", "gists_url": "https://api.github.com/users/maxpel/gists{/gist_id}", "starred_url": "https://api.github.com/users/maxpel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/maxpel/subscriptions", "organizations_url": "https://api.github.com/users/maxpel/orgs", "repos_url": "https://api.github.com/users/maxpel/repos", "events_url": "https://api.github.com/users/maxpel/events{/privacy}", "received_events_url": "https://api.github.com/users/maxpel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-07-23T12:25:32Z", "updated_at": "2021-07-26T09:30:55Z", "closed_at": "2021-07-26T09:30:55Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "On the info page of the wnut_17 data set (https://huggingface.co/datasets/wnut_17), the model output of ner-tags is only documented for these 5 cases:\r\n\r\n`ner_tags: a list of classification labels, with possible values including O (0), B-corporation (1), I-corporation (2), B-creative-work (3), I-creative-work (4).`\r\n\r\nI trained a model with the data and it gives me 13 classes:\r\n\r\n```\r\n\"id2label\": {\r\n    \"0\": 0,\r\n    \"1\": 1,\r\n    \"2\": 2,\r\n    \"3\": 3,\r\n    \"4\": 4,\r\n    \"5\": 5,\r\n    \"6\": 6,\r\n    \"7\": 7,\r\n    \"8\": 8,\r\n    \"9\": 9,\r\n    \"10\": 10,\r\n    \"11\": 11,\r\n    \"12\": 12\r\n  }\r\n\r\n  \"label2id\": {\r\n    \"0\": 0,\r\n    \"1\": 1,\r\n    \"10\": 10,\r\n    \"11\": 11,\r\n    \"12\": 12,\r\n    \"2\": 2,\r\n    \"3\": 3,\r\n    \"4\": 4,\r\n    \"5\": 5,\r\n    \"6\": 6,\r\n    \"7\": 7,\r\n    \"8\": 8,\r\n    \"9\": 9\r\n  }\r\n```\r\nThe paper (https://www.aclweb.org/anthology/W17-4418.pdf) explains those 6 categories, but the ordering does not match:\r\n\r\n```\r\n1. person\r\n2. location (including GPE, facility)\r\n3. corporation\r\n4. product (tangible goods, or well-defined\r\nservices)\r\n5. creative-work (song, movie, book and\r\nso on)\r\n6. group (subsuming music band, sports team,\r\nand non-corporate organisations)\r\n```\r\nI would be very helpful for me, if somebody could clarify the model ouputs and explain the \"B-\" and \"I-\" prefixes to me.\r\n\r\nReally great work with that and the other packages, I couldn't believe that training the model with that data was basically a one-liner!", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2709/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2709/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2708", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2708/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2708/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2708/events", "html_url": "https://github.com/huggingface/datasets/issues/2708", "id": 951092660, "node_id": "MDU6SXNzdWU5NTEwOTI2NjA=", "number": 2708, "title": "QASC: incomplete training set ", "user": {"login": "danyaljj", "id": 2441454, "node_id": "MDQ6VXNlcjI0NDE0NTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/2441454?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danyaljj", "html_url": "https://github.com/danyaljj", "followers_url": "https://api.github.com/users/danyaljj/followers", "following_url": "https://api.github.com/users/danyaljj/following{/other_user}", "gists_url": "https://api.github.com/users/danyaljj/gists{/gist_id}", "starred_url": "https://api.github.com/users/danyaljj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danyaljj/subscriptions", "organizations_url": "https://api.github.com/users/danyaljj/orgs", "repos_url": "https://api.github.com/users/danyaljj/repos", "events_url": "https://api.github.com/users/danyaljj/events{/privacy}", "received_events_url": "https://api.github.com/users/danyaljj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-22T21:59:44Z", "updated_at": "2021-07-23T13:30:07Z", "closed_at": "2021-07-23T13:30:07Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nThe training instances are not loaded properly. \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"qasc\", script_version='1.10.2')\r\n \r\ndef load_instances(split): \r\n    instances = dataset[split]\r\n    print(f\"split: {split} - size: {len(instances)}\")\r\n    for x in instances:\r\n        print(json.dumps(x))\r\n\r\n\r\nload_instances('test')\r\nload_instances('validation')\r\nload_instances('train')\r\n```\r\n\r\n##  results\r\nFor test and validation, we can see the examples in the output (which is good!): \r\n```\r\nsplit: test - size: 920\r\n{\"answerKey\": \"\", \"choices\": {\"label\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"], \"text\": [\"Anthax\", \"under water\", \"uterus\", \"wombs\", \"two\", \"moles\", \"live\", \"embryo\"]}, \"combinedfact\": \"\", \"fact1\": \"\", \"fact2\": \"\", \"formatted_question\": \"What type of birth do therian mammals have? (A) Anthax (B) under water (C) uterus (D) wombs (E) two (F) moles (G) live (H) embryo\", \"id\": \"3C44YUNSI1OBFBB8D36GODNOZN9DPA\", \"question\": \"What type of birth do therian mammals have?\"}\r\n{\"answerKey\": \"\", \"choices\": {\"label\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"], \"text\": [\"Corvidae\", \"arthropods\", \"birds\", \"backbones\", \"keratin\", \"Jurassic\", \"front paws\", \"Parakeets.\"]}, \"combinedfact\": \"\", \"fact1\": \"\", \"fact2\": \"\", \"formatted_question\": \"By what time had mouse-sized viviparous mammals evolved? (A) Corvidae (B) arthropods (C) birds (D) backbones (E) keratin (F) Jurassic (G) front paws (H) Parakeets.\", \"id\": \"3B1NLC6UGZVERVLZFT7OUYQLD1SGPZ\", \"question\": \"By what time had mouse-sized viviparous mammals evolved?\"}\r\n{\"answerKey\": \"\", \"choices\": {\"label\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"], \"text\": [\"Reduced friction\", \"causes infection\", \"vital to a good life\", \"prevents water loss\", \"camouflage from consumers\", \"Protection against predators\", \"spur the growth of the plant\", \"a smooth surface\"]}, \"combinedfact\": \"\", \"fact1\": \"\", \"fact2\": \"\", \"formatted_question\": \"What does a plant's skin do? (A) Reduced friction (B) causes infection (C) vital to a good life (D) prevents water loss (E) camouflage from consumers (F) Protection against predators (G) spur the growth of the plant (H) a smooth surface\", \"id\": \"3QRYMNZ7FYGITFVSJET3PS0F4S0NT9\", \"question\": \"What does a plant's skin do?\"}\r\n...\r\n```\r\nHowever, only a few instances are loaded for the training split, which is not correct. \r\n\r\n## Environment info\r\n- `datasets` version: '1.10.2' \r\n- Platform: MaxOS \r\n- Python version:3.7\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2708/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2708/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2705", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2705/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2705/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2705/events", "html_url": "https://github.com/huggingface/datasets/issues/2705", "id": 950488583, "node_id": "MDU6SXNzdWU5NTA0ODg1ODM=", "number": 2705, "title": "404 not found error on loading WIKIANN dataset", "user": {"login": "ronbutan", "id": 39296659, "node_id": "MDQ6VXNlcjM5Mjk2NjU5", "avatar_url": "https://avatars.githubusercontent.com/u/39296659?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ronbutan", "html_url": "https://github.com/ronbutan", "followers_url": "https://api.github.com/users/ronbutan/followers", "following_url": "https://api.github.com/users/ronbutan/following{/other_user}", "gists_url": "https://api.github.com/users/ronbutan/gists{/gist_id}", "starred_url": "https://api.github.com/users/ronbutan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ronbutan/subscriptions", "organizations_url": "https://api.github.com/users/ronbutan/orgs", "repos_url": "https://api.github.com/users/ronbutan/repos", "events_url": "https://api.github.com/users/ronbutan/events{/privacy}", "received_events_url": "https://api.github.com/users/ronbutan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-07-22T09:55:50Z", "updated_at": "2021-07-23T08:07:32Z", "closed_at": "2021-07-23T08:07:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nUnable to retreive wikiann English dataset\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import list_datasets, load_dataset, list_metrics, load_metric\r\nWIKIANN = load_dataset(\"wikiann\",\"en\")\r\n```\r\n\r\n## Expected results\r\nColab notebook should display successful download status\r\n\r\n## Actual results\r\nFileNotFoundError: Couldn't find file at https://www.dropbox.com/s/12h3qqog6q4bjve/panx_dataset.tar?dl=1\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.10.1\r\n- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.11\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2705/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2705/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2700", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2700/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2700/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2700/events", "html_url": "https://github.com/huggingface/datasets/issues/2700", "id": 950276325, "node_id": "MDU6SXNzdWU5NTAyNzYzMjU=", "number": 2700, "title": "from datasets import Dataset is failing ", "user": {"login": "kswamy15", "id": 5582286, "node_id": "MDQ6VXNlcjU1ODIyODY=", "avatar_url": "https://avatars.githubusercontent.com/u/5582286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kswamy15", "html_url": "https://github.com/kswamy15", "followers_url": "https://api.github.com/users/kswamy15/followers", "following_url": "https://api.github.com/users/kswamy15/following{/other_user}", "gists_url": "https://api.github.com/users/kswamy15/gists{/gist_id}", "starred_url": "https://api.github.com/users/kswamy15/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kswamy15/subscriptions", "organizations_url": "https://api.github.com/users/kswamy15/orgs", "repos_url": "https://api.github.com/users/kswamy15/repos", "events_url": "https://api.github.com/users/kswamy15/events{/privacy}", "received_events_url": "https://api.github.com/users/kswamy15/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-07-22T03:51:23Z", "updated_at": "2021-07-22T07:23:45Z", "closed_at": "2021-07-22T07:09:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import Dataset\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in <module>()\r\n     25 import posixpath\r\n     26 import requests\r\n---> 27 from tqdm.contrib.concurrent import thread_map\r\n     28 \r\n     29 from .. import __version__, config, utils\r\n\r\nModuleNotFoundError: No module named 'tqdm.contrib.concurrent'\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n\"Open Examples\" button below.\r\n---------------------------------------------------------------------------\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: latest version as of 07/21/2021\r\n- Platform: Google Colab\r\n- Python version: 3.7\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2700/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2700/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2695", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2695/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2695/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2695/events", "html_url": "https://github.com/huggingface/datasets/issues/2695", "id": 949864823, "node_id": "MDU6SXNzdWU5NDk4NjQ4MjM=", "number": 2695, "title": "Cannot import load_dataset on Colab", "user": {"login": "bayartsogt-ya", "id": 43239645, "node_id": "MDQ6VXNlcjQzMjM5NjQ1", "avatar_url": "https://avatars.githubusercontent.com/u/43239645?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bayartsogt-ya", "html_url": "https://github.com/bayartsogt-ya", "followers_url": "https://api.github.com/users/bayartsogt-ya/followers", "following_url": "https://api.github.com/users/bayartsogt-ya/following{/other_user}", "gists_url": "https://api.github.com/users/bayartsogt-ya/gists{/gist_id}", "starred_url": "https://api.github.com/users/bayartsogt-ya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bayartsogt-ya/subscriptions", "organizations_url": "https://api.github.com/users/bayartsogt-ya/orgs", "repos_url": "https://api.github.com/users/bayartsogt-ya/repos", "events_url": "https://api.github.com/users/bayartsogt-ya/events{/privacy}", "received_events_url": "https://api.github.com/users/bayartsogt-ya/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-07-21T15:52:51Z", "updated_at": "2021-07-22T07:26:25Z", "closed_at": "2021-07-22T07:09:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nGot tqdm concurrent module not found error during importing load_dataset from datasets.\r\n\r\n## Steps to reproduce the bug\r\nHere [colab notebook](https://colab.research.google.com/drive/1pErWWnVP4P4mVHjSFUtkePd8Na_Qirg4?usp=sharing) to reproduce the error\r\n\r\nOn colab:\r\n```python\r\n!pip install datasets\r\nfrom datasets import load_dataset\r\n```\r\n\r\n## Expected results\r\nWorks without error\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n```\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-2-8cc7de4c69eb> in <module>()\r\n----> 1 from datasets import load_dataset, load_metric, Metric, MetricInfo, Features, Value\r\n      2 from sklearn.metrics import mean_squared_error\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/__init__.py in <module>()\r\n     31     )\r\n     32 \r\n---> 33 from .arrow_dataset import Dataset, concatenate_datasets\r\n     34 from .arrow_reader import ArrowReader, ReadInstruction\r\n     35 from .arrow_writer import ArrowWriter\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in <module>()\r\n     40 from tqdm.auto import tqdm\r\n     41 \r\n---> 42 from datasets.tasks.text_classification import TextClassification\r\n     43 \r\n     44 from . import config, utils\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/tasks/__init__.py in <module>()\r\n      1 from typing import Optional\r\n      2 \r\n----> 3 from ..utils.logging import get_logger\r\n      4 from .automatic_speech_recognition import AutomaticSpeechRecognition\r\n      5 from .base import TaskTemplate\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/__init__.py in <module>()\r\n     19 \r\n     20 from . import logging\r\n---> 21 from .download_manager import DownloadManager, GenerateMode\r\n     22 from .file_utils import DownloadConfig, cached_path, hf_bucket_url, is_remote_url, temp_seed\r\n     23 from .mock_download_manager import MockDownloadManager\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/download_manager.py in <module>()\r\n     24 \r\n     25 from .. import config\r\n---> 26 from .file_utils import (\r\n     27     DownloadConfig,\r\n     28     cached_path,\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in <module>()\r\n     25 import posixpath\r\n     26 import requests\r\n---> 27 from tqdm.contrib.concurrent import thread_map\r\n     28 \r\n     29 from .. import __version__, config, utils\r\n\r\nModuleNotFoundError: No module named 'tqdm.contrib.concurrent'\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.10.0\r\n- Platform: Colab\r\n- Python version: 3.7.11\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2695/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2695/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2691", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2691/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2691/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2691/events", "html_url": "https://github.com/huggingface/datasets/issues/2691", "id": 949758379, "node_id": "MDU6SXNzdWU5NDk3NTgzNzk=", "number": 2691, "title": "xtreme / pan-x cannot be downloaded", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-07-21T14:18:05Z", "updated_at": "2021-07-26T09:34:22Z", "closed_at": "2021-07-26T09:34:22Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nDataset xtreme / pan-x cannot be loaded\r\n\r\nSeems related to https://github.com/huggingface/datasets/pull/2326\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\ndataset = load_dataset(\"xtreme\", \"PAN-X.fr\")\r\n```\r\n\r\n## Expected results\r\n\r\nLoad the dataset\r\n\r\n## Actual results\r\n\r\n```\r\nFileNotFoundError: Couldn't find file at https://www.dropbox.com/s/12h3qqog6q4bjve/panx_dataset.tar?dl=1\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.9.0\r\n- Platform: macOS-11.4-x86_64-i386-64bit\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2691/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2691/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2689", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2689/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2689/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2689/events", "html_url": "https://github.com/huggingface/datasets/issues/2689", "id": 949447104, "node_id": "MDU6SXNzdWU5NDk0NDcxMDQ=", "number": 2689, "title": "cannot save the dataset to disk after rename_column", "user": {"login": "PaulLerner", "id": 25532159, "node_id": "MDQ6VXNlcjI1NTMyMTU5", "avatar_url": "https://avatars.githubusercontent.com/u/25532159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PaulLerner", "html_url": "https://github.com/PaulLerner", "followers_url": "https://api.github.com/users/PaulLerner/followers", "following_url": "https://api.github.com/users/PaulLerner/following{/other_user}", "gists_url": "https://api.github.com/users/PaulLerner/gists{/gist_id}", "starred_url": "https://api.github.com/users/PaulLerner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PaulLerner/subscriptions", "organizations_url": "https://api.github.com/users/PaulLerner/orgs", "repos_url": "https://api.github.com/users/PaulLerner/repos", "events_url": "https://api.github.com/users/PaulLerner/events{/privacy}", "received_events_url": "https://api.github.com/users/PaulLerner/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-21T08:13:40Z", "updated_at": "2021-07-21T13:11:04Z", "closed_at": "2021-07-21T13:11:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nIf you use `rename_column` and do no other modification, you will be unable to save the dataset using `save_to_disk`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nIn [1]: from datasets import Dataset, load_from_disk\r\nIn [5]: dataset=Dataset.from_dict({'foo': [0]})\r\nIn [7]: dataset.save_to_disk('foo')\r\nIn [8]: dataset=load_from_disk('foo')\r\nIn [10]: dataset=dataset.rename_column('foo', 'bar')\r\nIn [11]: dataset.save_to_disk('foo')\r\n---------------------------------------------------------------------------\r\nPermissionError                           Traceback (most recent call last)\r\n<ipython-input-11-a3bc0d4fc339> in <module>\r\n----> 1 dataset.save_to_disk('foo')\r\n\r\n/mnt/beegfs/projects/meerqat/anaconda3/envs/meerqat/lib/python3.7/site-packages/datasets/arrow_dataset.py in save_to_disk(self, dataset_path\r\n, fs)\r\n    597             if Path(dataset_path, config.DATASET_ARROW_FILENAME) in cache_files_paths:\r\n    598                 raise PermissionError(\r\n--> 599                     f\"Tried to overwrite {Path(dataset_path, config.DATASET_ARROW_FILENAME)} but a dataset can't overwrite itself.\"\r\n    600                 )\r\n    601             if Path(dataset_path, config.DATASET_INDICES_FILENAME) in cache_files_paths:\r\n\r\nPermissionError: Tried to overwrite foo/dataset.arrow but a dataset can't overwrite itself.\r\n```\r\n\r\nN. B. I created the dataset from dict to enable easy reproduction but the same happens if you load an existing dataset (e.g. starting from `In [8]`)\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-centos-7.9.2009-Core\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2689/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2689/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2688", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2688/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2688/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2688/events", "html_url": "https://github.com/huggingface/datasets/issues/2688", "id": 949182074, "node_id": "MDU6SXNzdWU5NDkxODIwNzQ=", "number": 2688, "title": "hebrew language codes he and iw should be treated as aliases", "user": {"login": "eyaler", "id": 4436747, "node_id": "MDQ6VXNlcjQ0MzY3NDc=", "avatar_url": "https://avatars.githubusercontent.com/u/4436747?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eyaler", "html_url": "https://github.com/eyaler", "followers_url": "https://api.github.com/users/eyaler/followers", "following_url": "https://api.github.com/users/eyaler/following{/other_user}", "gists_url": "https://api.github.com/users/eyaler/gists{/gist_id}", "starred_url": "https://api.github.com/users/eyaler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eyaler/subscriptions", "organizations_url": "https://api.github.com/users/eyaler/orgs", "repos_url": "https://api.github.com/users/eyaler/repos", "events_url": "https://api.github.com/users/eyaler/events{/privacy}", "received_events_url": "https://api.github.com/users/eyaler/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-20T23:13:52Z", "updated_at": "2021-07-21T16:34:53Z", "closed_at": "2021-07-21T16:34:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://huggingface.co/datasets/mc4 not listed when searching for hebrew datasets (he) as it uses the older language code iw, preventing discoverability. ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2688/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2688/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2681", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2681/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2681/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2681/events", "html_url": "https://github.com/huggingface/datasets/issues/2681", "id": 948708645, "node_id": "MDU6SXNzdWU5NDg3MDg2NDU=", "number": 2681, "title": "5 duplicate datasets", "user": {"login": "severo", "id": 1676121, "node_id": "MDQ6VXNlcjE2NzYxMjE=", "avatar_url": "https://avatars.githubusercontent.com/u/1676121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/severo", "html_url": "https://github.com/severo", "followers_url": "https://api.github.com/users/severo/followers", "following_url": "https://api.github.com/users/severo/following{/other_user}", "gists_url": "https://api.github.com/users/severo/gists{/gist_id}", "starred_url": "https://api.github.com/users/severo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/severo/subscriptions", "organizations_url": "https://api.github.com/users/severo/orgs", "repos_url": "https://api.github.com/users/severo/repos", "events_url": "https://api.github.com/users/severo/events{/privacy}", "received_events_url": "https://api.github.com/users/severo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-20T14:25:00Z", "updated_at": "2021-07-20T15:44:17Z", "closed_at": "2021-07-20T15:44:17Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nIn 5 cases, I could find a dataset on Paperswithcode which references two Hugging Face datasets as dataset loaders. They are:\r\n\r\n- https://paperswithcode.com/dataset/multinli -> https://huggingface.co/datasets/multi_nli and https://huggingface.co/datasets/multi_nli_mismatch\r\n  \r\n  <img width=\"838\" alt=\"Capture d\u2019e\u0301cran 2021-07-20 a\u0300 16 33 58\" src=\"https://user-images.githubusercontent.com/1676121/126342757-4625522a-f788-41a3-bd1f-2a8b9817bbf5.png\">\r\n\r\n- https://paperswithcode.com/dataset/squad -> https://huggingface.co/datasets/squad and https://huggingface.co/datasets/squad_v2\r\n- https://paperswithcode.com/dataset/narrativeqa -> https://huggingface.co/datasets/narrativeqa and https://huggingface.co/datasets/narrativeqa_manual\r\n- https://paperswithcode.com/dataset/hate-speech-and-offensive-language -> https://huggingface.co/datasets/hate_offensive and https://huggingface.co/datasets/hate_speech_offensive\r\n- https://paperswithcode.com/dataset/newsph-nli -> https://huggingface.co/datasets/newsph and https://huggingface.co/datasets/newsph_nli\r\n\r\nPossible solutions:\r\n- don't fix (it works)\r\n- for each pair of duplicate datasets, remove one, and create an alias to the other.\r\n\r\n## Steps to reproduce the bug\r\n\r\nVisit the Paperswithcode links, and look at the \"Dataset Loaders\" section\r\n\r\n## Expected results\r\n\r\nThere should only be one reference to a Hugging Face dataset loader\r\n\r\n## Actual results\r\n\r\nTwo Hugging Face dataset loaders\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2681/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2679", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2679/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2679/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2679/events", "html_url": "https://github.com/huggingface/datasets/issues/2679", "id": 948506638, "node_id": "MDU6SXNzdWU5NDg1MDY2Mzg=", "number": 2679, "title": "Cannot load the blog_authorship_corpus due to codec errors", "user": {"login": "izaskr", "id": 38069449, "node_id": "MDQ6VXNlcjM4MDY5NDQ5", "avatar_url": "https://avatars.githubusercontent.com/u/38069449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/izaskr", "html_url": "https://github.com/izaskr", "followers_url": "https://api.github.com/users/izaskr/followers", "following_url": "https://api.github.com/users/izaskr/following{/other_user}", "gists_url": "https://api.github.com/users/izaskr/gists{/gist_id}", "starred_url": "https://api.github.com/users/izaskr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/izaskr/subscriptions", "organizations_url": "https://api.github.com/users/izaskr/orgs", "repos_url": "https://api.github.com/users/izaskr/repos", "events_url": "https://api.github.com/users/izaskr/events{/privacy}", "received_events_url": "https://api.github.com/users/izaskr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-07-20T10:13:20Z", "updated_at": "2021-07-21T17:02:21Z", "closed_at": "2021-07-21T13:11:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA codec error is raised while loading the blog_authorship_corpus. \r\n\r\n## Steps to reproduce the bug\r\n```\r\nfrom datasets import load_dataset\r\nraw_datasets = load_dataset(\"blog_authorship_corpus\")\r\n```\r\n\r\n\r\n## Expected results\r\nLoading the dataset without errors.\r\n\r\n## Actual results\r\nAn error similar to the one below was raised for (what seems like) every XML file.\r\n/home/izaskr/.cache/huggingface/datasets/downloads/extracted/7cf52524f6517e168604b41c6719292e8f97abbe8f731e638b13423f4212359a/blogs/788358.male.24.Arts.Libra.xml cannot be loaded. Error message: 'utf-8' codec can't decode byte 0xe7 in position 7551: invalid continuation byte\r\n\r\nTraceback (most recent call last):         \r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/load.py\", line 856, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/builder.py\", line 583, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/builder.py\", line 671, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/utils/info_utils.py\", line 74, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_splits))\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=610252351, num_examples=532812, dataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='train', num_bytes=614706451, num_examples=535568, dataset_name='blog_authorship_corpus')}, {'expected': SplitInfo(name='validation', num_bytes=37500394, num_examples=31277, dataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='validation', num_bytes=32553710, num_examples=28521, dataset_name='blog_authorship_corpus')}]\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-4.15.0-132-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.8\r\n- PyArrow version: 4.0.1\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2679/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2679/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2678", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2678/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2678/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2678/events", "html_url": "https://github.com/huggingface/datasets/issues/2678", "id": 948471222, "node_id": "MDU6SXNzdWU5NDg0NzEyMjI=", "number": 2678, "title": "Import Error in Kaggle notebook", "user": {"login": "prikmm", "id": 47216475, "node_id": "MDQ6VXNlcjQ3MjE2NDc1", "avatar_url": "https://avatars.githubusercontent.com/u/47216475?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prikmm", "html_url": "https://github.com/prikmm", "followers_url": "https://api.github.com/users/prikmm/followers", "following_url": "https://api.github.com/users/prikmm/following{/other_user}", "gists_url": "https://api.github.com/users/prikmm/gists{/gist_id}", "starred_url": "https://api.github.com/users/prikmm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prikmm/subscriptions", "organizations_url": "https://api.github.com/users/prikmm/orgs", "repos_url": "https://api.github.com/users/prikmm/repos", "events_url": "https://api.github.com/users/prikmm/events{/privacy}", "received_events_url": "https://api.github.com/users/prikmm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-07-20T09:28:38Z", "updated_at": "2021-07-21T13:59:26Z", "closed_at": "2021-07-21T13:03:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nNot able to import datasets library in kaggle notebooks\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n!pip install datasets\r\nimport datasets\r\n```\r\n\r\n## Expected results\r\nNo such error\r\n\r\n## Actual results\r\n```\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-9-652e886d387f> in <module>\r\n----> 1 import datasets\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/__init__.py in <module>\r\n     31     )\r\n     32 \r\n---> 33 from .arrow_dataset import Dataset, concatenate_datasets\r\n     34 from .arrow_reader import ArrowReader, ReadInstruction\r\n     35 from .arrow_writer import ArrowWriter\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py in <module>\r\n     36 import pandas as pd\r\n     37 import pyarrow as pa\r\n---> 38 import pyarrow.compute as pc\r\n     39 from multiprocess import Pool, RLock\r\n     40 from tqdm.auto import tqdm\r\n\r\n/opt/conda/lib/python3.7/site-packages/pyarrow/compute.py in <module>\r\n     16 # under the License.\r\n     17 \r\n---> 18 from pyarrow._compute import (  # noqa\r\n     19     Function,\r\n     20     FunctionOptions,\r\n\r\nImportError: /opt/conda/lib/python3.7/site-packages/pyarrow/_compute.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZNK5arrow7compute15KernelSignature8ToStringEv\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Kaggle\r\n- Python version: 3.7.10\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2678/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2677", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2677/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2677/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2677/events", "html_url": "https://github.com/huggingface/datasets/issues/2677", "id": 948429788, "node_id": "MDU6SXNzdWU5NDg0Mjk3ODg=", "number": 2677, "title": "Error when downloading C4", "user": {"login": "Aktsvigun", "id": 36672861, "node_id": "MDQ6VXNlcjM2NjcyODYx", "avatar_url": "https://avatars.githubusercontent.com/u/36672861?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aktsvigun", "html_url": "https://github.com/Aktsvigun", "followers_url": "https://api.github.com/users/Aktsvigun/followers", "following_url": "https://api.github.com/users/Aktsvigun/following{/other_user}", "gists_url": "https://api.github.com/users/Aktsvigun/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aktsvigun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aktsvigun/subscriptions", "organizations_url": "https://api.github.com/users/Aktsvigun/orgs", "repos_url": "https://api.github.com/users/Aktsvigun/repos", "events_url": "https://api.github.com/users/Aktsvigun/events{/privacy}", "received_events_url": "https://api.github.com/users/Aktsvigun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-07-20T08:37:30Z", "updated_at": "2021-07-20T14:41:31Z", "closed_at": "2021-07-20T14:38:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI am trying to download `en` corpus from C4 dataset. However, I get an error caused by validation files download (see image). My code is very primitive:\r\n`datasets.load_dataset('c4', 'en')`\r\n\r\nIs this a bug or do I have some configurations missing on my server? \r\nThanks!\r\n\r\n\r\n<img width=\"1014\" alt=\"\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 2021-07-20 \u0432 11 37 17\" src=\"https://user-images.githubusercontent.com/36672861/126289448-6e0db402-5f3f-485a-bf74-eb6e0271fc25.png\">", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2677/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2669", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2669/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2669/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2669/events", "html_url": "https://github.com/huggingface/datasets/issues/2669", "id": 946982998, "node_id": "MDU6SXNzdWU5NDY5ODI5OTg=", "number": 2669, "title": "Metric kwargs are not passed to underlying external metric f1_score", "user": {"login": "BramVanroy", "id": 2779410, "node_id": "MDQ6VXNlcjI3Nzk0MTA=", "avatar_url": "https://avatars.githubusercontent.com/u/2779410?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BramVanroy", "html_url": "https://github.com/BramVanroy", "followers_url": "https://api.github.com/users/BramVanroy/followers", "following_url": "https://api.github.com/users/BramVanroy/following{/other_user}", "gists_url": "https://api.github.com/users/BramVanroy/gists{/gist_id}", "starred_url": "https://api.github.com/users/BramVanroy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BramVanroy/subscriptions", "organizations_url": "https://api.github.com/users/BramVanroy/orgs", "repos_url": "https://api.github.com/users/BramVanroy/repos", "events_url": "https://api.github.com/users/BramVanroy/events{/privacy}", "received_events_url": "https://api.github.com/users/BramVanroy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-07-18T08:32:31Z", "updated_at": "2021-07-18T18:36:05Z", "closed_at": "2021-07-18T11:19:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen I want to use F1 score with average=\"min\", this keyword argument does not seem to be passed through to the underlying sklearn metric. This is evident because [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) throws an error telling me so.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nf1 = datasets.load_metric(\"f1\", keep_in_memory=True, average=\"min\")\r\nf1.add_batch(predictions=[0,2,3], references=[1, 2, 3])\r\nf1.compute()\r\n```\r\n\r\n## Expected results\r\nNo error, because `average=\"min\"` should be passed correctly to f1_score in sklearn.\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\datasets\\metric.py\", line 402, in compute\r\n    output = self._compute(predictions=predictions, references=references, **kwargs)\r\n  File \"C:\\Users\\bramv\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\f1\\82177930a325d4c28342bba0f116d73f6d92fb0c44cd67be32a07c1262b61cfe\\f1.py\", line 97, in _compute\r\n    \"f1\": f1_score(\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\r\n    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\r\n    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\r\n    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\r\n  File \"C:\\Users\\bramv\\.virtualenvs\\pipeline-TpEsXVex\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1294, in _check_set_wise_labels\r\n    raise ValueError(\"Target is %s but average='binary'. Please \"\r\nValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.9.2\r\n- PyArrow version: 4.0.1", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2669/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2669/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2651", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2651/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2651/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2651/events", "html_url": "https://github.com/huggingface/datasets/issues/2651", "id": 944796961, "node_id": "MDU6SXNzdWU5NDQ3OTY5NjE=", "number": 2651, "title": "Setting log level higher than warning does not suppress progress bar", "user": {"login": "Isa-rentacs", "id": 1147443, "node_id": "MDQ6VXNlcjExNDc0NDM=", "avatar_url": "https://avatars.githubusercontent.com/u/1147443?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Isa-rentacs", "html_url": "https://github.com/Isa-rentacs", "followers_url": "https://api.github.com/users/Isa-rentacs/followers", "following_url": "https://api.github.com/users/Isa-rentacs/following{/other_user}", "gists_url": "https://api.github.com/users/Isa-rentacs/gists{/gist_id}", "starred_url": "https://api.github.com/users/Isa-rentacs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Isa-rentacs/subscriptions", "organizations_url": "https://api.github.com/users/Isa-rentacs/orgs", "repos_url": "https://api.github.com/users/Isa-rentacs/repos", "events_url": "https://api.github.com/users/Isa-rentacs/events{/privacy}", "received_events_url": "https://api.github.com/users/Isa-rentacs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2021-07-14T21:06:51Z", "updated_at": "2022-07-08T14:51:57Z", "closed_at": "2021-07-15T03:41:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI would like to disable progress bars for `.map` method (and other methods like `.filter` and `load_dataset` as well).\r\nAccording to #1627 one can suppress it by setting log level higher than `warning`, however doing so doesn't suppress it with version 1.9.0.\r\n\r\nI also tried to set `DATASETS_VERBOSITY` environment variable to `error` or `critical` but it also didn't work.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\n\r\nfrom datasets.utils.logging import set_verbosity_error\r\n\r\nset_verbosity_error()\r\n\r\ndef dummy_map(batch):\r\n    return batch\r\n\r\ncommon_voice_train = datasets.load_dataset(\"common_voice\", \"de\", split=\"train\")\r\ncommon_voice_test = datasets.load_dataset(\"common_voice\", \"de\", split=\"test\")\r\n\r\ncommon_voice_train.map(dummy_map)\r\n```\r\n\r\n## Expected results\r\n- The progress bar for `.map` call won't be shown\r\n\r\n## Actual results\r\n- The progress bar for `.map` is still shown \r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-5.4.0-1045-aws-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.5\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2651/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2651/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2646", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2646/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2646/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2646/events", "html_url": "https://github.com/huggingface/datasets/issues/2646", "id": 944379954, "node_id": "MDU6SXNzdWU5NDQzNzk5NTQ=", "number": 2646, "title": "downloading of yahoo_answers_topics dataset failed", "user": {"login": "vikrant7k", "id": 66781249, "node_id": "MDQ6VXNlcjY2NzgxMjQ5", "avatar_url": "https://avatars.githubusercontent.com/u/66781249?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vikrant7k", "html_url": "https://github.com/vikrant7k", "followers_url": "https://api.github.com/users/vikrant7k/followers", "following_url": "https://api.github.com/users/vikrant7k/following{/other_user}", "gists_url": "https://api.github.com/users/vikrant7k/gists{/gist_id}", "starred_url": "https://api.github.com/users/vikrant7k/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vikrant7k/subscriptions", "organizations_url": "https://api.github.com/users/vikrant7k/orgs", "repos_url": "https://api.github.com/users/vikrant7k/repos", "events_url": "https://api.github.com/users/vikrant7k/events{/privacy}", "received_events_url": "https://api.github.com/users/vikrant7k/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-14T12:31:05Z", "updated_at": "2022-08-04T08:28:24Z", "closed_at": "2022-08-04T08:28:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI get an error datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files when I try to download the yahoo_answers_topics dataset\r\n\r\n## Steps to reproduce the bug\r\n   self.dataset = load_dataset(\r\n                'yahoo_answers_topics', cache_dir=self.config['yahoo_cache_dir'], split='train[:90%]')\r\n# Sample code to reproduce the bug\r\n   self.dataset = load_dataset(\r\n                'yahoo_answers_topics', cache_dir=self.config['yahoo_cache_dir'], split='train[:90%]')\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2646/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2646/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2645", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2645/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2645/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2645/events", "html_url": "https://github.com/huggingface/datasets/issues/2645", "id": 944374284, "node_id": "MDU6SXNzdWU5NDQzNzQyODQ=", "number": 2645, "title": "load_dataset processing failed with OS error after downloading a dataset", "user": {"login": "fake-warrior8", "id": 40395156, "node_id": "MDQ6VXNlcjQwMzk1MTU2", "avatar_url": "https://avatars.githubusercontent.com/u/40395156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fake-warrior8", "html_url": "https://github.com/fake-warrior8", "followers_url": "https://api.github.com/users/fake-warrior8/followers", "following_url": "https://api.github.com/users/fake-warrior8/following{/other_user}", "gists_url": "https://api.github.com/users/fake-warrior8/gists{/gist_id}", "starred_url": "https://api.github.com/users/fake-warrior8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fake-warrior8/subscriptions", "organizations_url": "https://api.github.com/users/fake-warrior8/orgs", "repos_url": "https://api.github.com/users/fake-warrior8/repos", "events_url": "https://api.github.com/users/fake-warrior8/events{/privacy}", "received_events_url": "https://api.github.com/users/fake-warrior8/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-14T12:23:53Z", "updated_at": "2021-07-15T09:34:02Z", "closed_at": "2021-07-15T09:34:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nAfter downloading a dataset like opus100, there is a bug that \r\nOSError: Cannot find data file.\r\nOriginal error:\r\ndlopen: cannot load any more object with static TLS\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nthis_dataset = load_dataset('opus100', 'af-en')\r\n```\r\n\r\n## Expected results\r\nthere is no error when running load_dataset.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py\", line 652, in _download_and_prep\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py\", line 989, in _prepare_split\r\n    example = self.info.features.encode_example(record)\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/features.py\", line 952, in encode_example\r\n    example = cast_to_python_objects(example)\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/features.py\", line 219, in cast_to_python_ob\r\n    return _cast_to_python_objects(obj)[0]\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/features.py\", line 165, in _cast_to_python_o\r\n    import torch\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/torch/__init__.py\", line 188, in <module>\r\n    _load_global_deps()\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/torch/__init__.py\", line 141, in _load_global_deps\r\n    ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\r\n  File \"/home/anaconda3/lib/python3.6/ctypes/__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: dlopen: cannot load any more object with static TLS\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"download_hub_opus100.py\", line 9, in <module>\r\n    this_dataset = load_dataset('opus100', language_pair)\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/load.py\", line 748, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py\", line 575, in download_and_prepa\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py\", line 658, in _download_and_prep\r\n    + str(e)\r\nOSError: Cannot find data file.\r\nOriginal error:\r\ndlopen: cannot load any more object with static TLS\r\n\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-3.13.0-32-generic-x86_64-with-debian-jessie-sid\r\n- Python version: 3.6.6\r\n- PyArrow version: 3.0.0\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2645/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2644", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2644/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2644/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2644/events", "html_url": "https://github.com/huggingface/datasets/issues/2644", "id": 944254748, "node_id": "MDU6SXNzdWU5NDQyNTQ3NDg=", "number": 2644, "title": "Batched `map` not allowed to return 0 items", "user": {"login": "pcuenca", "id": 1177582, "node_id": "MDQ6VXNlcjExNzc1ODI=", "avatar_url": "https://avatars.githubusercontent.com/u/1177582?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pcuenca", "html_url": "https://github.com/pcuenca", "followers_url": "https://api.github.com/users/pcuenca/followers", "following_url": "https://api.github.com/users/pcuenca/following{/other_user}", "gists_url": "https://api.github.com/users/pcuenca/gists{/gist_id}", "starred_url": "https://api.github.com/users/pcuenca/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pcuenca/subscriptions", "organizations_url": "https://api.github.com/users/pcuenca/orgs", "repos_url": "https://api.github.com/users/pcuenca/repos", "events_url": "https://api.github.com/users/pcuenca/events{/privacy}", "received_events_url": "https://api.github.com/users/pcuenca/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-07-14T09:58:19Z", "updated_at": "2021-07-26T14:55:15Z", "closed_at": "2021-07-26T14:55:15Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nI'm trying to use `map` to filter a large dataset by selecting rows that match an expensive condition (files referenced by one of the columns need to exist in the filesystem, so we have to `stat` them). According to [the documentation](https://huggingface.co/docs/datasets/processing.html#augmenting-the-dataset), `a batch mapped function can take as input a batch of size N and return a batch of size M where M can be greater or less than N and can even be zero`.\r\n\r\nHowever, when the returned batch has a size of zero (neither item in the batch fulfilled the condition), we get an `index out of bounds` error. I think that `arrow_writer.py` is [trying to infer the returned types using the first element returned](https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_writer.py#L100), but no elements were returned in this case.\r\n\r\nFor this error to happen, I'm returning a dictionary that contains empty lists for the keys I want to keep, see below. If I return an empty dictionary instead (no keys), then a different error eventually occurs.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndef select_rows(examples):\r\n    # `key` is a column name that exists in the original dataset\r\n    # The following line simulates no matches found, so we return an empty batch\r\n    result = {'key': []}\r\n    return result\r\n\r\nfiltered_dataset = dataset.map(\r\n    select_rows,\r\n    remove_columns = dataset.column_names,\r\n    batched = True,\r\n    num_proc = 1,\r\n    desc = \"Selecting rows with images that exist\"\r\n)\r\n```\r\n\r\nThe code above immediately triggers the exception. If we use the following instead:\r\n\r\n```python\r\ndef select_rows(examples):\r\n    # `key` is a column name that exists in the original dataset\r\n    result = {'key': []}   # or defaultdict or whatever\r\n    \r\n    # code to check for condition and append elements to result\r\n    # some_items_found will be set to True if there were any matching elements in the batch\r\n    \r\n    return result if some_items_found else {}\r\n```\r\n\r\nThen it _seems_ to work, but it eventually fails with some sort of schema error. I believe it may happen when an empty batch is followed by a non-empty one, but haven't set up a test to verify it.\r\n\r\nIn my opinion, returning a dictionary with empty lists and valid column names should be accepted as a valid result with zero items.\r\n\r\n## Expected results\r\nThe dataset would be filtered and only the matching fields would be returned.\r\n\r\n## Actual results\r\nAn exception is encountered, as described. Using a workaround makes it fail further along the line.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.1.dev0\r\n- Platform: Linux-5.4.0-53-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2644/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2644/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2641", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2641/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2641/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2641/events", "html_url": "https://github.com/huggingface/datasets/issues/2641", "id": 943838085, "node_id": "MDU6SXNzdWU5NDM4MzgwODU=", "number": 2641, "title": "load_dataset(\"financial_phrasebank\") NonMatchingChecksumError", "user": {"login": "courtmckay", "id": 13956255, "node_id": "MDQ6VXNlcjEzOTU2MjU1", "avatar_url": "https://avatars.githubusercontent.com/u/13956255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/courtmckay", "html_url": "https://github.com/courtmckay", "followers_url": "https://api.github.com/users/courtmckay/followers", "following_url": "https://api.github.com/users/courtmckay/following{/other_user}", "gists_url": "https://api.github.com/users/courtmckay/gists{/gist_id}", "starred_url": "https://api.github.com/users/courtmckay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/courtmckay/subscriptions", "organizations_url": "https://api.github.com/users/courtmckay/orgs", "repos_url": "https://api.github.com/users/courtmckay/repos", "events_url": "https://api.github.com/users/courtmckay/events{/privacy}", "received_events_url": "https://api.github.com/users/courtmckay/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-07-13T21:21:49Z", "updated_at": "2022-08-04T08:30:08Z", "closed_at": "2022-08-04T08:30:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nAttempting to download the financial_phrasebank dataset results in a NonMatchingChecksumError\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"financial_phrasebank\", 'sentences_allagree')\r\n```\r\n\r\n## Expected results\r\nI expect to see the financial_phrasebank dataset downloaded successfully\r\n\r\n## Actual results\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip']\r\n\r\n## Environment info\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-4.14.232-177.418.amzn2.x86_64-x86_64-with-debian-10.6\r\n- Python version: 3.7.10\r\n- PyArrow version: 4.0.1\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2641/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2641/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2630", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2630/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2630/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2630/events", "html_url": "https://github.com/huggingface/datasets/issues/2630", "id": 942102956, "node_id": "MDU6SXNzdWU5NDIxMDI5NTY=", "number": 2630, "title": "Progress bars are not properly rendered in Jupyter notebook", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-12T14:07:13Z", "updated_at": "2022-02-03T15:55:33Z", "closed_at": "2022-02-03T15:55:33Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nThe progress bars are not Jupyter widgets; regular progress bars appear (like in a terminal).\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nds.map(tokenize, num_proc=10)\r\n```\r\n\r\n## Expected results\r\nJupyter widgets displaying the progress bars.\r\n\r\n## Actual results\r\nSimple plane progress bars.\r\n\r\ncc: Reported by @thomwolf ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2630/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2624", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2624/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2624/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2624/events", "html_url": "https://github.com/huggingface/datasets/issues/2624", "id": 941318247, "node_id": "MDU6SXNzdWU5NDEzMTgyNDc=", "number": 2624, "title": "can't set verbosity for `metric.py`", "user": {"login": "thomas-happify", "id": 66082334, "node_id": "MDQ6VXNlcjY2MDgyMzM0", "avatar_url": "https://avatars.githubusercontent.com/u/66082334?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomas-happify", "html_url": "https://github.com/thomas-happify", "followers_url": "https://api.github.com/users/thomas-happify/followers", "following_url": "https://api.github.com/users/thomas-happify/following{/other_user}", "gists_url": "https://api.github.com/users/thomas-happify/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomas-happify/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomas-happify/subscriptions", "organizations_url": "https://api.github.com/users/thomas-happify/orgs", "repos_url": "https://api.github.com/users/thomas-happify/repos", "events_url": "https://api.github.com/users/thomas-happify/events{/privacy}", "received_events_url": "https://api.github.com/users/thomas-happify/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-07-10T20:23:45Z", "updated_at": "2021-07-12T05:54:29Z", "closed_at": "2021-07-12T05:54:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n```\r\n[2021-07-10 20:13:11,528][datasets.utils.filelock][INFO] - Lock 139705371374976 acquired on /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.lock\r\n[2021-07-10 20:13:11,529][datasets.arrow_writer][INFO] - Done writing 32 examples in 6100 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\r\n[2021-07-10 20:13:11,531][datasets.arrow_dataset][INFO] - Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\r\n[2021-07-10 20:13:11,543][/conda/envs/myenv/lib/python3.8/site-packages/datasets/metric.py][INFO] - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\r\n```\r\nAs you can see, `datasets` logging come from different places. \r\n`filelock`, `arrow_writer` & `arrow_dataset` comes from `datasets.*` which are expected \r\nHowever, `metric.py` logging comes from `/conda/envs/myenv/lib/python3.8/site-packages/datasets/`\r\n\r\nSo when setting `datasets.utils.logging.set_verbosity_error()`,  it still logs the last message which is annoying during evaluation. \r\n\r\nI had to do \r\n```\r\nlogging.getLogger(\"/conda/envs/myenv/lib/python3.8/site-packages/datasets/metric\").setLevel(logging.ERROR)\r\n``` \r\nto fully mute these messages\r\n\r\n## Expected results\r\nit shouldn't log these messages when setting `datasets.utils.logging.set_verbosity_error()`\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: tried both 1.8.0 & 1.9.0\r\n- Platform: Ubuntu 18.04.5 LTS \r\n- Python version: 3.8.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2624/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2615", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2615/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2615/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2615/events", "html_url": "https://github.com/huggingface/datasets/issues/2615", "id": 940794339, "node_id": "MDU6SXNzdWU5NDA3OTQzMzk=", "number": 2615, "title": "Jsonlines export error", "user": {"login": "TevenLeScao", "id": 26709476, "node_id": "MDQ6VXNlcjI2NzA5NDc2", "avatar_url": "https://avatars.githubusercontent.com/u/26709476?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TevenLeScao", "html_url": "https://github.com/TevenLeScao", "followers_url": "https://api.github.com/users/TevenLeScao/followers", "following_url": "https://api.github.com/users/TevenLeScao/following{/other_user}", "gists_url": "https://api.github.com/users/TevenLeScao/gists{/gist_id}", "starred_url": "https://api.github.com/users/TevenLeScao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TevenLeScao/subscriptions", "organizations_url": "https://api.github.com/users/TevenLeScao/orgs", "repos_url": "https://api.github.com/users/TevenLeScao/repos", "events_url": "https://api.github.com/users/TevenLeScao/events{/privacy}", "received_events_url": "https://api.github.com/users/TevenLeScao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2021-07-09T14:02:05Z", "updated_at": "2021-07-09T15:29:07Z", "closed_at": "2021-07-09T15:28:33Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen exporting large datasets in jsonlines (c4 in my case) the created file has an error every 9999 lines: the 9999th and 10000th are concatenated, thus breaking the jsonlines format. This sounds like it is related to batching, which is by 10000 by default\r\n\r\n## Steps to reproduce the bug\r\nThis what I'm running:\r\n\r\nin python:\r\n\r\n```\r\nfrom datasets import load_dataset\r\nptb = load_dataset(\"ptb_text_only\")\r\nptb[\"train\"].to_json(\"ptb.jsonl\")\r\n```\r\n\r\nthen out of python:\r\n\r\n```\r\nhead -10000 ptb.jsonl\r\n```\r\n\r\n## Expected results\r\nProperly separated lines\r\n\r\n## Actual results\r\nThe last line is a concatenation of two lines\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n\r\n- `datasets` version: 1.9.1.dev0\r\n- Platform: Linux-5.4.0-1046-gcp-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyArrow version: 4.0.1", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2615/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2607", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2607/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2607/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2607/events", "html_url": "https://github.com/huggingface/datasets/issues/2607", "id": 938796902, "node_id": "MDU6SXNzdWU5Mzg3OTY5MDI=", "number": 2607, "title": "Streaming local gzip compressed JSON line files is not working", "user": {"login": "thomwolf", "id": 7353373, "node_id": "MDQ6VXNlcjczNTMzNzM=", "avatar_url": "https://avatars.githubusercontent.com/u/7353373?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomwolf", "html_url": "https://github.com/thomwolf", "followers_url": "https://api.github.com/users/thomwolf/followers", "following_url": "https://api.github.com/users/thomwolf/following{/other_user}", "gists_url": "https://api.github.com/users/thomwolf/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomwolf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomwolf/subscriptions", "organizations_url": "https://api.github.com/users/thomwolf/orgs", "repos_url": "https://api.github.com/users/thomwolf/repos", "events_url": "https://api.github.com/users/thomwolf/events{/privacy}", "received_events_url": "https://api.github.com/users/thomwolf/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2021-07-07T11:36:33Z", "updated_at": "2021-07-20T09:50:19Z", "closed_at": "2021-07-08T16:08:41Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nUsing streaming to iterate on local gzip compressed JSON files raise a file not exist error\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nstreamed_dataset = load_dataset('json', split='train', data_files=data_files, streaming=True)\r\n\r\nnext(iter(streamed_dataset))\r\n```\r\n\r\n## Actual results\r\n```\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-6-27a664e29784> in <module>\r\n----> 1 next(iter(streamed_dataset))\r\n\r\n~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in __iter__(self)\r\n    336 \r\n    337     def __iter__(self):\r\n--> 338         for key, example in self._iter():\r\n    339             if self.features:\r\n    340                 # we encode the example for ClassLabel feature types for example\r\n\r\n~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in _iter(self)\r\n    333         else:\r\n    334             ex_iterable = self._ex_iterable\r\n--> 335         yield from ex_iterable\r\n    336 \r\n    337     def __iter__(self):\r\n\r\n~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in __iter__(self)\r\n     76 \r\n     77     def __iter__(self):\r\n---> 78         for key, example in self.generate_examples_fn(**self.kwargs):\r\n     79             yield key, example\r\n     80 \r\n\r\n~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in wrapper(**kwargs)\r\n    282     def wrapper(**kwargs):\r\n    283         python_formatter = PythonFormatter()\r\n--> 284         for key, table in generate_tables_fn(**kwargs):\r\n    285             batch = python_formatter.format_batch(table)\r\n    286             for i, example in enumerate(_batch_to_examples(batch)):\r\n\r\n~/Documents/GitHub/datasets/src/datasets/packaged_modules/json/json.py in _generate_tables(self, files, original_files)\r\n     85                         file,\r\n     86                         read_options=self.config.pa_read_options,\r\n---> 87                         parse_options=self.config.pa_parse_options,\r\n     88                     )\r\n     89                 except pa.ArrowInvalid as err:\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/_json.pyx in pyarrow._json.read_json()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/_json.pyx in pyarrow._json._get_reader()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.get_input_stream()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.get_native_file()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.OSFile.__cinit__()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.OSFile._open_readable()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nFileNotFoundError: [Errno 2] Failed to open local file 'gzip://file-000000000000.json::/Users/thomwolf/github-dataset/file-000000000000.json.gz'. Detail: [errno 2] No such file or directory\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.9.1.dev0\r\n- Platform: Darwin-19.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.7\r\n- PyArrow version: 1.0.0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2607/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2607/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2600", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2600/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2600/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2600/events", "html_url": "https://github.com/huggingface/datasets/issues/2600", "id": 938086745, "node_id": "MDU6SXNzdWU5MzgwODY3NDU=", "number": 2600, "title": "Crash when using multiprocessing (`num_proc` > 1) on `filter` and all samples are discarded", "user": {"login": "mxschmdt", "id": 4904985, "node_id": "MDQ6VXNlcjQ5MDQ5ODU=", "avatar_url": "https://avatars.githubusercontent.com/u/4904985?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mxschmdt", "html_url": "https://github.com/mxschmdt", "followers_url": "https://api.github.com/users/mxschmdt/followers", "following_url": "https://api.github.com/users/mxschmdt/following{/other_user}", "gists_url": "https://api.github.com/users/mxschmdt/gists{/gist_id}", "starred_url": "https://api.github.com/users/mxschmdt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mxschmdt/subscriptions", "organizations_url": "https://api.github.com/users/mxschmdt/orgs", "repos_url": "https://api.github.com/users/mxschmdt/repos", "events_url": "https://api.github.com/users/mxschmdt/events{/privacy}", "received_events_url": "https://api.github.com/users/mxschmdt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-07-06T16:53:25Z", "updated_at": "2021-07-07T12:50:31Z", "closed_at": "2021-07-07T12:50:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nIf `filter` is applied to a dataset using multiprocessing (`num_proc` > 1) and all sharded datasets are empty afterwards (due to all samples being discarded), the program crashes.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import Dataset\r\ndata = Dataset.from_dict({'id': [0,1]})\r\ndata.filter(lambda x: False, num_proc=2)\r\n```\r\n\r\n## Expected results\r\nAn empty table should be returned without crashing.\r\n\r\n## Actual results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/fingerprint.py\", line 397, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2143, in filter\r\n    return self.map(\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1738, in map\r\n    result = concatenate_datasets(transformed_shards)\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3267, in concatenate_datasets\r\n    table = concat_tables(tables_to_concat, axis=axis)\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/table.py\", line 853, in concat_tables\r\n    return ConcatenationTable.from_tables(tables, axis=axis)\r\n  File \"/home/user/venv/lib/python3.8/site-packages/datasets/table.py\", line 713, in from_tables\r\n    blocks = to_blocks(tables[0])\r\nIndexError: list index out of range\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.9.0\r\n- Platform: Linux-5.12.11-300.fc34.x86_64-x86_64-with-glibc2.2.5\r\n- Python version: 3.8.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2600/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2598", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2598/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2598/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2598/events", "html_url": "https://github.com/huggingface/datasets/issues/2598", "id": 937930632, "node_id": "MDU6SXNzdWU5Mzc5MzA2MzI=", "number": 2598, "title": "Unable to download omp dataset", "user": {"login": "erikadistefano", "id": 25797960, "node_id": "MDQ6VXNlcjI1Nzk3OTYw", "avatar_url": "https://avatars.githubusercontent.com/u/25797960?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erikadistefano", "html_url": "https://github.com/erikadistefano", "followers_url": "https://api.github.com/users/erikadistefano/followers", "following_url": "https://api.github.com/users/erikadistefano/following{/other_user}", "gists_url": "https://api.github.com/users/erikadistefano/gists{/gist_id}", "starred_url": "https://api.github.com/users/erikadistefano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erikadistefano/subscriptions", "organizations_url": "https://api.github.com/users/erikadistefano/orgs", "repos_url": "https://api.github.com/users/erikadistefano/repos", "events_url": "https://api.github.com/users/erikadistefano/events{/privacy}", "received_events_url": "https://api.github.com/users/erikadistefano/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-07-06T14:00:52Z", "updated_at": "2021-07-07T12:56:35Z", "closed_at": "2021-07-07T12:56:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe omp dataset cannot be downloaded because of a DuplicatedKeysError\r\n\r\n## Steps to reproduce the bug\r\nfrom datasets import load_dataset\r\nomp = load_dataset('omp', 'posts_labeled')\r\nprint(omp)\r\n\r\n## Expected results\r\nThis code should download the omp dataset and print the dictionary\r\n\r\n## Actual results\r\nDownloading and preparing dataset omp/posts_labeled (download: 1.27 MiB, generated: 13.31 MiB, post-processed: Unknown size, total: 14.58 MiB) to /home/erika_distefano/.cache/huggingface/datasets/omp/posts_labeled/1.1.0/2fe5b067be3bff1d4588d5b0cbb9b5b22ae1b9d5b026a8ff572cd389f862735b...\r\n0 examples [00:00, ? examples/s]2021-07-06 09:43:55.868815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\nTraceback (most recent call last):      \r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py\", line 990, in _prepare_split\r\n    writer.write(example, key)\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py\", line 338, in write\r\n    self.check_duplicate_keys()\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py\", line 349, in check_duplicate_keys\r\n    raise DuplicatedKeysError(key)\r\ndatasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 3326\r\nKeys should be unique and deterministic in nature\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"hf_datasets.py\", line 32, in <module>\r\n    omp = load_dataset('omp', 'posts_labeled')\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/load.py\", line 748, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py\", line 575, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py\", line 652, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py\", line 992, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py\", line 409, in finalize\r\n    self.check_duplicate_keys()\r\n  File \"/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py\", line 349, in check_duplicate_keys\r\n    raise DuplicatedKeysError(key)\r\ndatasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 3326\r\nKeys should be unique and deterministic in nature\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.0\r\n- Platform: Ubuntu 18.04.4 LTS\r\n- Python version: 3.6.9\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2598/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2598/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2595", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2595/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2595/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2595/events", "html_url": "https://github.com/huggingface/datasets/issues/2595", "id": 937483120, "node_id": "MDU6SXNzdWU5Mzc0ODMxMjA=", "number": 2595, "title": "ModuleNotFoundError: No module named 'datasets.tasks' while importing common voice datasets", "user": {"login": "profsatwinder", "id": 41314912, "node_id": "MDQ6VXNlcjQxMzE0OTEy", "avatar_url": "https://avatars.githubusercontent.com/u/41314912?v=4", "gravatar_id": "", "url": "https://api.github.com/users/profsatwinder", "html_url": "https://github.com/profsatwinder", "followers_url": "https://api.github.com/users/profsatwinder/followers", "following_url": "https://api.github.com/users/profsatwinder/following{/other_user}", "gists_url": "https://api.github.com/users/profsatwinder/gists{/gist_id}", "starred_url": "https://api.github.com/users/profsatwinder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/profsatwinder/subscriptions", "organizations_url": "https://api.github.com/users/profsatwinder/orgs", "repos_url": "https://api.github.com/users/profsatwinder/repos", "events_url": "https://api.github.com/users/profsatwinder/events{/privacy}", "received_events_url": "https://api.github.com/users/profsatwinder/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-06T03:20:55Z", "updated_at": "2021-07-06T05:59:49Z", "closed_at": "2021-07-06T05:59:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "Error traceback:\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-8-a7b592d3bca0> in <module>()\r\n      1 from datasets import load_dataset, load_metric\r\n      2 \r\n----> 3 common_voice_train = load_dataset(\"common_voice\", \"pa-IN\", split=\"train+validation\")\r\n      4 common_voice_test = load_dataset(\"common_voice\", \"pa-IN\", split=\"test\")\r\n\r\n9 frames\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/common_voice/078d412587e9efeb0ae2e574da99c31e18844c496008d53dc5c60f4159ed639b/common_voice.py in <module>()\r\n     19 \r\n     20 import datasets\r\n---> 21 from datasets.tasks import AutomaticSpeechRecognition\r\n     22 \r\n     23 \r\n\r\nModuleNotFoundError: No module named 'datasets.tasks'", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2595/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2595/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2585", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2585/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2585/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2585/events", "html_url": "https://github.com/huggingface/datasets/issues/2585", "id": 936484419, "node_id": "MDU6SXNzdWU5MzY0ODQ0MTk=", "number": 2585, "title": "sqaud_v2 dataset contains misalignment between the answer text and the context value at the answer index", "user": {"login": "mmajurski", "id": 9354454, "node_id": "MDQ6VXNlcjkzNTQ0NTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/9354454?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mmajurski", "html_url": "https://github.com/mmajurski", "followers_url": "https://api.github.com/users/mmajurski/followers", "following_url": "https://api.github.com/users/mmajurski/following{/other_user}", "gists_url": "https://api.github.com/users/mmajurski/gists{/gist_id}", "starred_url": "https://api.github.com/users/mmajurski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mmajurski/subscriptions", "organizations_url": "https://api.github.com/users/mmajurski/orgs", "repos_url": "https://api.github.com/users/mmajurski/repos", "events_url": "https://api.github.com/users/mmajurski/events{/privacy}", "received_events_url": "https://api.github.com/users/mmajurski/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-07-04T15:39:49Z", "updated_at": "2021-07-07T13:18:51Z", "closed_at": "2021-07-07T13:18:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe built in huggingface squad_v2 dataset that you can access via datasets.load_dataset contains mis-alignment between the answers['text'] and the characters in the context at the location specified by answers['answer_start'].\r\n\r\nFor example:\r\nid = '56d1f453e7d4791d009025bd'\r\nanswers = {'text': ['Pure Land'], 'answer_start': [146]}\r\nHowever the actual text in context at location 146 is 'ure Land,'\r\nWhich is an off-by-one error from the correct answer.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\n\r\ndef check_context_answer_alignment(example):\r\n    for a_idx in range(len(example['answers']['text'])):\r\n        # check raw dataset for answer consistency between context and answer\r\n        answer_text = example['answers']['text'][a_idx]\r\n        a_st_idx = example['answers']['answer_start'][a_idx]\r\n        a_end_idx = a_st_idx + len(example['answers']['text'][a_idx])\r\n        answer_text_from_context = example['context'][a_st_idx:a_end_idx]\r\n        if answer_text != answer_text_from_context:\r\n            #print(example['id'])\r\n            return False\r\n    return True\r\n\r\ndataset = datasets.load_dataset('squad_v2', split='train', keep_in_memory=True)\r\n\r\nstart_len = len(dataset)\r\ndataset = dataset.filter(check_context_answer_alignment,\r\n                                num_proc=1,\r\n                                keep_in_memory=True)\r\nend_len = len(dataset)\r\nprint('{} instances contain mis-alignment between the answer text and answer index.'.format(start_len - end_len))\r\n```\r\n\r\n## Expected results\r\nThis code should result in 0 rows being filtered out from the dataset.\r\n\r\n## Actual results\r\nThis filter command results in 258 rows being flagged as containing a discrepancy between the text contained within answers['text'] and the text in example['context'] at the answers['answer_start'] location.\r\n\r\nThis code will reproduce the problem and produce the following count:\r\n\"258 instances contain mis-alignment between the answer text and answer index.\"\r\n\r\n## Environment info\r\nSteps to rebuilt the Conda environment:\r\n```\r\n# create a virtual environment to stuff all these packages into\r\nconda create -n round8 python=3.8 -y\r\n\r\n# activate the virtual environment\r\nconda activate round8\r\n\r\n# install pytorch (best done through conda to handle cuda dependencies)\r\nconda install pytorch torchvision torchtext cudatoolkit=11.1 -c pytorch-lts -c nvidia\r\n\r\npip install jsonpickle transformers datasets matplotlib\r\n```\r\n\r\nOS: Ubuntu 20.04\r\nPython 3.8\r\n\r\nResult of `conda env export`:\r\n```\r\nname: round8\r\nchannels:\r\n  - pytorch-lts\r\n  - nvidia\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=4.5=1_gnu\r\n  - blas=1.0=mkl\r\n  - brotlipy=0.7.0=py38h27cfd23_1003\r\n  - bzip2=1.0.8=h7b6447c_0\r\n  - ca-certificates=2021.5.25=h06a4308_1\r\n  - certifi=2021.5.30=py38h06a4308_0\r\n  - cffi=1.14.5=py38h261ae71_0\r\n  - chardet=4.0.0=py38h06a4308_1003\r\n  - cryptography=3.4.7=py38hd23ed53_0\r\n  - cudatoolkit=11.1.74=h6bb024c_0\r\n  - ffmpeg=4.2.2=h20bf706_0\r\n  - freetype=2.10.4=h5ab3b9f_0\r\n  - gmp=6.2.1=h2531618_2\r\n  - gnutls=3.6.15=he1e5248_0\r\n  - idna=2.10=pyhd3eb1b0_0\r\n  - intel-openmp=2021.2.0=h06a4308_610\r\n  - jpeg=9b=h024ee3a_2\r\n  - lame=3.100=h7b6447c_0\r\n  - lcms2=2.12=h3be6417_0\r\n  - ld_impl_linux-64=2.35.1=h7274673_9\r\n  - libffi=3.3=he6710b0_2\r\n  - libgcc-ng=9.3.0=h5101ec6_17\r\n  - libgomp=9.3.0=h5101ec6_17\r\n  - libidn2=2.3.1=h27cfd23_0\r\n  - libopus=1.3.1=h7b6447c_0\r\n  - libpng=1.6.37=hbc83047_0\r\n  - libstdcxx-ng=9.3.0=hd4cf53a_17\r\n  - libtasn1=4.16.0=h27cfd23_0\r\n  - libtiff=4.2.0=h85742a9_0\r\n  - libunistring=0.9.10=h27cfd23_0\r\n  - libuv=1.40.0=h7b6447c_0\r\n  - libvpx=1.7.0=h439df22_0\r\n  - libwebp-base=1.2.0=h27cfd23_0\r\n  - lz4-c=1.9.3=h2531618_0\r\n  - mkl=2021.2.0=h06a4308_296\r\n  - mkl-service=2.3.0=py38h27cfd23_1\r\n  - mkl_fft=1.3.0=py38h42c9631_2\r\n  - mkl_random=1.2.1=py38ha9443f7_2\r\n  - ncurses=6.2=he6710b0_1\r\n  - nettle=3.7.3=hbbd107a_1\r\n  - ninja=1.10.2=hff7bd54_1\r\n  - numpy=1.20.2=py38h2d18471_0\r\n  - numpy-base=1.20.2=py38hfae3a4d_0\r\n  - olefile=0.46=py_0\r\n  - openh264=2.1.0=hd408876_0\r\n  - openssl=1.1.1k=h27cfd23_0\r\n  - pillow=8.2.0=py38he98fc37_0\r\n  - pip=21.1.2=py38h06a4308_0\r\n  - pycparser=2.20=py_2\r\n  - pyopenssl=20.0.1=pyhd3eb1b0_1\r\n  - pysocks=1.7.1=py38h06a4308_0\r\n  - python=3.8.10=h12debd9_8\r\n  - pytorch=1.8.1=py3.8_cuda11.1_cudnn8.0.5_0\r\n  - readline=8.1=h27cfd23_0\r\n  - requests=2.25.1=pyhd3eb1b0_0\r\n  - setuptools=52.0.0=py38h06a4308_0\r\n  - six=1.16.0=pyhd3eb1b0_0\r\n  - sqlite=3.35.4=hdfb4753_0\r\n  - tk=8.6.10=hbc83047_0\r\n  - torchtext=0.9.1=py38\r\n  - torchvision=0.9.1=py38_cu111\r\n  - typing_extensions=3.7.4.3=pyha847dfd_0\r\n  - urllib3=1.26.4=pyhd3eb1b0_0\r\n  - wheel=0.36.2=pyhd3eb1b0_0\r\n  - x264=1!157.20191217=h7b6447c_0\r\n  - xz=5.2.5=h7b6447c_0\r\n  - zlib=1.2.11=h7b6447c_3\r\n  - zstd=1.4.9=haebb681_0\r\n  - pip:\r\n    - click==8.0.1\r\n    - cycler==0.10.0\r\n    - datasets==1.8.0\r\n    - dill==0.3.4\r\n    - filelock==3.0.12\r\n    - fsspec==2021.6.0\r\n    - huggingface-hub==0.0.8\r\n    - joblib==1.0.1\r\n    - jsonpickle==2.0.0\r\n    - kiwisolver==1.3.1\r\n    - matplotlib==3.4.2\r\n    - multiprocess==0.70.12.2\r\n    - packaging==20.9\r\n    - pandas==1.2.4\r\n    - pyarrow==3.0.0\r\n    - pyparsing==2.4.7\r\n    - python-dateutil==2.8.1\r\n    - pytz==2021.1\r\n    - regex==2021.4.4\r\n    - sacremoses==0.0.45\r\n    - tokenizers==0.10.3\r\n    - tqdm==4.49.0\r\n    - transformers==4.6.1\r\n    - xxhash==2.0.2\r\nprefix: /home/mmajurski/anaconda3/envs/round8\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2585/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2585/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2583", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2583/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2583/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2583/events", "html_url": "https://github.com/huggingface/datasets/issues/2583", "id": 936034976, "node_id": "MDU6SXNzdWU5MzYwMzQ5NzY=", "number": 2583, "title": "Error iteration over IterableDataset using Torch DataLoader", "user": {"login": "LeenaShekhar", "id": 12227436, "node_id": "MDQ6VXNlcjEyMjI3NDM2", "avatar_url": "https://avatars.githubusercontent.com/u/12227436?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LeenaShekhar", "html_url": "https://github.com/LeenaShekhar", "followers_url": "https://api.github.com/users/LeenaShekhar/followers", "following_url": "https://api.github.com/users/LeenaShekhar/following{/other_user}", "gists_url": "https://api.github.com/users/LeenaShekhar/gists{/gist_id}", "starred_url": "https://api.github.com/users/LeenaShekhar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LeenaShekhar/subscriptions", "organizations_url": "https://api.github.com/users/LeenaShekhar/orgs", "repos_url": "https://api.github.com/users/LeenaShekhar/repos", "events_url": "https://api.github.com/users/LeenaShekhar/events{/privacy}", "received_events_url": "https://api.github.com/users/LeenaShekhar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-07-02T19:55:58Z", "updated_at": "2021-07-20T09:04:45Z", "closed_at": "2021-07-05T23:48:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI have an IterableDataset (created using streaming=True) and I am trying to create batches using Torch DataLoader class by passing this IterableDataset to it. This throws error which is pasted below. I can do the same by using Torch IterableDataset. One thing I noticed is that in the former case when I look at the dataloader.sampler class I get torch.utils.data.sampler.SequentialSampler while the latter one gives torch.utils.data.dataloader._InfiniteConstantSampler. \r\n\r\nI am not sure if this is how it is meant to be used, but that's what seemed reasonable to me. \r\n\r\n## Steps to reproduce the bug\r\n\r\n1. Does not work.\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\r\n>>> dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\r\n>>> dataloader.sampler\r\n<torch.utils.data.sampler.SequentialSampler object at 0x7f245a510208>\r\n>>> for batch in dataloader:\r\n...     print(batch)\r\n```\r\n\r\n2. Works.\r\n```python\r\nimport torch\r\nfrom torch.utils.data import Dataset, IterableDataset, DataLoader\r\nclass CustomIterableDataset(IterableDataset):\r\n  'Characterizes a dataset for PyTorch'\r\n  def __init__(self, data):\r\n        'Initialization'\r\n        self.data = data\r\n\r\n\r\n  def __iter__(self):\r\n        return iter(self.data)\r\n\r\n\r\ndata = list(range(12))\r\ndataset = CustomIterableDataset(data)\r\ndataloader = DataLoader(dataset, batch_size=4)\r\nprint(\"dataloader: \", dataloader.sampler)\r\nfor batch in dataloader:\r\n    print(batch)\r\n```\r\n\r\n## Expected results\r\nTo get batches of data with the batch size as 4. Output from the latter one (2) though Datasource is different here so actual data is different.\r\ndataloader:  <torch.utils.data.dataloader._InfiniteConstantSampler object at 0x7f1cc29e2c50>\r\ntensor([0, 1, 2, 3])\r\ntensor([4, 5, 6, 7])\r\ntensor([ 8,  9, 10, 11])\r\n\r\n## Actual results\r\n<torch.utils.data.sampler.SequentialSampler object at 0x7f245a510208>\r\n\r\n...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\r\n    data = self._next_data()\r\n  File \"/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 474, in _next_data\r\n    index = self._next_index()  # may raise StopIteration\r\n  File \"/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 427, in _next_index\r\n    return next(self._sampler_iter)  # may raise StopIteration\r\n  File \"/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/sampler.py\", line 227, in __iter__\r\n    for idx in self.sampler:\r\n  File \"/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/sampler.py\", line 67, in __iter__\r\n    return iter(range(len(self.data_source)))\r\nTypeError: object of type 'IterableDataset' has no len()\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: '1.8.1.dev0'\r\n- Platform: Linux\r\n- Python version: Python 3.6.8\r\n- PyArrow version: '3.0.0'\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2583/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2569", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2569/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2569/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2569/events", "html_url": "https://github.com/huggingface/datasets/issues/2569", "id": 933015797, "node_id": "MDU6SXNzdWU5MzMwMTU3OTc=", "number": 2569, "title": "Weights of model checkpoint not initialized for RobertaModel for Bertscore", "user": {"login": "suzyahyah", "id": 2980993, "node_id": "MDQ6VXNlcjI5ODA5OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/2980993?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suzyahyah", "html_url": "https://github.com/suzyahyah", "followers_url": "https://api.github.com/users/suzyahyah/followers", "following_url": "https://api.github.com/users/suzyahyah/following{/other_user}", "gists_url": "https://api.github.com/users/suzyahyah/gists{/gist_id}", "starred_url": "https://api.github.com/users/suzyahyah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suzyahyah/subscriptions", "organizations_url": "https://api.github.com/users/suzyahyah/orgs", "repos_url": "https://api.github.com/users/suzyahyah/repos", "events_url": "https://api.github.com/users/suzyahyah/events{/privacy}", "received_events_url": "https://api.github.com/users/suzyahyah/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-06-29T18:55:23Z", "updated_at": "2021-07-01T07:08:59Z", "closed_at": "2021-06-30T07:35:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "When applying bertscore out of the box, \r\n\r\n```Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']```\r\n\r\nFollowing the typical usage from https://huggingface.co/docs/datasets/loading_metrics.html\r\n\r\n```\r\nfrom datasets import load_metric\r\nmetric = load_metric('bertscore')\r\n\r\n# Example of typical usage\r\nfor batch in dataset:\r\n    inputs, references = batch\r\n    predictions = model(inputs)\r\n    metric.add_batch(predictions=predictions, references=references)\r\nscore = metric.compute(lang=\"en\")\r\n#score = metric.compute(model_type=\"roberta-large\") # gives the same error\r\n```\r\n\r\nI am concerned about this because my usage shouldn't require any further fine-tuning and most people would expect to use BertScore out of the box? I realised the huggingface code is a wrapper around https://github.com/Tiiiger/bert_score, but I think this repo is anyway relying on the model code and weights from huggingface repo.... \r\n\r\n## Environment info\r\n- `datasets` version: 1.7.0\r\n- Platform: Linux-5.4.0-1041-aws-x86_64-with-glibc2.27\r\n- Python version:  3.9.5\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2569/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2569/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2561", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2561/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2561/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2561/events", "html_url": "https://github.com/huggingface/datasets/issues/2561", "id": 932321725, "node_id": "MDU6SXNzdWU5MzIzMjE3MjU=", "number": 2561, "title": "Existing cache for local dataset builder file updates is ignored with `ignore_verifications=True`", "user": {"login": "apsdehal", "id": 3616806, "node_id": "MDQ6VXNlcjM2MTY4MDY=", "avatar_url": "https://avatars.githubusercontent.com/u/3616806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apsdehal", "html_url": "https://github.com/apsdehal", "followers_url": "https://api.github.com/users/apsdehal/followers", "following_url": "https://api.github.com/users/apsdehal/following{/other_user}", "gists_url": "https://api.github.com/users/apsdehal/gists{/gist_id}", "starred_url": "https://api.github.com/users/apsdehal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apsdehal/subscriptions", "organizations_url": "https://api.github.com/users/apsdehal/orgs", "repos_url": "https://api.github.com/users/apsdehal/repos", "events_url": "https://api.github.com/users/apsdehal/events{/privacy}", "received_events_url": "https://api.github.com/users/apsdehal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-06-29T07:43:03Z", "updated_at": "2022-08-04T11:58:36Z", "closed_at": "2022-08-04T11:58:36Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nIf i have local file defining a dataset builder class and I load it using `load_dataset` functionality, the existing cache is ignored whenever the file is update even with `ignore_verifications=True`. This slows down debugging and cache generator for very large datasets.\r\n\r\n## Steps to reproduce the bug\r\n\r\n- Create a local dataset builder class\r\n- load the local builder class file using `load_dataset` and let the cache build\r\n- update the file's content\r\n- The cache should rebuilt.\r\n\r\n## Expected results\r\n\r\nWith `ignore_verifications=True`, `load_dataset` should pick up existing cache.\r\n\r\n## Actual results\r\n\r\nCreates new cache.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-5.4.0-52-generic-x86_64-with-debian-bullseye-sid\r\n- Python version: 3.7.7\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2561/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2554", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2554/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2554/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2554/events", "html_url": "https://github.com/huggingface/datasets/issues/2554", "id": 931453855, "node_id": "MDU6SXNzdWU5MzE0NTM4NTU=", "number": 2554, "title": "Multilabel metrics not supported", "user": {"login": "GuillemGSubies", "id": 37592763, "node_id": "MDQ6VXNlcjM3NTkyNzYz", "avatar_url": "https://avatars.githubusercontent.com/u/37592763?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GuillemGSubies", "html_url": "https://github.com/GuillemGSubies", "followers_url": "https://api.github.com/users/GuillemGSubies/followers", "following_url": "https://api.github.com/users/GuillemGSubies/following{/other_user}", "gists_url": "https://api.github.com/users/GuillemGSubies/gists{/gist_id}", "starred_url": "https://api.github.com/users/GuillemGSubies/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GuillemGSubies/subscriptions", "organizations_url": "https://api.github.com/users/GuillemGSubies/orgs", "repos_url": "https://api.github.com/users/GuillemGSubies/repos", "events_url": "https://api.github.com/users/GuillemGSubies/events{/privacy}", "received_events_url": "https://api.github.com/users/GuillemGSubies/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-06-28T11:09:46Z", "updated_at": "2021-10-13T12:29:13Z", "closed_at": "2021-07-08T08:40:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I try to use a metric like F1 macro I get the following error:\r\n\r\n```\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'list'\r\n```\r\nThere is an explicit casting here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/fc79f61cbbcfa0e8c68b28c0a8257f17e768a075/src/datasets/features.py#L274\r\n\r\nAnd looks like this is because here\r\n\r\nhttps://github.com/huggingface/datasets/blob/fc79f61cbbcfa0e8c68b28c0a8257f17e768a075/metrics/f1/f1.py#L88\r\n\r\nthe features can only be integers, so we cannot use that F1 for multilabel. Instead, if I create the following F1 (ints replaced with sequence of ints), it will work:\r\n\r\n```python\r\nclass F1(datasets.Metric):\r\n    def _info(self):\r\n        return datasets.MetricInfo(\r\n            description=_DESCRIPTION,\r\n            citation=_CITATION,\r\n            inputs_description=_KWARGS_DESCRIPTION,\r\n            features=datasets.Features(\r\n                {\r\n                    \"predictions\": datasets.Sequence(datasets.Value(\"int32\")),\r\n                    \"references\": datasets.Sequence(datasets.Value(\"int32\")),\r\n                }\r\n            ),\r\n            reference_urls=[\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\"],\r\n        )\r\n\r\n    def _compute(self, predictions, references, labels=None, pos_label=1, average=\"binary\", sample_weight=None):\r\n        return {\r\n            \"f1\": f1_score(\r\n                references,\r\n                predictions,\r\n                labels=labels,\r\n                pos_label=pos_label,\r\n                average=average,\r\n                sample_weight=sample_weight,\r\n            ),\r\n        }\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2554/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2553", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2553/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2553/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2553/events", "html_url": "https://github.com/huggingface/datasets/issues/2553", "id": 931365926, "node_id": "MDU6SXNzdWU5MzEzNjU5MjY=", "number": 2553, "title": "load_dataset(\"web_nlg\") NonMatchingChecksumError", "user": {"login": "alxthm", "id": 33730312, "node_id": "MDQ6VXNlcjMzNzMwMzEy", "avatar_url": "https://avatars.githubusercontent.com/u/33730312?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alxthm", "html_url": "https://github.com/alxthm", "followers_url": "https://api.github.com/users/alxthm/followers", "following_url": "https://api.github.com/users/alxthm/following{/other_user}", "gists_url": "https://api.github.com/users/alxthm/gists{/gist_id}", "starred_url": "https://api.github.com/users/alxthm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alxthm/subscriptions", "organizations_url": "https://api.github.com/users/alxthm/orgs", "repos_url": "https://api.github.com/users/alxthm/repos", "events_url": "https://api.github.com/users/alxthm/events{/privacy}", "received_events_url": "https://api.github.com/users/alxthm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-06-28T09:26:46Z", "updated_at": "2021-06-28T17:23:39Z", "closed_at": "2021-06-28T17:23:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi! It seems the WebNLG dataset gives a NonMatchingChecksumError.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('web_nlg', name=\"release_v3.0_en\", split=\"dev\")\r\n```\r\n\r\nGives\r\n\r\n```\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://gitlab.com/shimorina/webnlg-dataset/-/archive/master/webnlg-dataset-master.zip']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.0\r\n- Platform: macOS-11.3.1-x86_64-i386-64bit\r\n- Python version: 3.9.4\r\n- PyArrow version: 3.0.0\r\n\r\nAlso tested on Linux, with python 3.6.8", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2553/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2552", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2552/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2552/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2552/events", "html_url": "https://github.com/huggingface/datasets/issues/2552", "id": 931354687, "node_id": "MDU6SXNzdWU5MzEzNTQ2ODc=", "number": 2552, "title": "Keys should be unique error on code_search_net", "user": {"login": "thomwolf", "id": 7353373, "node_id": "MDQ6VXNlcjczNTMzNzM=", "avatar_url": "https://avatars.githubusercontent.com/u/7353373?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomwolf", "html_url": "https://github.com/thomwolf", "followers_url": "https://api.github.com/users/thomwolf/followers", "following_url": "https://api.github.com/users/thomwolf/following{/other_user}", "gists_url": "https://api.github.com/users/thomwolf/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomwolf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomwolf/subscriptions", "organizations_url": "https://api.github.com/users/thomwolf/orgs", "repos_url": "https://api.github.com/users/thomwolf/repos", "events_url": "https://api.github.com/users/thomwolf/events{/privacy}", "received_events_url": "https://api.github.com/users/thomwolf/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2021-06-28T09:15:20Z", "updated_at": "2021-09-06T14:08:30Z", "closed_at": "2021-09-02T08:25:29Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nLoading `code_search_net` seems not possible at the moment.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n>>> load_dataset('code_search_net')\r\nDownloading: 8.50kB [00:00, 3.09MB/s]                                                                                                                                           \r\nDownloading: 19.1kB [00:00, 10.1MB/s]                                                                                                                                           \r\nNo config specified, defaulting to: code_search_net/all\r\nDownloading and preparing dataset code_search_net/all (download: 4.77 GiB, generated: 5.99 GiB, post-processed: Unknown size, total: 10.76 GiB) to /Users/thomwolf/.cache/huggingface/datasets/code_search_net/all/1.0.0/b3e8278faf5d67da1d06981efbeac3b76a2900693bd2239bbca7a4a3b0d6e52a...\r\nTraceback (most recent call last):         \r\n  File \"/Users/thomwolf/Documents/GitHub/datasets/src/datasets/builder.py\", line 1067, in _prepare_split\r\n    writer.write(example, key)\r\n  File \"/Users/thomwolf/Documents/GitHub/datasets/src/datasets/arrow_writer.py\", line 343, in write\r\n    self.check_duplicate_keys()\r\n  File \"/Users/thomwolf/Documents/GitHub/datasets/src/datasets/arrow_writer.py\", line 354, in check_duplicate_keys\r\n    raise DuplicatedKeysError(key)\r\ndatasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 48\r\nKeys should be unique and deterministic in nature\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.1.dev0\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: 2.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2552/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2548", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2548/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2548/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2548/events", "html_url": "https://github.com/huggingface/datasets/issues/2548", "id": 929232831, "node_id": "MDU6SXNzdWU5MjkyMzI4MzE=", "number": 2548, "title": "Field order issue in loading json", "user": {"login": "luyug", "id": 55288513, "node_id": "MDQ6VXNlcjU1Mjg4NTEz", "avatar_url": "https://avatars.githubusercontent.com/u/55288513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luyug", "html_url": "https://github.com/luyug", "followers_url": "https://api.github.com/users/luyug/followers", "following_url": "https://api.github.com/users/luyug/following{/other_user}", "gists_url": "https://api.github.com/users/luyug/gists{/gist_id}", "starred_url": "https://api.github.com/users/luyug/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luyug/subscriptions", "organizations_url": "https://api.github.com/users/luyug/orgs", "repos_url": "https://api.github.com/users/luyug/repos", "events_url": "https://api.github.com/users/luyug/events{/privacy}", "received_events_url": "https://api.github.com/users/luyug/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-06-24T13:29:53Z", "updated_at": "2021-06-24T14:36:43Z", "closed_at": "2021-06-24T14:34:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nThe `load_dataset` function expects columns in alphabetical order when loading json files.\r\n\r\nSimilar bug was previously reported for csv in #623 and fixed in #684.\r\n## Steps to reproduce the bug\r\n\r\nFor a json file `j.json`,\r\n```\r\n{\"c\":321, \"a\": 1, \"b\": 2}\r\n```\r\nRunning the following,\r\n```\r\nf= datasets.Features({'a': Value('int32'), 'b': Value('int32'), 'c': Value('int32')})\r\njson_data = datasets.load_dataset('json', data_files='j.json', features=f)\r\n```\r\n\r\n\r\n## Expected results\r\nA successful load.\r\n## Actual results\r\n```\r\nFile \"pyarrow/table.pxi\", line 1409, in pyarrow.lib.Table.cast\r\nValueError: Target schema's field names are not matching the table's field names: ['c', 'a', 'b'], ['a', 'b', 'c']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-3.10.0-957.1.3.el7.x86_64-x86_64-with-glibc2.10\r\n- Python version: 3.8.8\r\n- PyArrow version: 3.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2548/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2542", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2542/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2542/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2542/events", "html_url": "https://github.com/huggingface/datasets/issues/2542", "id": 928540382, "node_id": "MDU6SXNzdWU5Mjg1NDAzODI=", "number": 2542, "title": "`datasets.keyhash.DuplicatedKeysError` for `drop` and `adversarial_qa/adversarialQA`", "user": {"login": "VictorSanh", "id": 16107619, "node_id": "MDQ6VXNlcjE2MTA3NjE5", "avatar_url": "https://avatars.githubusercontent.com/u/16107619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VictorSanh", "html_url": "https://github.com/VictorSanh", "followers_url": "https://api.github.com/users/VictorSanh/followers", "following_url": "https://api.github.com/users/VictorSanh/following{/other_user}", "gists_url": "https://api.github.com/users/VictorSanh/gists{/gist_id}", "starred_url": "https://api.github.com/users/VictorSanh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VictorSanh/subscriptions", "organizations_url": "https://api.github.com/users/VictorSanh/orgs", "repos_url": "https://api.github.com/users/VictorSanh/repos", "events_url": "https://api.github.com/users/VictorSanh/events{/privacy}", "received_events_url": "https://api.github.com/users/VictorSanh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2021-06-23T18:41:16Z", "updated_at": "2021-06-25T21:50:05Z", "closed_at": "2021-06-24T14:57:08Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nFailure to generate the datasets (`drop` and subset `adversarialQA` from `adversarial_qa`) because of duplicate keys.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"drop\")\r\nload_dataset(\"adversarial_qa\", \"adversarialQA\")\r\n```\r\n\r\n## Expected results\r\nThe examples keys should be unique.\r\n\r\n## Actual results\r\n```bash\r\n>>> load_dataset(\"drop\")\r\nUsing custom data configuration default\r\nDownloading and preparing dataset drop/default (download: 7.92 MiB, generated: 111.88 MiB, post-processed: Unknown size, total: 119.80 MiB) to /home/hf/.cache/huggingface/datasets/drop/default/0.1.0/7a94f1e2bb26c4b5c75f89857c06982967d7416e5af935a9374b9bccf5068026...\r\nTraceback (most recent call last):         \r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/load.py\", line 751, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 575, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 652, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 992, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 409, in finalize\r\n    self.check_duplicate_keys()\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 349, in check_duplicate_keys\r\n    raise DuplicatedKeysError(key)\r\ndatasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 28553293-d719-441b-8f00-ce3dc6df5398\r\nKeys should be unique and deterministic in nature\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.7.0\r\n- Platform: Linux-5.4.0-1044-gcp-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2542/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2542/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2532", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2532/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2532/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2532/events", "html_url": "https://github.com/huggingface/datasets/issues/2532", "id": 927063196, "node_id": "MDU6SXNzdWU5MjcwNjMxOTY=", "number": 2532, "title": "Tokenizer's normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task", "user": {"login": "jerryIsHere", "id": 50871412, "node_id": "MDQ6VXNlcjUwODcxNDEy", "avatar_url": "https://avatars.githubusercontent.com/u/50871412?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jerryIsHere", "html_url": "https://github.com/jerryIsHere", "followers_url": "https://api.github.com/users/jerryIsHere/followers", "following_url": "https://api.github.com/users/jerryIsHere/following{/other_user}", "gists_url": "https://api.github.com/users/jerryIsHere/gists{/gist_id}", "starred_url": "https://api.github.com/users/jerryIsHere/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jerryIsHere/subscriptions", "organizations_url": "https://api.github.com/users/jerryIsHere/orgs", "repos_url": "https://api.github.com/users/jerryIsHere/repos", "events_url": "https://api.github.com/users/jerryIsHere/events{/privacy}", "received_events_url": "https://api.github.com/users/jerryIsHere/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-06-22T10:08:18Z", "updated_at": "2021-06-23T05:17:25Z", "closed_at": "2021-06-23T05:17:25Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "[This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).\r\n\r\nThe pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don't know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:\r\n![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)\r\n\r\nWithout the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)\r\n\r\nIt is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str('\u30ff')` return '\u30b3\u30c8'.\r\n\r\nOne workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.\r\n\r\nI guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don't need to include them in their code. But I don't understand the code of tokenizer well that I think I am not able to do this.\r\n\r\np.s.\r\n**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**\r\n`get_dataset `is just a simple wrapping for `load_dataset`\r\nand the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")`", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2532/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2528", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2528/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2528/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2528/events", "html_url": "https://github.com/huggingface/datasets/issues/2528", "id": 926314656, "node_id": "MDU6SXNzdWU5MjYzMTQ2NTY=", "number": 2528, "title": "Logging cannot be set to NOTSET similar to transformers", "user": {"login": "joshzwiebel", "id": 34662010, "node_id": "MDQ6VXNlcjM0NjYyMDEw", "avatar_url": "https://avatars.githubusercontent.com/u/34662010?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joshzwiebel", "html_url": "https://github.com/joshzwiebel", "followers_url": "https://api.github.com/users/joshzwiebel/followers", "following_url": "https://api.github.com/users/joshzwiebel/following{/other_user}", "gists_url": "https://api.github.com/users/joshzwiebel/gists{/gist_id}", "starred_url": "https://api.github.com/users/joshzwiebel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joshzwiebel/subscriptions", "organizations_url": "https://api.github.com/users/joshzwiebel/orgs", "repos_url": "https://api.github.com/users/joshzwiebel/repos", "events_url": "https://api.github.com/users/joshzwiebel/events{/privacy}", "received_events_url": "https://api.github.com/users/joshzwiebel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2021-06-21T15:04:54Z", "updated_at": "2021-06-24T14:42:47Z", "closed_at": "2021-06-24T14:42:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nIn the transformers library you can set the verbosity level to logging.NOTSET to work around the usage of tqdm and IPywidgets, however in Datasets this is no longer possible. This is because transformers set the verbosity level of tqdm with [this](https://github.com/huggingface/transformers/blob/b53bc55ba9bb10d5ee279eab51a2f0acc5af2a6b/src/transformers/file_utils.py#L1449) \r\n`disable=bool(logging.get_verbosity() == logging.NOTSET)`\r\nand datasets accomplishes this like [so](https://github.com/huggingface/datasets/blob/83554e410e1ab8c6f705cfbb2df7953638ad3ac1/src/datasets/utils/file_utils.py#L493)\r\n`not_verbose = bool(logger.getEffectiveLevel() > WARNING)`\r\n## Steps to reproduce the bug\r\n```python\r\nimport datasets\r\nimport logging\r\ndatasets.logging.get_verbosity = lambda : logging.NOTSET\r\ndatasets.load_dataset(\"patrickvonplaten/librispeech_asr_dummy\")\r\n```\r\n\r\n## Expected results\r\nThe code should download and load the dataset as normal without displaying progress bars\r\n\r\n## Actual results\r\n```ImportError                               Traceback (most recent call last)\r\n<ipython-input-4-aec65c0509c6> in <module>\r\n----> 1 datasets.load_dataset(\"patrickvonplaten/librispeech_asr_dummy\")\r\n\r\n~/venv/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, **config_kwargs)\r\n    713         dataset=True,\r\n    714         return_resolved_file_path=True,\r\n--> 715         use_auth_token=use_auth_token,\r\n    716     )\r\n    717     # Set the base path for downloads as the parent of the script location\r\n\r\n~/venv/lib/python3.7/site-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, **download_kwargs)\r\n    350                     file_path = hf_bucket_url(path, filename=name, dataset=False)\r\n    351                 try:\r\n--> 352                     local_path = cached_path(file_path, download_config=download_config)\r\n    353                 except FileNotFoundError:\r\n    354                     raise FileNotFoundError(\r\n\r\n~/venv/lib/python3.7/site-packages/datasets/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    289             use_etag=download_config.use_etag,\r\n    290             max_retries=download_config.max_retries,\r\n--> 291             use_auth_token=download_config.use_auth_token,\r\n    292         )\r\n    293     elif os.path.exists(url_or_filename):\r\n\r\n~/venv/lib/python3.7/site-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\r\n    668                     headers=headers,\r\n    669                     cookies=cookies,\r\n--> 670                     max_retries=max_retries,\r\n    671                 )\r\n    672 \r\n\r\n~/venv/lib/python3.7/site-packages/datasets/utils/file_utils.py in http_get(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries)\r\n    493         initial=resume_size,\r\n    494         desc=\"Downloading\",\r\n--> 495         disable=not_verbose,\r\n    496     )\r\n    497     for chunk in response.iter_content(chunk_size=1024):\r\n\r\n~/venv/lib/python3.7/site-packages/tqdm/notebook.py in __init__(self, *args, **kwargs)\r\n    217         total = self.total * unit_scale if self.total else self.total\r\n    218         self.container = self.status_printer(\r\n--> 219             self.fp, total, self.desc, self.ncols)\r\n    220         self.sp = self.display\r\n    221 \r\n\r\n~/venv/lib/python3.7/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols)\r\n     95         if IProgress is None:  # #187 #451 #558 #872\r\n     96             raise ImportError(\r\n---> 97                 \"IProgress not found. Please update jupyter and ipywidgets.\"\r\n     98                 \" See https://ipywidgets.readthedocs.io/en/stable\"\r\n     99                 \"/user_install.html\")\r\n\r\nImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-5.4.95-42.163.amzn2.x86_64-x86_64-with-debian-10.8\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\nI am running this code on Deepnote and which important to this issue **does not** support IPywidgets\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2528/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2522", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2522/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2522/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2522/events", "html_url": "https://github.com/huggingface/datasets/issues/2522", "id": 925334379, "node_id": "MDU6SXNzdWU5MjUzMzQzNzk=", "number": 2522, "title": "Documentation Mistakes in Dataset: emotion", "user": {"login": "GDGauravDutta", "id": 62606251, "node_id": "MDQ6VXNlcjYyNjA2MjUx", "avatar_url": "https://avatars.githubusercontent.com/u/62606251?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GDGauravDutta", "html_url": "https://github.com/GDGauravDutta", "followers_url": "https://api.github.com/users/GDGauravDutta/followers", "following_url": "https://api.github.com/users/GDGauravDutta/following{/other_user}", "gists_url": "https://api.github.com/users/GDGauravDutta/gists{/gist_id}", "starred_url": "https://api.github.com/users/GDGauravDutta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GDGauravDutta/subscriptions", "organizations_url": "https://api.github.com/users/GDGauravDutta/orgs", "repos_url": "https://api.github.com/users/GDGauravDutta/repos", "events_url": "https://api.github.com/users/GDGauravDutta/events{/privacy}", "received_events_url": "https://api.github.com/users/GDGauravDutta/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-06-19T07:08:57Z", "updated_at": "2023-01-02T12:04:58Z", "closed_at": "2023-01-02T12:04:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "As per documentation,\r\nDataset: emotion\r\nHomepage: https://github.com/dair-ai/emotion_dataset\r\n\r\nDataset: https://github.com/huggingface/datasets/blob/master/datasets/emotion/emotion.py\r\n\r\nPermalink: https://huggingface.co/datasets/viewer/?dataset=emotion\r\n\r\nEmotion is a dataset of English Twitter messages with eight basic emotions: anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. For more detailed information please refer to the paper.\r\n\r\nBut when we view the data, there are only 6 emotions, anger, fear, joy, sadness, surprise, and trust.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2522/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2522/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2512", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2512/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2512/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2512/events", "html_url": "https://github.com/huggingface/datasets/issues/2512", "id": 924069353, "node_id": "MDU6SXNzdWU5MjQwNjkzNTM=", "number": 2512, "title": "seqeval metric does not work with a recent version of sklearn: classification_report() got an unexpected keyword argument 'output_dict'", "user": {"login": "avidale", "id": 8642136, "node_id": "MDQ6VXNlcjg2NDIxMzY=", "avatar_url": "https://avatars.githubusercontent.com/u/8642136?v=4", "gravatar_id": "", "url": "https://api.github.com/users/avidale", "html_url": "https://github.com/avidale", "followers_url": "https://api.github.com/users/avidale/followers", "following_url": "https://api.github.com/users/avidale/following{/other_user}", "gists_url": "https://api.github.com/users/avidale/gists{/gist_id}", "starred_url": "https://api.github.com/users/avidale/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/avidale/subscriptions", "organizations_url": "https://api.github.com/users/avidale/orgs", "repos_url": "https://api.github.com/users/avidale/repos", "events_url": "https://api.github.com/users/avidale/events{/privacy}", "received_events_url": "https://api.github.com/users/avidale/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-06-17T15:36:02Z", "updated_at": "2021-06-17T15:46:07Z", "closed_at": "2021-06-17T15:46:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\nseqeval = load_metric(\"seqeval\")\r\nseqeval.compute(predictions=[['A']], references=[['A']])\r\n```\r\n\r\n## Expected results\r\nThe function computes a dict with metrics\r\n\r\n## Actual results\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-39-69a57f5cf06f> in <module>\r\n      1 from datasets import load_dataset, load_metric\r\n      2 seqeval = load_metric(\"seqeval\")\r\n----> 3 seqeval.compute(predictions=[['A']], references=[['A']])\r\n\r\n~/p3/lib/python3.7/site-packages/datasets/metric.py in compute(self, *args, **kwargs)\r\n    396             references = self.data[\"references\"]\r\n    397             with temp_seed(self.seed):\r\n--> 398                 output = self._compute(predictions=predictions, references=references, **kwargs)\r\n    399 \r\n    400             if self.buf_writer is not None:\r\n\r\n~/.cache/huggingface/modules/datasets_modules/metrics/seqeval/81eda1ff004361d4fa48754a446ec69bb7aa9cf4d14c7215f407d1475941c5ff/seqeval.py in _compute(self, predictions, references, suffix)\r\n     95 \r\n     96     def _compute(self, predictions, references, suffix=False):\r\n---> 97         report = classification_report(y_true=references, y_pred=predictions, suffix=suffix, output_dict=True)\r\n     98         report.pop(\"macro avg\")\r\n     99         report.pop(\"weighted avg\")\r\n\r\nTypeError: classification_report() got an unexpected keyword argument 'output_dict'\r\n```\r\n\r\n## Environment info\r\nsklearn=0.24\r\ndatasets=1.1.3\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2512/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2475", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2475/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2475/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2475/events", "html_url": "https://github.com/huggingface/datasets/issues/2475", "id": 917650882, "node_id": "MDU6SXNzdWU5MTc2NTA4ODI=", "number": 2475, "title": "Issue in timit_asr database", "user": {"login": "hrahamim", "id": 85702107, "node_id": "MDQ6VXNlcjg1NzAyMTA3", "avatar_url": "https://avatars.githubusercontent.com/u/85702107?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hrahamim", "html_url": "https://github.com/hrahamim", "followers_url": "https://api.github.com/users/hrahamim/followers", "following_url": "https://api.github.com/users/hrahamim/following{/other_user}", "gists_url": "https://api.github.com/users/hrahamim/gists{/gist_id}", "starred_url": "https://api.github.com/users/hrahamim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hrahamim/subscriptions", "organizations_url": "https://api.github.com/users/hrahamim/orgs", "repos_url": "https://api.github.com/users/hrahamim/repos", "events_url": "https://api.github.com/users/hrahamim/events{/privacy}", "received_events_url": "https://api.github.com/users/hrahamim/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-06-10T18:05:29Z", "updated_at": "2021-06-13T08:13:50Z", "closed_at": "2021-06-13T08:13:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nI am trying to load the timit_asr dataset however only the first record is shown (duplicated over all the rows).\r\nI am using the next code line\r\ndataset = load_dataset(\u201ctimit_asr\u201d, split=\u201ctest\u201d).shuffle().select(range(10))\r\n\r\nThe above code result with the same sentence duplicated ten times.\r\nIt also happens when I use the dataset viewer at Streamlit .\r\n\r\n## Steps to reproduce the bug\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\u201ctimit_asr\u201d, split=\u201ctest\u201d).shuffle().select(range(10))\r\ndata = dataset.to_pandas()\r\n\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\ntable with different row information\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n\r\n- `datasets` version: 1.4.1 (also occur in the latest version)\r\n- Platform: Linux-4.15.0-143-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.8.1+cu102 (False)\r\n- Tensorflow version (GPU?): 1.15.3 (False)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n- `datasets` version:\r\n- Platform:\r\n- Python version:\r\n- PyArrow version:\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2475/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2472", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2472/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2472/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2472/events", "html_url": "https://github.com/huggingface/datasets/issues/2472", "id": 917463821, "node_id": "MDU6SXNzdWU5MTc0NjM4MjE=", "number": 2472, "title": "Fix automatic generation of Zenodo DOI", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/huggingface/datasets/milestones/5", "html_url": "https://github.com/huggingface/datasets/milestone/5", "labels_url": "https://api.github.com/repos/huggingface/datasets/milestones/5/labels", "id": 6808903, "node_id": "MDk6TWlsZXN0b25lNjgwODkwMw==", "number": 5, "title": "1.9", "description": "Next minor release", "creator": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 12, "state": "closed", "created_at": "2021-05-31T16:13:06Z", "updated_at": "2021-07-12T14:12:00Z", "due_on": "2021-07-08T07:00:00Z", "closed_at": "2021-07-09T05:50:07Z"}, "comments": 4, "created_at": "2021-06-10T15:15:46Z", "updated_at": "2021-06-14T16:49:42Z", "closed_at": "2021-06-14T16:49:42Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "After the last release of Datasets (1.8.0), the automatic generation of the Zenodo DOI failed: it appears in yellow as \"Received\", instead of in green as \"Published\".\r\n\r\nI have contacted Zenodo support to fix this issue.\r\n\r\nTODO:\r\n- [x] Check with Zenodo to fix the issue\r\n- [x] Check BibTeX entry is right", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2472/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2471", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2471/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2471/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2471/events", "html_url": "https://github.com/huggingface/datasets/issues/2471", "id": 917067165, "node_id": "MDU6SXNzdWU5MTcwNjcxNjU=", "number": 2471, "title": "Fix PermissionError on Windows when using tqdm >=4.50.0", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/huggingface/datasets/milestones/5", "html_url": "https://github.com/huggingface/datasets/milestone/5", "labels_url": "https://api.github.com/repos/huggingface/datasets/milestones/5/labels", "id": 6808903, "node_id": "MDk6TWlsZXN0b25lNjgwODkwMw==", "number": 5, "title": "1.9", "description": "Next minor release", "creator": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 12, "state": "closed", "created_at": "2021-05-31T16:13:06Z", "updated_at": "2021-07-12T14:12:00Z", "due_on": "2021-07-08T07:00:00Z", "closed_at": "2021-07-09T05:50:07Z"}, "comments": 0, "created_at": "2021-06-10T08:31:49Z", "updated_at": "2021-06-11T15:11:50Z", "closed_at": "2021-06-11T15:11:50Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "See: https://app.circleci.com/pipelines/github/huggingface/datasets/235/workflows/cfb6a39f-68eb-4802-8b17-2cd5e8ea7369/jobs/1111\r\n\r\n```\r\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2471/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2470", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2470/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2470/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2470/events", "html_url": "https://github.com/huggingface/datasets/issues/2470", "id": 916724260, "node_id": "MDU6SXNzdWU5MTY3MjQyNjA=", "number": 2470, "title": "Crash when `num_proc` > dataset length for `map()` on a `datasets.Dataset`.", "user": {"login": "mbforbes", "id": 1170062, "node_id": "MDQ6VXNlcjExNzAwNjI=", "avatar_url": "https://avatars.githubusercontent.com/u/1170062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mbforbes", "html_url": "https://github.com/mbforbes", "followers_url": "https://api.github.com/users/mbforbes/followers", "following_url": "https://api.github.com/users/mbforbes/following{/other_user}", "gists_url": "https://api.github.com/users/mbforbes/gists{/gist_id}", "starred_url": "https://api.github.com/users/mbforbes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mbforbes/subscriptions", "organizations_url": "https://api.github.com/users/mbforbes/orgs", "repos_url": "https://api.github.com/users/mbforbes/repos", "events_url": "https://api.github.com/users/mbforbes/events{/privacy}", "received_events_url": "https://api.github.com/users/mbforbes/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-06-09T22:40:22Z", "updated_at": "2021-07-01T09:34:54Z", "closed_at": "2021-07-01T09:11:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nCrash if when using `num_proc` > 1 (I used 16) for `map()` on a `datasets.Dataset`.\r\n\r\nI believe I've had cases where `num_proc` > 1 works before, but now it seems either inconsistent, or depends on my data. I'm not sure whether the issue is on my end, because it's difficult for me to debug! Any tips greatly appreciated, I'm happy to provide more info if it would helps us diagnose.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# this function will be applied with map()\r\ndef tokenize_function(examples):\r\n    return tokenizer(\r\n        examples[\"text\"],\r\n        padding=PaddingStrategy.DO_NOT_PAD,\r\n        truncation=True,\r\n    )\r\n\r\n# data_files is a Dict[str, str] mapping name -> path\r\ndatasets = load_dataset(\"text\", data_files={...})  \r\n\r\n# this is where the error happens if num_proc = 16,\r\n# but is fine if num_proc = 1\r\ntokenized_datasets = datasets.map(\r\n    tokenize_function,\r\n    batched=True,\r\n    num_proc=num_workers,\r\n)\r\n```\r\n\r\n## Expected results\r\nThe `map()` function succeeds with `num_proc` > 1.\r\n\r\n## Actual results\r\n![image](https://user-images.githubusercontent.com/1170062/121404271-a6cc5200-c910-11eb-8e27-5c893bd04042.png)\r\n![image](https://user-images.githubusercontent.com/1170062/121404362-be0b3f80-c910-11eb-9117-658943029aef.png)\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.2\r\n- Platform: Linux-5.4.0-73-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.5\r\n- PyTorch version (GPU?): 1.8.1+cu111 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: Yes, but I think N/A for this issue\r\n- Using distributed or parallel set-up in script?: Multi-GPU on one machine, but I think also N/A for this issue\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2470/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2459", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2459/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2459/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2459/events", "html_url": "https://github.com/huggingface/datasets/issues/2459", "id": 915222015, "node_id": "MDU6SXNzdWU5MTUyMjIwMTU=", "number": 2459, "title": "`Proto_qa` hosting seems to be broken", "user": {"login": "VictorSanh", "id": 16107619, "node_id": "MDQ6VXNlcjE2MTA3NjE5", "avatar_url": "https://avatars.githubusercontent.com/u/16107619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VictorSanh", "html_url": "https://github.com/VictorSanh", "followers_url": "https://api.github.com/users/VictorSanh/followers", "following_url": "https://api.github.com/users/VictorSanh/following{/other_user}", "gists_url": "https://api.github.com/users/VictorSanh/gists{/gist_id}", "starred_url": "https://api.github.com/users/VictorSanh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VictorSanh/subscriptions", "organizations_url": "https://api.github.com/users/VictorSanh/orgs", "repos_url": "https://api.github.com/users/VictorSanh/repos", "events_url": "https://api.github.com/users/VictorSanh/events{/privacy}", "received_events_url": "https://api.github.com/users/VictorSanh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-06-08T16:16:32Z", "updated_at": "2021-06-10T08:31:09Z", "closed_at": "2021-06-10T08:31:09Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nThe hosting (on Github) of the `proto_qa` dataset seems broken. I haven't investigated more yet, just flagging it for now. \r\n\r\n@zaidalyafeai if you want to dive into it, I think it's just a matter of changing the links in `proto_qa.py`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"proto_qa\")\r\n```\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/load.py\", line 751, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 575, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py\", line 630, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/hf/.cache/huggingface/modules/datasets_modules/datasets/proto_qa/445346efaad5c5f200ecda4aa7f0fb50ff1b55edde3003be424a2112c3e8102e/proto_qa.py\", line 131, in _split_generators\r\n    train_fpath = dl_manager.download(_URLs[self.config.name][\"train\"])\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 199, in download\r\n    num_proc=download_config.num_proc,\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/py_utils.py\", line 195, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/download_manager.py\", line 218, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 291, in cached_path\r\n    use_auth_token=download_config.use_auth_token,\r\n  File \"/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/file_utils.py\", line 621, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/iesl/protoqa-data/master/data/train/protoqa_train.jsonl\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2459/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2459/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2452", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2452/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2452/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2452/events", "html_url": "https://github.com/huggingface/datasets/issues/2452", "id": 913603877, "node_id": "MDU6SXNzdWU5MTM2MDM4Nzc=", "number": 2452, "title": "MRPC test set differences between torch and tensorflow datasets", "user": {"login": "FredericOdermatt", "id": 50372080, "node_id": "MDQ6VXNlcjUwMzcyMDgw", "avatar_url": "https://avatars.githubusercontent.com/u/50372080?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FredericOdermatt", "html_url": "https://github.com/FredericOdermatt", "followers_url": "https://api.github.com/users/FredericOdermatt/followers", "following_url": "https://api.github.com/users/FredericOdermatt/following{/other_user}", "gists_url": "https://api.github.com/users/FredericOdermatt/gists{/gist_id}", "starred_url": "https://api.github.com/users/FredericOdermatt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FredericOdermatt/subscriptions", "organizations_url": "https://api.github.com/users/FredericOdermatt/orgs", "repos_url": "https://api.github.com/users/FredericOdermatt/repos", "events_url": "https://api.github.com/users/FredericOdermatt/events{/privacy}", "received_events_url": "https://api.github.com/users/FredericOdermatt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-06-07T14:20:26Z", "updated_at": "2021-06-07T14:34:32Z", "closed_at": "2021-06-07T14:34:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen using `load_dataset(\"glue\", \"mrpc\")` to load the MRPC dataset, the test set includes the labels. When using `tensorflow_datasets.load('glue/{}'.format('mrpc'))` to load the dataset the test set does not contain the labels. There should be consistency between torch and tensorflow ways of importing the GLUE datasets.\r\n\r\n## Steps to reproduce the bug\r\n\r\nMinimal working code  \r\n```python\r\nfrom datasets import load_dataset\r\nimport tensorflow as tf\r\nimport tensorflow_datasets\r\n\r\n# torch\r\ndataset = load_dataset(\"glue\", \"mrpc\")\r\n# tf\r\ndata = tensorflow_datasets.load('glue/{}'.format('mrpc'))\r\ndata = list(data['test'].as_numpy_iterator())\r\nfor i in range(40,50):\r\n  tf_sentence1 = data[i]['sentence1'].decode(\"utf-8\") \r\n  tf_sentence2 = data[i]['sentence2'].decode(\"utf-8\") \r\n\r\n  tf_label = data[i]['label']\r\n  \r\n  index = data[i]['idx']\r\n  print('Index {}'.format(index))\r\n  torch_sentence1 = dataset['test']['sentence1'][index]\r\n  torch_sentence2 = dataset['test']['sentence2'][index]\r\n\r\n  torch_label = dataset['test']['label'][index]\r\n  print('Tensorflow: \\n\\tSentence1 {}\\n\\tSentence2 {}\\n\\tLabel {}'.format(tf_sentence1, tf_sentence2, tf_label))\r\n  print('Torch: \\n\\tSentence1 {}\\n\\tSentence2 {}\\n\\tLabel {}'.format(torch_sentence1, torch_sentence2, torch_label))\r\n```\r\n\r\nSample output  \r\n```\r\nIndex 954\r\nTensorflow: \r\n\tSentence1 Sabri Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate yesterday on charges of violating U.S. arms-control laws .\r\n\tSentence2 The elder Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate Wednesday on charges of violating U.S. arms control laws .\r\n\tLabel -1\r\nTorch: \r\n\tSentence1 Sabri Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate yesterday on charges of violating U.S. arms-control laws .\r\n\tSentence2 The elder Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate Wednesday on charges of violating U.S. arms control laws .\r\n\tLabel 1\r\nIndex 711\r\nTensorflow: \r\n\tSentence1 Others keep records sealed for as little as five years or as much as 30 .\r\n\tSentence2 Some states make them available immediately ; others keep them sealed for as much as 30 years .\r\n\tLabel -1\r\nTorch: \r\n\tSentence1 Others keep records sealed for as little as five years or as much as 30 .\r\n\tSentence2 Some states make them available immediately ; others keep them sealed for as much as 30 years .\r\n\tLabel 0\r\n```\r\n\r\n## Expected results\r\nI would expect the datasets to be independent of whether I am working with torch or tensorflow.\r\n\r\n## Actual results\r\nTest set labels are provided in the `datasets.load_datasets()` for MRPC. However MRPC is the only task where the test set labels are not -1.\r\n\r\n## Environment info\r\n- `datasets` version: 1.7.0\r\n- Platform: Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2452/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2447", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2447/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2447/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2447/events", "html_url": "https://github.com/huggingface/datasets/issues/2447", "id": 912299527, "node_id": "MDU6SXNzdWU5MTIyOTk1Mjc=", "number": 2447, "title": "dataset adversarial_qa has no answers in the \"test\" set", "user": {"login": "bjascob", "id": 22728060, "node_id": "MDQ6VXNlcjIyNzI4MDYw", "avatar_url": "https://avatars.githubusercontent.com/u/22728060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bjascob", "html_url": "https://github.com/bjascob", "followers_url": "https://api.github.com/users/bjascob/followers", "following_url": "https://api.github.com/users/bjascob/following{/other_user}", "gists_url": "https://api.github.com/users/bjascob/gists{/gist_id}", "starred_url": "https://api.github.com/users/bjascob/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bjascob/subscriptions", "organizations_url": "https://api.github.com/users/bjascob/orgs", "repos_url": "https://api.github.com/users/bjascob/repos", "events_url": "https://api.github.com/users/bjascob/events{/privacy}", "received_events_url": "https://api.github.com/users/bjascob/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-06-05T14:57:38Z", "updated_at": "2021-06-07T11:13:07Z", "closed_at": "2021-06-07T11:13:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen loading the adversarial_qa dataset the 'test' portion has no answers.  Only the 'train' and 'validation' portions do.  This occurs with all four of the configs ('adversarialQA', 'dbidaf', 'dbert', 'droberta')\r\n\r\n## Steps to reproduce the bug\r\n```\r\nfrom   datasets import load_dataset\r\nexamples = load_dataset('adversarial_qa', 'adversarialQA', script_version=\"master\")['test']\r\nprint('Loaded {:,} examples'.format(len(examples)))\r\nhas_answers = 0\r\nfor e in examples:\r\n    if e['answers']['text']:\r\n        has_answers += 1\r\nprint('{:,} have answers'.format(has_answers))\r\n>>> Loaded 3,000 examples\r\n>>> 0 have answers\r\n\r\nexamples = load_dataset('adversarial_qa', 'adversarialQA', script_version=\"master\")['validation']\r\n<...code above...>\r\n>>> Loaded 3,000 examples\r\n>>> 3,000 have answers\r\n```\r\n\r\n## Expected results\r\nIf 'test' is a valid dataset, it should have answers. Also note that all of the 'train' and 'validation' sets have answers, there are no \"no answer\" questions with this set (not sure if this is correct or not).\r\n\r\n## Environment info\r\n- `datasets` version: 1.7.0\r\n- Platform: Linux-5.8.0-53-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.5\r\n- PyArrow version: 1.0.0\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2447/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2444", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2444/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2444/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2444/events", "html_url": "https://github.com/huggingface/datasets/issues/2444", "id": 911297139, "node_id": "MDU6SXNzdWU5MTEyOTcxMzk=", "number": 2444, "title": "Sentence Boundaries missing in Dataset: xtreme / udpos", "user": {"login": "jerryIsHere", "id": 50871412, "node_id": "MDQ6VXNlcjUwODcxNDEy", "avatar_url": "https://avatars.githubusercontent.com/u/50871412?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jerryIsHere", "html_url": "https://github.com/jerryIsHere", "followers_url": "https://api.github.com/users/jerryIsHere/followers", "following_url": "https://api.github.com/users/jerryIsHere/following{/other_user}", "gists_url": "https://api.github.com/users/jerryIsHere/gists{/gist_id}", "starred_url": "https://api.github.com/users/jerryIsHere/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jerryIsHere/subscriptions", "organizations_url": "https://api.github.com/users/jerryIsHere/orgs", "repos_url": "https://api.github.com/users/jerryIsHere/repos", "events_url": "https://api.github.com/users/jerryIsHere/events{/privacy}", "received_events_url": "https://api.github.com/users/jerryIsHere/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-06-04T09:10:26Z", "updated_at": "2021-06-18T11:53:43Z", "closed_at": "2021-06-18T11:53:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I was browsing through annotation guidelines, as suggested by the datasets introduction.\r\n\r\nThe guidlines saids \"There must be exactly one blank line after every sentence, including the last sentence in the file. Empty sentences are not allowed.\" in the [Sentence Boundaries and Comments section](https://universaldependencies.org/format.html#sentence-boundaries-and-comments)\r\n\r\nBut the sentence boundaries seems not to be represented by huggingface datasets features well. I found out that multiple sentence are concatenated together as a 1D array, without any delimiter.\r\n\r\nPAN-x, which is another token classification subset from xtreme do represent the sentence boundary using a 2D array.\r\n\r\nYou may compare in PAN-x.en and udpos.English in the explorer:\r\n https://huggingface.co/datasets/viewer/?dataset=xtreme", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2444/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2444/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2443", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2443/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2443/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2443/events", "html_url": "https://github.com/huggingface/datasets/issues/2443", "id": 909983574, "node_id": "MDU6SXNzdWU5MDk5ODM1NzQ=", "number": 2443, "title": "Some tests hang on Windows", "user": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-06-03T00:27:30Z", "updated_at": "2021-06-28T08:47:39Z", "closed_at": "2021-06-28T08:47:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Currently, several tests hang on Windows if the max path limit of 260 characters is not disabled. This happens due to the changes introduced by #2223 that cause an infinite loop in `WindowsFileLock` described in #2220.  This can be very tricky to debug, so I think now is a good time to address these issues/PRs. IMO throwing an error is too harsh, but maybe we can emit a warning in the top-level `__init__.py ` on startup if long paths are not enabled.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2443/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2441", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2441/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2441/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2441/events", "html_url": "https://github.com/huggingface/datasets/issues/2441", "id": 908554713, "node_id": "MDU6SXNzdWU5MDg1NTQ3MTM=", "number": 2441, "title": "DuplicatedKeysError on personal dataset", "user": {"login": "lucaguarro", "id": 22605313, "node_id": "MDQ6VXNlcjIyNjA1MzEz", "avatar_url": "https://avatars.githubusercontent.com/u/22605313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucaguarro", "html_url": "https://github.com/lucaguarro", "followers_url": "https://api.github.com/users/lucaguarro/followers", "following_url": "https://api.github.com/users/lucaguarro/following{/other_user}", "gists_url": "https://api.github.com/users/lucaguarro/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucaguarro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucaguarro/subscriptions", "organizations_url": "https://api.github.com/users/lucaguarro/orgs", "repos_url": "https://api.github.com/users/lucaguarro/repos", "events_url": "https://api.github.com/users/lucaguarro/events{/privacy}", "received_events_url": "https://api.github.com/users/lucaguarro/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-06-01T17:59:41Z", "updated_at": "2021-06-04T23:50:03Z", "closed_at": "2021-06-04T23:50:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nEver since today, I have been getting a DuplicatedKeysError while trying to load my dataset from my own script.\r\nError returned when running this line: `dataset = load_dataset('/content/drive/MyDrive/Thesis/Datasets/book_preprocessing/goodreads_maharjan_trimmed_and_nered/goodreadsnered.py')`\r\nNote that my script was working fine with earlier versions of the Datasets library. Cannot say with 100% certainty if I have been doing something wrong with my dataset script this whole time or if this is simply a bug with the new version of datasets.\r\n\r\n## Steps to reproduce the bug\r\nI cannot provide code to reproduce the error as I am working with my own dataset. I can however provide my script if requested.\r\n\r\n## Expected results\r\nFor my data to be loaded.\r\n\r\n## Actual results\r\n**DuplicatedKeysError** exception is raised\r\n```\r\nDownloading and preparing dataset good_reads_practice_dataset/main_domain (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/good_reads_practice_dataset/main_domain/1.1.0/64ff7c3fee2693afdddea75002eb6887d4fedc3d812ae3622128c8504ab21655...\r\n\r\n---------------------------------------------------------------------------\r\n\r\nDuplicatedKeysError                       Traceback (most recent call last)\r\n\r\n<ipython-input-6-c342ea0dae9d> in <module>()\r\n----> 1 dataset = load_dataset('/content/drive/MyDrive/Thesis/Datasets/book_preprocessing/goodreads_maharjan_trimmed_and_nered/goodreadsnered.py')\r\n\r\n5 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, **config_kwargs)\r\n    749         try_from_hf_gcs=try_from_hf_gcs,\r\n    750         base_path=base_path,\r\n--> 751         use_auth_token=use_auth_token,\r\n    752     )\r\n    753 \r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    573                     if not downloaded_from_gcs:\r\n    574                         self._download_and_prepare(\r\n--> 575                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    576                         )\r\n    577                     # Sync info\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    650             try:\r\n    651                 # Prepare split will record examples associated to the split\r\n--> 652                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    653             except OSError as e:\r\n    654                 raise OSError(\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _prepare_split(self, split_generator)\r\n    990                     writer.write(example, key)\r\n    991             finally:\r\n--> 992                 num_examples, num_bytes = writer.finalize()\r\n    993 \r\n    994         split_generator.split_info.num_examples = num_examples\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in finalize(self, close_stream)\r\n    407         # In case current_examples < writer_batch_size, but user uses finalize()\r\n    408         if self._check_duplicates:\r\n--> 409             self.check_duplicate_keys()\r\n    410             # Re-intializing to empty list for next batch\r\n    411             self.hkey_record = []\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in check_duplicate_keys(self)\r\n    347         for hash, key in self.hkey_record:\r\n    348             if hash in tmp_record:\r\n--> 349                 raise DuplicatedKeysError(key)\r\n    350             else:\r\n    351                 tmp_record.add(hash)\r\n\r\nDuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 0\r\nKeys should be unique and deterministic in nature\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.7.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.7.9\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2441/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2441/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2440", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2440/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2440/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2440/events", "html_url": "https://github.com/huggingface/datasets/issues/2440", "id": 908521954, "node_id": "MDU6SXNzdWU5MDg1MjE5NTQ=", "number": 2440, "title": "Remove `extended` field from dataset tagger", "user": {"login": "lewtun", "id": 26859204, "node_id": "MDQ6VXNlcjI2ODU5MjA0", "avatar_url": "https://avatars.githubusercontent.com/u/26859204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lewtun", "html_url": "https://github.com/lewtun", "followers_url": "https://api.github.com/users/lewtun/followers", "following_url": "https://api.github.com/users/lewtun/following{/other_user}", "gists_url": "https://api.github.com/users/lewtun/gists{/gist_id}", "starred_url": "https://api.github.com/users/lewtun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lewtun/subscriptions", "organizations_url": "https://api.github.com/users/lewtun/orgs", "repos_url": "https://api.github.com/users/lewtun/repos", "events_url": "https://api.github.com/users/lewtun/events{/privacy}", "received_events_url": "https://api.github.com/users/lewtun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-06-01T17:18:42Z", "updated_at": "2021-06-09T09:06:31Z", "closed_at": "2021-06-09T09:06:30Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nWhile working on #2435 I used the [dataset tagger](https://huggingface.co/datasets/tagging/) to generate the missing tags for the YAML metadata of each README.md file. However, it seems that our CI raises an error when the `extended` field is included:\r\n\r\n```\r\ndataset_name = 'arcd'\r\n\r\n    @pytest.mark.parametrize(\"dataset_name\", get_changed_datasets(repo_path))\r\n    def test_changed_dataset_card(dataset_name):\r\n        card_path = repo_path / \"datasets\" / dataset_name / \"README.md\"\r\n        assert card_path.exists()\r\n        error_messages = []\r\n        try:\r\n            ReadMe.from_readme(card_path)\r\n        except Exception as readme_error:\r\n            error_messages.append(f\"The following issues have been found in the dataset cards:\\nREADME:\\n{readme_error}\")\r\n        try:\r\n            DatasetMetadata.from_readme(card_path)\r\n        except Exception as metadata_error:\r\n            error_messages.append(\r\n                f\"The following issues have been found in the dataset cards:\\nYAML tags:\\n{metadata_error}\"\r\n            )\r\n    \r\n        if error_messages:\r\n>           raise ValueError(\"\\n\".join(error_messages))\r\nE           ValueError: The following issues have been found in the dataset cards:\r\nE           YAML tags:\r\nE           __init__() got an unexpected keyword argument 'extended'\r\n\r\ntests/test_dataset_cards.py:70: ValueError\r\n```\r\n\r\nConsider either removing this tag from the tagger or including it as part of the validation step in the CI.\r\n\r\ncc @yjernite ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2440/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2440/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2431", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2431/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2431/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2431/events", "html_url": "https://github.com/huggingface/datasets/issues/2431", "id": 907413691, "node_id": "MDU6SXNzdWU5MDc0MTM2OTE=", "number": 2431, "title": "DuplicatedKeysError when trying to load adversarial_qa", "user": {"login": "hanss0n", "id": 21348833, "node_id": "MDQ6VXNlcjIxMzQ4ODMz", "avatar_url": "https://avatars.githubusercontent.com/u/21348833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hanss0n", "html_url": "https://github.com/hanss0n", "followers_url": "https://api.github.com/users/hanss0n/followers", "following_url": "https://api.github.com/users/hanss0n/following{/other_user}", "gists_url": "https://api.github.com/users/hanss0n/gists{/gist_id}", "starred_url": "https://api.github.com/users/hanss0n/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hanss0n/subscriptions", "organizations_url": "https://api.github.com/users/hanss0n/orgs", "repos_url": "https://api.github.com/users/hanss0n/repos", "events_url": "https://api.github.com/users/hanss0n/events{/privacy}", "received_events_url": "https://api.github.com/users/hanss0n/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-05-31T12:11:19Z", "updated_at": "2021-06-01T08:54:03Z", "closed_at": "2021-06-01T08:52:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = load_dataset('adversarial_qa', 'adversarialQA')\r\n```\r\n\r\n## Expected results\r\nThe dataset should be loaded into memory\r\n\r\n## Actual results\r\n\r\n>DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\n>Found duplicate Key: 4d3cb5677211ee32895ca9c66dad04d7152254d4\r\n>Keys should be unique and deterministic in nature\r\n>\r\n>\r\n>During handling of the above exception, another exception occurred:\r\n>\r\n>DuplicatedKeysError                       Traceback (most recent call last)\r\n>\r\n>/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in check_duplicate_keys(self)\r\n>    347         for hash, key in self.hkey_record:\r\n>    348             if hash in tmp_record:\r\n>--> 349                 raise DuplicatedKeysError(key)\r\n>    350             else:\r\n>    351                 tmp_record.add(hash)\r\n>\r\n>DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\n>Found duplicate Key: 4d3cb5677211ee32895ca9c66dad04d7152254d4\r\n>Keys should be unique and deterministic in nature\r\n\r\n## Environment info\r\n- `datasets` version: 1.7.0\r\n- Platform: Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2431/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2415", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2415/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2415/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2415/events", "html_url": "https://github.com/huggingface/datasets/issues/2415", "id": 903923097, "node_id": "MDU6SXNzdWU5MDM5MjMwOTc=", "number": 2415, "title": "Cached dataset not loaded", "user": {"login": "borisdayma", "id": 715491, "node_id": "MDQ6VXNlcjcxNTQ5MQ==", "avatar_url": "https://avatars.githubusercontent.com/u/715491?v=4", "gravatar_id": "", "url": "https://api.github.com/users/borisdayma", "html_url": "https://github.com/borisdayma", "followers_url": "https://api.github.com/users/borisdayma/followers", "following_url": "https://api.github.com/users/borisdayma/following{/other_user}", "gists_url": "https://api.github.com/users/borisdayma/gists{/gist_id}", "starred_url": "https://api.github.com/users/borisdayma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/borisdayma/subscriptions", "organizations_url": "https://api.github.com/users/borisdayma/orgs", "repos_url": "https://api.github.com/users/borisdayma/repos", "events_url": "https://api.github.com/users/borisdayma/events{/privacy}", "received_events_url": "https://api.github.com/users/borisdayma/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-05-27T15:40:06Z", "updated_at": "2021-06-02T13:15:47Z", "closed_at": "2021-06-02T13:15:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nI have a large dataset (common_voice, english) where I use several map and filter functions.\r\nSometimes my cached datasets after specific functions are not loaded.\r\nI always use the same arguments, same functions, no seed\u2026\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndef filter_by_duration(batch):\r\n    return (\r\n        batch[\"duration\"] <= 10\r\n        and batch[\"duration\"] >= 1\r\n        and len(batch[\"target_text\"]) > 5\r\n    )\r\n\r\ndef prepare_dataset(batch):\r\n    batch[\"input_values\"] = processor(\r\n        batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]\r\n    ).input_values\r\n    with processor.as_target_processor():\r\n        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\r\n    return batch\r\n\r\ntrain_dataset = train_dataset.filter(\r\n    filter_by_duration,\r\n    remove_columns=[\"duration\"],\r\n    num_proc=data_args.preprocessing_num_workers,\r\n)\r\n\r\n# PROBLEM HERE -> below function is reexecuted and cache is not loaded\r\ntrain_dataset = train_dataset.map(\r\n    prepare_dataset,\r\n    remove_columns=train_dataset.column_names,\r\n    batch_size=training_args.per_device_train_batch_size,\r\n    batched=True,\r\n    num_proc=data_args.preprocessing_num_workers,\r\n)\r\n\r\n# Later in script\r\nset_caching_enabled(False)\r\n# apply map on trained model to eval/test sets\r\n\r\n```\r\n\r\n## Expected results\r\nThe cached dataset should always be reloaded.\r\n\r\n## Actual results\r\nThe function is reexecuted.\r\n\r\nI have access to cached files `cache-xxxxx.arrow`.\r\nIs there a way I can somehow load manually 2 versions and see how the hash was created for debug purposes (to know if it's an issue with dataset or function)?\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.2\r\n- Platform: Linux-5.8.0-45-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.5\r\n- PyTorch version (GPU?): 1.8.1+cu102 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: Yes\r\n- Using distributed or parallel set-up in script?: No", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2415/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2413", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2413/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2413/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2413/events", "html_url": "https://github.com/huggingface/datasets/issues/2413", "id": 903777557, "node_id": "MDU6SXNzdWU5MDM3Nzc1NTc=", "number": 2413, "title": "AttributeError: 'DatasetInfo' object has no attribute 'task_templates'", "user": {"login": "jungwhank", "id": 53588015, "node_id": "MDQ6VXNlcjUzNTg4MDE1", "avatar_url": "https://avatars.githubusercontent.com/u/53588015?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jungwhank", "html_url": "https://github.com/jungwhank", "followers_url": "https://api.github.com/users/jungwhank/followers", "following_url": "https://api.github.com/users/jungwhank/following{/other_user}", "gists_url": "https://api.github.com/users/jungwhank/gists{/gist_id}", "starred_url": "https://api.github.com/users/jungwhank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jungwhank/subscriptions", "organizations_url": "https://api.github.com/users/jungwhank/orgs", "repos_url": "https://api.github.com/users/jungwhank/repos", "events_url": "https://api.github.com/users/jungwhank/events{/privacy}", "received_events_url": "https://api.github.com/users/jungwhank/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-05-27T13:44:28Z", "updated_at": "2021-06-01T01:05:47Z", "closed_at": "2021-06-01T01:05:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nHello, \r\nI'm trying to add dataset and contribute, but test keep fail with below cli.\r\n` RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_<my_dataset>`\r\n\r\n## Steps to reproduce the bug\r\nIt seems like a bug when I see an error with the existing dataset, not the dataset I'm trying to add.\r\n\r\n` RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_<any_dataset>`\r\n\r\n\r\n## Expected results\r\nAll test passed\r\n\r\n## Actual results\r\n```\r\n                # check that dataset is not empty\r\n                self.parent.assertListEqual(sorted(dataset_builder.info.splits.keys()), sorted(dataset))\r\n                for split in dataset_builder.info.splits.keys():\r\n                    # check that loaded datset is not empty\r\n                    self.parent.assertTrue(len(dataset[split]) > 0)\r\n    \r\n                # check that we can cast features for each task template\r\n>               task_templates = dataset_builder.info.task_templates\r\nE               AttributeError: 'DatasetInfo' object has no attribute 'task_templates'\r\n\r\ntests/test_dataset_common.py:175: AttributeError\r\n```\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.2\r\n- Platform: Darwin-20.4.0-x86_64-i386-64bit\r\n- Python version: 3.7.7\r\n- PyTorch version (GPU?): 1.7.0 (False)\r\n- Tensorflow version (GPU?): 2.3.0 (False)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2413/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2413/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2407", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2407/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2407/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2407/events", "html_url": "https://github.com/huggingface/datasets/issues/2407", "id": 903111755, "node_id": "MDU6SXNzdWU5MDMxMTE3NTU=", "number": 2407, "title": ".map() function got an unexpected keyword argument 'cache_file_name'", "user": {"login": "cindyxinyiwang", "id": 7390482, "node_id": "MDQ6VXNlcjczOTA0ODI=", "avatar_url": "https://avatars.githubusercontent.com/u/7390482?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cindyxinyiwang", "html_url": "https://github.com/cindyxinyiwang", "followers_url": "https://api.github.com/users/cindyxinyiwang/followers", "following_url": "https://api.github.com/users/cindyxinyiwang/following{/other_user}", "gists_url": "https://api.github.com/users/cindyxinyiwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/cindyxinyiwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cindyxinyiwang/subscriptions", "organizations_url": "https://api.github.com/users/cindyxinyiwang/orgs", "repos_url": "https://api.github.com/users/cindyxinyiwang/repos", "events_url": "https://api.github.com/users/cindyxinyiwang/events{/privacy}", "received_events_url": "https://api.github.com/users/cindyxinyiwang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-05-27T01:54:26Z", "updated_at": "2021-05-27T13:46:40Z", "closed_at": "2021-05-27T13:46:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nI'm trying to save the result of datasets.map() to a specific file, so that I can easily share it among multiple computers without reprocessing the dataset. However, when I try to pass an argument 'cache_file_name' to the .map() function, it throws an error that \".map() function got an unexpected keyword argument 'cache_file_name'\". \r\n\r\nI believe I'm using the latest dataset 1.6.2. Also seems like the document and the actual code indicates there is an argument 'cache_file_name' for the .map() function.\r\n\r\nHere is the code I use\r\n## Steps to reproduce the bug\r\n```datasets = load_from_disk(dataset_path=my_path)\r\n\r\n[...]\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[text_column_name])\r\n\r\nlogger.info(\"Mapping dataset to tokenized dataset.\")\r\ntokenized_datasets = datasets.map(\r\n    tokenize_function,\r\n    batched=True,\r\n    num_proc=preprocessing_num_workers,\r\n    remove_columns=column_names,\r\n    load_from_cache_file=True,\r\n   cache_file_name=\"my_tokenized_file\"\r\n)\r\n```\r\n\r\n## Actual results\r\n    tokenized_datasets = datasets.map(\r\nTypeError: map() got an unexpected keyword argument 'cache_file_name'\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:1.6.2\r\n- Platform:Linux-4.18.0-193.28.1.el8_2.x86_64-x86_64-with-glibc2.10\r\n- Python version:3.8.5\r\n- PyArrow version:3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2407/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2402", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2402/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2402/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2402/events", "html_url": "https://github.com/huggingface/datasets/issues/2402", "id": 900025329, "node_id": "MDU6SXNzdWU5MDAwMjUzMjk=", "number": 2402, "title": "PermissionError on Windows when using temp dir for caching", "user": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-05-24T21:22:59Z", "updated_at": "2021-05-26T16:39:29Z", "closed_at": "2021-05-26T16:39:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Currently, the following code raises a PermissionError on master if working on Windows:\r\n\r\n```python\r\n# run as a script or call exit() in REPL to initiate the temp dir cleanup\r\nfrom datasets import *\r\nd = load_dataset(\"sst\", split=\"train\", keep_in_memory=False)\r\nset_caching_enabled(False)\r\nd.map(lambda ex: ex)\r\n```\r\n\r\nError stack trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\weakref.py\", line 624, in _exitfunc\r\n    f()\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\weakref.py\", line 548, in __call__\r\n    return info.func(*info.args, **(info.kwargs or {}))\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\tempfile.py\", line 799, in _cleanup\r\n    _shutil.rmtree(name)\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\shutil.py\", line 500, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\shutil.py\", line 395, in _rmtree_unsafe\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\shutil.py\", line 393, in _rmtree_unsafe\r\n    os.unlink(fullname)\r\nPermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Mario\\\\AppData\\\\Local\\\\Temp\\\\tmp20epyhmq\\\\cache-87a87ffb5a956e68.arrow'\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2402/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2401", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2401/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2401/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2401/events", "html_url": "https://github.com/huggingface/datasets/issues/2401", "id": 899910521, "node_id": "MDU6SXNzdWU4OTk5MTA1MjE=", "number": 2401, "title": "load_dataset('natural_questions') fails with \"ValueError: External features info don't match the dataset\"", "user": {"login": "jonrbates", "id": 15602718, "node_id": "MDQ6VXNlcjE1NjAyNzE4", "avatar_url": "https://avatars.githubusercontent.com/u/15602718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jonrbates", "html_url": "https://github.com/jonrbates", "followers_url": "https://api.github.com/users/jonrbates/followers", "following_url": "https://api.github.com/users/jonrbates/following{/other_user}", "gists_url": "https://api.github.com/users/jonrbates/gists{/gist_id}", "starred_url": "https://api.github.com/users/jonrbates/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jonrbates/subscriptions", "organizations_url": "https://api.github.com/users/jonrbates/orgs", "repos_url": "https://api.github.com/users/jonrbates/repos", "events_url": "https://api.github.com/users/jonrbates/events{/privacy}", "received_events_url": "https://api.github.com/users/jonrbates/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2021-05-24T18:38:53Z", "updated_at": "2021-06-09T09:07:25Z", "closed_at": "2021-06-09T09:07:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nload_dataset('natural_questions') throws ValueError\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndatasets = load_dataset('natural_questions', split='validation[:10]')\r\n```\r\n\r\n## Expected results\r\nCall to load_dataset returns data.\r\n\r\n## Actual results\r\n```\r\nUsing custom data configuration default\r\nReusing dataset natural_questions (/mnt/d/huggingface/datasets/natural_questions/default/0.0.2/19bc04755018a3ad02ee74f7045cde4ba9b4162cb64450a87030ab786b123b76)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-d55ab8a8cc1c> in <module>\r\n----> 1 datasets = load_dataset('natural_questions', split='validation[:10]', cache_dir='/mnt/d/huggingface/datasets')\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\r\n    756         keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n    757     )\r\n--> 758     ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)\r\n    759     if save_infos:\r\n    760         builder_instance._save_infos()\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/builder.py in as_dataset(self, split, run_post_process, ignore_verifications, in_memory)\r\n    735 \r\n    736         # Create a dataset for each of the given splits\r\n--> 737         datasets = utils.map_nested(\r\n    738             partial(\r\n    739                 self._build_single_dataset,\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types)\r\n    193     # Singleton\r\n    194     if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 195         return function(data_struct)\r\n    196 \r\n    197     disable_tqdm = bool(logger.getEffectiveLevel() > INFO)\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/builder.py in _build_single_dataset(self, split, run_post_process, ignore_verifications, in_memory)\r\n    762 \r\n    763         # Build base dataset\r\n--> 764         ds = self._as_dataset(\r\n    765             split=split,\r\n    766             in_memory=in_memory,\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/builder.py in _as_dataset(self, split, in_memory)\r\n    838             in_memory=in_memory,\r\n    839         )\r\n--> 840         return Dataset(**dataset_kwargs)\r\n    841 \r\n    842     def _post_process(self, dataset: Dataset, resources_paths: Dict[str, str]) -> Optional[Dataset]:\r\n\r\n~/miniconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py in __init__(self, arrow_table, info, split, indices_table, fingerprint)\r\n    271         assert self._fingerprint is not None, \"Fingerprint can't be None in a Dataset object\"\r\n    272         if self.info.features.type != inferred_features.type:\r\n--> 273             raise ValueError(\r\n    274                 \"External features info don't match the dataset:\\nGot\\n{}\\nwith type\\n{}\\n\\nbut expected something like\\n{}\\nwith type\\n{}\".format(\r\n    275                     self.info.features, self.info.features.type, inferred_features, inferred_features.type\r\n\r\nValueError: External features info don't match the dataset:\r\nGot\r\n{'id': Value(dtype='string', id=None), 'document': {'title': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'html': Value(dtype='string', id=None), 'tokens': Sequence(feature={'token': Value(dtype='string', id=None), 'is_html': Value(dtype='bool', id=None)}, length=-1, id=None)}, 'question': {'text': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'annotations': Sequence(feature={'id': Value(dtype='string', id=None), 'long_answer': {'start_token': Value(dtype='int64', id=None), 'end_token': Value(dtype='int64', id=None), 'start_byte': Value(dtype='int64', id=None), 'end_byte': Value(dtype='int64', id=None)}, 'short_answers': Sequence(feature={'start_token': Value(dtype='int64', id=None), 'end_token': Value(dtype='int64', id=None), 'start_byte': Value(dtype='int64', id=None), 'end_byte': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}, length=-1, id=None), 'yes_no_answer': ClassLabel(num_classes=2, names=['NO', 'YES'], names_file=None, id=None)}, length=-1, id=None)}\r\nwith type\r\nstruct<annotations: struct<id: list<item: string>, long_answer: list<item: struct<start_token: int64, end_token: int64, start_byte: int64, end_byte: int64>>, short_answers: list<item: struct<end_byte: list<item: int64>, end_token: list<item: int64>, start_byte: list<item: int64>, start_token: list<item: int64>, text: list<item: string>>>, yes_no_answer: list<item: int64>>, document: struct<title: string, url: string, html: string, tokens: struct<is_html: list<item: bool>, token: list<item: string>>>, id: string, question: struct<text: string, tokens: list<item: string>>>\r\n\r\nbut expected something like\r\n{'id': Value(dtype='string', id=None), 'document': {'html': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'tokens': {'is_html': Sequence(feature=Value(dtype='bool', id=None), length=-1, id=None), 'token': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'url': Value(dtype='string', id=None)}, 'question': {'text': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'annotations': {'id': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'long_answer': [{'end_byte': Value(dtype='int64', id=None), 'end_token': Value(dtype='int64', id=None), 'start_byte': Value(dtype='int64', id=None), 'start_token': Value(dtype='int64', id=None)}], 'short_answers': [{'end_byte': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'end_token': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'start_byte': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'start_token': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}], 'yes_no_answer': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}}\r\nwith type\r\nstruct<annotations: struct<id: list<item: string>, long_answer: list<item: struct<end_byte: int64, end_token: int64, start_byte: int64, start_token: int64>>, short_answers: list<item: struct<end_byte: list<item: int64>, end_token: list<item: int64>, start_byte: list<item: int64>, start_token: list<item: int64>, text: list<item: string>>>, yes_no_answer: list<item: int64>>, document: struct<html: string, title: string, tokens: struct<is_html: list<item: bool>, token: list<item: string>>, url: string>, id: string, question: struct<text: string, tokens: list<item: string>>>\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.2\r\n- Platform: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.10\r\n- Python version: 3.8.3\r\n- PyTorch version (GPU?): 1.6.0 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2401/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2400", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2400/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2400/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2400/events", "html_url": "https://github.com/huggingface/datasets/issues/2400", "id": 899867212, "node_id": "MDU6SXNzdWU4OTk4NjcyMTI=", "number": 2400, "title": "Concatenate several datasets with removed columns is not working.", "user": {"login": "philschmid", "id": 32632186, "node_id": "MDQ6VXNlcjMyNjMyMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/32632186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/philschmid", "html_url": "https://github.com/philschmid", "followers_url": "https://api.github.com/users/philschmid/followers", "following_url": "https://api.github.com/users/philschmid/following{/other_user}", "gists_url": "https://api.github.com/users/philschmid/gists{/gist_id}", "starred_url": "https://api.github.com/users/philschmid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/philschmid/subscriptions", "organizations_url": "https://api.github.com/users/philschmid/orgs", "repos_url": "https://api.github.com/users/philschmid/repos", "events_url": "https://api.github.com/users/philschmid/events{/privacy}", "received_events_url": "https://api.github.com/users/philschmid/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-05-24T17:40:15Z", "updated_at": "2021-05-25T05:52:01Z", "closed_at": "2021-05-25T05:51:59Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nYou can't concatenate datasets when you removed columns before.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, concatenate_datasets\r\n\r\nwikiann= load_dataset(\"wikiann\",\"en\")\r\n\r\nwikiann[\"train\"] = wikiann[\"train\"].remove_columns([\"langs\",\"spans\"])\r\nwikiann[\"test\"] = wikiann[\"test\"].remove_columns([\"langs\",\"spans\"])\r\n\r\nassert wikiann[\"train\"].features.type == wikiann[\"test\"].features.type\r\n\r\nconcate = concatenate_datasets([wikiann[\"train\"],wikiann[\"test\"]])\r\n```\r\n\r\n## Expected results\r\nMerged dataset \r\n\r\n\r\n## Actual results\r\n```python\r\nValueError: External features info don't match the dataset:\r\nGot\r\n{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None), length=-1, id=None), 'langs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'spans': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\nwith type\r\nstruct<langs: list<item: string>, ner_tags: list<item: int64>, spans: list<item: string>, tokens: list<item: string>>\r\n\r\nbut expected something like\r\n{'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\nwith type\r\nstruct<ner_tags: list<item: int64>, tokens: list<item: string>>\r\n```\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: ~1.6.2~ 1.5.0\r\n- Platform: macos\r\n- Python version: 3.8.5\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2400/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2400/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2398", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2398/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2398/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2398/events", "html_url": "https://github.com/huggingface/datasets/issues/2398", "id": 899511837, "node_id": "MDU6SXNzdWU4OTk1MTE4Mzc=", "number": 2398, "title": "News_commentary Dataset Translation Pairs are of Incorrect Language Specified Pairs", "user": {"login": "anassalamah", "id": 8571003, "node_id": "MDQ6VXNlcjg1NzEwMDM=", "avatar_url": "https://avatars.githubusercontent.com/u/8571003?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anassalamah", "html_url": "https://github.com/anassalamah", "followers_url": "https://api.github.com/users/anassalamah/followers", "following_url": "https://api.github.com/users/anassalamah/following{/other_user}", "gists_url": "https://api.github.com/users/anassalamah/gists{/gist_id}", "starred_url": "https://api.github.com/users/anassalamah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anassalamah/subscriptions", "organizations_url": "https://api.github.com/users/anassalamah/orgs", "repos_url": "https://api.github.com/users/anassalamah/repos", "events_url": "https://api.github.com/users/anassalamah/events{/privacy}", "received_events_url": "https://api.github.com/users/anassalamah/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-05-24T10:03:34Z", "updated_at": "2022-10-05T17:13:49Z", "closed_at": "2022-10-05T17:13:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "I used load_dataset to load the news_commentary dataset for \"ar-en\" translation pairs but found translations from Arabic to Hindi.  \r\n\r\n```\r\ntrain_ds = load_dataset(\"news_commentary\", \"ar-en\", split='train[:98%]')\r\nval_ds = load_dataset(\"news_commentary\", \"ar-en\", split='train[98%:]')\r\n\r\n# filtering out examples that are not ar-en translations but ar-hi\r\nval_ds = val_ds.filter(lambda example, indice: indice not in chain(range(1312,1327) ,range(1384,1399), range(1030,1042)), with_indices=True)\r\n```\r\n\r\n* I'm fairly new to using datasets so I might be doing something wrong", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2398/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2398/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2391", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2391/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2391/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2391/events", "html_url": "https://github.com/huggingface/datasets/issues/2391", "id": 898128099, "node_id": "MDU6SXNzdWU4OTgxMjgwOTk=", "number": 2391, "title": "Missing original answers in kilt-TriviaQA", "user": {"login": "PaulLerner", "id": 25532159, "node_id": "MDQ6VXNlcjI1NTMyMTU5", "avatar_url": "https://avatars.githubusercontent.com/u/25532159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PaulLerner", "html_url": "https://github.com/PaulLerner", "followers_url": "https://api.github.com/users/PaulLerner/followers", "following_url": "https://api.github.com/users/PaulLerner/following{/other_user}", "gists_url": "https://api.github.com/users/PaulLerner/gists{/gist_id}", "starred_url": "https://api.github.com/users/PaulLerner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PaulLerner/subscriptions", "organizations_url": "https://api.github.com/users/PaulLerner/orgs", "repos_url": "https://api.github.com/users/PaulLerner/repos", "events_url": "https://api.github.com/users/PaulLerner/events{/privacy}", "received_events_url": "https://api.github.com/users/PaulLerner/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-05-21T14:57:07Z", "updated_at": "2021-06-14T17:29:11Z", "closed_at": "2021-06-14T17:29:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I previously opened an issue at https://github.com/facebookresearch/KILT/issues/42 but from the answer of @fabiopetroni it seems that the problem comes from HF-datasets\r\n\r\n## Describe the bug\r\nThe `answer` field in kilt-TriviaQA, e.g. `kilt_tasks['train_triviaqa'][0]['output']['answer']` contains a list of alternative answer which are accepted for the question.  \r\nHowever it'd be nice to know the original answer to the question (the only fields in `output` are `'answer', 'meta', 'provenance'`)\r\n\r\n## How to fix\r\nIt can be fixed by retrieving the original answer from the original TriviaQA (e.g. `trivia_qa['train'][0]['answer']['value']`), perhaps at the same place as here where one retrieves the questions https://github.com/huggingface/datasets/blob/master/datasets/kilt_tasks/README.md#loading-the-kilt-knowledge-source-and-task-data\r\n\r\ncc @yjernite who previously answered to an issue about KILT and TriviaQA :)\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2391/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2391/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2388", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2388/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2388/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2388/events", "html_url": "https://github.com/huggingface/datasets/issues/2388", "id": 897767470, "node_id": "MDU6SXNzdWU4OTc3Njc0NzA=", "number": 2388, "title": "Incorrect URLs for some datasets", "user": {"login": "lewtun", "id": 26859204, "node_id": "MDQ6VXNlcjI2ODU5MjA0", "avatar_url": "https://avatars.githubusercontent.com/u/26859204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lewtun", "html_url": "https://github.com/lewtun", "followers_url": "https://api.github.com/users/lewtun/followers", "following_url": "https://api.github.com/users/lewtun/following{/other_user}", "gists_url": "https://api.github.com/users/lewtun/gists{/gist_id}", "starred_url": "https://api.github.com/users/lewtun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lewtun/subscriptions", "organizations_url": "https://api.github.com/users/lewtun/orgs", "repos_url": "https://api.github.com/users/lewtun/repos", "events_url": "https://api.github.com/users/lewtun/events{/privacy}", "received_events_url": "https://api.github.com/users/lewtun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lewtun", "id": 26859204, "node_id": "MDQ6VXNlcjI2ODU5MjA0", "avatar_url": "https://avatars.githubusercontent.com/u/26859204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lewtun", "html_url": "https://github.com/lewtun", "followers_url": "https://api.github.com/users/lewtun/followers", "following_url": "https://api.github.com/users/lewtun/following{/other_user}", "gists_url": "https://api.github.com/users/lewtun/gists{/gist_id}", "starred_url": "https://api.github.com/users/lewtun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lewtun/subscriptions", "organizations_url": "https://api.github.com/users/lewtun/orgs", "repos_url": "https://api.github.com/users/lewtun/repos", "events_url": "https://api.github.com/users/lewtun/events{/privacy}", "received_events_url": "https://api.github.com/users/lewtun/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lewtun", "id": 26859204, "node_id": "MDQ6VXNlcjI2ODU5MjA0", "avatar_url": "https://avatars.githubusercontent.com/u/26859204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lewtun", "html_url": "https://github.com/lewtun", "followers_url": "https://api.github.com/users/lewtun/followers", "following_url": "https://api.github.com/users/lewtun/following{/other_user}", "gists_url": "https://api.github.com/users/lewtun/gists{/gist_id}", "starred_url": "https://api.github.com/users/lewtun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lewtun/subscriptions", "organizations_url": "https://api.github.com/users/lewtun/orgs", "repos_url": "https://api.github.com/users/lewtun/repos", "events_url": "https://api.github.com/users/lewtun/events{/privacy}", "received_events_url": "https://api.github.com/users/lewtun/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-05-21T07:22:35Z", "updated_at": "2021-06-04T17:39:45Z", "closed_at": "2021-06-04T17:39:45Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\nIt seems that the URLs for the following datasets are invalid: \r\n\r\n- [ ]  `bn_hate_speech` has been renamed: https://github.com/rezacsedu/Bengali-Hate-Speech-Dataset/commit/c67ecfc4184911e12814f6b36901f9828df8a63a\r\n- [ ] `covid_tweets_japanese` has been renamed: http://www.db.info.gifu-u.ac.jp/covid-19-twitter-dataset/\r\n\r\nAs a result we can no longer load these datasets using `load_dataset`. The simple fix is to rename the URL in the dataset script - will do this asap.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n# pick one of the datasets from the list above\r\nds = load_dataset(\"bn_hate_speech\")\r\n```\r\n\r\n## Expected results\r\nDataset loads without error.\r\n\r\n## Actual results\r\n```\r\nDownloading: 3.36kB [00:00, 1.07MB/s]                                                                                                                                                                     \r\nDownloading: 2.03kB [00:00, 678kB/s]                                                                                                                                                                      \r\nUsing custom data configuration default\r\nDownloading and preparing dataset bn_hate_speech/default (download: 951.48 KiB, generated: 949.84 KiB, post-processed: Unknown size, total: 1.86 MiB) to /Users/lewtun/.cache/huggingface/datasets/bn_hate_speech/default/0.0.0/a2dc726e511a2177523301bcad196af05d4d8a2cff30d2769ba8aacc1f5fdb5c...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/load.py\", line 744, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/builder.py\", line 574, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/builder.py\", line 630, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/Users/lewtun/.cache/huggingface/modules/datasets_modules/datasets/bn_hate_speech/a2dc726e511a2177523301bcad196af05d4d8a2cff30d2769ba8aacc1f5fdb5c/bn_hate_speech.py\", line 76, in _split_generators\r\n    train_path = dl_manager.download_and_extract(_URL)\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 287, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 195, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 195, in map_nested\r\n    return function(data_struct)\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/download_manager.py\", line 218, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 281, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/Users/lewtun/miniconda3/envs/hf-hub_eval/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 621, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/rezacsedu/Bengali-Hate-Speech-Dataset/main/Bengali_%20Hate_Speech_Dataset_Subset.csv\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.6.2.dev0\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.8\r\n- PyArrow version: 3.0.0\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2388/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2387", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2387/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2387/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2387/events", "html_url": "https://github.com/huggingface/datasets/issues/2387", "id": 897566666, "node_id": "MDU6SXNzdWU4OTc1NjY2NjY=", "number": 2387, "title": "datasets 1.6 ignores cache", "user": {"login": "stas00", "id": 10676103, "node_id": "MDQ6VXNlcjEwNjc2MTAz", "avatar_url": "https://avatars.githubusercontent.com/u/10676103?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stas00", "html_url": "https://github.com/stas00", "followers_url": "https://api.github.com/users/stas00/followers", "following_url": "https://api.github.com/users/stas00/following{/other_user}", "gists_url": "https://api.github.com/users/stas00/gists{/gist_id}", "starred_url": "https://api.github.com/users/stas00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stas00/subscriptions", "organizations_url": "https://api.github.com/users/stas00/orgs", "repos_url": "https://api.github.com/users/stas00/repos", "events_url": "https://api.github.com/users/stas00/events{/privacy}", "received_events_url": "https://api.github.com/users/stas00/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2021-05-21T00:12:58Z", "updated_at": "2021-05-26T16:07:54Z", "closed_at": "2021-05-26T16:07:54Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Moving from https://github.com/huggingface/transformers/issues/11801#issuecomment-845546612 \r\n\r\nQuoting @VictorSanh:\r\n\r\n> \r\n> I downgraded datasets to `1.5.0` and printed `tokenized_datasets.cache_files` (L335):\r\n> \r\n> > `{'train': [{'filename': '/home/victor/.cache/huggingface/datasets/openwebtext10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b/cache-c6aefe81ca4e5152.arrow'}], 'validation': [{'filename': '/home/victor/.cache/huggingface/datasets/openwebtext10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b/cache-97cf4c813e6469c6.arrow'}]}`\r\n> \r\n> while the same command with the latest version of datasets (actually starting at `1.6.0`) gives:\r\n> > `{'train': [], 'validation': []}`\r\n> \r\n\r\nI also confirm that downgrading to `datasets==1.5.0` makes things fast again - i.e. cache is used.\r\n\r\nto reproduce:\r\n```\r\nUSE_TF=0 python  examples/pytorch/language-modeling/run_clm.py \\\r\n    --model_name_or_path gpt2 \\\r\n    --dataset_name \"stas/openwebtext-10k\" \\\r\n    --output_dir output_dir \\\r\n    --overwrite_output_dir \\\r\n    --do_train \\\r\n    --do_eval \\\r\n    --max_train_samples 1000 \\\r\n    --max_eval_samples 200 \\\r\n    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --num_train_epochs 1 \\\r\n    --warmup_steps 8 \\\r\n    --block_size 64 \\\r\n    --fp16 \\\r\n    --report_to none\r\n```\r\n\r\nthe first time the startup is slow and some 5 tqdm bars. It shouldn't do it on consequent runs. but with `datasets>1.5.0` it rebuilds on every run.\r\n\r\n@lhoestq \r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2387/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2387/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2386", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2386/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2386/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2386/events", "html_url": "https://github.com/huggingface/datasets/issues/2386", "id": 897560049, "node_id": "MDU6SXNzdWU4OTc1NjAwNDk=", "number": 2386, "title": "Accessing Arrow dataset cache_files", "user": {"login": "Mehrad0711", "id": 28717374, "node_id": "MDQ6VXNlcjI4NzE3Mzc0", "avatar_url": "https://avatars.githubusercontent.com/u/28717374?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mehrad0711", "html_url": "https://github.com/Mehrad0711", "followers_url": "https://api.github.com/users/Mehrad0711/followers", "following_url": "https://api.github.com/users/Mehrad0711/following{/other_user}", "gists_url": "https://api.github.com/users/Mehrad0711/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mehrad0711/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mehrad0711/subscriptions", "organizations_url": "https://api.github.com/users/Mehrad0711/orgs", "repos_url": "https://api.github.com/users/Mehrad0711/repos", "events_url": "https://api.github.com/users/Mehrad0711/events{/privacy}", "received_events_url": "https://api.github.com/users/Mehrad0711/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-05-20T23:57:43Z", "updated_at": "2021-05-21T19:18:03Z", "closed_at": "2021-05-21T19:18:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nIn datasets 1.5.0 the following code snippet would have printed the cache_files:\r\n\r\n```\r\ntrain_data = load_dataset('conll2003', split='train', cache_dir='data')\r\nprint(train_data.cache_files[0]['filename'])\r\n\r\n```\r\n\r\nHowever, in the newest release (1.6.1), it prints an empty list.\r\n\r\nI also tried loading the dataset with `keep_in_memory=True` argument but still `cache_files` is empty.\r\n\r\nWas wondering if this is a bug or I need to pass additional arguments so I can access the cache_files.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2386/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2386/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2366", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2366/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2366/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2366/events", "html_url": "https://github.com/huggingface/datasets/issues/2366", "id": 893185266, "node_id": "MDU6SXNzdWU4OTMxODUyNjY=", "number": 2366, "title": "Json loader fails if user-specified features don't match the json data fields order", "user": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-05-17T10:26:08Z", "updated_at": "2021-06-16T10:47:49Z", "closed_at": "2021-06-16T10:47:49Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "If you do\r\n```python\r\ndataset = load_dataset(\"json\", data_files=data_files, features=features)\r\n```\r\nThen depending on the order of the features in the json data field it fails:\r\n```python\r\n[...]\r\n~/Desktop/hf/datasets/src/datasets/packaged_modules/json/json.py in _generate_tables(self, files)\r\n     94             if self.config.schema:\r\n     95                 # Cast allows str <-> int/float, while parse_option explicit_schema does NOT\r\n---> 96                 pa_table = pa_table.cast(self.config.schema)\r\n     97             yield i, pa_table\r\n[...]\r\nValueError: Target schema's field names are not matching the table's field names: ['tokens', 'ner_tags'], ['ner_tags', 'tokens']\r\n```\r\n\r\nThis is because one must first re-order the columns of the table to match the `self.config.schema` before calling cast.\r\n\r\nOne way to fix the `cast` would be to replace it with:\r\n```python\r\n# reorder the arrays if necessary + cast to schema\r\n# we can't simply use .cast here because we may need to change the order of the columns\r\npa_table = pa.Table.from_arrays([pa_table[name] for name in schema.names], schema=schema)\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2366/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2365", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2365/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2365/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2365/events", "html_url": "https://github.com/huggingface/datasets/issues/2365", "id": 893179697, "node_id": "MDU6SXNzdWU4OTMxNzk2OTc=", "number": 2365, "title": "Missing ClassLabel encoding in Json loader", "user": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/huggingface/datasets/milestones/5", "html_url": "https://github.com/huggingface/datasets/milestone/5", "labels_url": "https://api.github.com/repos/huggingface/datasets/milestones/5/labels", "id": 6808903, "node_id": "MDk6TWlsZXN0b25lNjgwODkwMw==", "number": 5, "title": "1.9", "description": "Next minor release", "creator": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 12, "state": "closed", "created_at": "2021-05-31T16:13:06Z", "updated_at": "2021-07-12T14:12:00Z", "due_on": "2021-07-08T07:00:00Z", "closed_at": "2021-07-09T05:50:07Z"}, "comments": 0, "created_at": "2021-05-17T10:19:10Z", "updated_at": "2021-06-28T15:05:34Z", "closed_at": "2021-06-28T15:05:34Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Currently if you want to load a json dataset this way\r\n```python\r\ndataset = load_dataset(\"json\", data_files=data_files, features=features)\r\n```\r\nThen if your features has ClassLabel types and if your json data needs class label encoding (i.e. if the labels in the json files are strings and not integers), then it would fail:\r\n```python\r\n[...]\r\n~/Desktop/hf/datasets/src/datasets/packaged_modules/json/json.py in _generate_tables(self, files)\r\n     94             if self.config.schema:\r\n     95                 # Cast allows str <-> int/float, while parse_option explicit_schema does NOT\r\n---> 96                 pa_table = pa_table.cast(self.config.schema)\r\n     97             yield i, pa_table\r\n[...]\r\nArrowInvalid: Failed to parse string: 'O' as a scalar of type int64\r\n```\r\n\r\nThis is because it just tries to cast the string data to integers, without applying the mapping str->int first\r\n\r\nThe current workaround is to do instead\r\n```python\r\ndataset = load_dataset(\"json\", data_files=data_files)\r\ndataset = dataset.map(features.encode_example, features=features)\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2365/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2365/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2350", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2350/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2350/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2350/events", "html_url": "https://github.com/huggingface/datasets/issues/2350", "id": 889580247, "node_id": "MDU6SXNzdWU4ODk1ODAyNDc=", "number": 2350, "title": "`FaissIndex.save` throws error on GPU", "user": {"login": "Guitaricet", "id": 2821124, "node_id": "MDQ6VXNlcjI4MjExMjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/2821124?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Guitaricet", "html_url": "https://github.com/Guitaricet", "followers_url": "https://api.github.com/users/Guitaricet/followers", "following_url": "https://api.github.com/users/Guitaricet/following{/other_user}", "gists_url": "https://api.github.com/users/Guitaricet/gists{/gist_id}", "starred_url": "https://api.github.com/users/Guitaricet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Guitaricet/subscriptions", "organizations_url": "https://api.github.com/users/Guitaricet/orgs", "repos_url": "https://api.github.com/users/Guitaricet/repos", "events_url": "https://api.github.com/users/Guitaricet/events{/privacy}", "received_events_url": "https://api.github.com/users/Guitaricet/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-05-12T03:41:56Z", "updated_at": "2021-05-17T13:41:41Z", "closed_at": "2021-05-17T13:41:41Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nAfter training an index with a factory string `OPQ16_128,IVF512,PQ32` on GPU, `.save_faiss_index` throws this error.\r\n\r\n```\r\n  File \"index_wikipedia.py\", line 119, in <module>\r\n    data[\"train\"].save_faiss_index(\"text_emb\", index_save_path)\r\n  File \"/home/vlialin/miniconda3/envs/cat/lib/python3.8/site-packages/datasets/search.py\", line 470, in save_faiss_index\r\n    index.save(file)\r\n  File \"/home/vlialin/miniconda3/envs/cat/lib/python3.8/site-packages/datasets/search.py\", line 334, in save\r\n    faiss.write_index(index, str(file))\r\n  File \"/home/vlialin/miniconda3/envs/cat/lib/python3.8/site-packages/faiss/swigfaiss_avx2.py\", line 5654, in write_index\r\n    return _swigfaiss.write_index(*args)\r\nRuntimeError: Error in void faiss::write_index(const faiss::Index*, faiss::IOWriter*) at /root/miniconda3/conda-bld/faiss-pkg_1613235005464/work/faiss/impl/index_write.cpp:453: don't know how to serialize this type of index\r\n```\r\n\r\n## Steps to reproduce the bug\r\n\r\nAny dataset will do, I just selected a familiar one.\r\n\r\n```python\r\nimport numpy as np\r\nimport datasets\r\nINDEX_STR = \"OPQ16_128,IVF512,PQ32\"\r\nINDEX_SAVE_PATH = \"will_not_save.faiss\"\r\n\r\ndata = datasets.load_dataset(\"Fraser/news-category-dataset\", split=f\"train[:10000]\")\r\n\r\ndef encode(item):\r\n    return {\"text_emb\": np.random.randn(768).astype(np.float32)}\r\n\r\ndata = data.map(encode)\r\n\r\ndata.add_faiss_index(column=\"text_emb\", string_factory=INDEX_STR, train_size=10_000, device=0)\r\ndata.save_faiss_index(\"text_emb\", INDEX_SAVE_PATH)\r\n```\r\n\r\n## Expected results\r\nSaving the index\r\n\r\n## Actual results\r\nError in void faiss::write_index(const faiss::Index*, faiss::IOWriter*) ... don't know how to serialize this type of index\r\n\r\n## Environment info\r\n- `datasets` version: 1.6.2\r\n- Platform: Linux-4.15.0-142-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.8\r\n- PyTorch version (GPU?): 1.8.1+cu111 (True)\r\n- Tensorflow version (GPU?): 2.2.0 (False)\r\n- Using GPU in script?: Yes\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n\r\nI will be proposing a fix in a couple of minutes", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2350/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2337", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2337/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2337/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2337/events", "html_url": "https://github.com/huggingface/datasets/issues/2337", "id": 881610567, "node_id": "MDU6SXNzdWU4ODE2MTA1Njc=", "number": 2337, "title": "NonMatchingChecksumError for web_of_science dataset", "user": {"login": "nbroad1881", "id": 24982805, "node_id": "MDQ6VXNlcjI0OTgyODA1", "avatar_url": "https://avatars.githubusercontent.com/u/24982805?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nbroad1881", "html_url": "https://github.com/nbroad1881", "followers_url": "https://api.github.com/users/nbroad1881/followers", "following_url": "https://api.github.com/users/nbroad1881/following{/other_user}", "gists_url": "https://api.github.com/users/nbroad1881/gists{/gist_id}", "starred_url": "https://api.github.com/users/nbroad1881/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nbroad1881/subscriptions", "organizations_url": "https://api.github.com/users/nbroad1881/orgs", "repos_url": "https://api.github.com/users/nbroad1881/repos", "events_url": "https://api.github.com/users/nbroad1881/events{/privacy}", "received_events_url": "https://api.github.com/users/nbroad1881/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-05-09T02:02:02Z", "updated_at": "2021-05-10T13:35:53Z", "closed_at": "2021-05-10T13:35:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "NonMatchingChecksumError when trying to download the web_of_science dataset. \r\n\r\n>NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://data.mendeley.com/datasets/9rw3vkcfy4/6/files/c9ea673d-5542-44c0-ab7b-f1311f7d61df/WebOfScience.zip?dl=1']\r\n\r\nSetting `ignore_verfications=True` results in OSError.\r\n\r\n>OSError: Cannot find data file. \r\nOriginal error:\r\n[Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/37ab2c42f50d553c1d0ea432baca3e9e11fedea4aeec63a81e6b7e25dd10d4e7/WOS5736/X.txt'\r\n\r\n```python\r\ndataset = load_dataset('web_of_science', 'WOS5736')\r\n```\r\nThere are 3 data instances and they all don't work. 'WOS5736', 'WOS11967', 'WOS46985'\r\n\r\ndatasets 1.6.2\r\npython 3.7.10\r\nUbuntu 18.04.5 LTS", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2337/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2337/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2335", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2335/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2335/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2335/events", "html_url": "https://github.com/huggingface/datasets/issues/2335", "id": 881291887, "node_id": "MDU6SXNzdWU4ODEyOTE4ODc=", "number": 2335, "title": "Index error in Dataset.map", "user": {"login": "mariosasko", "id": 47462742, "node_id": "MDQ6VXNlcjQ3NDYyNzQy", "avatar_url": "https://avatars.githubusercontent.com/u/47462742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosasko", "html_url": "https://github.com/mariosasko", "followers_url": "https://api.github.com/users/mariosasko/followers", "following_url": "https://api.github.com/users/mariosasko/following{/other_user}", "gists_url": "https://api.github.com/users/mariosasko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosasko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosasko/subscriptions", "organizations_url": "https://api.github.com/users/mariosasko/orgs", "repos_url": "https://api.github.com/users/mariosasko/repos", "events_url": "https://api.github.com/users/mariosasko/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosasko/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-05-08T20:44:57Z", "updated_at": "2021-05-10T13:26:12Z", "closed_at": "2021-05-10T13:26:12Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "The following code, if executed on master, raises an IndexError (due to overflow):\r\n```python\r\n>>> from datasets import *\r\n>>> d = load_dataset(\"bookcorpus\", split=\"train\")\r\nReusing dataset bookcorpus (C:\\Users\\Mario\\.cache\\huggingface\\datasets\\bookcorpus\\plain_text\\1.0.0\\44662c4a114441c35200992bea923b170e6f13f2f0beb7c14e43759cec498700)\r\n2021-05-08 21:23:46.859818: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n>>> d.map(lambda ex: ex)\r\n  0%|\u258e                                                                    | 289430/74004228 [00:13<58:41, 20935.33ex/s]c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\table.py:84: RuntimeWarning: overflow encountered in int_scalars\r\n  k = i + ((j - i) * (x - arr[i]) // (arr[j] - arr[i]))\r\n  0%|\u258e                                                                    | 290162/74004228 [00:13<59:11, 20757.23ex/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\arrow_dataset.py\", line 1498, in map\r\n    new_fingerprint=new_fingerprint,\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\arrow_dataset.py\", line 174, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\fingerprint.py\", line 340, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\arrow_dataset.py\", line 1799, in _map_single\r\n    for i, example in enumerate(pbar):\r\n  File \"C:\\Users\\Mario\\Anaconda3\\envs\\hf-datasets\\lib\\site-packages\\tqdm\\std.py\", line 1133, in __iter__\r\n    for obj in iterable:\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\arrow_dataset.py\", line 1145, in __iter__\r\n    format_kwargs=format_kwargs,\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\arrow_dataset.py\", line 1337, in _getitem\r\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\formatting\\formatting.py\", line 368, in query_table\r\n    pa_subtable = _query_table(table, key)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\formatting\\formatting.py\", line 79, in _query_table\r\n    return table.fast_slice(key % table.num_rows, 1)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\table.py\", line 128, in fast_slice\r\n    i = _interpolation_search(self._offsets, offset)\r\n  File \"c:\\users\\mario\\desktop\\projects\\datasets-1\\src\\datasets\\table.py\", line 91, in _interpolation_search\r\n    raise IndexError(f\"Invalid query '{x}' for size {arr[-1] if len(arr) else 'none'}.\")\r\nIndexError: Invalid query '290162' for size 74004228.\r\n```\r\nTested on Windows, can run on Linux if needed.\r\n\r\nEDIT:\r\nIt seems like for this to happen, the default NumPy dtype has to be np.int32.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2335/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2327", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2327/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2327/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2327/events", "html_url": "https://github.com/huggingface/datasets/issues/2327", "id": 877565831, "node_id": "MDU6SXNzdWU4Nzc1NjU4MzE=", "number": 2327, "title": "A syntax error in example", "user": {"login": "mymusise", "id": 6883957, "node_id": "MDQ6VXNlcjY4ODM5NTc=", "avatar_url": "https://avatars.githubusercontent.com/u/6883957?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mymusise", "html_url": "https://github.com/mymusise", "followers_url": "https://api.github.com/users/mymusise/followers", "following_url": "https://api.github.com/users/mymusise/following{/other_user}", "gists_url": "https://api.github.com/users/mymusise/gists{/gist_id}", "starred_url": "https://api.github.com/users/mymusise/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mymusise/subscriptions", "organizations_url": "https://api.github.com/users/mymusise/orgs", "repos_url": "https://api.github.com/users/mymusise/repos", "events_url": "https://api.github.com/users/mymusise/events{/privacy}", "received_events_url": "https://api.github.com/users/mymusise/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-05-06T14:34:44Z", "updated_at": "2021-05-20T03:04:19Z", "closed_at": "2021-05-20T03:04:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "![image](https://user-images.githubusercontent.com/6883957/117315905-b47a5c00-aeba-11eb-91eb-b2a4a0212a56.png)\r\n\r\nSorry to report with an image, I can't find the template source code of this snippet.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2327/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2323", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2323/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2323/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2323/events", "html_url": "https://github.com/huggingface/datasets/issues/2323", "id": 876438507, "node_id": "MDU6SXNzdWU4NzY0Mzg1MDc=", "number": 2323, "title": "load_dataset(\"timit_asr\") gives back duplicates of just one sample text", "user": {"login": "ekeleshian", "id": 33647474, "node_id": "MDQ6VXNlcjMzNjQ3NDc0", "avatar_url": "https://avatars.githubusercontent.com/u/33647474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ekeleshian", "html_url": "https://github.com/ekeleshian", "followers_url": "https://api.github.com/users/ekeleshian/followers", "following_url": "https://api.github.com/users/ekeleshian/following{/other_user}", "gists_url": "https://api.github.com/users/ekeleshian/gists{/gist_id}", "starred_url": "https://api.github.com/users/ekeleshian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ekeleshian/subscriptions", "organizations_url": "https://api.github.com/users/ekeleshian/orgs", "repos_url": "https://api.github.com/users/ekeleshian/repos", "events_url": "https://api.github.com/users/ekeleshian/events{/privacy}", "received_events_url": "https://api.github.com/users/ekeleshian/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-05-05T13:14:48Z", "updated_at": "2021-05-07T10:32:30Z", "closed_at": "2021-05-07T10:32:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen you look up on key [\"train\"] and then ['text'], you get back a list  with just one sentence duplicated 4620 times. Namely, the sentence \"Would such an act of refusal be useful?\". Similarly when you look up ['test'] and then ['text'], the list is one sentence repeated \"The bungalow was pleasantly situated near the shore.\" 1680 times. \r\n\r\nI tried to work around the issue by downgrading to datasets version 1.3.0, inspired by [this post](https://www.gitmemory.com/issue/huggingface/datasets/2052/798904836) and removing the entire huggingface directory from ~/.cache, but I still get the same issue.  \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ntimit = load_dataset(\"timit_asr\")\r\nprint(timit['train']['text'])\r\nprint(timit['test']['text'])\r\n```\r\n\r\n## Expected Result\r\nRows of diverse text, like how it is shown in the [wav2vec2.0 tutorial](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb)\r\n<img width=\"485\" alt=\"Screen Shot 2021-05-05 at 9 09 57 AM\" src=\"https://user-images.githubusercontent.com/33647474/117146094-d9b77f00-ad81-11eb-8306-f281850c127a.png\">\r\n\r\n\r\n## Actual results\r\nRows of repeated text.\r\n<img width=\"319\" alt=\"Screen Shot 2021-05-05 at 9 11 53 AM\" src=\"https://user-images.githubusercontent.com/33647474/117146231-f8b61100-ad81-11eb-834a-fc10410b0c9c.png\">\r\n\r\n\r\n## Versions\r\n- Datasets: 1.3.0\r\n- Python: 3.9.1\r\n- Platform: macOS-11.2.1-x86_64-i386-64bit}\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2323/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2323/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2322", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2322/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2322/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2322/events", "html_url": "https://github.com/huggingface/datasets/issues/2322", "id": 876383853, "node_id": "MDU6SXNzdWU4NzYzODM4NTM=", "number": 2322, "title": "Calls to map are not cached.", "user": {"login": "villmow", "id": 2743060, "node_id": "MDQ6VXNlcjI3NDMwNjA=", "avatar_url": "https://avatars.githubusercontent.com/u/2743060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/villmow", "html_url": "https://github.com/villmow", "followers_url": "https://api.github.com/users/villmow/followers", "following_url": "https://api.github.com/users/villmow/following{/other_user}", "gists_url": "https://api.github.com/users/villmow/gists{/gist_id}", "starred_url": "https://api.github.com/users/villmow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/villmow/subscriptions", "organizations_url": "https://api.github.com/users/villmow/orgs", "repos_url": "https://api.github.com/users/villmow/repos", "events_url": "https://api.github.com/users/villmow/events{/privacy}", "received_events_url": "https://api.github.com/users/villmow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-05-05T12:11:27Z", "updated_at": "2021-06-08T19:10:02Z", "closed_at": "2021-06-08T19:08:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nSomehow caching does not work for me anymore. Am I doing something wrong, or is there anything that I missed?\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\nimport datasets\r\ndatasets.set_caching_enabled(True)\r\nsst = datasets.load_dataset(\"sst\")\r\n\r\ndef foo(samples, i):\r\n    print(\"executed\", i[:10])\r\n    return samples\r\n\r\n# first call\r\nx = sst.map(foo, batched=True, with_indices=True,  num_proc=2)\r\n\r\nprint('\\n'*3, \"#\" * 30, '\\n'*3)\r\n\r\n# second call\r\ny = sst.map(foo, batched=True, with_indices=True, num_proc=2)\r\n\r\n# print version\r\nimport sys\r\nimport platform\r\nprint(f\"\"\"\r\n- Datasets: {datasets.__version__}\r\n- Python: {sys.version}\r\n- Platform: {platform.platform()}\r\n\"\"\")\r\n```\r\n\r\n## Actual results\r\nThis code prints the following output for me:\r\n```bash\r\nNo config specified, defaulting to: sst/default\r\nReusing dataset sst (/home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\r\n#0:   0%|          | 0/5 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/5 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281]\r\nexecuted [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\r\nexecuted [5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281]\r\nexecuted [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]\r\nexecuted [6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281]\r\nexecuted [3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009]\r\nexecuted [7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281]\r\nexecuted [4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 59.85ba/s]\r\nexecuted [8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 60.85ba/s]\r\n#0:   0%|          | 0/1 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/1 [00:00<?, ?ba/s]executed [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 69.32ba/s]\r\nexecuted [551, 552, 553, 554, 555, 556, 557, 558, 559, 560]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 70.93ba/s]\r\n#0:   0%|          | 0/2 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/2 [00:00<?, ?ba/s]executed [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 63.25ba/s]\r\nexecuted [1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114]\r\nexecuted [2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 57.69ba/s]\r\n\r\n\r\n\r\n ############################## \r\n\r\n\r\n\r\n#0:   0%|          | 0/5 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/5 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281]\r\nexecuted [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\r\nexecuted [5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281]\r\nexecuted [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]\r\nexecuted [6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281]\r\nexecuted [3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009]\r\nexecuted [4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 58.10ba/s]\r\nexecuted [7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281]\r\nexecuted [8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 57.19ba/s]\r\n#0:   0%|          | 0/1 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/1 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 60.10ba/s]\r\nexecuted [551, 552, 553, 554, 555, 556, 557, 558, 559, 560]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 53.82ba/s]\r\n#0:   0%|          | 0/2 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/2 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\r\nexecuted [1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114]\r\n#0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 72.76ba/s]\r\nexecuted [2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114]\r\n#1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 71.55ba/s]\r\n\r\n- Datasets: 1.6.1\r\n- Python: 3.8.3 (default, May 19 2020, 18:47:26) \r\n[GCC 7.3.0]\r\n- Platform: Linux-5.4.0-72-generic-x86_64-with-glibc2.10\r\n```\r\n\r\n## Expected results\r\nCaching should work.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2322/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2322/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2319", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2319/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2319/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2319/events", "html_url": "https://github.com/huggingface/datasets/issues/2319", "id": 876251376, "node_id": "MDU6SXNzdWU4NzYyNTEzNzY=", "number": 2319, "title": "UnicodeDecodeError for OSCAR (Afrikaans)", "user": {"login": "sgraaf", "id": 8904453, "node_id": "MDQ6VXNlcjg5MDQ0NTM=", "avatar_url": "https://avatars.githubusercontent.com/u/8904453?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sgraaf", "html_url": "https://github.com/sgraaf", "followers_url": "https://api.github.com/users/sgraaf/followers", "following_url": "https://api.github.com/users/sgraaf/following{/other_user}", "gists_url": "https://api.github.com/users/sgraaf/gists{/gist_id}", "starred_url": "https://api.github.com/users/sgraaf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sgraaf/subscriptions", "organizations_url": "https://api.github.com/users/sgraaf/orgs", "repos_url": "https://api.github.com/users/sgraaf/repos", "events_url": "https://api.github.com/users/sgraaf/events{/privacy}", "received_events_url": "https://api.github.com/users/sgraaf/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-05-05T09:22:52Z", "updated_at": "2021-05-05T10:57:31Z", "closed_at": "2021-05-05T10:50:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen loading the [OSCAR dataset](https://huggingface.co/datasets/oscar) (specifically `unshuffled_deduplicated_af`), I encounter a `UnicodeDecodeError`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_af\")\r\n```\r\n\r\n## Expected results\r\nAnything but an error, really.\r\n\r\n## Actual results\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_af\")\r\nDownloading: 14.7kB [00:00, 4.91MB/s]\r\nDownloading: 3.07MB [00:00, 32.6MB/s]\r\nDownloading and preparing dataset oscar/unshuffled_deduplicated_af (download: 62.93 MiB, generated: 163.38 MiB, post-processed: Unknown size, total: 226.32 MiB) to C:\\Users\\sgraaf\\.cache\\huggingface\\datasets\\oscar\\unshuffled_deduplicated_af\\1.0.0\\bd4f96df5b4512007ef9fd17bbc1ecde459fa53d2fc0049cf99392ba2efcc464...\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81.0/81.0 [00:00<00:00, 40.5kB/s]\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 66.0M/66.0M [00:18<00:00, 3.50MB/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\load.py\", line 745, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\builder.py\", line 574, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\builder.py\", line 652, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\builder.py\", line 979, in _prepare_split\r\n    for key, record in utils.tqdm(\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\std.py\", line 1133, in __iter__\r\n    for obj in iterable:\r\n  File \"C:\\Users\\sgraaf\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\oscar\\bd4f96df5b4512007ef9fd17bbc1ecde459fa53d2fc0049cf99392ba2efcc464\\oscar.py\", line 359, in _generate_examples\r\n    for line in f:\r\n  File \"C:\\Users\\sgraaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\encodings\\cp1252.py\", line 23, in decode\r\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 7454: character maps to <undefined>\r\n```\r\n\r\n## Versions\r\nPaste the output of the following code:\r\n```python\r\nimport datasets\r\nimport sys\r\nimport platform\r\n\r\nprint(f\"\"\"\r\n- Datasets: {datasets.__version__}\r\n- Python: {sys.version}\r\n- Platform: {platform.platform()}\r\n\"\"\")\r\n```\r\n- Datasets: 1.6.2\r\n- Python: 3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]\r\n- Platform: Windows-10-10.0.19041-SP0", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2319/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2319/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2316", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2316/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2316/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2316/events", "html_url": "https://github.com/huggingface/datasets/issues/2316", "id": 875756353, "node_id": "MDU6SXNzdWU4NzU3NTYzNTM=", "number": 2316, "title": "Incorrect version specification for pyarrow", "user": {"login": "cemilcengiz", "id": 32267027, "node_id": "MDQ6VXNlcjMyMjY3MDI3", "avatar_url": "https://avatars.githubusercontent.com/u/32267027?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cemilcengiz", "html_url": "https://github.com/cemilcengiz", "followers_url": "https://api.github.com/users/cemilcengiz/followers", "following_url": "https://api.github.com/users/cemilcengiz/following{/other_user}", "gists_url": "https://api.github.com/users/cemilcengiz/gists{/gist_id}", "starred_url": "https://api.github.com/users/cemilcengiz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cemilcengiz/subscriptions", "organizations_url": "https://api.github.com/users/cemilcengiz/orgs", "repos_url": "https://api.github.com/users/cemilcengiz/repos", "events_url": "https://api.github.com/users/cemilcengiz/events{/privacy}", "received_events_url": "https://api.github.com/users/cemilcengiz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-05-04T19:15:11Z", "updated_at": "2021-05-05T10:10:03Z", "closed_at": "2021-05-05T10:10:03Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\nThe pyarrow dependency is incorrectly specified in setup.py file, in [this line](https://github.com/huggingface/datasets/blob/3a3e5a4da20bfcd75f8b6a6869b240af8feccc12/setup.py#L77).\r\nAlso as a snippet:\r\n```python\r\n \"pyarrow>=1.0.0<4.0.0\",\r\n```\r\n\r\n## Steps to reproduce the bug\r\n```bash\r\n pip install \"pyarrow>=1.0.0<4.0.0\"\r\n```\r\n\r\n## Expected results\r\nIt is expected to get a pyarrow version between 1.0.0 (inclusive) and 4.0.0 (exclusive).\r\n\r\n## Actual results\r\npip ignores the specified versions since there is a missing comma between the lower and upper limits. Therefore, pip installs the latest pyarrow version from PYPI, which is 4.0.0.\r\nThis is especially problematic since \"conda env export\" fails due to incorrect version specification. Here is the conda error as well:\r\n```bash\r\nconda env export\r\nInvalidVersionSpec: Invalid version '1.0.0<4.0.0': invalid character(s)\r\n```\r\n\r\n\r\n## Fix suggestion\r\nPut a comma between the version limits which means replacing the line in setup.py file with the following:\r\n```python\r\n \"pyarrow>=1.0.0,<4.0.0\",\r\n```\r\n\r\n## Versions\r\nPaste the output of the following code:\r\n```python\r\n- Datasets: 1.6.2\r\n- Python: 3.7.10 (default, Feb 26 2021, 18:47:35) \r\n[GCC 7.3.0]\r\n- Platform: Linux-5.4.0-42-generic-x86_64-with-debian-buster-sid\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2316/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2316/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2288", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2288/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2288/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2288/events", "html_url": "https://github.com/huggingface/datasets/issues/2288", "id": 871111235, "node_id": "MDU6SXNzdWU4NzExMTEyMzU=", "number": 2288, "title": "Load_dataset for local CSV files", "user": {"login": "sstojanoska", "id": 17052700, "node_id": "MDQ6VXNlcjE3MDUyNzAw", "avatar_url": "https://avatars.githubusercontent.com/u/17052700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sstojanoska", "html_url": "https://github.com/sstojanoska", "followers_url": "https://api.github.com/users/sstojanoska/followers", "following_url": "https://api.github.com/users/sstojanoska/following{/other_user}", "gists_url": "https://api.github.com/users/sstojanoska/gists{/gist_id}", "starred_url": "https://api.github.com/users/sstojanoska/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sstojanoska/subscriptions", "organizations_url": "https://api.github.com/users/sstojanoska/orgs", "repos_url": "https://api.github.com/users/sstojanoska/repos", "events_url": "https://api.github.com/users/sstojanoska/events{/privacy}", "received_events_url": "https://api.github.com/users/sstojanoska/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-04-29T15:01:10Z", "updated_at": "2021-06-15T13:49:26Z", "closed_at": "2021-06-15T13:49:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "The method load_dataset fails to correctly load a dataset from csv. \r\n\r\nMoreover, I am working on a token-classification task ( POS tagging) , where each row in my CSV contains two columns each of them having a list of strings.\r\nrow example:\r\n```tokens  |  labels\r\n['I' , 'am', 'John']  |  ['PRON', 'AUX', 'PROPN' ] \r\n```\r\nThe method, loads each list as a string:  (i.g \"['I' , 'am', 'John']\").\r\nTo solve this issue, I copied the Datasets.Features, created Sequence types ( instead of Value)  and tried to cast the features type\r\n```\r\nnew_features['tokens'] = Sequence(feature=Value(dtype='string', id=None))\r\nnew_features['labels'] = Sequence(feature=ClassLabel(num_classes=len(tag2idx), names=list(unique_tags)))\r\ndataset = dataset.cast(new_features)\r\n```\r\nbut I got the following error \r\n```\r\nArrowNotImplementedError: Unsupported cast from string to list using function cast_list\r\n```\r\nMoreover, I tried to set feature parameter in load_dataset method, to my new_features, but this fails as well.\r\nHow can this be solved ?", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2288/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2288/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2279", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2279/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2279/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2279/events", "html_url": "https://github.com/huggingface/datasets/issues/2279", "id": 870431662, "node_id": "MDU6SXNzdWU4NzA0MzE2NjI=", "number": 2279, "title": "Compatibility with Ubuntu 18 and GLIBC 2.27?", "user": {"login": "tginart", "id": 11379648, "node_id": "MDQ6VXNlcjExMzc5NjQ4", "avatar_url": "https://avatars.githubusercontent.com/u/11379648?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tginart", "html_url": "https://github.com/tginart", "followers_url": "https://api.github.com/users/tginart/followers", "following_url": "https://api.github.com/users/tginart/following{/other_user}", "gists_url": "https://api.github.com/users/tginart/gists{/gist_id}", "starred_url": "https://api.github.com/users/tginart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tginart/subscriptions", "organizations_url": "https://api.github.com/users/tginart/orgs", "repos_url": "https://api.github.com/users/tginart/repos", "events_url": "https://api.github.com/users/tginart/events{/privacy}", "received_events_url": "https://api.github.com/users/tginart/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-04-28T22:08:07Z", "updated_at": "2021-04-29T07:42:42Z", "closed_at": "2021-04-29T07:42:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nFor use on Ubuntu systems, it seems that datasets requires GLIBC 2.29. However, Ubuntu 18 runs with GLIBC 2.27 and it seems [non-trivial to upgrade GLIBC to 2.29 for Ubuntu 18 users](https://www.digitalocean.com/community/questions/how-install-glibc-2-29-or-higher-in-ubuntu-18-04). \r\n\r\nI'm not sure if there is anything that can be done about this, but I'd like to confirm that using huggingface/datasets requires either an upgrade to Ubuntu 19/20 or a hand-rolled install of a higher version of GLIBC.\r\n\r\n## Steps to reproduce the bug\r\n1. clone the transformers repo\r\n2. move to examples/pytorch/language-modeling\r\n3. run example command:\r\n```python run_clm.py     --model_name_or_path gpt2     --dataset_name wikitext     --dataset_config_name wikitext-2-raw-v1     --do_train     --do_eval     --output_dir /tmp/test-clm```\r\n\r\n\r\n## Expected results\r\nAs described in the transformers repo.\r\n\r\n## Actual results\r\n```Traceback (most recent call last):\r\n  File \"run_clm.py\", line 34, in <module>\r\n    from transformers import (\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/__init__.py\", line 2487, in __getattr__\r\n    return super().__getattr__(name)\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/file_utils.py\", line 1699, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/__init__.py\", line 2481, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/__init__.py\", line 19, in <module>\r\n    from . import (\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/layoutlm/__init__.py\", line 23, in <module>\r\n    from .tokenization_layoutlm import LayoutLMTokenizer\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/layoutlm/tokenization_layoutlm.py\", line 19, in <module>\r\n    from ..bert.tokenization_bert import BertTokenizer\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\", line 23, in <module>\r\n    from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/tokenization_utils.py\", line 26, in <module>\r\n    from .tokenization_utils_base import (\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 68, in <module>\r\n    from tokenizers import AddedToken\r\n  File \"/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/tokenizers/__init__.py\", line 79, in <module>\r\n    from .tokenizers import (\r\nImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/tokenizers/tokenizers.cpython-37m-x86_64-linux-gnu.so)\r\n```\r\n\r\n## Versions\r\nPaste the output of the following code:\r\n```\r\n- Datasets: 1.6.1\r\n- Python: 3.7.10 (default, Feb 26 2021, 18:47:35) \r\n[GCC 7.3.0]\r\n- Platform: Linux-4.15.0-128-generic-x86_64-with-debian-buster-sid\r\n\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2279/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2279/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2276", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2276/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2276/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2276/events", "html_url": "https://github.com/huggingface/datasets/issues/2276", "id": 870010511, "node_id": "MDU6SXNzdWU4NzAwMTA1MTE=", "number": 2276, "title": "concatenate_datasets loads all the data into memory", "user": {"login": "TaskManager91", "id": 7063207, "node_id": "MDQ6VXNlcjcwNjMyMDc=", "avatar_url": "https://avatars.githubusercontent.com/u/7063207?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TaskManager91", "html_url": "https://github.com/TaskManager91", "followers_url": "https://api.github.com/users/TaskManager91/followers", "following_url": "https://api.github.com/users/TaskManager91/following{/other_user}", "gists_url": "https://api.github.com/users/TaskManager91/gists{/gist_id}", "starred_url": "https://api.github.com/users/TaskManager91/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TaskManager91/subscriptions", "organizations_url": "https://api.github.com/users/TaskManager91/orgs", "repos_url": "https://api.github.com/users/TaskManager91/repos", "events_url": "https://api.github.com/users/TaskManager91/events{/privacy}", "received_events_url": "https://api.github.com/users/TaskManager91/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2021-04-28T14:27:21Z", "updated_at": "2021-05-03T08:41:55Z", "closed_at": "2021-05-03T08:41:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nWhen I try to concatenate 2 datasets (10GB each) , the entire data is loaded into memory instead of being written directly to disk.\r\n\r\nInterestingly, this happens when trying to save the new dataset to disk or concatenating it again.\r\n\r\n![image](https://user-images.githubusercontent.com/7063207/116420321-2b21b480-a83e-11eb-9006-8f6ca729fb6f.png)\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import concatenate_datasets, load_from_disk\r\n\r\ntest_sampled_pro = load_from_disk(\"test_sampled_pro\")\r\nval_sampled_pro = load_from_disk(\"val_sampled_pro\")\r\n\r\nbig_set = concatenate_datasets([test_sampled_pro, val_sampled_pro])\r\n\r\n# Loaded to memory\r\nbig_set.save_to_disk(\"big_set\")\r\n\r\n# Loaded to memory\r\nbig_set = concatenate_datasets([big_set, val_sampled_pro])\r\n```\r\n\r\n## Expected results\r\nThe data should be loaded into memory in batches and then saved directly to disk.\r\n\r\n## Actual results\r\nThe entire data set is loaded into the memory and then saved to the hard disk.\r\n\r\n## Versions\r\nPaste the output of the following code:\r\n```python\r\n- Datasets: 1.6.1\r\n- Python: 3.8.8 (default, Apr 13 2021, 19:58:26) \r\n[GCC 7.3.0]\r\n- Platform: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.10\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2276/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2276/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2272", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2272/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2272/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2272/events", "html_url": "https://github.com/huggingface/datasets/issues/2272", "id": 869017977, "node_id": "MDU6SXNzdWU4NjkwMTc5Nzc=", "number": 2272, "title": "Bug in Dataset.class_encode_column", "user": {"login": "albertvillanova", "id": 8515462, "node_id": "MDQ6VXNlcjg1MTU0NjI=", "avatar_url": "https://avatars.githubusercontent.com/u/8515462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertvillanova", "html_url": "https://github.com/albertvillanova", "followers_url": "https://api.github.com/users/albertvillanova/followers", "following_url": "https://api.github.com/users/albertvillanova/following{/other_user}", "gists_url": "https://api.github.com/users/albertvillanova/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertvillanova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertvillanova/subscriptions", "organizations_url": "https://api.github.com/users/albertvillanova/orgs", "repos_url": "https://api.github.com/users/albertvillanova/repos", "events_url": "https://api.github.com/users/albertvillanova/events{/privacy}", "received_events_url": "https://api.github.com/users/albertvillanova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-04-27T16:13:18Z", "updated_at": "2021-04-30T12:54:27Z", "closed_at": "2021-04-30T12:54:27Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nAll the rest of the columns except the one passed to `Dataset.class_encode_column` are discarded.\r\n\r\n## Expected results\r\n\r\nAll the original columns should be kept.\r\n\r\nThis needs regression tests.\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2272/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2272/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2256", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2256/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2256/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2256/events", "html_url": "https://github.com/huggingface/datasets/issues/2256", "id": 866708609, "node_id": "MDU6SXNzdWU4NjY3MDg2MDk=", "number": 2256, "title": "Running `datase.map` with `num_proc > 1` uses a lot of memory", "user": {"login": "roskoN", "id": 8143425, "node_id": "MDQ6VXNlcjgxNDM0MjU=", "avatar_url": "https://avatars.githubusercontent.com/u/8143425?v=4", "gravatar_id": "", "url": "https://api.github.com/users/roskoN", "html_url": "https://github.com/roskoN", "followers_url": "https://api.github.com/users/roskoN/followers", "following_url": "https://api.github.com/users/roskoN/following{/other_user}", "gists_url": "https://api.github.com/users/roskoN/gists{/gist_id}", "starred_url": "https://api.github.com/users/roskoN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/roskoN/subscriptions", "organizations_url": "https://api.github.com/users/roskoN/orgs", "repos_url": "https://api.github.com/users/roskoN/repos", "events_url": "https://api.github.com/users/roskoN/events{/privacy}", "received_events_url": "https://api.github.com/users/roskoN/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2021-04-24T09:56:20Z", "updated_at": "2021-04-26T17:12:15Z", "closed_at": "2021-04-26T17:12:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\nRunning `datase.map` with `num_proc > 1`  leads to a tremendous memory usage that requires swapping on disk and it becomes very slow.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndstc8_datset = load_dataset(\"roskoN/dstc8-reddit-corpus\", keep_in_memory=False)\r\n\r\n\r\ndef _prepare_sample(batch):\r\n    return {\"input_ids\": list(), \"attention_mask\": list()}\r\n\r\n\r\nfor split_name, dataset_split in list(dstc8_datset.items()):\r\n    print(f\"Processing {split_name}\")\r\n    encoded_dataset_split = dataset_split.map(\r\n        function=_prepare_sample,\r\n        batched=True,\r\n        num_proc=4,\r\n        remove_columns=dataset_split.column_names,\r\n        batch_size=10,\r\n        writer_batch_size=10,\r\n        keep_in_memory=False,\r\n    )\r\n    print(encoded_dataset_split)\r\n\r\n    path = f\"./data/encoded_{split_name}\"\r\n\r\n    encoded_dataset_split.save_to_disk(path)\r\n```\r\n\r\n## Expected results\r\nMemory usage should stay within reasonable boundaries.\r\n\r\n\r\n## Actual results\r\nThis is htop-output from running the provided script.\r\n\r\n![image](https://user-images.githubusercontent.com/8143425/115954836-66954980-a4f3-11eb-8340-0153bdc3a475.png)\r\n\r\n## Versions\r\n```\r\n- Datasets: 1.6.0\r\n- Python: 3.8.8 (default, Apr 13 2021, 19:58:26)\r\n[GCC 7.3.0]\r\n- Platform: Linux-4.19.128-microsoft-standard-x86_64-with-glibc2.10\r\n```\r\nRunning on WSL2\r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2256/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2256/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2243", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2243/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2243/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2243/events", "html_url": "https://github.com/huggingface/datasets/issues/2243", "id": 862909389, "node_id": "MDU6SXNzdWU4NjI5MDkzODk=", "number": 2243, "title": "Map is slow and processes batches one after another", "user": {"login": "villmow", "id": 2743060, "node_id": "MDQ6VXNlcjI3NDMwNjA=", "avatar_url": "https://avatars.githubusercontent.com/u/2743060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/villmow", "html_url": "https://github.com/villmow", "followers_url": "https://api.github.com/users/villmow/followers", "following_url": "https://api.github.com/users/villmow/following{/other_user}", "gists_url": "https://api.github.com/users/villmow/gists{/gist_id}", "starred_url": "https://api.github.com/users/villmow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/villmow/subscriptions", "organizations_url": "https://api.github.com/users/villmow/orgs", "repos_url": "https://api.github.com/users/villmow/repos", "events_url": "https://api.github.com/users/villmow/events{/privacy}", "received_events_url": "https://api.github.com/users/villmow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-04-20T14:58:20Z", "updated_at": "2021-05-03T17:54:33Z", "closed_at": "2021-05-03T17:54:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nI have a somewhat unclear bug to me, where I can't figure out what the problem is. The code works as expected on a small subset of my dataset (2000 samples) on my local machine, but when I execute the same code with a larger dataset (1.4 million samples) this problem occurs. Thats why I can't give exact steps to reproduce, I'm sorry. \r\n\r\nI process a large dataset in a two step process. I first call map on a dataset I load from disk and create a new dataset from it. This works like expected and `map` uses all workers I started it with. Then I process the dataset created by the first step, again with `map`, which is really slow and starting only one or two process at a time. Number of processes is the same for both steps.\r\n\r\npseudo code:\r\n```python\r\nds = datasets.load_from_disk(\"path\")\r\nnew_dataset = ds.map(work, batched=True, ...)  # fast uses all processes\r\nfinal_dataset = new_dataset.map(work2, batched=True, ...)  # slow starts one process after another\r\n```\r\n\r\n## Expected results\r\nSecond stage should be as fast as the first stage.\r\n\r\n## Versions\r\nPaste the output of the following code:\r\n- Datasets: 1.5.0\r\n- Python: 3.8.8 (default, Feb 24 2021, 21:46:12)\r\n- Platform: Linux-5.4.0-60-generic-x86_64-with-glibc2.10    \r\n\r\nDo you guys have any idea? Thanks a lot!", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2243/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2242", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2242/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2242/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2242/events", "html_url": "https://github.com/huggingface/datasets/issues/2242", "id": 862870205, "node_id": "MDU6SXNzdWU4NjI4NzAyMDU=", "number": 2242, "title": "Link to datasets viwer on Quick Tour page returns \"502 Bad Gateway\"", "user": {"login": "martavillegas", "id": 6735707, "node_id": "MDQ6VXNlcjY3MzU3MDc=", "avatar_url": "https://avatars.githubusercontent.com/u/6735707?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martavillegas", "html_url": "https://github.com/martavillegas", "followers_url": "https://api.github.com/users/martavillegas/followers", "following_url": "https://api.github.com/users/martavillegas/following{/other_user}", "gists_url": "https://api.github.com/users/martavillegas/gists{/gist_id}", "starred_url": "https://api.github.com/users/martavillegas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martavillegas/subscriptions", "organizations_url": "https://api.github.com/users/martavillegas/orgs", "repos_url": "https://api.github.com/users/martavillegas/repos", "events_url": "https://api.github.com/users/martavillegas/events{/privacy}", "received_events_url": "https://api.github.com/users/martavillegas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-04-20T14:19:51Z", "updated_at": "2021-04-20T15:02:45Z", "closed_at": "2021-04-20T15:02:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Link to datasets viwer (https://huggingface.co/datasets/viewer/)  on Quick Tour page  (https://huggingface.co/docs/datasets/quicktour.html) returns \"502 Bad Gateway\"\r\n\r\nThe same error with https://huggingface.co/datasets/viewer/?dataset=glue&config=mrpc ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2242/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2239", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2239/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2239/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2239/events", "html_url": "https://github.com/huggingface/datasets/issues/2239", "id": 861904306, "node_id": "MDU6SXNzdWU4NjE5MDQzMDY=", "number": 2239, "title": "Error loading wikihow dataset", "user": {"login": "odellus", "id": 4686956, "node_id": "MDQ6VXNlcjQ2ODY5NTY=", "avatar_url": "https://avatars.githubusercontent.com/u/4686956?v=4", "gravatar_id": "", "url": "https://api.github.com/users/odellus", "html_url": "https://github.com/odellus", "followers_url": "https://api.github.com/users/odellus/followers", "following_url": "https://api.github.com/users/odellus/following{/other_user}", "gists_url": "https://api.github.com/users/odellus/gists{/gist_id}", "starred_url": "https://api.github.com/users/odellus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/odellus/subscriptions", "organizations_url": "https://api.github.com/users/odellus/orgs", "repos_url": "https://api.github.com/users/odellus/repos", "events_url": "https://api.github.com/users/odellus/events{/privacy}", "received_events_url": "https://api.github.com/users/odellus/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-04-19T21:02:31Z", "updated_at": "2021-04-20T16:33:11Z", "closed_at": "2021-04-20T16:33:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Describe the bug\r\n\r\nWhen attempting to load wikihow into a dataset with\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('wikihow', data_dir='./wikihow')\r\n```\r\nI get the message:\r\n```\r\nAttributeError: 'BuilderConfig' object has no attribute 'filename'\r\n```\r\nat the end of a [full stack trace](https://gist.github.com/odellus/602c3b2de52f541d353b1022f320ffc2).\r\n\r\n## Steps to reproduce the bug\r\n\r\nI have followed the instructions for creating a wikihow dataset. The [wikihow dataset site](https://huggingface.co/datasets/wikihow) says to use \r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('wikihow')\r\n```\r\nto load the dataset. I do so and I get the message\r\n```\r\nAssertionError: The dataset wikihow with config all requires manual data.\r\n Please follow the manual download instructions:   You need to manually download two wikihow files. An overview of which files to download can be seen at https://github.com/mahnazkoupaee/WikiHow-Dataset.\r\n  You need to download the following two files manually:\r\n    1) https://ucsb.app.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358 and save the file under <path/to/folder>/wikihowAll.csv\r\n    2) https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag and save the file under <path/to/folder>/wikihowSep.csv\r\n\r\n  The <path/to/folder> can e.g. be \"~/manual_wikihow_data\".\r\n\r\n  Wikihow can then be loaded using the following command `datasets.load_dataset(\"wikihow\", data_dir=\"<path/to/folder>\")`.\r\n  .\r\n Manual data can be loaded with `datasets.load_dataset(wikihow, data_dir='<path/to/manual/data>')\r\n```\r\n\r\nSo I create a directory `./wikihow` and download `wikihowAll.csv` and `wikihowSep.csv` into the new directory.\r\n\r\nThen I run \r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('wikihow', data_dir='./wikihow')\r\n```\r\n\r\nthat's when I get the [stack trace](https://gist.github.com/odellus/602c3b2de52f541d353b1022f320ffc2)\r\n\r\n## Expected results\r\nI expected it to load the downloaded files into a dataset.\r\n\r\n## Actual results\r\n```python\r\nUsing custom data configuration default-data_dir=.%2Fwikihow\r\nDownloading and preparing dataset wikihow/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/azureuser/.cache/huggingface/datasets/wikihow/default-data_dir=.%2Fwikihow/0.0.0/58f42f8f0e4d459811a0f69aaab35870093830ccd58006769e7e1eb3e0e686c2...                                                    ---------------------------------------------------------------------------\r\nAttributeError\r\nTraceback (most recent call last)\r\n<ipython-input-9-5e4d40142f30> in <module>\r\n----> 1 dataset = load_dataset('wikihow',data_dir='./wikihow')\r\n~/.local/lib/python3.6/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\r\n745         try_from_hf_gcs=try_from_hf_gcs,\r\n746         base_path=base_path,--> \r\n747         use_auth_token=use_auth_token,\r\n748     )\r\n749 \r\n~/.local/lib/python3.6/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n577                     if not downloaded_from_gcs:\r\n578                         self._download_and_prepare(                                                             -->\r\n579                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs        \r\n580                         )                                                                                          \r\n581                     # Sync info\r\n~/.local/lib/python3.6/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n632         split_dict = SplitDict(dataset_name=self.name)\r\n633         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)                      -->\r\n634         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)                            \r\n635                                                                                                                     \r\n636         # Checksums verification\r\n~/.cache/huggingface/modules/datasets_modules/datasets/wikihow/58f42f8f0e4d459811a0f69aaab35870093830ccd58006769e7e1eb3e0e686c2/wikihow.py in _split_generators(self, dl_manager)\r\n132\r\n133         path_to_manual_file = os.path.join(\r\n--> 134             os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), self.config.filename                        \r\n135         )                                                                                                           \r\n136\r\nAttributeError: 'BuilderConfig' object has no attribute 'filename'\r\n```\r\n## Versions\r\nPaste the output of the following code:\r\n```python\r\nimport datasets\r\nimport sys\r\nimport platform\r\n\r\nprint(f\"\"\"\r\n- Datasets: {datasets.__version__}\r\n- Python: {sys.version}\r\n- Platform: {platform.platform()}\r\n\"\"\")\r\n```\r\n```\r\n- Datasets: 1.5.0\r\n- Python: 3.6.9 (default, Jan 26 2021, 15:33:00)                                      [GCC 8.4.0]\r\n- Platform: Linux-5.4.0-1046-azure-x86_64-with-Ubuntu-18.04-bionic\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2239/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2239/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2226", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2226/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2226/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2226/events", "html_url": "https://github.com/huggingface/datasets/issues/2226", "id": 859720302, "node_id": "MDU6SXNzdWU4NTk3MjAzMDI=", "number": 2226, "title": "Batched map fails when removing all columns", "user": {"login": "villmow", "id": 2743060, "node_id": "MDQ6VXNlcjI3NDMwNjA=", "avatar_url": "https://avatars.githubusercontent.com/u/2743060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/villmow", "html_url": "https://github.com/villmow", "followers_url": "https://api.github.com/users/villmow/followers", "following_url": "https://api.github.com/users/villmow/following{/other_user}", "gists_url": "https://api.github.com/users/villmow/gists{/gist_id}", "starred_url": "https://api.github.com/users/villmow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/villmow/subscriptions", "organizations_url": "https://api.github.com/users/villmow/orgs", "repos_url": "https://api.github.com/users/villmow/repos", "events_url": "https://api.github.com/users/villmow/events{/privacy}", "received_events_url": "https://api.github.com/users/villmow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-04-16T11:17:01Z", "updated_at": "2022-10-05T17:32:15Z", "closed_at": "2022-10-05T17:32:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi @lhoestq ,\r\n\r\nI'm hijacking this issue, because I'm currently trying to do the approach you recommend:\r\n\r\n> Currently the optimal setup for single-column computations is probably to do something like\r\n> \r\n> ```python\r\n> result = dataset.map(f, input_columns=\"my_col\", remove_columns=dataset.column_names)\r\n> ```\r\n\r\nHere is my code: (see edit, in which I added a simplified version\r\n\r\n```\r\nThis is the error:\r\n```bash\r\npyarrow.lib.ArrowInvalid: Column 1 named tokens expected length 8964 but got length 1000\r\n```\r\nI wonder why this error occurs, when I delete every column? Can you give me a hint?\r\n\r\n### Edit:\r\nI preprocessed my dataset before (using map with the features argument) and saved it to disk. May this be part of the error?  I can iterate over the\r\ncomplete dataset and print every sample before calling map. There seems to be no other problem with the dataset.\r\n\r\nI tried to simplify the code that crashes:\r\n\r\n```python\r\n# works\r\nlog.debug(dataset.column_names)\r\nlog.debug(dataset)\r\nfor i, sample in enumerate(dataset):\r\n    log.debug(i, sample)\r\n\r\n# crashes\r\ncounted_dataset = dataset.map(\r\n    lambda x: {\"a\": list(range(20))},\r\n    input_columns=column,\r\n    remove_columns=dataset.column_names,\r\n    load_from_cache_file=False,\r\n    num_proc=num_workers,\r\n    batched=True,\r\n)\r\n```\r\n\r\n```\r\npyarrow.lib.ArrowInvalid: Column 1 named tokens expected length 20 but got length 1000\r\n```\r\n\r\nEdit2: \r\n\r\nMay this be a problem with a schema I set when preprocessing the dataset before? I tried to add the `features` argument to the function and then I get a new error:\r\n\r\n```python\r\n# crashes\r\ncounted_dataset = dataset.map(\r\n    lambda x: {\"a\": list(range(20))},\r\n    input_columns=column,\r\n    remove_columns=dataset.column_names,\r\n    load_from_cache_file=False,\r\n    num_proc=num_workers,\r\n    batched=True,\r\n    features=datasets.Features(\r\n        {\r\n              \"a\": datasets.Sequence(datasets.Value(\"int32\"))\r\n         }\r\n    )\r\n)\r\n```\r\n\r\n```\r\n File \"env/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1704, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"env/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 312, in write_batch\r\n    col_type = schema.field(col).type if schema is not None else None\r\n  File \"pyarrow/types.pxi\", line 1341, in pyarrow.lib.Schema.field\r\nKeyError: 'Column tokens does not exist in schema'\r\n```\r\n\r\n_Originally posted by @villmow in https://github.com/huggingface/datasets/issues/2193#issuecomment-820230874_", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2226/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2226/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2214", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2214/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2214/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2214/events", "html_url": "https://github.com/huggingface/datasets/issues/2214", "id": 856333657, "node_id": "MDU6SXNzdWU4NTYzMzM2NTc=", "number": 2214, "title": "load_metric error: module 'datasets.utils.file_utils' has no attribute 'add_start_docstrings'", "user": {"login": "nsaphra", "id": 414788, "node_id": "MDQ6VXNlcjQxNDc4OA==", "avatar_url": "https://avatars.githubusercontent.com/u/414788?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nsaphra", "html_url": "https://github.com/nsaphra", "followers_url": "https://api.github.com/users/nsaphra/followers", "following_url": "https://api.github.com/users/nsaphra/following{/other_user}", "gists_url": "https://api.github.com/users/nsaphra/gists{/gist_id}", "starred_url": "https://api.github.com/users/nsaphra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nsaphra/subscriptions", "organizations_url": "https://api.github.com/users/nsaphra/orgs", "repos_url": "https://api.github.com/users/nsaphra/repos", "events_url": "https://api.github.com/users/nsaphra/events{/privacy}", "received_events_url": "https://api.github.com/users/nsaphra/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-04-12T20:26:01Z", "updated_at": "2021-04-23T15:20:02Z", "closed_at": "2021-04-23T15:20:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm having the same problem as [Notebooks issue 10](https://github.com/huggingface/notebooks/issues/10) on datasets 1.2.1, and it seems to be an issue with the datasets package.\r\n\r\n```python\r\n>>> from datasets import load_metric\r\n>>> metric = load_metric(\"glue\", \"sst2\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/ext3/miniconda3/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg/datasets/load.py\", line 502, in load_metric\r\n  File \"/ext3/miniconda3/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg/datasets/load.py\", line 66, in import_main_class\r\n  File \"/ext3/miniconda3/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/ns4008/.cache/huggingface/modules/datasets_modules/metrics/glue/e4606ab9804a36bcd5a9cebb2cb65bb14b6ac78ee9e6d5981fa679a495dd55de/glue.py\", line 105, in <module>\r\n    @datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\r\nAttributeError: module 'datasets.utils.file_utils' has no attribute 'add_start_docstrings'\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2214/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2214/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2206", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2206/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2206/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2206/events", "html_url": "https://github.com/huggingface/datasets/issues/2206", "id": 855252415, "node_id": "MDU6SXNzdWU4NTUyNTI0MTU=", "number": 2206, "title": "Got pyarrow error when loading a dataset while adding special tokens into the tokenizer", "user": {"login": "yana-xuyan", "id": 38536635, "node_id": "MDQ6VXNlcjM4NTM2NjM1", "avatar_url": "https://avatars.githubusercontent.com/u/38536635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yana-xuyan", "html_url": "https://github.com/yana-xuyan", "followers_url": "https://api.github.com/users/yana-xuyan/followers", "following_url": "https://api.github.com/users/yana-xuyan/following{/other_user}", "gists_url": "https://api.github.com/users/yana-xuyan/gists{/gist_id}", "starred_url": "https://api.github.com/users/yana-xuyan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yana-xuyan/subscriptions", "organizations_url": "https://api.github.com/users/yana-xuyan/orgs", "repos_url": "https://api.github.com/users/yana-xuyan/repos", "events_url": "https://api.github.com/users/yana-xuyan/events{/privacy}", "received_events_url": "https://api.github.com/users/yana-xuyan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2021-04-11T08:40:09Z", "updated_at": "2021-11-10T12:18:30Z", "closed_at": "2021-11-10T12:04:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "I added five more special tokens into the GPT2 tokenizer. But after that, when I try to pre-process the data using my previous code, I got an error shown below:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1687, in _map_single\r\n    writer.write(example)\r\n  File \"/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 296, in write\r\n    self.write_on_file()\r\n  File \"/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 270, in write_on_file\r\n    pa_array = pa.array(typed_sequence)\r\n  File \"pyarrow/array.pxi\", line 222, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_writer.py\", line 108, in __arrow_array__\r\n    out = out.cast(pa.list_(self.optimized_int_type))\r\n  File \"pyarrow/array.pxi\", line 810, in pyarrow.lib.Array.cast\r\n  File \"/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/pyarrow/compute.py\", line 281, in cast\r\n    return call_function(\"cast\", [arr], options)\r\n  File \"pyarrow/_compute.pyx\", line 465, in pyarrow._compute.call_function\r\n  File \"pyarrow/_compute.pyx\", line 294, in pyarrow._compute.Function.call\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Integer value 50259 not in range: -128 to 127\r\n\r\nDo you have any idea about it?", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2206/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2206/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2195", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2195/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2195/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2195/events", "html_url": "https://github.com/huggingface/datasets/issues/2195", "id": 854070194, "node_id": "MDU6SXNzdWU4NTQwNzAxOTQ=", "number": 2195, "title": "KeyError: '_indices_files' in `arrow_dataset.py`", "user": {"login": "samsontmr", "id": 15007950, "node_id": "MDQ6VXNlcjE1MDA3OTUw", "avatar_url": "https://avatars.githubusercontent.com/u/15007950?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samsontmr", "html_url": "https://github.com/samsontmr", "followers_url": "https://api.github.com/users/samsontmr/followers", "following_url": "https://api.github.com/users/samsontmr/following{/other_user}", "gists_url": "https://api.github.com/users/samsontmr/gists{/gist_id}", "starred_url": "https://api.github.com/users/samsontmr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samsontmr/subscriptions", "organizations_url": "https://api.github.com/users/samsontmr/orgs", "repos_url": "https://api.github.com/users/samsontmr/repos", "events_url": "https://api.github.com/users/samsontmr/events{/privacy}", "received_events_url": "https://api.github.com/users/samsontmr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-04-09T01:37:12Z", "updated_at": "2021-04-09T09:55:09Z", "closed_at": "2021-04-09T09:54:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "After pulling the latest master, I'm getting a crash when `load_from_disk` tries to load my local dataset.\r\n\r\nTrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"load_data.py\", line 11, in <module>\r\n    dataset = load_from_disk(SRC)\r\n  File \"/opt/conda/envs/py38/lib/python3.8/site-packages/datasets/load.py\", line 784, in load_from_disk\r\n    return DatasetDict.load_from_disk(dataset_path, fs, keep_in_memory=keep_in_memory)\r\n  File \"/opt/conda/envs/py38/lib/python3.8/site-packages/datasets/dataset_dict.py\", line 692, in load_from_disk\r\n    dataset_dict[k] = Dataset.load_from_disk(dataset_dict_split_path, fs, keep_in_memory=keep_in_memory)\r\n  File \"/opt/conda/envs/py38/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 634, in load_from_disk\r\n    if state[\"_indices_files\"]:\r\nKeyError: '_indices_files'\r\n```\r\n\r\nI believe this is the line causing the error since there may not be a `_indices_files` key in the older versions:\r\nhttps://github.com/huggingface/datasets/blob/b70141e3c5149430951773aaa0155555c5fb3e76/src/datasets/arrow_dataset.py#L634\r\n\r\nMay I suggest using `state.get()` instead of directly indexing the dictionary?\r\n\r\n@lhoestq ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2195/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2195/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2153", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2153/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2153/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2153/events", "html_url": "https://github.com/huggingface/datasets/issues/2153", "id": 846181502, "node_id": "MDU6SXNzdWU4NDYxODE1MDI=", "number": 2153, "title": "load_dataset ignoring features", "user": {"login": "GuillemGSubies", "id": 37592763, "node_id": "MDQ6VXNlcjM3NTkyNzYz", "avatar_url": "https://avatars.githubusercontent.com/u/37592763?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GuillemGSubies", "html_url": "https://github.com/GuillemGSubies", "followers_url": "https://api.github.com/users/GuillemGSubies/followers", "following_url": "https://api.github.com/users/GuillemGSubies/following{/other_user}", "gists_url": "https://api.github.com/users/GuillemGSubies/gists{/gist_id}", "starred_url": "https://api.github.com/users/GuillemGSubies/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GuillemGSubies/subscriptions", "organizations_url": "https://api.github.com/users/GuillemGSubies/orgs", "repos_url": "https://api.github.com/users/GuillemGSubies/repos", "events_url": "https://api.github.com/users/GuillemGSubies/events{/privacy}", "received_events_url": "https://api.github.com/users/GuillemGSubies/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-03-31T08:30:09Z", "updated_at": "2022-10-05T13:29:12Z", "closed_at": "2022-10-05T13:29:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "First of all, I'm sorry if it is a repeated issue or the changes are already in master, I searched and I didn't find anything. \r\n\r\nI'm using datasets 1.5.0\r\n\r\n![image](https://user-images.githubusercontent.com/37592763/113114369-8f376580-920b-11eb-900d-94365b59f04b.png)\r\n\r\nAs you can see, when I load the dataset, the ClassLabels are ignored, I have to cast the dataset in order to make it work.\r\n\r\nCode to reproduce:\r\n\r\n```python\r\nimport datasets\r\ndata_location = \"/data/prueba_multiclase\"\r\nfeatures = datasets.Features(\r\n        {\"texto\": datasets.Value(\"string\"), \"label\": datasets.features.ClassLabel(names=[\"false\", \"true\"])}\r\n    )\r\ndataset = datasets.load_dataset(\r\n        \"csv\", data_files=data_location, delimiter=\"\\t\", features=features\r\n    )\r\n```\r\n\r\nDataset I used:\r\n\r\n\r\n[prueba_multiclase.zip](https://github.com/huggingface/datasets/files/6235022/prueba_multiclase.zip) (it has to be unzipped)\r\n\r\n\r\nThank you! \u2764\ufe0f \r\n", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2153/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2153/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2134", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2134/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2134/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2134/events", "html_url": "https://github.com/huggingface/datasets/issues/2134", "id": 843242849, "node_id": "MDU6SXNzdWU4NDMyNDI4NDk=", "number": 2134, "title": "Saving large in-memory datasets with save_to_disk crashes because of pickling", "user": {"login": "prokopCerny", "id": 5815801, "node_id": "MDQ6VXNlcjU4MTU4MDE=", "avatar_url": "https://avatars.githubusercontent.com/u/5815801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prokopCerny", "html_url": "https://github.com/prokopCerny", "followers_url": "https://api.github.com/users/prokopCerny/followers", "following_url": "https://api.github.com/users/prokopCerny/following{/other_user}", "gists_url": "https://api.github.com/users/prokopCerny/gists{/gist_id}", "starred_url": "https://api.github.com/users/prokopCerny/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prokopCerny/subscriptions", "organizations_url": "https://api.github.com/users/prokopCerny/orgs", "repos_url": "https://api.github.com/users/prokopCerny/repos", "events_url": "https://api.github.com/users/prokopCerny/events{/privacy}", "received_events_url": "https://api.github.com/users/prokopCerny/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2021-03-29T10:43:15Z", "updated_at": "2021-05-03T17:59:21Z", "closed_at": "2021-05-03T17:59:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Using Datasets 1.5.0 on Python 3.7.\r\nRecently I've been working on medium to large size datasets (pretokenized raw text sizes from few gigabytes to low tens of gigabytes), and have found out that several preprocessing steps are massively faster when done in memory, and I have the ability to requisition a lot of RAM, so I decided to do these steps completely out of the datasets library.\r\n\r\n So my workflow is to do several .map() on datasets object, then for the operation which is faster in memory to extract the necessary columns from the dataset and then drop it whole, do the transformation in memory, and then create a fresh Dataset object using .from_dict() or other method. \r\n\r\nWhen I then try to call save_to_disk(path) on the dataset, it crashes because of pickling, which appears to be because of using old pickle protocol which doesn't support large files (over 4 GiB).\r\n```\r\nTraceback (most recent call last):\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 80, in <module>\r\n    main()\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 75, in main\r\n    tokenize_and_chunkify(config)\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 60, in tokenize_and_chunkify\r\n    contexts_dataset.save_to_disk(chunked_path)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 457, in save_to_disk\r\n    self = pickle.loads(pickle.dumps(self))\r\nOverflowError: cannot serialize a bytes object larger than 4 GiB\r\n```\r\nFrom what I've seen this issue may be possibly fixed, as the line `self = pickle.loads(pickle.dumps(self))` does not appear to be present in the current state of the repository.\r\n\r\nTo save these datasets to disk, I've resorted to calling .map() over them with `function=None` and specifying the .arrow cache file, and then creating a new dataset using the .from_file() method, which I can then safely save to disk.\r\n\r\nAdditional issue when working with these large in-memory datasets is when using multiprocessing, is again to do with pickling. I've tried to speed up the mapping with function=None by specifying num_proc to the available cpu count, and I again get issues with transferring the dataset, with the following traceback. I am not sure if I should open a separate issue for that.\r\n```\r\nTraceback (most recent call last):\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 94, in <module>\r\n    main()\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 89, in main\r\n    tokenize_and_chunkify(config)\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 67, in tokenize_and_chunkify\r\n    contexts_dataset.map(function=None, cache_file_name=str(output_dir_path / \"tmp.arrow\"), writer_batch_size=50000, num_proc=config.threads)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1485, in map\r\n    transformed_shards = [r.get() for r in results]\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1485, in <listcomp>\r\n    transformed_shards = [r.get() for r in results]\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py\", line 657, in get\r\n    raise self._value\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py\", line 431, in _handle_tasks\r\n    put(task)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/connection.py\", line 209, in send\r\n    self._send_bytes(_ForkingPickler.dumps(obj))\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/reduction.py\", line 54, in dumps\r\n    cls(buf, protocol, *args, **kwds).dump(obj)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 454, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(x)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(x)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 732, in save_bytes\r\n    self._write_large_bytes(BINBYTES + pack(\"<I\", n), obj)\r\nstruct.error: 'I' format requires 0 <= number <= 4294967295Traceback (most recent call last):\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 94, in <module>\r\n    main()\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 89, in main\r\n    tokenize_and_chunkify(config)\r\n  File \"./tokenize_and_chunkify_in_memory.py\", line 67, in tokenize_and_chunkify\r\n    contexts_dataset.map(function=None, cache_file_name=str(output_dir_path / \"tmp.arrow\"), writer_batch_size=50000, num_proc=config.threads)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1485, in map\r\n    transformed_shards = [r.get() for r in results]\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 1485, in <listcomp>\r\n    transformed_shards = [r.get() for r in results]\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py\", line 657, in get\r\n    raise self._value\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py\", line 431, in _handle_tasks\r\n    put(task)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/connection.py\", line 209, in send\r\n    self._send_bytes(_ForkingPickler.dumps(obj))\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/reduction.py\", line 54, in dumps\r\n    cls(buf, protocol, *args, **kwds).dump(obj)\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 454, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(x)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(x)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py\", line 732, in save_bytes\r\n    self._write_large_bytes(BINBYTES + pack(\"<I\", n), obj)\r\nstruct.error: 'I' format requires 0 <= number <= 4294967295\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2134/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2134/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2131", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2131/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2131/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2131/events", "html_url": "https://github.com/huggingface/datasets/issues/2131", "id": 843133112, "node_id": "MDU6SXNzdWU4NDMxMzMxMTI=", "number": 2131, "title": "When training with Multi-Node Multi-GPU the worker 2 has TypeError: 'NoneType' object", "user": {"login": "andy-yangz", "id": 23011317, "node_id": "MDQ6VXNlcjIzMDExMzE3", "avatar_url": "https://avatars.githubusercontent.com/u/23011317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andy-yangz", "html_url": "https://github.com/andy-yangz", "followers_url": "https://api.github.com/users/andy-yangz/followers", "following_url": "https://api.github.com/users/andy-yangz/following{/other_user}", "gists_url": "https://api.github.com/users/andy-yangz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andy-yangz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andy-yangz/subscriptions", "organizations_url": "https://api.github.com/users/andy-yangz/orgs", "repos_url": "https://api.github.com/users/andy-yangz/repos", "events_url": "https://api.github.com/users/andy-yangz/events{/privacy}", "received_events_url": "https://api.github.com/users/andy-yangz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-03-29T08:45:58Z", "updated_at": "2021-04-10T11:08:55Z", "closed_at": "2021-04-10T11:08:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "\bversion: 1.5.0\r\nmet a very strange error, I am training large scale language model, and need train on 2 machines(workers).\r\nAnd sometimes I will get this error `TypeError: 'NoneType' object is not iterable`\r\nThis is traceback\r\n```\r\n\r\n71 | \u00a0 | Traceback (most recent call last):\r\n-- | -- | --\r\n72 | \u00a0 | File \"run_gpt.py\", line 316, in <module>\r\n73 | \u00a0 | main()\r\n74 | \u00a0 | File \"run_gpt.py\", line 222, in main\r\n75 | \u00a0 | delimiter=\"\\t\", column_names=[\"input_ids\", \"attention_mask\", \"chinese_ref\"])\r\n76 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/datasets/load.py\", line 747, in load_dataset\r\n77 | \u00a0 | use_auth_token=use_auth_token,\r\n78 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/datasets/builder.py\", line 513, in download_and_prepare\r\n79 | \u00a0 | self.download_post_processing_resources(dl_manager)\r\n80 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/datasets/builder.py\", line 673, in download_post_processing_resources\r\n81 | \u00a0 | for split in self.info.splits:\r\n82 | \u00a0 | TypeError: 'NoneType' object is not iterable\r\n83 | \u00a0 | WARNING:datasets.builder:Reusing dataset csv (/usr/local/app/.cache/huggingface/datasets/csv/default-1c257ebd48e225e7/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2)\r\n84 | \u00a0 | Traceback (most recent call last):\r\n85 | \u00a0 | File \"/data/miniconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n86 | \u00a0 | \"__main__\", mod_spec)\r\n87 | \u00a0 | File \"/data/miniconda3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n88 | \u00a0 | exec(code, run_globals)\r\n89 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/torch/distributed/launch.py\", line 340, in <module>\r\n90 | \u00a0 | main()\r\n91 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/torch/distributed/launch.py\", line 326, in main\r\n92 | \u00a0 | sigkill_handler(signal.SIGTERM, None)  # not coming back\r\n93 | \u00a0 | File \"/data/miniconda3/lib/python3.7/site-packages/torch/distributed/launch.py\", line 301, in sigkill_handler\r\n94 | \u00a0 | raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)\r\n\r\n```\r\nOn worker 1 it loads the dataset well, however on worker 2 will get this error. \r\nAnd I will meet this error from time to time, sometimes it just goes well.", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2131/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 1, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2131/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2071", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2071/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2071/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2071/events", "html_url": "https://github.com/huggingface/datasets/issues/2071", "id": 833950824, "node_id": "MDU6SXNzdWU4MzM5NTA4MjQ=", "number": 2071, "title": "Multiprocessing is slower than single process", "user": {"login": "theo-m", "id": 17948980, "node_id": "MDQ6VXNlcjE3OTQ4OTgw", "avatar_url": "https://avatars.githubusercontent.com/u/17948980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/theo-m", "html_url": "https://github.com/theo-m", "followers_url": "https://api.github.com/users/theo-m/followers", "following_url": "https://api.github.com/users/theo-m/following{/other_user}", "gists_url": "https://api.github.com/users/theo-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/theo-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/theo-m/subscriptions", "organizations_url": "https://api.github.com/users/theo-m/orgs", "repos_url": "https://api.github.com/users/theo-m/repos", "events_url": "https://api.github.com/users/theo-m/events{/privacy}", "received_events_url": "https://api.github.com/users/theo-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-03-17T16:08:58Z", "updated_at": "2021-03-18T09:10:23Z", "closed_at": "2021-03-18T09:10:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "```python\r\n# benchmark_filter.py\r\nimport logging\r\nimport sys\r\nimport time\r\n\r\nfrom datasets import load_dataset, set_caching_enabled\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    set_caching_enabled(False)\r\n    logging.basicConfig(level=logging.DEBUG)\r\n\r\n    bc = load_dataset(\"bookcorpus\")\r\n\r\n    now = time.time()\r\n    try:\r\n        bc[\"train\"].filter(lambda x: len(x[\"text\"]) < 64, num_proc=int(sys.argv[1]))\r\n    except Exception as e:\r\n        print(f\"cancelled: {e}\")\r\n    elapsed = time.time() - now\r\n\r\n    print(elapsed)\r\n```\r\n\r\nRunning `python benchmark_filter.py 1` (20min+) is faster than `python benchmark_filter.py 2` (2hrs+)", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2071/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/2010", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/2010/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/2010/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/2010/events", "html_url": "https://github.com/huggingface/datasets/issues/2010", "id": 825567635, "node_id": "MDU6SXNzdWU4MjU1Njc2MzU=", "number": 2010, "title": "Local testing fails", "user": {"login": "theo-m", "id": 17948980, "node_id": "MDQ6VXNlcjE3OTQ4OTgw", "avatar_url": "https://avatars.githubusercontent.com/u/17948980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/theo-m", "html_url": "https://github.com/theo-m", "followers_url": "https://api.github.com/users/theo-m/followers", "following_url": "https://api.github.com/users/theo-m/following{/other_user}", "gists_url": "https://api.github.com/users/theo-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/theo-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/theo-m/subscriptions", "organizations_url": "https://api.github.com/users/theo-m/orgs", "repos_url": "https://api.github.com/users/theo-m/repos", "events_url": "https://api.github.com/users/theo-m/events{/privacy}", "received_events_url": "https://api.github.com/users/theo-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2021-03-09T09:01:38Z", "updated_at": "2021-03-09T14:06:03Z", "closed_at": "2021-03-09T14:06:03Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I'm following the CI setup as described in \r\n\r\nhttps://github.com/huggingface/datasets/blob/8eee4fa9e133fe873a7993ba746d32ca2b687551/.circleci/config.yml#L16-L19\r\n\r\nin a new conda environment, at commit https://github.com/huggingface/datasets/commit/4de6dbf84e93dad97e1000120d6628c88954e5d4\r\n\r\nand getting\r\n\r\n```\r\nFAILED tests/test_caching.py::RecurseDumpTest::test_dump_ipython_function - TypeError: an integer is required (got type bytes)\r\n1 failed, 2321 passed, 5109 skipped, 10 warnings in 124.32s (0:02:04)\r\n```\r\n\r\nSeems like a discrepancy with CI, perhaps a lib version that's not controlled? \r\nTried with `pyarrow=={1.0.0,0.17.1,2.0.0}`", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/2010/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/2010/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/1996", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/1996/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/1996/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/1996/events", "html_url": "https://github.com/huggingface/datasets/issues/1996", "id": 823573410, "node_id": "MDU6SXNzdWU4MjM1NzM0MTA=", "number": 1996, "title": "Error when exploring `arabic_speech_corpus`", "user": {"login": "elgeish", "id": 6879673, "node_id": "MDQ6VXNlcjY4Nzk2NzM=", "avatar_url": "https://avatars.githubusercontent.com/u/6879673?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elgeish", "html_url": "https://github.com/elgeish", "followers_url": "https://api.github.com/users/elgeish/followers", "following_url": "https://api.github.com/users/elgeish/following{/other_user}", "gists_url": "https://api.github.com/users/elgeish/gists{/gist_id}", "starred_url": "https://api.github.com/users/elgeish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elgeish/subscriptions", "organizations_url": "https://api.github.com/users/elgeish/orgs", "repos_url": "https://api.github.com/users/elgeish/repos", "events_url": "https://api.github.com/users/elgeish/events{/privacy}", "received_events_url": "https://api.github.com/users/elgeish/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 2107841032, "node_id": "MDU6TGFiZWwyMTA3ODQxMDMy", "url": "https://api.github.com/repos/huggingface/datasets/labels/nlp-viewer", "name": "nlp-viewer", "color": "94203D", "default": false, "description": ""}, {"id": 2725241052, "node_id": "MDU6TGFiZWwyNzI1MjQxMDUy", "url": "https://api.github.com/repos/huggingface/datasets/labels/speech", "name": "speech", "color": "d93f0b", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-03-06T05:55:20Z", "updated_at": "2022-10-05T13:24:26Z", "closed_at": "2022-10-05T13:24:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Navigate to https://huggingface.co/datasets/viewer/?dataset=arabic_speech_corpus\r\n\r\nError:\r\n```\r\nImportError: To be able to use this dataset, you need to install the following dependencies['soundfile'] using 'pip install soundfile' for instance'\r\nTraceback:\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/script_runner.py\", line 332, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/home/sasha/nlp-viewer/run.py\", line 233, in <module>\r\n    configs = get_confs(option)\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/caching.py\", line 604, in wrapped_func\r\n    return get_or_create_cached_value()\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/caching.py\", line 588, in get_or_create_cached_value\r\n    return_value = func(*args, **kwargs)\r\nFile \"/home/sasha/nlp-viewer/run.py\", line 145, in get_confs\r\n    module_path = nlp.load.prepare_module(path, dataset=True\r\nFile \"/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/datasets/load.py\", line 342, in prepare_module\r\n    f\"To be able to use this {module_type}, you need to install the following dependencies\"\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/1996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/1996/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/1610", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/1610/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/1610/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/1610/events", "html_url": "https://github.com/huggingface/datasets/issues/1610", "id": 771453599, "node_id": "MDU6SXNzdWU3NzE0NTM1OTk=", "number": 1610, "title": "shuffle does not accept seed ", "user": {"login": "rabeehk", "id": 6278280, "node_id": "MDQ6VXNlcjYyNzgyODA=", "avatar_url": "https://avatars.githubusercontent.com/u/6278280?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rabeehk", "html_url": "https://github.com/rabeehk", "followers_url": "https://api.github.com/users/rabeehk/followers", "following_url": "https://api.github.com/users/rabeehk/following{/other_user}", "gists_url": "https://api.github.com/users/rabeehk/gists{/gist_id}", "starred_url": "https://api.github.com/users/rabeehk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rabeehk/subscriptions", "organizations_url": "https://api.github.com/users/rabeehk/orgs", "repos_url": "https://api.github.com/users/rabeehk/repos", "events_url": "https://api.github.com/users/rabeehk/events{/privacy}", "received_events_url": "https://api.github.com/users/rabeehk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-12-19T20:59:39Z", "updated_at": "2021-01-04T10:00:03Z", "closed_at": "2021-01-04T10:00:03Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi\r\nI need to shuffle the dataset, but this needs to be based on epoch+seed to be consistent across the cores, when I pass seed to shuffle, this does not accept seed, could you assist me with this? thanks  @lhoestq\r\n ", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/1610/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/1610/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/1064", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/1064/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/1064/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/1064/events", "html_url": "https://github.com/huggingface/datasets/issues/1064", "id": 756382186, "node_id": "MDU6SXNzdWU3NTYzODIxODY=", "number": 1064, "title": "Not support links with 302 redirect ", "user": {"login": "chameleonTK", "id": 6429850, "node_id": "MDQ6VXNlcjY0Mjk4NTA=", "avatar_url": "https://avatars.githubusercontent.com/u/6429850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chameleonTK", "html_url": "https://github.com/chameleonTK", "followers_url": "https://api.github.com/users/chameleonTK/followers", "following_url": "https://api.github.com/users/chameleonTK/following{/other_user}", "gists_url": "https://api.github.com/users/chameleonTK/gists{/gist_id}", "starred_url": "https://api.github.com/users/chameleonTK/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chameleonTK/subscriptions", "organizations_url": "https://api.github.com/users/chameleonTK/orgs", "repos_url": "https://api.github.com/users/chameleonTK/repos", "events_url": "https://api.github.com/users/chameleonTK/events{/privacy}", "received_events_url": "https://api.github.com/users/chameleonTK/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 1935892871, "node_id": "MDU6TGFiZWwxOTM1ODkyODcx", "url": "https://api.github.com/repos/huggingface/datasets/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-12-03T17:04:43Z", "updated_at": "2021-01-14T02:51:25Z", "closed_at": "2021-01-14T02:51:25Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I have an issue adding this download link https://github.com/jitkapat/thailitcorpus/releases/download/v.2.0/tlc_v.2.0.tar.gz\r\n\r\nit might be because it is not a direct link (it returns 302 and redirects to aws that returns 403 for head requests). \r\n\r\n```\r\nr.head(\"https://github.com/jitkapat/thailitcorpus/releases/download/v.2.0/tlc_v.2.0.tar.gz\", allow_redirects=True)                                      \r\n# <Response [403]>\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/1064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/1064/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/794", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/794/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/794/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/794/events", "html_url": "https://github.com/huggingface/datasets/issues/794", "id": 735158725, "node_id": "MDU6SXNzdWU3MzUxNTg3MjU=", "number": 794, "title": "self.options cannot be converted to a Python object for pickling", "user": {"login": "hzqjyyx", "id": 9635713, "node_id": "MDQ6VXNlcjk2MzU3MTM=", "avatar_url": "https://avatars.githubusercontent.com/u/9635713?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hzqjyyx", "html_url": "https://github.com/hzqjyyx", "followers_url": "https://api.github.com/users/hzqjyyx/followers", "following_url": "https://api.github.com/users/hzqjyyx/following{/other_user}", "gists_url": "https://api.github.com/users/hzqjyyx/gists{/gist_id}", "starred_url": "https://api.github.com/users/hzqjyyx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hzqjyyx/subscriptions", "organizations_url": "https://api.github.com/users/hzqjyyx/orgs", "repos_url": "https://api.github.com/users/hzqjyyx/repos", "events_url": "https://api.github.com/users/hzqjyyx/events{/privacy}", "received_events_url": "https://api.github.com/users/hzqjyyx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-11-03T09:27:34Z", "updated_at": "2020-11-19T17:35:38Z", "closed_at": "2020-11-19T17:35:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nCurrently I am trying to load csv file with customized read_options. And the latest master seems broken if we pass the ReadOptions object.\r\n\r\nHere is a code snippet\r\n```python\r\nfrom datasets import load_dataset\r\nfrom pyarrow.csv import ReadOptions\r\nload_dataset(\"csv\", data_files=[\"out.csv\"], read_options=ReadOptions(block_size=16*1024*1024))\r\n```\r\nerror is `self.options cannot be converted to a Python object for pickling`\r\nWould you mind to take a look? Thanks!\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-28-ab83fec2ded4> in <module>\r\n----> 1 load_dataset(\"csv\", data_files=[\"out.csv\"], read_options=ReadOptions(block_size=16*1024*1024))\r\n\r\n/tmp/datasets/src/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\r\n    602         hash=hash,\r\n    603         features=features,\r\n--> 604         **config_kwargs,\r\n    605     )\r\n    606 \r\n\r\n/tmp/datasets/src/datasets/builder.py in __init__(self, cache_dir, name, hash, features, **config_kwargs)\r\n    162             name,\r\n    163             custom_features=features,\r\n--> 164             **config_kwargs,\r\n    165         )\r\n    166 \r\n\r\n/tmp/datasets/src/datasets/builder.py in _create_builder_config(self, name, custom_features, **config_kwargs)\r\n    281                 )\r\n    282             else:\r\n--> 283                 suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\r\n    284 \r\n    285         if builder_config.data_files is not None:\r\n\r\n/tmp/datasets/src/datasets/fingerprint.py in hash(cls, value)\r\n     51             return cls.dispatch[type(value)](cls, value)\r\n     52         else:\r\n---> 53             return cls.hash_default(value)\r\n     54 \r\n     55     def update(self, value):\r\n\r\n/tmp/datasets/src/datasets/fingerprint.py in hash_default(cls, value)\r\n     44     @classmethod\r\n     45     def hash_default(cls, value):\r\n---> 46         return cls.hash_bytes(dumps(value))\r\n     47 \r\n     48     @classmethod\r\n\r\n/tmp/datasets/src/datasets/utils/py_utils.py in dumps(obj)\r\n    365     file = StringIO()\r\n    366     with _no_cache_fields(obj):\r\n--> 367         dump(obj, file)\r\n    368     return file.getvalue()\r\n    369 \r\n\r\n/tmp/datasets/src/datasets/utils/py_utils.py in dump(obj, file)\r\n    337 def dump(obj, file):\r\n    338     \"\"\"pickle an object to a file\"\"\"\r\n--> 339     Pickler(file, recurse=True).dump(obj)\r\n    340     return\r\n    341 \r\n\r\n~/.local/lib/python3.6/site-packages/dill/_dill.py in dump(self, obj)\r\n    444             raise PicklingError(msg)\r\n    445         else:\r\n--> 446             StockPickler.dump(self, obj)\r\n    447         stack.clear()  # clear record of 'recursion-sensitive' pickled objects\r\n    448         return\r\n\r\n/usr/lib/python3.6/pickle.py in dump(self, obj)\r\n    407         if self.proto >= 4:\r\n    408             self.framer.start_framing()\r\n--> 409         self.save(obj)\r\n    410         self.write(STOP)\r\n    411         self.framer.end_framing()\r\n\r\n/usr/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n~/.local/lib/python3.6/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    931             # we only care about session the first pass thru\r\n    932             pickler._session = False\r\n--> 933         StockPickler.save_dict(pickler, obj)\r\n    934         log.info(\"# D2\")\r\n    935     return\r\n\r\n/usr/lib/python3.6/pickle.py in save_dict(self, obj)\r\n    819 \r\n    820         self.memoize(obj)\r\n--> 821         self._batch_setitems(obj.items())\r\n    822 \r\n    823     dispatch[dict] = save_dict\r\n\r\n/usr/lib/python3.6/pickle.py in _batch_setitems(self, items)\r\n    850                 k, v = tmp[0]\r\n    851                 save(k)\r\n--> 852                 save(v)\r\n    853                 write(SETITEM)\r\n    854             # else tmp is empty, and we're done\r\n\r\n/usr/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    494             reduce = getattr(obj, \"__reduce_ex__\", None)\r\n    495             if reduce is not None:\r\n--> 496                 rv = reduce(self.proto)\r\n    497             else:\r\n    498                 reduce = getattr(obj, \"__reduce__\", None)\r\n\r\n~/.local/lib/python3.6/site-packages/pyarrow/_csv.cpython-36m-x86_64-linux-gnu.so in pyarrow._csv.ReadOptions.__reduce_cython__()\r\n\r\nTypeError: self.options cannot be converted to a Python object for pickling\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/794/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/794/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/730", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/730/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/730/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/730/events", "html_url": "https://github.com/huggingface/datasets/issues/730", "id": 721073812, "node_id": "MDU6SXNzdWU3MjEwNzM4MTI=", "number": 730, "title": "Possible caching bug", "user": {"login": "ArneBinder", "id": 3375489, "node_id": "MDQ6VXNlcjMzNzU0ODk=", "avatar_url": "https://avatars.githubusercontent.com/u/3375489?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ArneBinder", "html_url": "https://github.com/ArneBinder", "followers_url": "https://api.github.com/users/ArneBinder/followers", "following_url": "https://api.github.com/users/ArneBinder/following{/other_user}", "gists_url": "https://api.github.com/users/ArneBinder/gists{/gist_id}", "starred_url": "https://api.github.com/users/ArneBinder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ArneBinder/subscriptions", "organizations_url": "https://api.github.com/users/ArneBinder/orgs", "repos_url": "https://api.github.com/users/ArneBinder/repos", "events_url": "https://api.github.com/users/ArneBinder/events{/privacy}", "received_events_url": "https://api.github.com/users/ArneBinder/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-10-14T02:02:34Z", "updated_at": "2022-11-22T01:45:54Z", "closed_at": "2020-10-29T09:36:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "The following code with `test1.txt` containing just \"\ud83e\udd17\ud83e\udd17\ud83e\udd17\":\r\n```\r\ndataset = datasets.load_dataset('text', data_files=['test1.txt'], split=\"train\", encoding=\"latin_1\")\r\nprint(dataset[0])\r\ndataset = datasets.load_dataset('text', data_files=['test1.txt'], split=\"train\", encoding=\"utf-8\")\r\nprint(dataset[0])\r\n``` \r\nproduces this output:\r\n```\r\nDownloading and preparing dataset text/default-15600e4d83254059 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155...\r\nDataset text downloaded and prepared to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155. Subsequent calls will reuse this data.\r\n{'text': '\u00f0\\x9f\u00a4\\x97\u00f0\\x9f\u00a4\\x97\u00f0\\x9f\u00a4\\x97'}\r\nUsing custom data configuration default\r\nReusing dataset text (/home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155)\r\n{'text': '\u00f0\\x9f\u00a4\\x97\u00f0\\x9f\u00a4\\x97\u00f0\\x9f\u00a4\\x97'}\r\n```\r\nJust changing the order (and deleting the temp files):\r\n```\r\ndataset = datasets.load_dataset('text', data_files=['test1.txt'], split=\"train\", encoding=\"utf-8\")\r\nprint(dataset[0])\r\ndataset = datasets.load_dataset('text', data_files=['test1.txt'], split=\"train\", encoding=\"latin_1\")\r\nprint(dataset[0])\r\n```\r\nproduces this:\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset text/default-15600e4d83254059 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155...\r\nDataset text downloaded and prepared to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155. Subsequent calls will reuse this data.\r\n{'text': '\ud83e\udd17\ud83e\udd17\ud83e\udd17'}\r\nUsing custom data configuration default\r\nReusing dataset text (/home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155)\r\n{'text': '\ud83e\udd17\ud83e\udd17\ud83e\udd17'}\r\n```\r\n\r\nIs it intended that the cache path does not depend on the config entries?\r\n\r\ntested with datasets==1.1.2 and python==3.8.5", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/730/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/649", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/649/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/649/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/649/events", "html_url": "https://github.com/huggingface/datasets/issues/649", "id": 704838415, "node_id": "MDU6SXNzdWU3MDQ4Mzg0MTU=", "number": 649, "title": "Inconsistent behavior in map", "user": {"login": "krandiash", "id": 10166085, "node_id": "MDQ6VXNlcjEwMTY2MDg1", "avatar_url": "https://avatars.githubusercontent.com/u/10166085?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krandiash", "html_url": "https://github.com/krandiash", "followers_url": "https://api.github.com/users/krandiash/followers", "following_url": "https://api.github.com/users/krandiash/following{/other_user}", "gists_url": "https://api.github.com/users/krandiash/gists{/gist_id}", "starred_url": "https://api.github.com/users/krandiash/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krandiash/subscriptions", "organizations_url": "https://api.github.com/users/krandiash/orgs", "repos_url": "https://api.github.com/users/krandiash/repos", "events_url": "https://api.github.com/users/krandiash/events{/privacy}", "received_events_url": "https://api.github.com/users/krandiash/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-09-19T08:41:12Z", "updated_at": "2020-09-21T16:13:05Z", "closed_at": "2020-09-21T16:13:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm observing inconsistent behavior when applying .map(). This happens specifically when I'm incrementally adding onto a feature that is a nested dictionary. Here's a simple example that reproduces the problem.\r\n\r\n```python\r\nimport datasets\r\n\r\n# Dataset with a single feature called 'field' consisting of two examples\r\ndataset = datasets.Dataset.from_dict({'field': ['a', 'b']})\r\nprint(dataset[0])\r\n# outputs\r\n{'field': 'a'}\r\n\r\n# Map this dataset to create another feature called 'otherfield', which is a dictionary containing a key called 'capital'\r\ndataset = dataset.map(lambda example: {'otherfield': {'capital': example['field'].capitalize()}})\r\nprint(dataset[0])\r\n# output is okay\r\n{'field': 'a', 'otherfield': {'capital': 'A'}}\r\n\r\n# Now I want to map again to modify 'otherfield', by adding another key called 'append_x' to the dictionary under 'otherfield'\r\nprint(dataset.map(lambda example: {'otherfield': {'append_x': example['field'] + 'x'}})[0])\r\n# printing out the first example after applying the map shows that the new key 'append_x' doesn't get added\r\n# it also messes up the value stored at 'capital'\r\n{'field': 'a', 'otherfield': {'capital': None}}\r\n\r\n# Instead, I try to do the same thing by using a different mapped fn\r\nprint(dataset.map(lambda example:  {'otherfield': {'append_x': example['field'] + 'x', 'capital': example['otherfield']['capital']}})[0])\r\n# this preserves the value under capital, but still no 'append_x'\r\n{'field': 'a', 'otherfield': {'capital': 'A'}}\r\n\r\n# Instead, I try to pass 'otherfield' to remove_columns\r\nprint(dataset.map(lambda example:  {'otherfield': {'append_x': example['field'] + 'x', 'capital': example['otherfield']['capital']}}, remove_columns=['otherfield'])[0])\r\n# this still doesn't fix the problem\r\n{'field': 'a', 'otherfield': {'capital': 'A'}}\r\n\r\n# Alternately, here's what happens if I just directly map both 'capital' and 'append_x' on a fresh dataset.\r\n\r\n# Recreate the dataset\r\ndataset = datasets.Dataset.from_dict({'field': ['a', 'b']})\r\n# Now map the entire 'otherfield' dict directly, instead of incrementally as before\r\nprint(dataset.map(lambda example:  {'otherfield': {'append_x': example['field'] + 'x', 'capital': example['field'].capitalize()}})[0])\r\n# This looks good!\r\n{'field': 'a', 'otherfield': {'append_x': 'ax', 'capital': 'A'}}\r\n```\r\n\r\nThis might be a new issue, because I didn't see this behavior in the `nlp` library. \r\n\r\nAny help is appreciated!", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/649/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/648", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/648/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/648/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/648/events", "html_url": "https://github.com/huggingface/datasets/issues/648", "id": 704753123, "node_id": "MDU6SXNzdWU3MDQ3NTMxMjM=", "number": 648, "title": "offset overflow when multiprocessing batched map on large datasets.", "user": {"login": "richarddwang", "id": 17963619, "node_id": "MDQ6VXNlcjE3OTYzNjE5", "avatar_url": "https://avatars.githubusercontent.com/u/17963619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/richarddwang", "html_url": "https://github.com/richarddwang", "followers_url": "https://api.github.com/users/richarddwang/followers", "following_url": "https://api.github.com/users/richarddwang/following{/other_user}", "gists_url": "https://api.github.com/users/richarddwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/richarddwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/richarddwang/subscriptions", "organizations_url": "https://api.github.com/users/richarddwang/orgs", "repos_url": "https://api.github.com/users/richarddwang/repos", "events_url": "https://api.github.com/users/richarddwang/events{/privacy}", "received_events_url": "https://api.github.com/users/richarddwang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-09-19T02:15:11Z", "updated_at": "2020-09-19T16:47:07Z", "closed_at": "2020-09-19T16:46:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "It only happened when \"multiprocessing\" + \"batched\" + \"large dataset\" at the same time.\r\n\r\n```\r\ndef bprocess(examples):\r\n  examples['len'] = []\r\n  for text in examples['text']:\r\n    examples['len'].append(len(text))\r\n  return examples\r\nwiki.map(brpocess, batched=True, num_proc=8)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nRemoteTraceback                           Traceback (most recent call last)\r\nRemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 153, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/yisiang/datasets/src/datasets/fingerprint.py\", line 163, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 1486, in _map_single\r\n    batch = self[i : i + batch_size]\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 1071, in __getitem__\r\n    format_kwargs=self._format_kwargs,\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 972, in _getitem\r\n    data_subset = self._data.take(indices_array)\r\n  File \"pyarrow/table.pxi\", line 1145, in pyarrow.lib.Table.take\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/pyarrow/compute.py\", line 268, in take\r\n    return call_function('take', [data, indices], options)\r\n  File \"pyarrow/_compute.pyx\", line 298, in pyarrow._compute.call_function\r\n  File \"pyarrow/_compute.pyx\", line 192, in pyarrow._compute.Function.call\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nArrowInvalid                              Traceback (most recent call last)\r\n in \r\n     30   owt = datasets.load_dataset('/home/yisiang/datasets/datasets/openwebtext/openwebtext.py', cache_dir='./datasets')['train']\r\n     31   print('load/create data from OpenWebText Corpus for ELECTRA')\r\n---> 32   e_owt = ELECTRAProcessor(owt, apply_cleaning=False).map(cache_file_name=f\"electra_owt_{c.max_length}.arrow\")\r\n     33   dsets.append(e_owt)\r\n     34 \r\n\r\n~/Reexamine_Attention/electra_pytorch/_utils/utils.py in map(self, **kwargs)\r\n    126       writer_batch_size=10**4,\r\n    127       num_proc=num_proc,\r\n--> 128       **kwargs\r\n    129     )\r\n    130 \r\n\r\n~/hugdatafast/hugdatafast/transform.py in my_map(self, *args, **kwargs)\r\n     21     if not cache_file_name.endswith('.arrow'): cache_file_name += '.arrow'\r\n     22     if '/' not in cache_file_name: cache_file_name = os.path.join(self.cache_directory(), cache_file_name)\r\n---> 23   return self.map(*args, cache_file_name=cache_file_name, **kwargs)\r\n     24 \r\n     25 @patch\r\n\r\n~/datasets/src/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1285                 logger.info(\"Spawning {} processes\".format(num_proc))\r\n   1286                 results = [pool.apply_async(self.__class__._map_single, kwds=kwds) for kwds in kwds_per_shard]\r\n-> 1287                 transformed_shards = [r.get() for r in results]\r\n   1288                 logger.info(\"Concatenating {} shards from multiprocessing\".format(num_proc))\r\n   1289                 result = concatenate_datasets(transformed_shards)\r\n\r\n~/datasets/src/datasets/arrow_dataset.py in (.0)\r\n   1285                 logger.info(\"Spawning {} processes\".format(num_proc))\r\n   1286                 results = [pool.apply_async(self.__class__._map_single, kwds=kwds) for kwds in kwds_per_shard]\r\n-> 1287                 transformed_shards = [r.get() for r in results]\r\n   1288                 logger.info(\"Concatenating {} shards from multiprocessing\".format(num_proc))\r\n   1289                 result = concatenate_datasets(transformed_shards)\r\n\r\n~/miniconda3/envs/ml/lib/python3.7/multiprocessing/pool.py in get(self, timeout)\r\n    655             return self._value\r\n    656         else:\r\n--> 657             raise self._value\r\n    658 \r\n    659     def _set(self, i, obj):\r\n\r\nArrowInvalid: offset overflow while concatenating arrays\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/648/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/648/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/643", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/643/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/643/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/643/events", "html_url": "https://github.com/huggingface/datasets/issues/643", "id": 704477164, "node_id": "MDU6SXNzdWU3MDQ0NzcxNjQ=", "number": 643, "title": "Caching processed dataset at wrong folder", "user": {"login": "mrm8488", "id": 3653789, "node_id": "MDQ6VXNlcjM2NTM3ODk=", "avatar_url": "https://avatars.githubusercontent.com/u/3653789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrm8488", "html_url": "https://github.com/mrm8488", "followers_url": "https://api.github.com/users/mrm8488/followers", "following_url": "https://api.github.com/users/mrm8488/following{/other_user}", "gists_url": "https://api.github.com/users/mrm8488/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrm8488/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrm8488/subscriptions", "organizations_url": "https://api.github.com/users/mrm8488/orgs", "repos_url": "https://api.github.com/users/mrm8488/repos", "events_url": "https://api.github.com/users/mrm8488/events{/privacy}", "received_events_url": "https://api.github.com/users/mrm8488/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2020-09-18T15:41:26Z", "updated_at": "2022-02-16T14:53:29Z", "closed_at": "2022-02-16T14:53:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi guys, I run this on my Colab (PRO):\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('text', data_files='/content/corpus.txt', cache_dir='/content/drive/My Drive', split='train')\r\n\r\ndef encode(examples):\r\n  return tokenizer(examples['text'], truncation=True, padding='max_length')\r\n\r\ndataset = dataset.map(encode, batched=True)\r\n```\r\nThe file is about 4 GB, so I cannot process it on the Colab HD because there is no enough space. So I decided to mount my Google Drive fs and do it on it.\r\nThe dataset is cached in the right place but by processing it (applying `encode` function) seems to use a different folder because Colab HD starts to grow and it crashes when it should be done in the Drive fs.\r\n\r\nWhat gets me crazy, it prints it is processing/encoding the dataset in the right folder:\r\n```\r\nTesting the mapped function outputs\r\nTesting finished, running the mapping function on the dataset\r\nCaching processed dataset at /content/drive/My Drive/text/default-ad3e69d6242ee916/0.0.0/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/cache-b16341780a59747d.arrow\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/643/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/huggingface/datasets/issues/620", "repository_url": "https://api.github.com/repos/huggingface/datasets", "labels_url": "https://api.github.com/repos/huggingface/datasets/issues/620/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/datasets/issues/620/comments", "events_url": "https://api.github.com/repos/huggingface/datasets/issues/620/events", "html_url": "https://github.com/huggingface/datasets/issues/620", "id": 699815135, "node_id": "MDU6SXNzdWU2OTk4MTUxMzU=", "number": 620, "title": "map/filter multiprocessing raises errors and corrupts datasets", "user": {"login": "timothyjlaurent", "id": 2000204, "node_id": "MDQ6VXNlcjIwMDAyMDQ=", "avatar_url": "https://avatars.githubusercontent.com/u/2000204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/timothyjlaurent", "html_url": "https://github.com/timothyjlaurent", "followers_url": "https://api.github.com/users/timothyjlaurent/followers", "following_url": "https://api.github.com/users/timothyjlaurent/following{/other_user}", "gists_url": "https://api.github.com/users/timothyjlaurent/gists{/gist_id}", "starred_url": "https://api.github.com/users/timothyjlaurent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/timothyjlaurent/subscriptions", "organizations_url": "https://api.github.com/users/timothyjlaurent/orgs", "repos_url": "https://api.github.com/users/timothyjlaurent/repos", "events_url": "https://api.github.com/users/timothyjlaurent/events{/privacy}", "received_events_url": "https://api.github.com/users/timothyjlaurent/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1935892857, "node_id": "MDU6TGFiZWwxOTM1ODkyODU3", "url": "https://api.github.com/repos/huggingface/datasets/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 22, "created_at": "2020-09-11T22:30:06Z", "updated_at": "2020-10-08T16:31:47Z", "closed_at": "2020-10-08T16:31:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "After upgrading to the 1.0 started seeing errors in my data loading script after enabling multiprocessing.\r\n\r\n```python\r\n    ...\r\n    ner_ds_dict = ner_ds.train_test_split(test_size=test_pct, shuffle=True, seed=seed)\r\n    ner_ds_dict[\"validation\"] = ner_ds_dict[\"test\"]\r\n    rel_ds_dict = rel_ds.train_test_split(test_size=test_pct, shuffle=True, seed=seed)\r\n    rel_ds_dict[\"validation\"] = rel_ds_dict[\"test\"]\r\n    return ner_ds_dict, rel_ds_dict\r\n```\r\n\r\nThe first train_test_split, `ner_ds`/`ner_ds_dict`, returns a `train` and `test` split that are iterable.\r\nThe second, `rel_ds`/`rel_ds_dict` in this case, returns a Dataset dict that has rows but if selected from or sliced into into returns an empty dictionary. eg `rel_ds_dict['train'][0] == {}` and `rel_ds_dict['train'][0:100] == {}`.\r\n\r\nOk I think I know the problem -- the rel_ds was mapped though a mapper with `num_proc=12`. If I remove `num_proc`. The dataset loads.\r\n\r\nI also see errors with other map and filter functions when `num_proc` is set.\r\n\r\n```\r\nDone writing 67 indices in 536 bytes .\r\nDone writing 67 indices in 536 bytes .\r\nFatal Python error: PyCOND_WAIT(gil_cond) failed\r\n```", "reactions": {"url": "https://api.github.com/repos/huggingface/datasets/issues/620/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/huggingface/datasets/issues/620/timeline", "performed_via_github_app": null, "state_reason": "completed"}]