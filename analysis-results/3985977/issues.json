[{"url": "https://api.github.com/repos/dask/distributed/issues/7798", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7798/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7798/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7798/events", "html_url": "https://github.com/dask/distributed/issues/7798", "id": 1682613631, "node_id": "I_kwDOAocYk85kSqV_", "number": 7798, "title": "Data loss possible with P2P shuffle when worker returns with same address", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4691806577, "node_id": "LA_kwDOAocYk88AAAABF6dJcQ", "url": "https://api.github.com/repos/dask/distributed/labels/shuffle", "name": "shuffle", "color": "E338E3", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2023-04-25T07:41:40Z", "updated_at": "2023-04-27T12:49:42Z", "closed_at": "2023-04-27T12:49:42Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "With P2P shuffling, data loss can occur if a worker returns with the same address if the `remove_worker` hook of the `ShuffleSchedulerExtension` is delayed. This may happen if another plugin with a slow async `remove_worker` hook is installed and its hook is executed and therefore awaited before the `ShuffleSchedulerExtension`'s hook. \r\n\r\nWhile this is unlikely to happen (it would require users to install a plugin before the shuffle extension is installed by default or to manipulate insertion order into `Scheduler.plugins`, it is a possible failure case that must not happen.\r\n\r\nTo avoid this situation, we can delay awaiting async `remove_worker` hooks until all sync ones have been completed. More generally speaking, this is yet another instance of #6392, where we use an address in the worker restrictions even though we want to restrict on a specific instance (see #7346 for a similar discussion).\r\n\r\nReproducer (to be added to `distributed/shuffle/tests/test_shuffle.py`:\r\n\r\n```python3\r\nfrom contextlib import AsyncExitStack\r\nfrom distributed.diagnostics.plugin import SchedulerPlugin\r\nfrom distributed.shuffle._scheduler_extension import ShuffleSchedulerExtension\r\n\r\nclass BlockedRemoveWorkerSchedulerPlugin(SchedulerPlugin):\r\n    def __init__(self, scheduler: Scheduler, *args, **kwargs):\r\n        self.scheduler = scheduler\r\n        super().__init__(*args, **kwargs)\r\n        self.in_remove_worker = asyncio.Event()\r\n        self.block_remove_worker = asyncio.Event()\r\n        self.scheduler.add_plugin(self)\r\n\r\n    async def remove_worker(self, *args, **kwargs) -> None:\r\n        self.in_remove_worker.set()\r\n        await self.block_remove_worker.wait()\r\n\r\n\r\nclass BlockedBarrierSchedulerExtension(ShuffleSchedulerExtension):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.in_barrier = asyncio.Event()\r\n        self.block_barrier = asyncio.Event()\r\n\r\n    async def barrier(self, *args, **kwargs) -> None:\r\n        self.in_barrier.set()\r\n        await self.block_barrier.wait()\r\n        await super().barrier(*args, **kwargs)\r\n\r\n\r\n@gen_cluster(\r\n    client=True,\r\n    nthreads=[],\r\n    scheduler_kwargs={\r\n        \"extensions\": {\r\n            \"blocking\": BlockedRemoveWorkerSchedulerPlugin,\r\n            \"shuffle\": BlockedBarrierSchedulerExtension,\r\n        }\r\n    },\r\n)\r\nasync def test_closed_worker_returns_before_barrier(c, s):\r\n    async with AsyncExitStack() as stack:\r\n        workers = [await stack.enter_async_context(Worker(s.address)) for _ in range(2)]\r\n\r\n        df = dask.datasets.timeseries(\r\n            start=\"2000-01-01\",\r\n            end=\"2000-01-10\",\r\n            dtypes={\"x\": float, \"y\": float},\r\n            freq=\"10 s\",\r\n        )\r\n        out = dd.shuffle.shuffle(df, \"x\", shuffle=\"p2p\")\r\n        out = out.persist()\r\n        shuffle_id = await wait_until_new_shuffle_is_initialized(s)\r\n        key = barrier_key(shuffle_id)\r\n        await wait_for_state(key, \"processing\", s)\r\n        scheduler_extension = s.extensions[\"shuffle\"]\r\n        await scheduler_extension.in_barrier.wait()\r\n\r\n        flushes = [w.extensions[\"shuffle\"].shuffles[shuffle_id]._flush_comm() for w in workers]\r\n        await asyncio.gather(*flushes)\r\n        \r\n        ts = s.tasks[key]\r\n        to_close = None\r\n        for worker in workers:\r\n            if ts.processing_on.address != worker.address:\r\n                to_close = worker\r\n                break\r\n        assert to_close \r\n        closed_port = to_close.port\r\n        await to_close.close()\r\n\r\n        blocking_extension = s.extensions[\"blocking\"]\r\n        assert blocking_extension.in_remove_worker.is_set()\r\n\r\n        workers.append(await stack.enter_async_context(Worker(s.address, port=closed_port)))\r\n\r\n        scheduler_extension.block_barrier.set()\r\n\r\n        await wait_for_state(key, \"memory\", s)\r\n\r\n        out = await c.compute(out.x.size)\r\n        y = await c.compute(df.x.size)\r\n        assert out == y\r\n```\r\n\r\nfails with\r\n```\r\nFAILED distributed/shuffle/tests/test_shuffle.py::test_closed_worker_returns_before_barrier - assert 43118 == 77760\r\n```", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7798/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7798/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7753", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7753/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7753/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7753/events", "html_url": "https://github.com/dask/distributed/issues/7753", "id": 1655610230, "node_id": "I_kwDOAocYk85irpt2", "number": 7753, "title": "Broken `test_processing_chain` on CI", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3798450396, "node_id": "LA_kwDOAocYk87iZ8Dc", "url": "https://api.github.com/repos/dask/distributed/labels/tests", "name": "tests", "color": "a0f9b4", "default": false, "description": "Unit tests and/or continuous integration"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-04-05T13:20:21Z", "updated_at": "2023-04-05T15:10:37Z", "closed_at": "2023-04-05T15:10:37Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "`test_processing_chain` currently fails for all builds on CI with\r\n\r\n```\r\n____________________________ test_processing_chain ____________________________\r\n\r\n    def test_processing_chain():\r\n        \"\"\"\r\n        This is a serial version of the entire compute chain\r\n    \r\n        In practice this takes place on many different workers.\r\n        Here we verify its accuracy in a single threaded situation.\r\n        \"\"\"\r\n        np = pytest.importorskip(\"numpy\")\r\n        pa = pytest.importorskip(\"pyarrow\")\r\n    \r\n        class Stub:\r\n            def __init__(self, value: int) -> None:\r\n                self.value = value\r\n    \r\n        counter = count()\r\n        workers = [\"a\", \"b\", \"c\"]\r\n        npartitions = 5\r\n    \r\n        # Test the processing chain with a dataframe that contains all supported dtypes\r\n        df = pd.DataFrame(\r\n            {\r\n                # numpy dtypes\r\n                f\"col{next(counter)}\": pd.array([True, False] * 50, dtype=\"bool\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"int8\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"int16\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"int32\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"int64\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"uint8\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"uint16\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"uint32\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"uint64\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"float16\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"float32\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"float64\"),\r\n                f\"col{next(counter)}\": pd.array(\r\n                    [np.datetime64(\"2022-01-01\") + i for i in range(100)],\r\n                    dtype=\"datetime64\",\r\n                ),\r\n                f\"col{next(counter)}\": pd.array(\r\n                    [np.timedelta64(1, \"D\") + i for i in range(100)], dtype=\"timedelta64\"\r\n                ),\r\n                # FIXME: PyArrow does not support complex numbers: https://issues.apache.org/jira/browse/ARROW-638\r\n                # f\"col{next(counter)}\": pd.array(range(100), dtype=\"csingle\"),\r\n                # f\"col{next(counter)}\": pd.array(range(100), dtype=\"cdouble\"),\r\n                # f\"col{next(counter)}\": pd.array(range(100), dtype=\"clongdouble\"),\r\n                # Nullable dtypes\r\n                f\"col{next(counter)}\": pd.array([True, False] * 50, dtype=\"boolean\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"Int8\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"Int16\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"Int32\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"Int64\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"UInt8\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"UInt16\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"UInt32\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"UInt64\"),\r\n                # pandas dtypes\r\n                f\"col{next(counter)}\": pd.array(\r\n                    [np.datetime64(\"2022-01-01\") + i for i in range(100)],\r\n                    dtype=pd.DatetimeTZDtype(tz=\"Europe/Berlin\"),\r\n                ),\r\n                f\"col{next(counter)}\": pd.array(\r\n                    [pd.Period(\"2022-01-01\", freq=\"D\") + i for i in range(100)],\r\n                    dtype=\"period[D]\",\r\n                ),\r\n                f\"col{next(counter)}\": pd.array(\r\n                    [pd.Interval(left=i, right=i + 2) for i in range(100)], dtype=\"Interval\"\r\n                ),\r\n                f\"col{next(counter)}\": pd.array([\"x\", \"y\"] * 50, dtype=\"category\"),\r\n                f\"col{next(counter)}\": pd.array([\"lorem ipsum\"] * 100, dtype=\"string\"),\r\n                # FIXME: PyArrow does not support sparse data: https://issues.apache.org/jira/browse/ARROW-8679\r\n                # f\"col{next(counter)}\": pd.array(\r\n                #     [np.nan, np.nan, 1.0, np.nan, np.nan] * 20,\r\n                #     dtype=\"Sparse[float64]\",\r\n                # ),\r\n                # PyArrow dtypes\r\n                f\"col{next(counter)}\": pd.array([True, False] * 50, dtype=\"bool[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"int8[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"int16[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"int32[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"int64[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"uint8[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"uint16[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"uint32[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"uint64[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"float32[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(range(100), dtype=\"float64[pyarrow]\"),\r\n                f\"col{next(counter)}\": pd.array(\r\n                    [pd.Timestamp.fromtimestamp(1641034800 + i) for i in range(100)],\r\n                    dtype=pd.ArrowDtype(pa.timestamp(\"ms\")),\r\n                ),\r\n                # FIXME: distributed#7420\r\n                # f\"col{next(counter)}\": pd.array(\r\n                #     [\"lorem ipsum\"] * 100,\r\n                #     dtype=\"string[pyarrow]\",\r\n                # ),\r\n                # f\"col{next(counter)}\": pd.array(\r\n                #     [\"lorem ipsum\"] * 100,\r\n                #     dtype=pd.StringDtype(\"pyarrow\"),\r\n                # ),\r\n                # custom objects\r\n                # FIXME: Serializing custom objects is not supported in P2P shuffling\r\n                # f\"col{next(counter)}\": pd.array(\r\n                #     [Stub(i) for i in range(100)], dtype=\"object\"\r\n                # ),\r\n            }\r\n        )\r\n        df[\"_partitions\"] = df.col4 % npartitions\r\n        schema = pa.Schema.from_pandas(df)\r\n        worker_for = {i: random.choice(workers) for i in list(range(npartitions))}\r\n        worker_for = pd.Series(worker_for, name=\"_worker\").astype(\"category\")\r\n    \r\n        data = split_by_worker(df, \"_partitions\", worker_for=worker_for)\r\n        assert set(data) == set(worker_for.cat.categories)\r\n        assert sum(map(len, data.values())) == len(df)\r\n    \r\n        batches = {worker: [serialize_table(t)] for worker, t in data.items()}\r\n    \r\n        # Typically we communicate to different workers at this stage\r\n        # We then receive them back and reconstute them\r\n    \r\n        by_worker = {\r\n            worker: list_of_buffers_to_table(list_of_batches)\r\n            for worker, list_of_batches in batches.items()\r\n        }\r\n        assert sum(map(len, by_worker.values())) == len(df)\r\n    \r\n        # We split them again, and then dump them down to disk\r\n    \r\n        splits_by_worker = {\r\n            worker: split_by_partition(t, \"_partitions\") for worker, t in by_worker.items()\r\n        }\r\n    \r\n        splits_by_worker = {\r\n            worker: {partition: [t] for partition, t in d.items()}\r\n            for worker, d in splits_by_worker.items()\r\n        }\r\n    \r\n        # No two workers share data from any partition\r\n        assert not any(\r\n            set(a) & set(b)\r\n            for w1, a in splits_by_worker.items()\r\n            for w2, b in splits_by_worker.items()\r\n            if w1 is not w2\r\n        )\r\n    \r\n        # Our simple file system\r\n        filesystem = defaultdict(io.BytesIO)\r\n    \r\n        for partitions in splits_by_worker.values():\r\n            for partition, tables in partitions.items():\r\n                for table in tables:\r\n                    filesystem[partition].write(serialize_table(table))\r\n    \r\n        out = {}\r\n        for k, bio in filesystem.items():\r\n            bio.seek(0)\r\n            out[k] = convert_partition(bio.read())\r\n    \r\n        shuffled_df = pd.concat(table.to_pandas() for table in out.values())\r\n>       pd.testing.assert_frame_equal(\r\n            df,\r\n            shuffled_df,\r\n            check_like=True,\r\n            check_exact=True,\r\n        )\r\nE       AssertionError: Attributes of DataFrame.iloc[:, 12] (column name=\"col12\") are different\r\nE       \r\nE       Attribute \"dtype\" are different\r\nE       [left]:  datetime64[s]\r\nE       [right]: datetime64[ns]\r\n\r\ndistributed\\shuffle\\tests\\test_shuffle.py:781: AssertionError\r\n```\r\n\r\nCI: https://github.com/dask/distributed/actions/runs/4615417846/jobs/8159285277#step:19:2425", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7753/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7753/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7682", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7682/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7682/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7682/events", "html_url": "https://github.com/dask/distributed/issues/7682", "id": 1629872605, "node_id": "I_kwDOAocYk85hJeHd", "number": 7682, "title": "Task stream dashboard plot broken on `main`? ", "user": {"login": "jrbourbeau", "id": 11656932, "node_id": "MDQ6VXNlcjExNjU2OTMy", "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrbourbeau", "html_url": "https://github.com/jrbourbeau", "followers_url": "https://api.github.com/users/jrbourbeau/followers", "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}", "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions", "organizations_url": "https://api.github.com/users/jrbourbeau/orgs", "repos_url": "https://api.github.com/users/jrbourbeau/repos", "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}", "received_events_url": "https://api.github.com/users/jrbourbeau/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 5277649523, "node_id": "LA_kwDOAocYk88AAAABOpKKcw", "url": "https://api.github.com/repos/dask/distributed/labels/dashboard", "name": "dashboard", "color": "A5BCD1", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2023-03-17T20:02:49Z", "updated_at": "2023-03-20T15:50:22Z", "closed_at": "2023-03-20T15:50:22Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "While following up on https://github.com/dask/distributed/issues/7663#issuecomment-1474055091, I noticed that the task stream plot on the dashboard is sporadically broken when using the `main` branch of `distributed` (it appears okay when using the latest release). \r\n\r\nWith `bokeh=2.4.3` and `dask` + `distributed` `main` I ran the following little snippet locally\r\n\r\n```python\r\nimport time\r\n\r\nfrom dask import delayed\r\nfrom dask.distributed import Client, wait, performance_report\r\n\r\n@delayed\r\ndef inc(x):\r\n    time.sleep(0.1)\r\n    return x + 1\r\n\r\n@delayed\r\ndef double(x):\r\n    time.sleep(0.1)\r\n    return 2 * x\r\n\r\n@delayed\r\ndef add(x, y):\r\n    time.sleep(0.1)\r\n    return x + y\r\n\r\nif __name__ == \"__main__\":\r\n    with Client(n_workers=4, threads_per_worker=2, memory_limit=\"4 GiB\") as client:\r\n        print(f\"{client.dashboard_link = }\")\r\n        with performance_report():\r\n            data = list(range(500))\r\n            output = []\r\n            for x in data:\r\n                a = inc(x)\r\n                b = double(x)\r\n                c = add(a, b)\r\n                output.append(c)\r\n\r\n            total = delayed(sum)(output)\r\n            total = total.persist()\r\n            wait(total)\r\n            del total\r\n```\r\n\r\nWhile that script is running, I navigate to the dashboard link in my browser (have used Firefox, Safari, and Chrome). Frequently, maybe 60% of the time?, the task stream is broken and looks like this\r\n\r\n![Screenshot 2023-03-17 at 2 45 36 PM](https://user-images.githubusercontent.com/11656932/226018932-56f74b84-0170-462b-8238-75e17d704304.png)\r\n\r\nSometimes it's not broken, but becomes broken if I refresh the dashboard page enough. \r\n\r\nAre others able to reproduce this same behavior?\r\n\r\nManually going through commits in the `distributed` git history since the last release, it looks like https://github.com/dask/distributed/pull/7586 is where things appear to start breaking. But looking through that PR I don't see anything that looks obviously related. ", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7682/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7654", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7654/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7654/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7654/events", "html_url": "https://github.com/dask/distributed/issues/7654", "id": 1625679853, "node_id": "I_kwDOAocYk85g5eft", "number": 7654, "title": "Test report CI job failing", "user": {"login": "jrbourbeau", "id": 11656932, "node_id": "MDQ6VXNlcjExNjU2OTMy", "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrbourbeau", "html_url": "https://github.com/jrbourbeau", "followers_url": "https://api.github.com/users/jrbourbeau/followers", "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}", "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions", "organizations_url": "https://api.github.com/users/jrbourbeau/orgs", "repos_url": "https://api.github.com/users/jrbourbeau/repos", "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}", "received_events_url": "https://api.github.com/users/jrbourbeau/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-03-15T14:51:02Z", "updated_at": "2023-03-22T14:05:28Z", "closed_at": "2023-03-22T14:05:28Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "We've been getting \r\n\r\n```python\r\nTraceback (most recent call last):\r\nGetting list of workflow runs...\r\nFound [15](https://github.com/dask/distributed/actions/runs/4424054212/jobs/7757420843#step:7:16)1 workflow runs\r\n  File \"/home/runner/work/distributed/distributed/continuous_integration/scripts/test_report.py\", line 513, in <module>\r\nFetching artifact listing for the 30 most recent workflow runs\r\n    main()\r\nDownloading and parsing 805 artifacts...\r\n  File \"/home/runner/work/distributed/distributed/continuous_integration/scripts/test_report.py\", line 397, in main\r\n    dfs = list(\r\n  File \"/home/runner/work/distributed/distributed/continuous_integration/scripts/test_report.py\", line 367, in download_and_parse_artifacts\r\n    assert (\r\nAssertionError: Artifact suite name ubuntu-latest-mindeps--ci1 did not match any jobs dataframe:\r\n[nan 'ubuntu-latest-mindeps-queue-ci1'\r\n 'ubuntu-latest-mindeps-queue-notci1' 'ubuntu-latest-3.8-queue-ci1'\r\n 'ubuntu-latest-3.8-queue-notci1' 'ubuntu-latest-3.9-queue-ci1'\r\n 'ubuntu-latest-3.9-queue-notci1' 'ubuntu-latest-3.10-queue-ci1'\r\n 'ubuntu-latest-3.10-queue-notci1' 'ubuntu-latest-3.11-queue-ci1'\r\n 'ubuntu-latest-3.11-queue-notci1' 'windows-latest-3.8-queue-ci1'\r\n 'windows-latest-3.8-queue-notci1' 'windows-latest-3.9-queue-ci1'\r\n 'windows-latest-3.9-queue-notci1' 'windows-latest-3.10-queue-ci1'\r\n 'windows-latest-3.10-queue-notci1' 'windows-latest-3.11-queue-ci1'\r\n 'windows-latest-3.11-queue-notci1' 'macos-latest-3.8-queue-ci1'\r\n 'macos-latest-3.8-queue-notci1' 'macos-latest-3.11-queue-ci1'\r\n 'macos-latest-3.11-queue-notci1' 'ubuntu-latest-3.9-no_queue-ci1'\r\n 'ubuntu-latest-3.9-no_queue-notci1' 'ubuntu-latest-mindeps-numpy-ci1'\r\n 'ubuntu-latest-mindeps-numpy-notci1']\r\n```\r\n\r\nin our test report jobs for a while (see [this build](https://github.com/dask/distributed/actions/runs/4424054212/jobs/7757420843) for an example) ", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7654/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7654/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7647", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7647/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7647/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7647/events", "html_url": "https://github.com/dask/distributed/issues/7647", "id": 1621992176, "node_id": "I_kwDOAocYk85graLw", "number": 7647, "title": "`OverflowError` in `Cluster._sync_cluster_info()`", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2023-03-13T18:02:17Z", "updated_at": "2023-03-14T14:58:25Z", "closed_at": "2023-03-14T14:58:25Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "When `_sync_cluster_info` fails over an extended period, it raises an `OverflowError`:\r\n\r\n```\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-128673' coro=<Cluster._sync_cluster_info() done, defined at /opt/homebrew/Caskroom/mambaforge/base/envs/benchmarking/lib/python3.10/site-packages/distributed/deploy/cluster.py:144> exception=OverflowError(34, 'Result too large')>\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/benchmarking/lib/python3.10/site-packages/distributed/deploy/cluster.py\", line 178, in _sync_cluster_info\r\n    interval = min(max_interval, self._sync_interval * 1.5**err_count)\r\nOverflowError: (34, 'Result too large')\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-186646' coro=<Cluster._sync_cluster_info() done, defined at /opt/homebrew/Caskroom/mambaforge/base/envs/benchmarking/lib/python3.10/site-packages/distributed/deploy/cluster.py:144> exception=OverflowError(34, 'Result too large')>\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/benchmarking/lib/python3.10/site-packages/distributed/deploy/cluster.py\", line 178, in _sync_cluster_info\r\n    interval = min(max_interval, self._sync_interval * 1.5**err_count)\r\nOverflowError: (34, 'Result too large')\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-187847' coro=<Cluster._sync_cluster_info() done, defined at /opt/homebrew/Caskroom/mambaforge/base/envs/benchmarking/lib/python3.10/site-packages/distributed/deploy/cluster.py:144> exception=OverflowError(34, 'Result too large')>\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/benchmarking/lib/python3.10/site-packages/distributed/deploy/cluster.py\", line 178, in _sync_cluster_info\r\n    interval = min(max_interval, self._sync_interval * 1.5**err_count)\r\nOverflowError: (34, 'Result too large')\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7647/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7625", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7625/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7625/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7625/events", "html_url": "https://github.com/dask/distributed/issues/7625", "id": 1614931076, "node_id": "I_kwDOAocYk85gQeSE", "number": 7625, "title": "Non deterministic groupby result when using disk/ partd shuffle method ", "user": {"login": "fjetter", "id": 8629629, "node_id": "MDQ6VXNlcjg2Mjk2Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjetter", "html_url": "https://github.com/fjetter", "followers_url": "https://api.github.com/users/fjetter/followers", "following_url": "https://api.github.com/users/fjetter/following{/other_user}", "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions", "organizations_url": "https://api.github.com/users/fjetter/orgs", "repos_url": "https://api.github.com/users/fjetter/repos", "events_url": "https://api.github.com/users/fjetter/events{/privacy}", "received_events_url": "https://api.github.com/users/fjetter/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-03-08T09:27:36Z", "updated_at": "2023-03-08T09:27:54Z", "closed_at": "2023-03-08T09:27:46Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "When running a groupby using the default disk backend I am receiving non-deterministic results\r\n\r\n```python\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\r\n        'plain_int64': [1, 2, 3],\r\n        'dup_strings': [0, 1, 0]\r\n    })\r\n\r\nddf = dd.from_pandas(df, npartitions=2)\r\ndd_series = ddf.groupby('dup_strings').transform('first')\r\ndd_series.compute()\r\n```\r\n\r\nMost of the time, this example returns\r\n\r\n /  | plain_int64               \r\n-- | --\r\n1 | 2\r\n0 | 1\r\n2 | 1\r\n\r\nbut in about 2-3% I get a different result\r\n /  | plain_int64               \r\n-- | --\r\n1 | 2\r\n2 | 3\r\n0 | 3\r\n\r\n\r\npandas output is\r\n\r\n /  | plain_int64               \r\n-- | --\r\n0 | 1\r\n1 | 2\r\n2 | 1\r\n\r\n\r\nWhen switching to the `tasks` backend using\r\n\r\n```python\r\nimport dask\r\ndask.config.set({\"dataframe.shuffle.method\": \"tasks\"})\r\n``` \r\n\r\nthis goes away.\r\n\r\nNote: whoever is using a distributed cluster will automatically use the `tasks` or `p2p` backend and will not be affected by this", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7625/timeline", "performed_via_github_app": null, "state_reason": "not_planned"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7589", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7589/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7589/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7589/events", "html_url": "https://github.com/dask/distributed/issues/7589", "id": 1601733014, "node_id": "I_kwDOAocYk85feIGW", "number": 7589, "title": "get_data() never offloads serialization", "user": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 1814519071, "node_id": "MDU6TGFiZWwxODE0NTE5MDcx", "url": "https://api.github.com/repos/dask/distributed/labels/performance", "name": "performance", "color": "1d76db", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-02-27T18:29:34Z", "updated_at": "2023-03-13T22:14:22Z", "closed_at": "2023-03-13T22:14:22Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "In theory, when a message is >10 MiB, both gather_dep and get_data should offload (de)serialization to a separate thread.\r\nThis works for gather_dep, but doesn't for get_data:\r\n\r\n```python\r\nimport random\r\nimport threading\r\nfrom distributed.utils_test import gen_cluster\r\n\r\n@gen_cluster(client=True)\r\nasync def test1(c, s, a, b):\r\n    print(\"main thread\", threading.get_ident())\r\n    n = 200_000_000\r\n    class C:\r\n        def __sizeof__(self):\r\n            return n\r\n\r\n        def __getstate__(self):\r\n            print(\"__getstate__\", threading.get_ident())\r\n            return random.randbytes(n)\r\n\r\n        def __setstate__(self, state):\r\n            print(\"__setstate__\", threading.get_ident())\r\n\r\n    x = c.submit(C, key=\"x\", workers=[a.address])\r\n    y = c.submit(lambda x: None, x, key=\"y\", workers=[b.address])\r\n    await y\r\n```\r\nOutput:\r\n```\r\nmain thread 139750127130432\r\n__getstate__ 139750127130432\r\n__getstate__ 139750127130432\r\n__getstate__ 139750127130432\r\n__setstate__ 139749812926016\r\n```\r\nIf you put a breakpoint in `__getstate__`, the stack trace confirms that offload is not involved.\r\n\r\n\r\nThis can cause the worker to become poorly responsive, for the following types of data:\r\n- Dataframes with pure-python string types\r\n- Bespoke pure-python data\r\n- In general, any data where the output of `pickle.dumps` is non-negligible compared to the size of the pickle5 buffers (if any)\r\n- Compressible numpy and pandas data, even when it uses pickle5 buffers. This last use case is particularly bad because compression releases the GIL, so running it on a different thread would leave the main thread completely free.\r\n\r\nNotably, as of today the [coiled-runtime benchmarks](https://github.com/coiled/coiled-runtime) do not capture the last use case, as most of them start from `da.random.random` which produces uncompressible output.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7589/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7589/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7548", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7548/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7548/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7548/events", "html_url": "https://github.com/dask/distributed/issues/7548", "id": 1586200013, "node_id": "I_kwDOAocYk85ei33N", "number": 7548, "title": "`'NoneType' object has no attribute 'send'` in client logs", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-02-15T16:49:18Z", "updated_at": "2023-03-03T09:11:23Z", "closed_at": "2023-03-03T09:11:23Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "```python-traceback\r\nERROR:tornado.application:Exception in callback <bound method Client._heartbeat of <Client: 'tls://10.0.32.52:8786' processes=8 threads=32, memory=119.47 GiB>>\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/Library/Caches/pypoetry/virtualenvs/geo-example-e13jEwDC-py3.9/lib/python3.9/site-packages/tornado/ioloop.py\", line 921, in _run\r\n    val = self.callback()\r\n  File \"/Users/gabe/Library/Caches/pypoetry/virtualenvs/geo-example-e13jEwDC-py3.9/lib/python3.9/site-packages/distributed/client.py\", line 1445, in _heartbeat\r\n    self.scheduler_comm.send({\"op\": \"heartbeat-client\"})\r\nAttributeError: 'NoneType' object has no attribute 'send'\r\n```\r\nThis is repeated about 20 times. It showed up in the Jupyter logs, but I've seen it plenty of times in the past in other places.\r\n\r\nI suspect this happened after the cluster shut itself down, but the notebook was still running. My laptop may also have gone to sleep before the cluster shut itself down, then woken back up after. Can't confirm though, and don't have a reproducer.\r\n\r\n* distributed: 2023.2.0\r\n* macOS\r\n\r\nThe obvious culprit is the `or` statement here:\r\nhttps://github.com/dask/distributed/blob/be3a62ecf0ad94fdf694d02cc5eeae8c31661d86/distributed/client.py#L1442-L1445\r\n\r\nIf `self.scheduler_comm` is None, we should not proceed with the rest of the statement, regardless of the cluster status. xref https://github.com/dask/distributed/pull/7429 cc @jrbourbeau.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7548/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7401", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7401/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7401/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7401/events", "html_url": "https://github.com/dask/distributed/issues/7401", "id": 1495155196, "node_id": "I_kwDOAocYk85ZHkH8", "number": 7401, "title": "Cluster with queued tasks and multiple clients underutilized after one client closes", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4230901184, "node_id": "LA_kwDOAocYk878Lm3A", "url": "https://api.github.com/repos/dask/distributed/labels/scheduling", "name": "scheduling", "color": "E4C52C", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-12-13T21:04:53Z", "updated_at": "2022-12-14T18:29:30Z", "closed_at": "2022-12-14T18:29:30Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "If there are queued tasks, and you cancel some tasks which are running on multiple workers, you'd expect all the threads they vacated are filled by tasks from the queue. However, it's possible this won't happen.\r\n\r\nBecause we just peek at the first N tasks on the queue (instead of `pop`) in [`_next_queued_tasks_for_worker`](https://github.com/dask/distributed/blob/7fb9c484d9ecc1398754c027ecdabc754f5b9fe8/distributed/scheduler.py#L3143-L3156), if transitions result in multiple workers having open slots, we'll keep peeking at the same first few tasks each time, and recommending those same tasks to `processing`. In the end, only `max(newly_open_slots_per_worker)` will get scheduled, instead of `sum(newly_open_slots_per_worker)`.\r\n\r\nIn practice, the only way to cause multiple workers to have open slots in a single transition is via `client.close`, which would mean you'd have to have multiple clients connected (otherwise the `client.close` would cancel everything and render the problem irrelevant). So probably not that common right now, but still should be fixed.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7401/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7327", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7327/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7327/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7327/events", "html_url": "https://github.com/dask/distributed/issues/7327", "id": 1453723419, "node_id": "I_kwDOAocYk85Wpg8b", "number": 7327, "title": "Dashboard is wonky with bokeh 3", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3918158031, "node_id": "LA_kwDOAocYk87piljP", "url": "https://api.github.com/repos/dask/distributed/labels/regression", "name": "regression", "color": "B60205", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-11-17T17:33:09Z", "updated_at": "2022-11-18T01:41:16Z", "closed_at": "2022-11-17T17:35:38Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "With bokeh 3.0.2, the task stream doesn't show up, nor do the 'tasks processing' bars. (I've more commonly seen that a few tasks intermittently show up on the task stream, but most are missing. The 'tasks processing' bars always seem to be broken.)\r\n\r\n![bokeh-3](https://user-images.githubusercontent.com/3309802/202515763-d5727a66-be81-46c0-952b-3a17f35badd2.gif)\r\n\r\nAfter `pip install 'bokeh<3'`:\r\n\r\n![bokeh-2_4_3](https://user-images.githubusercontent.com/3309802/202515741-633ae2c7-4b4b-42c0-9a7a-1e74c58975e5.gif)\r\n\r\n```python\r\nIn [1]: import distributed\r\n\r\nIn [2]: import dask.array as da\r\n\r\nIn [3]: client = distributed.Client()\r\n\r\nIn [4]: da.random.random(200_000, chunks=100).sum().compute()\r\n```\r\n\r\nI see this with both macOS and linux running the scheduler, both Chrome and Safari as the browser.\r\n\r\n**Environment**:\r\n\r\n- Distributed version: 176a84cb2bb86d6c9fc1507fbceccd63178be260, or 2022.11.0\r\n- Python version: 3.9.1\r\n- Operating System: macOS\r\n- Install method (conda, pip, source): source\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7327/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7327/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7223", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7223/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7223/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7223/events", "html_url": "https://github.com/dask/distributed/issues/7223", "id": 1427762003, "node_id": "I_kwDOAocYk85VGetT", "number": 7223, "title": "Queued tasks could deadlock in combination with `secede`", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4057255458, "node_id": "LA_kwDOAocYk87x1M4i", "url": "https://api.github.com/repos/dask/distributed/labels/deadlock", "name": "deadlock", "color": "B60205", "default": false, "description": "The cluster appears to not make any progress"}, {"id": 4230901184, "node_id": "LA_kwDOAocYk878Lm3A", "url": "https://api.github.com/repos/dask/distributed/labels/scheduling", "name": "scheduling", "color": "E4C52C", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-10-28T20:00:52Z", "updated_at": "2022-10-31T13:00:43Z", "closed_at": "2022-10-31T13:00:43Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "When a task secedes, we aren't scheduling any queued tasks right now. So if a root-ish task calls `secede`, and all workers are full, the next task on the queue won't be scheduled, even though a slot has opened up on the worker. This could cause a deadlock if the long-running task has created another task and is waiting for it to run, but it's stuck in the queue.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7223/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7209", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7209/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7209/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7209/events", "html_url": "https://github.com/dask/distributed/issues/7209", "id": 1426057862, "node_id": "I_kwDOAocYk85U_-qG", "number": 7209, "title": "Remove `Scheduler.reschedule` from public API", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3453380961, "node_id": "LA_kwDOAocYk87N1mlh", "url": "https://api.github.com/repos/dask/distributed/labels/discussion", "name": "discussion", "color": "bebaf4", "default": false, "description": "Discussing a topic with no specific actions yet"}, {"id": 4309005681, "node_id": "MDU6TGFiZWw0MzA5MDA1Njgx", "url": "https://api.github.com/repos/dask/distributed/labels/scheduler", "name": "scheduler", "color": "D10945", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-10-27T17:53:00Z", "updated_at": "2022-10-28T14:56:00Z", "closed_at": "2022-10-28T14:56:00Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "I don't know if `Scheduler.reschedule` is even considered public API. There's no client method for it. It does show up in [Scheduler API docs](https://distributed.dask.org/en/stable/scheduling-state.html#distributed.scheduler.Scheduler.reschedule).\r\n\r\nBut `Scheduler.reschedule` presents misleading functionality. It looks like it can be used in general, in any circumstances, to reschedule any task: https://github.com/dask/distributed/blob/09837312ccb3a9f1a14ea068ba5825963fec82cc/distributed/scheduler.py#L7266-L7269\r\n\r\nReading the implementation, this function should actually only be used in one extremely specific circumstance: when the task has already been released in some way on the worker it's assigned to\u2014either via cancellation or a `Reschedule` exception\u2014and you are certain the worker will not send any further updates about the task to the scheduler.\r\n\r\nCalling `reschedule` outside of that case can cause a task to be executing on multiple workers at once, and the scheduler only knows about it on one.\r\n\r\nThis inconsistency creates the need for \"unexpected worker completed task\" logic, and strange and un-tested transitions like `transition_waiting_memory`, `transition_no_worker_memory`, `transition_queued_memory`.\r\n\r\n`reschedule` is really just a handler for the worker informing the scheduler of a `Reschedule` exception being raised. I'm not even 100% sure that work stealing's [use of it](https://github.com/dask/distributed/blob/09837312ccb3a9f1a14ea068ba5825963fec82cc/distributed/stealing.py#L363) is appropriate.\r\n\r\nWe should note in the docstring that this shouldn't be called directly.\r\n\r\ncc @crusaderky @fjetter", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7209/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7209/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7204", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7204/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7204/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7204/events", "html_url": "https://github.com/dask/distributed/issues/7204", "id": 1425400151, "node_id": "I_kwDOAocYk85U9eFX", "number": 7204, "title": "`AssertionError: waiting on released dep` when worker-saturation < .inf", "user": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4309005681, "node_id": "MDU6TGFiZWw0MzA5MDA1Njgx", "url": "https://api.github.com/repos/dask/distributed/labels/scheduler", "name": "scheduler", "color": "D10945", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-10-27T10:24:04Z", "updated_at": "2022-10-28T14:37:58Z", "closed_at": "2022-10-27T17:35:35Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "`test_decide_worker_rootish_while_last_worker_is_retiring` (#7065) fails with `worker-saturation: 1.5`:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/home/crusaderky/github/distributed/distributed/scheduler.py\", line 1418, in validate\r\n    validate_task_state(self)\r\n  File \"/home/crusaderky/github/distributed/distributed/scheduler.py\", line 8173, in validate_task_state\r\n    assert dts.state != \"released\", (\"waiting on released dep\", str(ts), str(dts))\r\nAssertionError: ('waiting on released dep', \"<TaskState 'y-1' waiting>\", \"<TaskState 'x-0' released>\")\r\n2022-10-27 11:21:06,662 - distributed.scheduler - ERROR - \r\nTraceback (most recent call last):\r\n  File \"/home/crusaderky/github/distributed/distributed/scheduler.py\", line 5053, in validate_key\r\n    func(key)\r\n  File \"/home/crusaderky/github/distributed/distributed/scheduler.py\", line 4977, in validate_waiting\r\n    assert ts in dts.waiters  # XXX even if dts._who_has?\r\nAssertionError\r\n```", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7204/timeline", "performed_via_github_app": null, "state_reason": "not_planned"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7200", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7200/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7200/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7200/events", "html_url": "https://github.com/dask/distributed/issues/7200", "id": 1424377851, "node_id": "I_kwDOAocYk85U5kf7", "number": 7200, "title": "Transition queued->memory causes AssertionError", "user": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4057255458, "node_id": "LA_kwDOAocYk87x1M4i", "url": "https://api.github.com/repos/dask/distributed/labels/deadlock", "name": "deadlock", "color": "B60205", "default": false, "description": "The cluster appears to not make any progress"}, {"id": 4309005681, "node_id": "MDU6TGFiZWw0MzA5MDA1Njgx", "url": "https://api.github.com/repos/dask/distributed/labels/scheduler", "name": "scheduler", "color": "D10945", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2022-10-26T16:55:26Z", "updated_at": "2022-11-25T15:59:18Z", "closed_at": "2022-11-25T15:59:18Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "While trying to reproduce #7063, I came across a different error, this one with queueing enabled.\r\nThe below reproducer is NOT minimal - there is likely quite a bit of simplification possible. \r\n\r\n```python\r\n@gen_cluster(client=True, nthreads=[(\"\", 1)], config={\"distributed.scheduler.worker-saturation\": 1.5})\r\nasync def test_steal_rootish_while_retiring(c, s, a):\r\n    \"\"\"https://github.com/dask/distributed/issues/7063\r\n\r\n    Note that this applies to both tasks that raise Reschedule as well as work stealing.\r\n    \"\"\"\r\n    ev = Event()\r\n\r\n    # Put a task in memory on a, which will be retired, and prevent b from acquiring\r\n    # a replica. This will cause a to be stuck in closing_gracefully state until we\r\n    # set b.block_gather_dep.\r\n    m = c.submit(inc, 1, key=\"m\", workers=[a.address])\r\n    await wait(m)\r\n\r\n    async with BlockedGatherDep(s.address, nthreads=1) as b:\r\n        # Large number of tasks to make sure they're rootish\r\n        futures = c.map(\r\n            lambda i, ev: ev.wait(), range(10), ev=ev, key=[f\"x-{i}\" for i in range(10)]\r\n        )\r\n\r\n        while a.state.executing_count != 1 or b.state.executing_count != 1:\r\n            await asyncio.sleep(0.01)\r\n\r\n        assert s.is_rootish(s.tasks[futures[0].key])\r\n\r\n        retire_task = asyncio.create_task(c.retire_workers([a.address]))\r\n        # Wait until AMM sends AcquireReplicasEvent to b to move away m\r\n        await b.in_gather_dep.wait()\r\n        assert s.workers[a.address].status == Status.closing_gracefully\r\n\r\n        # Steal any of the tasks on a\r\n        steal_key = next(iter(a.state.executing)).key\r\n        s.reschedule(steal_key, stimulus_id=\"steal\")\r\n        await ev.set()\r\n\r\n        # The stolen task can now complete on the other worker\r\n        await wait_for_state(steal_key, \"memory\", b)\r\n        await wait_for_state(steal_key, \"memory\", s)\r\n\r\n        # Let graceful retirement of a complete.\r\n        # This in turn reschedules whatever tasks were still processing on a to b.\r\n        b.block_gather_dep.set()\r\n        await retire_task\r\n        await wait(futures)\r\n```\r\n\r\nThe test is green; however I read in the log:\r\n```\r\n  File \"/home/crusaderky/github/distributed/distributed/scheduler.py\", line 5284, in handle_task_finished\r\n    r: tuple = self.stimulus_task_finished(\r\n  File \"/home/crusaderky/github/distributed/distributed/scheduler.py\", line 4649, in stimulus_task_finished\r\n    r: tuple = self._transition(\r\n  File \"/home/crusaderky/github/distributed/distributed/scheduler.py\", line 1813, in _transition\r\n    assert not args and not kwargs, (args, kwargs, start, finish)\r\nAssertionError: ((), {'worker': 'tcp://127.0.0.1:45929', 'nbytes': 28, 'type': b'\\x80\\x04\\x95\\x15\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08builtins\\x94\\x8c\\x04bool\\x94\\x93\\x94.', 'typename': 'bool', 'metadata': {}, 'thread': 139862053221952, 'startstops': ({'action': 'compute', 'start': 1666802403.9580944, 'stop': 1666802403.9590282},), 'status': 'OK'}, 'queued', 'memory')\r\n```\r\n\r\nWhat is happening:\r\n1. steal_key is processing on a\r\n2. steal_key is rescheduled, which causes the scheduler to send a free-keys message to a and put the task back in queue\r\n3. before the free-keys message can reach a, steal_key finishes on a\r\n4. steal_key transitions to memory on a, sending a TaskFinishedMsg to the scheduler. \r\n5. a queued->memory transition happens which, I suspect, is otherwise untested.\r\n\r\nThis is timing-sensitive; if free-keys reached a before the task end, then steal_key would be cancelled and transition to forgotten without any messaging when it ends.\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7200/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7200/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7108", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7108/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7108/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7108/events", "html_url": "https://github.com/dask/distributed/issues/7108", "id": 1397576864, "node_id": "I_kwDOAocYk85TTVSg", "number": 7108, "title": "`Client.get_metadata` using list of keys raises `KeyError` despite `default` if intermediate level does not exist", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [{"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-10-05T10:27:49Z", "updated_at": "2022-10-05T13:13:56Z", "closed_at": "2022-10-05T13:13:56Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "**Reproducer**\r\n\r\n```python3\r\nfrom distributed import Client\r\nclient = Client()\r\nclient.get_metadata([\"x\", \"y\"], False)\r\n```\r\n\r\n**Expected result**\r\n`Client.get_metadata` should return the `default` on a `KeyError` at _any_ level. This is what the docstring suggests:\r\nhttps://github.com/dask/distributed/blob/61353b70c177e09782a2911d26d3b03680e9184c/distributed/client.py#L4057-L4059\r\n\r\nNote that this code returns `False` if we do `client.set_metadata([\"x\"], {})` before `client.get_metadata([\"x\", \"y\"], False)`, i.e., it only catches a `KeyError` on the last level:\r\nhttps://github.com/dask/distributed/blob/61353b70c177e09782a2911d26d3b03680e9184c/distributed/scheduler.py#L6941-L6950\r\n\r\n**Actual behavior**\r\nThe reproducer raises `KeyError: 'x'`\r\n\r\n<details><summary>Full traceback</summary>\r\n<p>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\nInput In [5], in <cell line: 1>()\r\n----> 1 client.get_metadata([\"x\", \"y\"], False)\r\n\r\nFile ~/projects/dask/distributed/distributed/client.py:4067, in Client.get_metadata(self, keys, default)\r\n   4065 if not isinstance(keys, (list, tuple)):\r\n   4066     keys = (keys,)\r\n-> 4067 return self.sync(self.scheduler.get_metadata, keys=keys, default=default)\r\n\r\nFile ~/projects/dask/distributed/distributed/utils.py:339, in SyncMethodMixin.sync(self, func, asynchronous, callback_timeout, *args, **kwargs)\r\n    337     return future\r\n    338 else:\r\n--> 339     return sync(\r\n    340         self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\r\n    341     )\r\n\r\nFile ~/projects/dask/distributed/distributed/utils.py:406, in sync(loop, func, callback_timeout, *args, **kwargs)\r\n    404 if error:\r\n    405     typ, exc, tb = error\r\n--> 406     raise exc.with_traceback(tb)\r\n    407 else:\r\n    408     return result\r\n\r\nFile ~/projects/dask/distributed/distributed/utils.py:379, in sync.<locals>.f()\r\n    377         future = asyncio.wait_for(future, callback_timeout)\r\n    378     future = asyncio.ensure_future(future)\r\n--> 379     result = yield future\r\n    380 except Exception:\r\n    381     error = sys.exc_info()\r\n\r\nFile /opt/homebrew/Caskroom/mambaforge/base/envs/dask-distributed-py3.9/lib/python3.9/site-packages/tornado/gen.py:762, in Runner.run(self)\r\n    759 exc_info = None\r\n    761 try:\r\n--> 762     value = future.result()\r\n    763 except Exception:\r\n    764     exc_info = sys.exc_info()\r\n\r\nFile ~/projects/dask/distributed/distributed/core.py:1154, in PooledRPCCall.__getattr__.<locals>.send_recv_from_rpc(**kwargs)\r\n   1152 prev_name, comm.name = comm.name, \"ConnectionPool.\" + key\r\n   1153 try:\r\n-> 1154     return await send_recv(comm=comm, op=key, **kwargs)\r\n   1155 finally:\r\n   1156     self.pool.reuse(self.addr, comm)\r\n\r\nFile ~/projects/dask/distributed/distributed/core.py:944, in send_recv(comm, reply, serializers, deserializers, **kwargs)\r\n    942     _, exc, tb = clean_exception(**response)\r\n    943     assert exc\r\n--> 944     raise exc.with_traceback(tb)\r\n    945 else:\r\n    946     raise Exception(response[\"exception_text\"])\r\n\r\nFile ~/projects/dask/distributed/distributed/core.py:768, in _handle_comm()\r\n    766     result = handler(comm, **msg)\r\n    767 else:\r\n--> 768     result = handler(**msg)\r\n    769 if inspect.iscoroutine(result):\r\n    770     result = await result\r\n\r\nFile ~/projects/dask/distributed/distributed/scheduler.py:6938, in get_metadata()\r\n   6936 metadata = self.task_metadata\r\n   6937 for key in keys[:-1]:\r\n-> 6938     metadata = metadata[key]\r\n   6939 try:\r\n   6940     return metadata[keys[-1]]\r\n\r\nKeyError: 'x'\r\n```\r\n</p>\r\n</details>\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7108/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7108/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7085", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7085/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7085/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7085/events", "html_url": "https://github.com/dask/distributed/issues/7085", "id": 1390760043, "node_id": "I_kwDOAocYk85S5VBr", "number": 7085, "title": "`worker-saturation` impacts balancing in work-stealing", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4230901184, "node_id": "LA_kwDOAocYk878Lm3A", "url": "https://api.github.com/repos/dask/distributed/labels/scheduling", "name": "scheduling", "color": "E4C52C", "default": false, "description": ""}, {"id": 4230901446, "node_id": "LA_kwDOAocYk878Lm7G", "url": "https://api.github.com/repos/dask/distributed/labels/stealing", "name": "stealing", "color": "EE9CBE", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-09-29T12:07:49Z", "updated_at": "2022-11-10T16:28:12Z", "closed_at": "2022-11-10T16:28:12Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "When `worker-saturation` is not `inf`, then workers are only classified as idle if they are not full:\r\n\r\nhttps://github.com/dask/distributed/blob/482941ebe6c0d5fd851efd4b193ea3392b7ce4a9/distributed/scheduler.py#L2899-L2903\r\n\r\nWhile this behavior is desired for withholding root-tasks (it was introduced in #6614), work-stealing also relies on the classification of idle tasks to identify thieves. Limiting this to workers that are not saturated according to `worker-saturation` delays balancing decisions until workers are almost out of work and reduces our ability to interleave computation of remaining tasks with gathering dependencies of stolen ones. \r\n\r\n**Reproducer**\r\n_Add the following test case to `test_steal.py`_\r\n```python3\r\n@pytest.mark.parametrize(\"queue\", [True, False])\r\n@pytest.mark.parametrize(\"recompute_saturation\", [True, False])\r\n@pytest.mark.parametrize(\r\n    \"inp,expected\",\r\n    [\r\n        (\r\n            [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]],\r\n            [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0]],\r\n        ),  # balance many tasks\r\n    ],\r\n)\r\ndef test_balance_interacts_with_worker_saturation(\r\n    inp, expected, queue, recompute_saturation\r\n):\r\n    async def test_balance_(*args, **kwargs):\r\n        await assert_balanced(inp, expected, recompute_saturation, *args, **kwargs)\r\n\r\n    config = {\r\n        \"distributed.scheduler.default-task-durations\": {str(i): 1 for i in range(10)},\r\n        \"distributed.scheduler.worker-saturation\": 1.0 if queue else float(\"inf\"),\r\n    }\r\n    gen_cluster(client=True, nthreads=[(\"\", 1)] * len(inp), config=config)(\r\n        test_balance_\r\n    )()\r\n```\r\n\r\n```\r\nFAILED distributed/tests/test_steal.py::test_balance_interacts_with_worker_saturation[inp0-expected0-True-True] - Exception: Expected: [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0]]; got: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]]\r\nFAILED distributed/tests/test_steal.py::test_balance_interacts_with_worker_saturation[inp0-expected0-False-True] - Exception: Expected: [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0]]; got: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]]\r\n```\r\n\r\ncc @gjoseph92 ", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7085/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7061", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7061/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7061/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7061/events", "html_url": "https://github.com/dask/distributed/issues/7061", "id": 1384265984, "node_id": "I_kwDOAocYk85SgjkA", "number": 7061, "title": "`No dispatch for <class 'numpy.ndarray'>`", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2022-09-23T20:25:01Z", "updated_at": "2022-10-14T02:43:53Z", "closed_at": "2022-10-14T02:43:53Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "This deserialization error then caused https://github.com/dask/distributed/issues/7060:\r\n\r\n```python-traceback\r\n2022-09-23 19:01:53,760 - distributed.protocol.core - CRITICAL - Failed to deserialize\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/distributed/protocol/core.py\", line 158, in loads\r\n    return msgpack.loads(\r\n  File \"msgpack/_unpacker.pyx\", line 194, in msgpack._cmsgpack.unpackb\r\n  File \"/usr/local/lib/python3.9/site-packages/distributed/protocol/core.py\", line 138, in _decode_default\r\n    return merge_and_deserialize(\r\n  File \"/usr/local/lib/python3.9/site-packages/distributed/protocol/serialize.py\", line 497, in merge_and_deserialize\r\n    return deserialize(header, merged_frames, deserializers=deserializers)\r\n  File \"/usr/local/lib/python3.9/site-packages/distributed/protocol/serialize.py\", line 426, in deserialize\r\n    return loads(header, frames)\r\n  File \"/usr/local/lib/python3.9/site-packages/distributed/protocol/serialize.py\", line 58, in dask_loads\r\n    loads = dask_deserialize.dispatch(typ)\r\n  File \"/usr/local/lib/python3.9/site-packages/dask/utils.py\", line 632, in dispatch\r\n    raise TypeError(f\"No dispatch for {cls}\")\r\nTypeError: No dispatch for <class 'numpy.ndarray'>\r\n```\r\n\r\nAFAICT, NumPy was installed on the worker. There are plenty of other NumPy keys in memory on the same worker. So I'm not sure why this happened.\r\n\r\nUnfortunately, I don't have more information here. Not sure how easily reproducible it is.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7061/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/7018", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7018/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7018/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7018/events", "html_url": "https://github.com/dask/distributed/pull/7018", "id": 1364872951, "node_id": "PR_kwDOAocYk84-hbLI", "number": 7018, "title": "Detect mismatching Python version in scheduler", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-09-07T15:50:26Z", "updated_at": "2022-09-26T16:02:35Z", "closed_at": "2022-09-26T16:02:29Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "draft": false, "pull_request": {"url": "https://api.github.com/repos/dask/distributed/pulls/7018", "html_url": "https://github.com/dask/distributed/pull/7018", "diff_url": "https://github.com/dask/distributed/pull/7018.diff", "patch_url": "https://github.com/dask/distributed/pull/7018.patch", "merged_at": "2022-09-26T16:02:29Z"}, "body": "Detects a mismatching Python version on the scheduler\r\n\r\n- [x] Tests added / passed\r\n- [x] Passes `pre-commit run --all-files`\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7018/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7018/timeline", "performed_via_github_app": null, "state_reason": null}, {"url": "https://api.github.com/repos/dask/distributed/issues/7002", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/7002/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/7002/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/7002/events", "html_url": "https://github.com/dask/distributed/issues/7002", "id": 1361993706, "node_id": "I_kwDOAocYk85RLl_q", "number": 7002, "title": "Stealing balance not accounting for recent decisions", "user": {"login": "fjetter", "id": 8629629, "node_id": "MDQ6VXNlcjg2Mjk2Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjetter", "html_url": "https://github.com/fjetter", "followers_url": "https://api.github.com/users/fjetter/followers", "following_url": "https://api.github.com/users/fjetter/following{/other_user}", "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions", "organizations_url": "https://api.github.com/users/fjetter/orgs", "repos_url": "https://api.github.com/users/fjetter/repos", "events_url": "https://api.github.com/users/fjetter/events{/privacy}", "received_events_url": "https://api.github.com/users/fjetter/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3621597059, "node_id": "LA_kwDOAocYk87X3S-D", "url": "https://api.github.com/repos/dask/distributed/labels/stability", "name": "stability", "color": "310DBB", "default": false, "description": "Issue or feature related to cluster stability (e.g. deadlock)"}, {"id": 3798450386, "node_id": "LA_kwDOAocYk87iZ8DS", "url": "https://api.github.com/repos/dask/distributed/labels/p1", "name": "p1", "color": "ff5233", "default": false, "description": "Affects a large population and inhibits work"}, {"id": 3918158031, "node_id": "LA_kwDOAocYk87piljP", "url": "https://api.github.com/repos/dask/distributed/labels/regression", "name": "regression", "color": "B60205", "default": false, "description": ""}, {"id": 4230901184, "node_id": "LA_kwDOAocYk878Lm3A", "url": "https://api.github.com/repos/dask/distributed/labels/scheduling", "name": "scheduling", "color": "E4C52C", "default": false, "description": ""}, {"id": 4230901446, "node_id": "LA_kwDOAocYk878Lm7G", "url": "https://api.github.com/repos/dask/distributed/labels/stealing", "name": "stealing", "color": "EE9CBE", "default": false, "description": ""}, {"id": 4309005681, "node_id": "MDU6TGFiZWw0MzA5MDA1Njgx", "url": "https://api.github.com/repos/dask/distributed/labels/scheduler", "name": "scheduler", "color": "D10945", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2022-09-05T13:39:37Z", "updated_at": "2022-09-16T12:35:38Z", "closed_at": "2022-09-16T12:35:38Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "The [Stealing.balance](https://github.com/dask/distributed/blob/b133009cee88fd48c8a345cffde0a8e9163426a6/distributed/stealing.py#L378-L502) implementation is currently using a greedy for-loop with multiple early termination conditions. These break statements are de-facto dead code causing too greedy and potentially destructive work stealing.\r\n\r\nStealing is highly sensitive to the definition of idle and saturated workers which is defined in [`Scheduler.check_idle_saturated`](https://github.com/dask/distributed/blob/b133009cee88fd48c8a345cffde0a8e9163426a6/distributed/scheduler.py#L2812-L2863).\r\n\r\n<details>\r\n\r\n<summary> Explanation of saturated </summary>\r\n\r\nSee also https://github.com/dask/distributed/pull/6614/files/36a60a5e358ea2a5d16597651126ac5892203b01#r952608704 \r\n\r\nI think the new definition of idle to incorporate whether there are free slots makes a lot of sense.\r\n\r\n> I didn't change the saturated definition because I don't honestly understand what it's trying to measure: https://github.com/dask/distributed/blob/a1d2011e7d5d5ba26bea8bd4b0dec65f485f2b43/distributed/scheduler.py#L2611-L2613\r\n\r\noff-topic but I spent some time on this before and it's worth sharing\r\n\r\nyou can rewrite this statement to\r\n\r\n```python\r\npending: float = occ * (p - nc) / (p * nc)\r\npending = occ / nc - occ / p\r\npending = occupancy / n_threads - occupancy / processing\r\npending = avg_occ_per_thread - avg_occ_currently_processing\r\n```\r\n\r\ni.e. `pending` is \"average occupancy per thread that is in state `ready` / not, yet executing\"\r\nor put into context of stealing, which is the only place that `saturated` matters, \"occupancy per thread that could be stolen\"\r\nI guess what's written there is more stable in terms of floating point arithmetic.\r\n\r\nSo, the definition of saturated is spelled out something like\r\n\r\nA worker is classified as saturated if it's `ready` occupancy is at least 0.4s per thread and at least 1.9 times the average cluster wide occupancy.\r\n\r\ni.e. \"there is something queued up / in ready and it is about two times more than average\"\r\n\r\n\r\n</details>\r\n\r\nSimplified, this is how the algorithm is _intended_ to work:\r\n`Stealing.balance` iterates over _saturated_ workers in descending occupancy and victimizes them for stealing. An _idle_ worker will act as the thief and work will be rebalanced. This continues until there are no longer saturated workers or there are no longer idle workers.\r\n\r\nHowever, this pseudo algorithm is no longer implemented. The current code only shows hints at how this was formerly achieved and it likely strongly depended on dicts, sets, etc. being passed by reference such that any changes to the fundamental collections are immediately propagated to the balance loop which then allows for early termination. \r\n\r\n`Stealing.maybe_move_task ` is calling `Scheduler. check_idle_saturated` with an additional keyword `occ` which is *only used here*. This keyword allows to _override_ the actual occupancy of the worker which would allow us to reclassify thieves and victims based on their `combined_occupancy`, i.e. the `Scheduler.idle` and `Scheduler.saturated` collections are updated under the assumption that in-flight tasks are stolen successfully. https://github.com/dask/distributed/blob/b133009cee88fd48c8a345cffde0a8e9163426a6/distributed/stealing.py#L409-L410\r\n\r\nAssuming that `idle` in the balance for-loop was referencing `Scheduler.idle` and not be a derivative, this would allow us to break out of the loop at various places, e.g. [here](https://github.com/dask/distributed/blob/b133009cee88fd48c8a345cffde0a8e9163426a6/distributed/stealing.py#L436-L437), [here](https://github.com/dask/distributed/blob/b133009cee88fd48c8a345cffde0a8e9163426a6/distributed/stealing.py#L451-L452) or [here](https://github.com/dask/distributed/blob/b133009cee88fd48c8a345cffde0a8e9163426a6/distributed/stealing.py#L471-L472). However, the loop variable `idle` is *always* [defined as a list](https://github.com/dask/distributed/blob/b133009cee88fd48c8a345cffde0a8e9163426a6/distributed/stealing.py#L415) since https://github.com/dask/distributed/pull/5665 and even before [is _sometimes_ a list due to sorting](https://github.com/dask/distributed/blob/b133009cee88fd48c8a345cffde0a8e9163426a6/distributed/stealing.py#L432-L433) ever since https://github.com/dask/distributed/pull/754\r\nThis effectively renders any `if not idle; break` statements dead code. This can be confirmed by codecoverage measurements\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/8629629/188457636-3b59c89b-9d1e-4b4a-8287-3f2e103f52ad.png)\r\n\r\nGoing back to https://github.com/dask/distributed/pull/750 it appears that a similar mechanism existed for early termination if saturated became empty. The implementation changed drastically and I do not see any leftover termination mechanism that ensures a potential _victim_ is still saturated if one accounts for combined_occupancy.\r\n\r\nI suspect this lack of early termination to be a major contributor to at least the following tickets\r\n\r\n- https://github.com/dask/distributed/issues/6573\r\n- https://github.com/dask/distributed/issues/5243", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/7002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/7002/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6951", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6951/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6951/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6951/events", "html_url": "https://github.com/dask/distributed/issues/6951", "id": 1349908600, "node_id": "I_kwDOAocYk85Qdfh4", "number": 6951, "title": "`HeapSet.sorted()` can return duplicate elements", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-08-24T19:14:23Z", "updated_at": "2022-08-25T10:47:33Z", "closed_at": "2022-08-25T10:47:33Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "If an element is discarded and re-added, `HeapSet.sorted()` will yield it twice:\r\n```python\r\n    def test_heapset_sort_duplicate():\r\n        heap = HeapSet(key=operator.attrgetter(\"i\"))\r\n    \r\n        c1 = C(\"x\", 1)\r\n        c2 = C(\"2\", 2)\r\n    \r\n        heap.add(c1)\r\n        heap.add(c2)\r\n        heap.discard(c1)\r\n        heap.add(c1)\r\n    \r\n>       assert list(heap.sorted()) == [c1, c2]\r\nE       assert [C(x, 1), C(x, 1), C(2, 2)] == [C(x, 1), C(2, 2)]\r\nE         At index 1 diff: C(x, 1) != C(2, 2)\r\nE         Left contains one more item: C(2, 2)\r\nE         Full diff:\r\nE         - [C(x, 1), C(2, 2)]\r\nE         + [C(x, 1), C(x, 1), C(2, 2)]\r\nE         ?          +++++++++\r\n```\r\n\r\nSimilar to https://github.com/dask/distributed/issues/6950.\r\n\r\ncc @crusaderky", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6951/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6950", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6950/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6950/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6950/events", "html_url": "https://github.com/dask/distributed/issues/6950", "id": 1349906492, "node_id": "I_kwDOAocYk85QdfA8", "number": 6950, "title": "`HeapSet` may not update position of item re-added with different priority", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-08-24T19:12:10Z", "updated_at": "2022-08-25T10:47:33Z", "closed_at": "2022-08-25T10:47:33Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Just a small thing I noticed:\r\n\r\n```python\r\n    def test_heapset_update_key():\r\n        heap = HeapSet(key=operator.attrgetter(\"i\"))\r\n    \r\n        cx = C(\"x\", 1)\r\n        c2 = C(\"2\", 2)\r\n    \r\n        heap.add(cx)\r\n        heap.add(c2)\r\n    \r\n        heap.discard(cx)\r\n        cx.i = 3\r\n        heap.add(cx)\r\n    \r\n>       assert heap.pop() is c2\r\nE       assert C(x, 3) is C(2, 2)\r\nE        +  where C(x, 3) = <bound method HeapSet.pop of <HeapSet: 1 items>>()\r\nE        +    where <bound method HeapSet.pop of <HeapSet: 1 items>> = <HeapSet: 1 items>.pop\r\n```\r\n\r\n**I don't think this actually affects us currently** since we currently only track tasks in HeapSets, and their priorities can't change. So it might be fine to fix this just by mentioning it in the docstring.\r\n\r\nOtherwise, to actually fix, we'd probably need to make `_data` into a dict instead of a set, and store the `_inc` for each element. When retrieving elements, we wouldn't just check `if value in self._data`, we'd check `if self._data.get(value) == v_tuple[1]`.\r\n\r\ncc @crusaderky", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6950/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6950/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6927", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6927/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6927/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6927/events", "html_url": "https://github.com/dask/distributed/issues/6927", "id": 1344813063, "node_id": "I_kwDOAocYk85QKDgH", "number": 6927, "title": "p2p shuffle fails when partition index is pandas extension dtype", "user": {"login": "ian-r-rose", "id": 5728311, "node_id": "MDQ6VXNlcjU3MjgzMTE=", "avatar_url": "https://avatars.githubusercontent.com/u/5728311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ian-r-rose", "html_url": "https://github.com/ian-r-rose", "followers_url": "https://api.github.com/users/ian-r-rose/followers", "following_url": "https://api.github.com/users/ian-r-rose/following{/other_user}", "gists_url": "https://api.github.com/users/ian-r-rose/gists{/gist_id}", "starred_url": "https://api.github.com/users/ian-r-rose/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ian-r-rose/subscriptions", "organizations_url": "https://api.github.com/users/ian-r-rose/orgs", "repos_url": "https://api.github.com/users/ian-r-rose/repos", "events_url": "https://api.github.com/users/ian-r-rose/events{/privacy}", "received_events_url": "https://api.github.com/users/ian-r-rose/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": {"login": "jrbourbeau", "id": 11656932, "node_id": "MDQ6VXNlcjExNjU2OTMy", "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrbourbeau", "html_url": "https://github.com/jrbourbeau", "followers_url": "https://api.github.com/users/jrbourbeau/followers", "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}", "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions", "organizations_url": "https://api.github.com/users/jrbourbeau/orgs", "repos_url": "https://api.github.com/users/jrbourbeau/repos", "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}", "received_events_url": "https://api.github.com/users/jrbourbeau/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jrbourbeau", "id": 11656932, "node_id": "MDQ6VXNlcjExNjU2OTMy", "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrbourbeau", "html_url": "https://github.com/jrbourbeau", "followers_url": "https://api.github.com/users/jrbourbeau/followers", "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}", "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions", "organizations_url": "https://api.github.com/users/jrbourbeau/orgs", "repos_url": "https://api.github.com/users/jrbourbeau/repos", "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}", "received_events_url": "https://api.github.com/users/jrbourbeau/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-08-19T19:14:37Z", "updated_at": "2022-09-21T16:15:25Z", "closed_at": "2022-09-21T16:15:25Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "I was experimenting with some p2p shuffle workloads, and ran into the following failure when the index is a pandas extension dtype (e.g., the new-ish `python[string]` dtype).\r\n\r\n```python\r\nimport distributed\r\nimport dask.datasets\r\n\r\ncluster = distributed.LocalCluster()\r\nclient = distributed.Client(cluster)\r\n\r\nddf = dask.datasets.timeseries()\r\nddf.index = ddf.index.astype(\"string[python]\")\r\n\r\nddf.shuffle(\"name\", shuffle=\"p2p\").compute()\r\n```\r\nProduces:\r\n<details>\r\n\r\n```python-traceback\r\nAttributeError                            Traceback (most recent call last)\r\nInput In [45], in <cell line: 10>()\r\n      7 ddf = dask.datasets.timeseries()\r\n      8 ddf.index = ddf.index.astype(\"string[python]\")\r\n---> 10 ddf.shuffle(\"name\", shuffle=\"p2p\").compute()\r\n\r\nFile ~/dask/dask/dask/base.py:315, in DaskMethodsMixin.compute(self, **kwargs)\r\n    291 def compute(self, **kwargs):\r\n    292     \"\"\"Compute this dask collection\r\n    293 \r\n    294     This turns a lazy Dask collection into its in-memory equivalent.\r\n   (...)\r\n    313     dask.base.compute\r\n    314     \"\"\"\r\n--> 315     (result,) = compute(self, traverse=False, **kwargs)\r\n    316     return result\r\n\r\nFile ~/dask/dask/dask/base.py:600, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    597     keys.append(x.__dask_keys__())\r\n    598     postcomputes.append(x.__dask_postcompute__())\r\n--> 600 results = schedule(dsk, keys, **kwargs)\r\n    601 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile ~/dask/distributed/distributed/client.py:3036, in Client.get(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\r\n   3034         should_rejoin = False\r\n   3035 try:\r\n-> 3036     results = self.gather(packed, asynchronous=asynchronous, direct=direct)\r\n   3037 finally:\r\n   3038     for f in futures.values():\r\n\r\nFile ~/dask/distributed/distributed/client.py:2210, in Client.gather(self, futures, errors, direct, asynchronous)\r\n   2208 else:\r\n   2209     local_worker = None\r\n-> 2210 return self.sync(\r\n   2211     self._gather,\r\n   2212     futures,\r\n   2213     errors=errors,\r\n   2214     direct=direct,\r\n   2215     local_worker=local_worker,\r\n   2216     asynchronous=asynchronous,\r\n   2217 )\r\n\r\nFile ~/dask/distributed/distributed/utils.py:338, in SyncMethodMixin.sync(self, func, asynchronous, callback_timeout, *args, **kwargs)\r\n    336     return future\r\n    337 else:\r\n--> 338     return sync(\r\n    339         self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\r\n    340     )\r\n\r\nFile ~/dask/distributed/distributed/utils.py:405, in sync(loop, func, callback_timeout, *args, **kwargs)\r\n    403 if error:\r\n    404     typ, exc, tb = error\r\n--> 405     raise exc.with_traceback(tb)\r\n    406 else:\r\n    407     return result\r\n\r\nFile ~/dask/distributed/distributed/utils.py:378, in sync.<locals>.f()\r\n    376         future = asyncio.wait_for(future, callback_timeout)\r\n    377     future = asyncio.ensure_future(future)\r\n--> 378     result = yield future\r\n    379 except Exception:\r\n    380     error = sys.exc_info()\r\n\r\nFile ~/miniconda3/envs/dask/lib/python3.8/site-packages/tornado/gen.py:769, in Runner.run(self)\r\n    766 exc_info = None\r\n    768 try:\r\n--> 769     value = future.result()\r\n    770 except Exception:\r\n    771     exc_info = sys.exc_info()\r\n\r\nFile ~/dask/distributed/distributed/client.py:2073, in Client._gather(self, futures, errors, direct, local_worker)\r\n   2071         exc = CancelledError(key)\r\n   2072     else:\r\n-> 2073         raise exception.with_traceback(traceback)\r\n   2074     raise exc\r\n   2075 if errors == \"skip\":\r\n\r\nFile ~/dask/dask/dask/optimization.py:990, in __call__()\r\n    988 if not len(args) == len(self.inkeys):\r\n    989     raise ValueError(\"Expected %d args, got %d\" % (len(self.inkeys), len(args)))\r\n--> 990 return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\r\n\r\nFile ~/dask/dask/dask/core.py:149, in get()\r\n    147 for key in toposort(dsk):\r\n    148     task = dsk[key]\r\n--> 149     result = _execute_task(task, cache)\r\n    150     cache[key] = result\r\n    151 result = _execute_task(out, cache)\r\n\r\nFile ~/dask/dask/dask/core.py:119, in _execute_task()\r\n    115     func, args = arg[0], arg[1:]\r\n    116     # Note: Don't assign the subtask results to a variable. numpy detects\r\n    117     # temporaries by their reference count and can execute certain\r\n    118     # operations in-place.\r\n--> 119     return func(*(_execute_task(a, cache) for a in args))\r\n    120 elif not ishashable(arg):\r\n    121     return arg\r\n\r\nFile ~/dask/dask/dask/utils.py:71, in apply()\r\n     40 \"\"\"Apply a function given its positional and keyword arguments.\r\n     41 \r\n     42 Equivalent to ``func(*args, **kwargs)``\r\n   (...)\r\n     68 >>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\r\n     69 \"\"\"\r\n     70 if kwargs:\r\n---> 71     return func(*args, **kwargs)\r\n     72 else:\r\n     73     return func(*args)\r\n\r\nFile ~/dask/distributed/distributed/shuffle/shuffle.py:42, in shuffle_transfer()\r\n     36 def shuffle_transfer(\r\n     37     input: pd.DataFrame,\r\n     38     id: ShuffleId,\r\n     39     npartitions: int | None = None,\r\n     40     column: str | None = None,\r\n     41 ) -> None:\r\n---> 42     get_ext().add_partition(input, id, npartitions=npartitions, column=column)\r\n\r\nFile ~/dask/distributed/distributed/shuffle/shuffle_extension.py:286, in add_partition()\r\n    279 def add_partition(\r\n    280     self,\r\n    281     data: pd.DataFrame,\r\n   (...)\r\n    284     column: str | None = None,\r\n    285 ) -> None:\r\n--> 286     shuffle = self.get_shuffle(\r\n    287         shuffle_id, empty=data, npartitions=npartitions, column=column\r\n    288     )\r\n    289     shuffle.add_partition(data=data)\r\n\r\nFile ~/dask/distributed/distributed/shuffle/shuffle_extension.py:389, in get_shuffle()\r\n    382 def get_shuffle(\r\n    383     self,\r\n    384     shuffle_id: ShuffleId,\r\n   (...)\r\n    387     npartitions: int | None = None,\r\n    388 ) -> Shuffle:\r\n--> 389     return sync(\r\n    390         self.worker.loop, self._get_shuffle, shuffle_id, empty, column, npartitions\r\n    391     )\r\n\r\nFile ~/dask/distributed/distributed/utils.py:405, in sync()\r\n    403 if error:\r\n    404     typ, exc, tb = error\r\n--> 405     raise exc.with_traceback(tb)\r\n    406 else:\r\n    407     return result\r\n\r\nFile ~/dask/distributed/distributed/utils.py:378, in f()\r\n    376         future = asyncio.wait_for(future, callback_timeout)\r\n    377     future = asyncio.ensure_future(future)\r\n--> 378     result = yield future\r\n    379 except Exception:\r\n    380     error = sys.exc_info()\r\n\r\nFile ~/miniconda3/envs/dask/lib/python3.8/site-packages/tornado/gen.py:769, in run()\r\n    766 exc_info = None\r\n    768 try:\r\n--> 769     value = future.result()\r\n    770 except Exception:\r\n    771     exc_info = sys.exc_info()\r\n\r\nFile ~/dask/distributed/distributed/shuffle/shuffle_extension.py:352, in _get_shuffle()\r\n    348 except KeyError:\r\n    349     try:\r\n    350         result = await self.worker.scheduler.shuffle_get(\r\n    351             id=shuffle_id,\r\n--> 352             schema=pa.Schema.from_pandas(empty).serialize().to_pybytes()\r\n    353             if empty is not None\r\n    354             else None,\r\n    355             npartitions=npartitions,\r\n    356             column=column,\r\n    357         )\r\n    358     except KeyError:\r\n    359         # Even the scheduler doesn't know about this shuffle\r\n    360         # Let's hand this back to the scheduler and let it figure\r\n    361         # things out\r\n    362         logger.info(\r\n    363             \"Worker Shuffle unable to get information from scheduler, rescheduling\"\r\n    364         )\r\n\r\nFile ~/miniconda3/envs/dask/lib/python3.8/site-packages/pyarrow/types.pxi:1492, in pyarrow.lib.Schema.from_pandas()\r\n   1490 \"\"\"\r\n   1491 from pyarrow.pandas_compat import dataframe_to_types\r\n-> 1492 names, types, metadata = dataframe_to_types(\r\n   1493     df,\r\n   1494     preserve_index=preserve_index\r\n\r\nFile ~/miniconda3/envs/dask/lib/python3.8/site-packages/pyarrow/pandas_compat.py:529, in dataframe_to_types()\r\n    527     type_ = pa.array(c, from_pandas=True).type\r\n    528 elif _pandas_api.is_extension_array_dtype(values):\r\n--> 529     type_ = pa.array(c.head(0), from_pandas=True).type\r\n    530 else:\r\n    531     values, type_ = get_datetimetz_type(values, c.dtype, None)\r\n\r\nAttributeError: 'Index' object has no attribute 'head'\r\n```\r\n</details>\r\n\r\nThis is seemingly due to the same upstream bug in pyarrow as https://github.com/dask/dask/issues/9186\r\n\r\nI opened an upstream bug report in pyarrow over [here](https://issues.apache.org/jira/browse/ARROW-16838) a couple of months ago, but it hasn't gotten too much attention. Perhaps @jorisvandenbossche knows if there is anyone who could take a look?\r\n\r\nIn the meantime, a workaround is to fall back on the old `\"object\"` dtype for strings.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6927/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6877", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6877/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6877/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6877/events", "html_url": "https://github.com/dask/distributed/issues/6877", "id": 1336030828, "node_id": "I_kwDOAocYk85PojZs", "number": 6877, "title": "`AssertionError` in `WorkerState._transition_cancelled_error`", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-08-11T14:19:10Z", "updated_at": "2022-08-18T17:01:46Z", "closed_at": "2022-08-18T17:01:46Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "```\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: Exception in callback Worker._handle_stimulus_from_task(<Task finishe... to SSL\")\\n')>) at /opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py:1844\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: handle: <Handle Worker._handle_stimulus_from_task(<Task finishe... to SSL\")\\n')>) at /opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py:1844>\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: Traceback (most recent call last):\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/asyncio/events.py\", line 80, in _run\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     self._context.run(self._callback, *self._args)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 193, in wrapper\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     return method(self, *args, **kwargs)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 1854, in _handle_stimulus_from_task\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     super()._handle_stimulus_from_task(task)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker_state_machine.py\", line 3341, in _handle_stimulus_from_task\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     self.handle_stimulus(stim)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 193, in wrapper\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     return method(self, *args, **kwargs)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 1866, in handle_stimulus\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     super().handle_stimulus(*stims)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker_state_machine.py\", line 3354, in handle_stimulus\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     instructions = self.state.handle_stimulus(*stims)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker_state_machine.py\", line 1279, in handle_stimulus\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     instructions += self._transitions(recs, stimulus_id=stim.stimulus_id)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker_state_machine.py\", line 2531, in _transitions\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     process_recs(recommendations.copy())\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker_state_machine.py\", line 2525, in process_recs\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     a_recs, a_instructions = self._transition(\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker_state_machine.py\", line 2443, in _transition\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     recs, instructions = func(self, ts, *args, stimulus_id=stimulus_id)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker_state_machine.py\", line 1921, in _transition_cancelled_error\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     assert ts.previous in (\"executing\", \"long-running\")\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: AssertionError\r\n```\r\n\r\n(https://cloud.coiled.io/dask-engineering/clusters/48635/details on `hendrik-debug-worker-057e4c65bd`)\r\n\r\nNotably, the following log messages directly preceded the error:\r\n```\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: 2022-08-11 10:00:05,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: 2022-08-11 10:00:05,225 - distributed.worker - INFO - Stopping worker at tls://10.0.3.173:42801\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: 2022-08-11 10:00:05,240 - distributed.core - ERROR - Exception while handling op get_data\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: Traceback (most recent call last):\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 223, in read\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     frames_nbytes = await stream.read_bytes(fmt_size)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: tornado.iostream.StreamClosedError: Stream is closed\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: During handling of the above exception, another exception occurred:\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: Traceback (most recent call last):\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/core.py\", line 769, in _handle_comm\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     result = await result\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 1692, in get_data\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     response = await comm.read(deserializers=serializers)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 239, in read\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     convert_stream_closed_error(self, e)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     if \"UNKNOWN_CA\" in exc.reason:\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: TypeError: argument of type 'NoneType' is not iterable\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: 2022-08-11 10:00:05,268 - distributed.worker - ERROR - IOStream is not idle; cannot convert to SSL\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: Traceback (most recent call last):\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/core.py\", line 1362, in connect\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     await done.wait()\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/asyncio/locks.py\", line 214, in wait\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     await fut\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: asyncio.exceptions.CancelledError\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: During handling of the above exception, another exception occurred:\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: Traceback (most recent call last):\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 1983, in gather_dep\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     response = await get_data_from_worker(\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 2725, in get_data_from_worker\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/utils_comm.py\", line 383, in retry_operation\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     return await retry(\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/utils_comm.py\", line 368, in retry\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     return await coro()\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 2702, in _get_data\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     comm = await rpc.connect(worker)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/core.py\", line 1367, in connect\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     await connect_attempt\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/core.py\", line 1307, in _connect\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     comm = await connect(\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/core.py\", line 291, in connect\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     comm = await asyncio.wait_for(\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     return fut.result()\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 449, in connect\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     stream = await self.client.connect(\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/tcpclient.py\", line 288, in connect\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     stream = await stream.start_tls(\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/iostream.py\", line 1270, in start_tls\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     raise ValueError(\"IOStream is not idle; cannot convert to SSL\")\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: ValueError: IOStream is not idle; cannot convert to SSL\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: Task exception was never retrieved\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: future: <Task finished name='Task-338' coro=<Server._handle_comm() done, defined at /opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/core.py:675> exception=TypeError(\"argument of type 'NoneType' is not iterable\")>\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: Traceback (most recent call last):\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 223, in read\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     frames_nbytes = await stream.read_bytes(fmt_size)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: tornado.iostream.StreamClosedError: Stream is closed\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: During handling of the above exception, another exception occurred:\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: Traceback (most recent call last):\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/core.py\", line 769, in _handle_comm\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     result = await result\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 1692, in get_data\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     response = await comm.read(deserializers=serializers)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 239, in read\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     convert_stream_closed_error(self, e)\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]:     if \"UNKNOWN_CA\" in exc.reason:\r\nAug 11 10:00:05 ip-10-0-3-173 cloud-init[1264]: TypeError: argument of type 'NoneType' is not iterable\r\n```", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6877/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6877/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6876", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6876/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6876/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6876/events", "html_url": "https://github.com/dask/distributed/issues/6876", "id": 1336024748, "node_id": "I_kwDOAocYk85Poh6s", "number": 6876, "title": "`KeyError` in `ProfileTimePlot.update`", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2022-08-11T14:14:43Z", "updated_at": "2022-12-22T19:06:36Z", "closed_at": "2022-12-22T19:06:36Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "```\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]: 2022-08-11 09:49:57,015 - bokeh.core.property.validation - ERROR - 'start'\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]: Traceback (most recent call last):\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/utils.py\", line 805, in wrapper\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     return func(*args, **kwargs)\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/dashboard/components/shared.py\", line 295, in update\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     ts = metadata[\"keys\"][self.key]\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]: KeyError: 'start'\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]: 2022-08-11 09:49:57,015 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f455b4cd0c0>>, <Task finished name='Task-5714' coro=<ServerSession.with_document_locked() done, defined at /opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/server/session.py:78> exception=KeyError('start')>)\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]: Traceback (most recent call last):\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/ioloop.py\", line 741, in _run_callback\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     ret = callback()\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     future.result()\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/server/session.py\", line 95, in _needs_document_lock_wrapper\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     result = func(self, *args, **kwargs)\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/server/session.py\", line 229, in with_document_locked\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     return func(*args, **kwargs)\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/document/callbacks.py\", line 450, in wrapper\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     return invoke_with_curdoc(doc, invoke)\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/document/callbacks.py\", line 408, in invoke_with_curdoc\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     return f()\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/document/callbacks.py\", line 449, in invoke\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     return f(*args, **kwargs)\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/document/callbacks.py\", line 179, in remove_then_invoke\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     return callback()\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/dashboard/components/shared.py\", line 316, in <lambda>\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     self.doc().add_next_tick_callback(lambda: self.update(prof, metadata))\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/core/property/validation.py\", line 95, in func\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     return input_function(*args, **kwargs)\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/utils.py\", line 805, in wrapper\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     return func(*args, **kwargs)\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/dashboard/components/shared.py\", line 295, in update\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]:     ts = metadata[\"keys\"][self.key]\r\nAug 11 09:49:57 ip-10-0-12-62 cloud-init[1268]: KeyError: 'start'\r\n```\r\n(https://cloud.coiled.io/dask-engineering/clusters/48635/details)", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6876/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6876/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6875", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6875/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6875/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6875/events", "html_url": "https://github.com/dask/distributed/issues/6875", "id": 1336008609, "node_id": "I_kwDOAocYk85Pod-h", "number": 6875, "title": "`TypeError` in `distributed.comm.tcp.convert_stream_closed_error`", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-08-11T14:02:47Z", "updated_at": "2022-08-12T14:13:39Z", "closed_at": "2022-08-12T14:13:39Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "```\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]: Task exception was never retrieved\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]: future: <Task finished name='Task-103' coro=<Server._handle_comm() done, defined at /opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/core.py:675> exception=TypeError(\"argument of type 'NoneType' is not iterable\")>\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]: Traceback (most recent call last):\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 223, in read\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]:     frames_nbytes = await stream.read_bytes(fmt_size)\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]: tornado.iostream.StreamClosedError: Stream is closed\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]: During handling of the above exception, another exception occurred:\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]: Traceback (most recent call last):\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/core.py\", line 769, in _handle_comm\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]:     result = await result\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 1692, in get_data\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]:     response = await comm.read(deserializers=serializers)\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 239, in read\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]:     convert_stream_closed_error(self, e)\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]:     if \"UNKNOWN_CA\" in exc.reason:\r\nAug 11 13:37:25 ip-10-0-4-2 cloud-init[1264]: TypeError: argument of type 'NoneType' is not iterable\r\n```\r\n(https://cloud.coiled.io/dask-engineering/clusters/48659/details on `hendrik-debug-worker-e73a905446`)\r\n\r\n\r\n```\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: 2022-08-11 14:00:05,617 - distributed.core - ERROR - argument of type 'NoneType' is not iterable\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: Traceback (most recent call last):\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 223, in read\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     frames_nbytes = await stream.read_bytes(fmt_size)\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/iostream.py\", line 426, in read_bytes\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     self._try_inline_read()\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/iostream.py\", line 841, in _try_inline_read\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     self._check_closed()\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/iostream.py\", line 1017, in _check_closed\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     raise StreamClosedError(real_error=self.error)\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: tornado.iostream.StreamClosedError: Stream is closed\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: During handling of the above exception, another exception occurred:\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: Traceback (most recent call last):\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/core.py\", line 821, in handle_stream\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     msgs = await comm.read()\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 239, in read\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     convert_stream_closed_error(self, e)\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     if \"UNKNOWN_CA\" in exc.reason:\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: TypeError: argument of type 'NoneType' is not iterable\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: 2022-08-11 14:00:05,627 - distributed.worker - ERROR - argument of type 'NoneType' is not iterable\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: Traceback (most recent call last):\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 223, in read\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     frames_nbytes = await stream.read_bytes(fmt_size)\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/iostream.py\", line 426, in read_bytes\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     self._try_inline_read()\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/iostream.py\", line 841, in _try_inline_read\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     self._check_closed()\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/iostream.py\", line 1017, in _check_closed\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     raise StreamClosedError(real_error=self.error)\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: tornado.iostream.StreamClosedError: Stream is closed\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: During handling of the above exception, another exception occurred:\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: Traceback (most recent call last):\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 180, in wrapper\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     return await method(self, *args, **kwargs)  # type: ignore\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/worker.py\", line 1211, in handle_scheduler\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     await self.handle_stream(comm)\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/core.py\", line 821, in handle_stream\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     msgs = await comm.read()\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 239, in read\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     convert_stream_closed_error(self, e)\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]:     if \"UNKNOWN_CA\" in exc.reason:\r\nAug 11 14:00:05 ip-10-0-6-45 cloud-init[1267]: TypeError: argument of type 'NoneType' is not iterable\r\n```\r\n(https://cloud.coiled.io/dask-engineering/clusters/48659/details on `hendrik-debug-worker-8ede19223f`)", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6875/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6875/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6874", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6874/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6874/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6874/events", "html_url": "https://github.com/dask/distributed/issues/6874", "id": 1336001736, "node_id": "I_kwDOAocYk85PocTI", "number": 6874, "title": "`TypeError` in `ExceptionsTable.update`", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 1342348975, "node_id": "MDU6TGFiZWwxMzQyMzQ4OTc1", "url": "https://api.github.com/repos/dask/distributed/labels/diagnostics", "name": "diagnostics", "color": "d93f0b", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-08-11T13:57:41Z", "updated_at": "2022-08-23T06:54:33Z", "closed_at": "2022-08-23T06:54:33Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "https://github.com/dask/distributed/blob/main/distributed/dashboard/components/scheduler.py#L3293 fails if `ts.erred_on` is `None`.\r\n\r\n```\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]: 2022-08-11 13:40:51,814 - bokeh.util.tornado - ERROR - Error thrown from periodic callback:\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]: 2022-08-11 13:40:51,816 - bokeh.util.tornado - ERROR - Traceback (most recent call last):\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/tornado/gen.py\", line 526, in callback\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     result_list.append(f.result())\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/server/session.py\", line 95, in _needs_document_lock_wrapper\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     result = func(self, *args, **kwargs)\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/server/session.py\", line 229, in with_document_locked\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     return func(*args, **kwargs)\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/document/callbacks.py\", line 450, in wrapper\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     return invoke_with_curdoc(doc, invoke)\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/document/callbacks.py\", line 408, in invoke_with_curdoc\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     return f()\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/document/callbacks.py\", line 449, in invoke\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     return f(*args, **kwargs)\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/dashboard/components/__init__.py\", line 41, in <lambda>\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     doc.add_periodic_callback(lambda: update(ref), interval)\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/core/property/validation.py\", line 95, in func\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     return input_function(*args, **kwargs)\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/dashboard/components/__init__.py\", line 49, in update\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     comp.update()\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/bokeh/core/property/validation.py\", line 95, in func\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     return input_function(*args, **kwargs)\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:   File \"/opt/conda/envs/coiled/lib/python3.10/site-packages/distributed/dashboard/components/scheduler.py\", line 3293, in update\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]:     new_data[\"Worker(s)\"].append(\",\\n\".join(ts.erred_on))\r\nAug 11 13:40:51 ip-10-0-9-12 cloud-init[1262]: TypeError: sequence item 0: expected str instance, NoneType found\r\n```\r\n\r\n(https://cloud.coiled.io/dask-engineering/clusters/48659/details)", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6874/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6874/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6788", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6788/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6788/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6788/events", "html_url": "https://github.com/dask/distributed/pull/6788", "id": 1317956267, "node_id": "PR_kwDOAocYk848GQU5", "number": 6788, "title": "Robust thread termination in `test_watch` and `test_watch_requires_lock_to_run`", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-07-26T09:26:13Z", "updated_at": "2022-08-01T21:49:17Z", "closed_at": "2022-08-01T21:49:17Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "draft": false, "pull_request": {"url": "https://api.github.com/repos/dask/distributed/pulls/6788", "html_url": "https://github.com/dask/distributed/pull/6788", "diff_url": "https://github.com/dask/distributed/pull/6788.diff", "patch_url": "https://github.com/dask/distributed/pull/6788.patch", "merged_at": "2022-08-01T21:49:17Z"}, "body": "Addresses CI failure (https://github.com/dask/distributed/runs/7199711175?check_suite_focus=true#step:11:1868) mentioned in https://github.com/dask/distributed/pull/6033#issuecomment-1190760897\r\n\r\n- [x] Tests added / passed\r\n- [x] Passes `pre-commit run --all-files`\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6788/timeline", "performed_via_github_app": null, "state_reason": null}, {"url": "https://api.github.com/repos/dask/distributed/issues/6783", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6783/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6783/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6783/events", "html_url": "https://github.com/dask/distributed/issues/6783", "id": 1317142989, "node_id": "I_kwDOAocYk85OggHN", "number": 6783, "title": "`SpillBuffer.spilled_total` appears to return incorrect results ", "user": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": {"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hendrikmakait", "id": 2699097, "node_id": "MDQ6VXNlcjI2OTkwOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/2699097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hendrikmakait", "html_url": "https://github.com/hendrikmakait", "followers_url": "https://api.github.com/users/hendrikmakait/followers", "following_url": "https://api.github.com/users/hendrikmakait/following{/other_user}", "gists_url": "https://api.github.com/users/hendrikmakait/gists{/gist_id}", "starred_url": "https://api.github.com/users/hendrikmakait/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hendrikmakait/subscriptions", "organizations_url": "https://api.github.com/users/hendrikmakait/orgs", "repos_url": "https://api.github.com/users/hendrikmakait/repos", "events_url": "https://api.github.com/users/hendrikmakait/events{/privacy}", "received_events_url": "https://api.github.com/users/hendrikmakait/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-07-25T17:25:04Z", "updated_at": "2022-07-28T17:40:38Z", "closed_at": "2022-07-28T17:40:38Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "When spilling data to disk, the `SpillBuffer` appears to return the incorrect size of the data on disk. For example, when spilling an ~8 MB random matrix onto disk, the `data` file created by the `SpillBuffer` is also ~8 MB in size, yet the SpillBuffer only returns ~1 MB.\r\n\r\nReproducer:\r\n```python3\r\nfrom __future__ import annotations\r\n\r\nimport os\r\nimport tempfile\r\n\r\nimport numpy as np\r\n\r\nfrom distributed.spill import SpillBuffer\r\n\r\n\r\ndef test_spill_size():\r\n    tmpdir = tempfile.mkdtemp()\r\n    buf = SpillBuffer(spill_directory=tmpdir, target=0, max_spill=False)\r\n    data = np.random.random((1024, 1024))\r\n    buf[\"data\"] = data\r\n    spill_size = os.stat(os.path.join(tmpdir,  \"data\")).st_size\r\n    assert buf.spilled_total.disk == spill_size\r\n```\r\n\r\nfails with\r\n```\r\n>       assert buf.spilled_total.disk == spill_size\r\nE       assert 1048808 == 8388840\r\nE        +  where 1048808 = SpilledSize(memory=8388608, disk=1048808).disk\r\nE        +    where SpilledSize(memory=8388608, disk=1048808) = Buffer<<LRU: 0/0 on dict>, <zict.cache.Cache object at 0x11f2413d0>>.spilled_total\r\n```", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6783/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6783/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6694", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6694/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6694/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6694/events", "html_url": "https://github.com/dask/distributed/issues/6694", "id": 1298903448, "node_id": "I_kwDOAocYk85Na7GY", "number": 6694, "title": "The dashboard is not cleaning Bokeh Sessions after the browser disconnects.", "user": {"login": "z4m0", "id": 476386, "node_id": "MDQ6VXNlcjQ3NjM4Ng==", "avatar_url": "https://avatars.githubusercontent.com/u/476386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/z4m0", "html_url": "https://github.com/z4m0", "followers_url": "https://api.github.com/users/z4m0/followers", "following_url": "https://api.github.com/users/z4m0/following{/other_user}", "gists_url": "https://api.github.com/users/z4m0/gists{/gist_id}", "starred_url": "https://api.github.com/users/z4m0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/z4m0/subscriptions", "organizations_url": "https://api.github.com/users/z4m0/orgs", "repos_url": "https://api.github.com/users/z4m0/repos", "events_url": "https://api.github.com/users/z4m0/events{/privacy}", "received_events_url": "https://api.github.com/users/z4m0/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 1342348975, "node_id": "MDU6TGFiZWwxMzQyMzQ4OTc1", "url": "https://api.github.com/repos/dask/distributed/labels/diagnostics", "name": "diagnostics", "color": "d93f0b", "default": false, "description": ""}, {"id": 4309005681, "node_id": "MDU6TGFiZWw0MzA5MDA1Njgx", "url": "https://api.github.com/repos/dask/distributed/labels/scheduler", "name": "scheduler", "color": "D10945", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-07-04T14:57:36Z", "updated_at": "2022-07-25T10:06:57Z", "closed_at": "2022-07-25T10:06:57Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**What happened**:\r\nThe Dask Scheduler Dashboard CPU usage increases when the dashboard is used but it keeps being high after the user closes the dashboard.  I think that this could be related with other issues like dask/dask#8493.\r\n\r\nSteps to reproduce: \r\n1. Start dask-scheduler\r\n2. Open the browser at localhost:8787\r\n3. Reload several times. \r\n\r\nYou can expect the CPU usage to increase the more you reload.\r\n\r\n**What you expected to happen**:\r\nThe CPU usage should be stable or at least decrease when the user closes the tab.\r\n\r\n**More information:**\r\nI've added logs to see the active Bokeh sessions\r\n```python\r\n#dashboard/scheduler.py\r\n...\r\ndef connect(application, http_server, scheduler, prefix=\"\"):\r\n    bokeh_app = BokehApplication(\r\n        applications, scheduler, prefix=prefix, template_variables=template_variables\r\n    )\r\n    application.add_application(bokeh_app)\r\n    bokeh_app.initialize(IOLoop.current())\r\n    bokeh_app.add_handlers(\r\n        r\".*\",\r\n        [\r\n            (\r\n                r\"/\",\r\n                web.RedirectHandler,\r\n                {\"url\": urljoin((prefix or \"\").strip(\"/\") + \"/\", r\"status\")},\r\n            )\r\n        ],\r\n    )\r\n\r\n    import threading\r\n    threading.Thread(\r\n        target=run_stuff,\r\n        args=(bokeh_app,),\r\n    ).start()\r\n\r\nfrom time import sleep\r\n\r\ndef run_stuff(bokeh_app):\r\n    while True:\r\n        sleep(1)\r\n        sessions = bokeh_app.get_sessions(\"/status\")\r\n        print('Total sessions', len(sessions))\r\n        for (c, session) in enumerate(sessions):\r\n            print('conn:', c, session.connection_count)\r\n```\r\n\r\nEach time the browser reloads the number of sessions increases even though the connection count for all sessions but one is 0.\r\n\r\n**How to fix this? (I think)**\r\n\r\nThe problem is that the cleaning callback loop is not started. To start one simply has to call `bokeh_app.start()` after the initialization.\r\n\r\n```python\r\n#dashboard/scheduler.py\r\n...\r\ndef connect(application, http_server, scheduler, prefix=\"\"):\r\n    bokeh_app = BokehApplication(\r\n        applications, scheduler, prefix=prefix, template_variables=template_variables\r\n    )\r\n    application.add_application(bokeh_app)\r\n    bokeh_app.initialize(IOLoop.current())\r\n    bokeh_app.add_handlers(\r\n        r\".*\",\r\n        [\r\n            (\r\n                r\"/\",\r\n                web.RedirectHandler,\r\n                {\"url\": urljoin((prefix or \"\").strip(\"/\") + \"/\", r\"status\")},\r\n            )\r\n        ],\r\n    )\r\n    bokeh_app.start()\r\n```\r\n\r\nAlso setting the option `unused_session_lifetime_milliseconds` to 500 helps cleaning the sessions faster.\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2022.6.1\r\n- Python version: 3.10\r\n- Operating System: Ubuntu 22.04\r\n- Install method (conda, pip, source): pip\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6694/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6694/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6624", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6624/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6624/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6624/events", "html_url": "https://github.com/dask/distributed/issues/6624", "id": 1283137177, "node_id": "I_kwDOAocYk85Mex6Z", "number": 6624, "title": "`map_overlap` tasks fail to deserialize on workers - `keywords must be strings`", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3918158031, "node_id": "LA_kwDOAocYk87piljP", "url": "https://api.github.com/repos/dask/distributed/labels/regression", "name": "regression", "color": "B60205", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "fjetter", "id": 8629629, "node_id": "MDQ6VXNlcjg2Mjk2Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjetter", "html_url": "https://github.com/fjetter", "followers_url": "https://api.github.com/users/fjetter/followers", "following_url": "https://api.github.com/users/fjetter/following{/other_user}", "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions", "organizations_url": "https://api.github.com/users/fjetter/orgs", "repos_url": "https://api.github.com/users/fjetter/repos", "events_url": "https://api.github.com/users/fjetter/events{/privacy}", "received_events_url": "https://api.github.com/users/fjetter/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fjetter", "id": 8629629, "node_id": "MDQ6VXNlcjg2Mjk2Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjetter", "html_url": "https://github.com/fjetter", "followers_url": "https://api.github.com/users/fjetter/followers", "following_url": "https://api.github.com/users/fjetter/following{/other_user}", "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions", "organizations_url": "https://api.github.com/users/fjetter/orgs", "repos_url": "https://api.github.com/users/fjetter/repos", "events_url": "https://api.github.com/users/fjetter/events{/privacy}", "received_events_url": "https://api.github.com/users/fjetter/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2022-06-24T01:20:55Z", "updated_at": "2022-06-24T14:33:43Z", "closed_at": "2022-06-24T14:33:43Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "```python\r\nimport distributed\r\nimport numpy as np\r\nimport dask.array as da\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    v = da.random.random((20, 20), chunks=(5, 5))\r\n\r\n    overlapped = da.map_overlap(np.sum, v, depth=2, boundary=\"reflect\")\r\n\r\n    client = distributed.Client()\r\n    overlapped.compute()\r\n```\r\n```\r\n2022-06-23 19:16:55,060 - distributed.core - ERROR - keywords must be strings\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/dev/distributed/distributed/core.py\", line 849, in handle_stream\r\n    handler(**merge(extra, msg))\r\n  File \"/Users/gabe/dev/distributed/distributed/worker.py\", line 1818, in _\r\n    event = cls(**kwargs)\r\n  File \"<string>\", line 13, in __init__\r\n  File \"/Users/gabe/dev/distributed/distributed/worker_state_machine.py\", line 667, in __post_init__\r\n    self.run_spec = SerializedTask(**self.run_spec)  # type: ignore[unreachable]\r\nTypeError: keywords must be strings\r\n\r\n...\r\n\r\n2022-06-23 19:16:55,068 - distributed.nanny - ERROR - Worker process died unexpectedly\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/Users/gabe/dev/distributed/distributed/nanny.py\", line 846, in watch_stop_q\r\n    child_stop_q.close()\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/multiprocessing/queues.py\", line 143, in close\r\n    self._reader.close()\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/multiprocessing/connection.py\", line 182, in close\r\n    self._close()\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/multiprocessing/connection.py\", line 366, in _close\r\n    _close(self._handle)\r\n\r\n...\r\n\r\ndistributed.scheduler.KilledWorker: (\"('random_sample-concatenate-1086077ac09ace0ac4330fce33825511', 2, 3)\", <WorkerState 'tcp://127.0.0.1:65387', name: 2, status: closed, memory: 0, processing: 32>)\r\n```\r\n\r\ncc @crusaderky @fjetter\r\n\r\n**Environment**:\r\n\r\n- Dask version: dc019ed2398411549cdf738be4327a601ec7dfca\r\n- Python version: 3.9.5\r\n- Operating System: macOS\r\n- Install method (conda, pip, source): source", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6624/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6573", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6573/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6573/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6573/events", "html_url": "https://github.com/dask/distributed/issues/6573", "id": 1270161175, "node_id": "I_kwDOAocYk85LtR8X", "number": 6573, "title": "Root-ish tasks all schedule onto one worker", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 1814519071, "node_id": "MDU6TGFiZWwxODE0NTE5MDcx", "url": "https://api.github.com/repos/dask/distributed/labels/performance", "name": "performance", "color": "1d76db", "default": false, "description": ""}, {"id": 4230901184, "node_id": "LA_kwDOAocYk878Lm3A", "url": "https://api.github.com/repos/dask/distributed/labels/scheduling", "name": "scheduling", "color": "E4C52C", "default": false, "description": ""}, {"id": 4230901446, "node_id": "LA_kwDOAocYk878Lm7G", "url": "https://api.github.com/repos/dask/distributed/labels/stealing", "name": "stealing", "color": "EE9CBE", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2022-06-14T02:15:26Z", "updated_at": "2022-10-18T16:25:15Z", "closed_at": "2022-10-18T16:25:15Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "```python\r\nimport time\r\nimport dask\r\nimport distributed\r\n\r\nclient = distributed.Client(n_workers=4, threads_per_worker=1)\r\n\r\nroot = dask.delayed(lambda n: \"x\" * n)(dask.utils.parse_bytes(\"1MiB\"), dask_key_name=\"root\")\r\nresults = [dask.delayed(lambda *args: None)(root, i) for i in range(10000)]\r\ndask.compute(results)\r\n```\r\nInitially a few `results` tasks run on other workers, but after about .5 sec, all tasks are just running on a single worker and the other three are idle.\r\n![Screen Shot 2022-06-13 at 8 08 48 PM](https://user-images.githubusercontent.com/3309802/173478512-4dc5332e-c577-4159-90b1-94de3ef0f52e.png)\r\n\r\nI would have expected these tasks to be evenly assigned to all workers up front\r\n\r\nSome variables to play with:\r\n* If the size of the root task is smaller, tasks will be assigned to other workers\r\n* If you remove `dask_key_name=\"root\"`, then _all_ tasks (including the root) will all run on the same worker. I assume this is because they have similar same key names (`lambda`) and therefore the same task group, and some scheduling heuristics are based not on graph structure but on naming heuristics\r\n\r\nDistributed version: 2022.6.0", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6573/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6568", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6568/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6568/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6568/events", "html_url": "https://github.com/dask/distributed/issues/6568", "id": 1269283513, "node_id": "I_kwDOAocYk85Lp7q5", "number": 6568, "title": "Active Memory Manager ignores nbytes thresholds", "user": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3621597059, "node_id": "LA_kwDOAocYk87X3S-D", "url": "https://api.github.com/repos/dask/distributed/labels/stability", "name": "stability", "color": "310DBB", "default": false, "description": "Issue or feature related to cluster stability (e.g. deadlock)"}, {"id": 3965379432, "node_id": "LA_kwDOAocYk87sWuNo", "url": "https://api.github.com/repos/dask/distributed/labels/memory", "name": "memory", "color": "5158E6", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2022-06-13T11:15:01Z", "updated_at": "2022-06-16T09:46:01Z", "closed_at": "2022-06-16T09:46:01Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "The `acquire-replicas` event does not include a `nbytes` dict, like `compute-task` does.\r\nThis means that all tasks fetched by AMM replicate have their size set to 1kb, which in turn means that `comm_threshold_bytes` and `target_message_size` are ignored and you'll potentially have several GiBs worth of tasks in a single gather_dep command.\r\n\r\nThis in turn can kill off the receiving worker, as tasks won't have a chance to be spilled to disk. \r\nTo be more precise, the `target` and `pause` thresholds will be completely bypassed until after all tasks have landed into RAM; the `spill` threshold *may* kick in and save the day but it's going to be a race condition between network transfer speed and disk write speed.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6568/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6568/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6565", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6565/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6565/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6565/events", "html_url": "https://github.com/dask/distributed/issues/6565", "id": 1268233174, "node_id": "I_kwDOAocYk85Ll7PW", "number": 6565, "title": "Seceded tasks don't release resources if they fail", "user": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4057255458, "node_id": "LA_kwDOAocYk87x1M4i", "url": "https://api.github.com/repos/dask/distributed/labels/deadlock", "name": "deadlock", "color": "B60205", "default": false, "description": "The cluster appears to not make any progress"}], "state": "closed", "locked": false, "assignee": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2022-06-11T10:48:17Z", "updated_at": "2022-07-08T16:22:49Z", "closed_at": "2022-07-08T16:22:49Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "The long-running -> error worker transition does not release worker resources.\r\n\r\nOn a related note, there's an unused method `transition_long_running_rescheduled` that should be cleaned up.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6565/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6556", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6556/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6556/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6556/events", "html_url": "https://github.com/dask/distributed/issues/6556", "id": 1266640281, "node_id": "I_kwDOAocYk85Lf2WZ", "number": 6556, "title": "SSL Overflow Errors on distributed 2022.05", "user": {"login": "hhuuggoo", "id": 249407, "node_id": "MDQ6VXNlcjI0OTQwNw==", "avatar_url": "https://avatars.githubusercontent.com/u/249407?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hhuuggoo", "html_url": "https://github.com/hhuuggoo", "followers_url": "https://api.github.com/users/hhuuggoo/followers", "following_url": "https://api.github.com/users/hhuuggoo/following{/other_user}", "gists_url": "https://api.github.com/users/hhuuggoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/hhuuggoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hhuuggoo/subscriptions", "organizations_url": "https://api.github.com/users/hhuuggoo/orgs", "repos_url": "https://api.github.com/users/hhuuggoo/repos", "events_url": "https://api.github.com/users/hhuuggoo/events{/privacy}", "received_events_url": "https://api.github.com/users/hhuuggoo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2022-06-09T20:47:54Z", "updated_at": "2022-06-24T14:33:06Z", "closed_at": "2022-06-24T14:33:06Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "\r\n**What happened**:\r\nI tried to send a ridiculously large numpy array to a dask cluster. This resulted in an OverflowError.\r\n\r\n```\r\n2022-06-09 20:24:23,931 - distributed.batched - ERROR - Error in batched write\r\nTraceback (most recent call last):\r\n  File \"/srv/conda/envs/saturn/lib/python3.9/site-packages/distributed/batched.py\", line 94, in _background_send\r\n    nbytes = yield self.comm.write(\r\n  File \"/srv/conda/envs/saturn/lib/python3.9/site-packages/tornado/gen.py\", line 762, in run\r\n    value = future.result()\r\n  File \"/srv/conda/envs/saturn/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 314, in write\r\n    stream.write(b\"\")\r\n  File \"/srv/conda/envs/saturn/lib/python3.9/site-packages/tornado/iostream.py\", line 544, in write\r\n    self._handle_write()\r\n  File \"/srv/conda/envs/saturn/lib/python3.9/site-packages/tornado/iostream.py\", line 1486, in _handle_write\r\n    super()._handle_write()\r\n  File \"/srv/conda/envs/saturn/lib/python3.9/site-packages/tornado/iostream.py\", line 971, in _handle_write\r\n    num_bytes = self.write_to_fd(self._write_buffer.peek(size))\r\n  File \"/srv/conda/envs/saturn/lib/python3.9/site-packages/tornado/iostream.py\", line 1568, in write_to_fd\r\n    return self.socket.send(data)  # type: ignore\r\n  File \"/srv/conda/envs/saturn/lib/python3.9/ssl.py\", line 1173, in send\r\n    return self._sslobj.write(data)\r\nOverflowError: string longer than 2147483647 bytes\r\n```\r\n**What you expected to happen**:\r\n```\r\nNo errors.\r\n```\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport numpy as np\r\nfrom dask.distributed import Client\r\n\r\narr = np.random.random(300000000)\r\nc = Client(...) # assuming you have a cluster that supports TLS\r\nprint(c.scheduler)\r\ndef func(x):\r\n    return x.sum()\r\nfut = c.submit(func, arr)\r\nprint('submitted')\r\nresult = fut.result()\r\nprint('done')\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nThere was a related fix in #5141, however that fix works by passing in `frame_split_size`, which gets passed to `distributed.protocol.core.dumps`.  `frame_split_size` [is only used if msgpack encounters a type it does not understand](https://github.com/dask/distributed/blob/main/distributed/protocol/core.py#L110). The numpy array becomes a bytearray by the time it makes it to `dumps`, so `frame_split_size` is not used and we end up with a 2.4GB message.\r\n\r\nTornado actually has a fix that does resolve this issue, however it's not on the latest release of tornado which is quite old.\r\n\r\nhttps://github.com/tornadoweb/tornado/blob/master/tornado/iostream.py#L1565\r\n\r\nWhen I monkey patch `write_to_fd`, the problem is resolved. \r\n\r\n**Environment**:\r\n\r\n- Dask version: 2022.5\r\n- Python version:3.9\r\n- Operating System: Linux\r\n- Install method (conda, pip, source): pip\r\n\r\n\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6556/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6548", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6548/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6548/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6548/events", "html_url": "https://github.com/dask/distributed/issues/6548", "id": 1266382949, "node_id": "I_kwDOAocYk85Le3hl", "number": 6548, "title": "Comm objects do not handle cancellation correctly", "user": {"login": "graingert", "id": 413772, "node_id": "MDQ6VXNlcjQxMzc3Mg==", "avatar_url": "https://avatars.githubusercontent.com/u/413772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/graingert", "html_url": "https://github.com/graingert", "followers_url": "https://api.github.com/users/graingert/followers", "following_url": "https://api.github.com/users/graingert/following{/other_user}", "gists_url": "https://api.github.com/users/graingert/gists{/gist_id}", "starred_url": "https://api.github.com/users/graingert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/graingert/subscriptions", "organizations_url": "https://api.github.com/users/graingert/orgs", "repos_url": "https://api.github.com/users/graingert/repos", "events_url": "https://api.github.com/users/graingert/events{/privacy}", "received_events_url": "https://api.github.com/users/graingert/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 668505972, "node_id": "MDU6TGFiZWw2Njg1MDU5NzI=", "url": "https://api.github.com/repos/dask/distributed/labels/networking", "name": "networking", "color": "FBCA04", "default": false, "description": ""}, {"id": 4166426941, "node_id": "LA_kwDOAocYk874VqE9", "url": "https://api.github.com/repos/dask/distributed/labels/asyncio", "name": "asyncio", "color": "bfd4f2", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "graingert", "id": 413772, "node_id": "MDQ6VXNlcjQxMzc3Mg==", "avatar_url": "https://avatars.githubusercontent.com/u/413772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/graingert", "html_url": "https://github.com/graingert", "followers_url": "https://api.github.com/users/graingert/followers", "following_url": "https://api.github.com/users/graingert/following{/other_user}", "gists_url": "https://api.github.com/users/graingert/gists{/gist_id}", "starred_url": "https://api.github.com/users/graingert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/graingert/subscriptions", "organizations_url": "https://api.github.com/users/graingert/orgs", "repos_url": "https://api.github.com/users/graingert/repos", "events_url": "https://api.github.com/users/graingert/events{/privacy}", "received_events_url": "https://api.github.com/users/graingert/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "graingert", "id": 413772, "node_id": "MDQ6VXNlcjQxMzc3Mg==", "avatar_url": "https://avatars.githubusercontent.com/u/413772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/graingert", "html_url": "https://github.com/graingert", "followers_url": "https://api.github.com/users/graingert/followers", "following_url": "https://api.github.com/users/graingert/following{/other_user}", "gists_url": "https://api.github.com/users/graingert/gists{/gist_id}", "starred_url": "https://api.github.com/users/graingert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/graingert/subscriptions", "organizations_url": "https://api.github.com/users/graingert/orgs", "repos_url": "https://api.github.com/users/graingert/repos", "events_url": "https://api.github.com/users/graingert/events{/privacy}", "received_events_url": "https://api.github.com/users/graingert/received_events", "type": "User", "site_admin": false}, {"login": "fjetter", "id": 8629629, "node_id": "MDQ6VXNlcjg2Mjk2Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjetter", "html_url": "https://github.com/fjetter", "followers_url": "https://api.github.com/users/fjetter/followers", "following_url": "https://api.github.com/users/fjetter/following{/other_user}", "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions", "organizations_url": "https://api.github.com/users/fjetter/orgs", "repos_url": "https://api.github.com/users/fjetter/repos", "events_url": "https://api.github.com/users/fjetter/events{/privacy}", "received_events_url": "https://api.github.com/users/fjetter/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2022-06-09T16:24:21Z", "updated_at": "2022-06-16T13:30:13Z", "closed_at": "2022-06-16T13:24:50Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "```python\r\nimport logging\r\nimport asyncio\r\nimport sys\r\n\r\nfrom distributed.comm import (\r\n    connect,\r\n    listen\r\n)\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nasync def get_comm_pair(listen_addr, listen_args={}, connect_args={}, **kwargs):\r\n    q = asyncio.Queue()\r\n\r\n    async def handle_comm(comm):\r\n        await q.put(comm)\r\n\r\n    async with listen(listen_addr, handle_comm, **listen_args, **kwargs) as listener:\r\n        comm = await connect(listener.contact_address, **connect_args, **kwargs)\r\n\r\n    serv_comm = await q.get()\r\n    return (comm, serv_comm)\r\n\r\n\r\nasync def check_comm_cancel(addr):\r\n    a, b = await get_comm_pair(addr)\r\n    try:\r\n        try:\r\n            await asyncio.wait_for(a.read(), 0.05)\r\n        except asyncio.TimeoutError:\r\n            pass\r\n        else:\r\n            raise Exception(\"did not raise\")\r\n        await a.read()\r\n    finally:\r\n        await a.close()\r\n        await b.close()\r\n\r\nasync def amain():\r\n    try:\r\n        await check_comm_cancel(\"tcp://\")\r\n    except AssertionError:\r\n        logger.exception(\"tcp failed\")\r\n    try:\r\n        await check_comm_cancel(\"inproc://\")\r\n    except AssertionError:\r\n        logger.exception(\"inproc failed\")\r\n\r\ndef main():\r\n    return asyncio.run(amain())\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n```\r\n\r\noutput:\r\n\r\n```\r\ntcp failed\r\nTraceback (most recent call last):\r\n  File \"/home/graingert/projects/distributed/demo.py\", line 41, in amain\r\n    await check_comm_cancel(\"tcp://\")\r\n  File \"/home/graingert/projects/distributed/demo.py\", line 34, in check_comm_cancel\r\n    await a.read()\r\n  File \"/home/graingert/projects/distributed/distributed/comm/tcp.py\", line 226, in read\r\n    frames_nbytes = await stream.read_bytes(fmt_size)\r\n  File \"/home/graingert/miniconda3/envs/dask-distributed/lib/python3.10/site-packages/tornado/iostream.py\", line 421, in read_bytes\r\n    future = self._start_read()\r\n  File \"/home/graingert/miniconda3/envs/dask-distributed/lib/python3.10/site-packages/tornado/iostream.py\", line 809, in _start_read\r\n    assert self._read_future is None, \"Already reading\"\r\nAssertionError: Already reading\r\nTraceback (most recent call last):\r\n  File \"/home/graingert/projects/distributed/demo.py\", line 34, in check_comm_cancel\r\n    await a.read()\r\n  File \"/home/graingert/projects/distributed/distributed/comm/inproc.py\", line 196, in read\r\n    msg = await self._read_q.get()\r\n  File \"/home/graingert/projects/distributed/distributed/comm/inproc.py\", line 107, in get\r\n    assert not self._read_future, \"Only one reader allowed\"\r\nAssertionError: Only one reader allowed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/graingert/projects/distributed/demo.py\", line 53, in <module>\r\n    sys.exit(main())\r\n  File \"/home/graingert/projects/distributed/demo.py\", line 50, in main\r\n    return asyncio.run(amain())\r\n  File \"/home/graingert/miniconda3/envs/dask-distributed/lib/python3.10/asyncio/runners.py\", line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/home/graingert/miniconda3/envs/dask-distributed/lib/python3.10/asyncio/base_events.py\", line 646, in run_until_complete\r\n    return future.result()\r\n  File \"/home/graingert/projects/distributed/demo.py\", line 45, in amain\r\n    await check_comm_cancel(\"inproc://\")\r\n  File \"/home/graingert/projects/distributed/demo.py\", line 36, in check_comm_cancel\r\n    await a.close()\r\n  File \"/home/graingert/projects/distributed/distributed/comm/inproc.py\", line 216, in close\r\n    self.abort()\r\n  File \"/home/graingert/projects/distributed/distributed/comm/inproc.py\", line 222, in abort\r\n    self._read_q.put_nowait(_EOF)\r\n  File \"/home/graingert/projects/distributed/distributed/comm/inproc.py\", line 122, in put_nowait\r\n    fut.set_result(value)\r\nasyncio.exceptions.InvalidStateError: invalid state\r\n```", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6548/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6494", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6494/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6494/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6494/events", "html_url": "https://github.com/dask/distributed/issues/6494", "id": 1258866008, "node_id": "I_kwDOAocYk85LCMVY", "number": 6494, "title": "`client.restart()` may cause workers to shut down instead of restarting", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3621597059, "node_id": "LA_kwDOAocYk87X3S-D", "url": "https://api.github.com/repos/dask/distributed/labels/stability", "name": "stability", "color": "310DBB", "default": false, "description": "Issue or feature related to cluster stability (e.g. deadlock)"}], "state": "closed", "locked": false, "assignee": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-06-02T22:27:41Z", "updated_at": "2022-06-07T12:10:49Z", "closed_at": "2022-06-07T12:10:49Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "`Client.restart()` immediately removes WorkerStates before the workers have fully shut down (https://github.com/dask/distributed/issues/6390) [by calling `remove_worker`](https://github.com/dask/distributed/blob/a3414329458e0bb1eb7efa9d948dfe1c91d53bf9/distributed/scheduler.py#L5096-L5106). However, this doesn't flush the `BatchedSend` or wait for confirmation that the worker has received the message. So if the worker heartbeats in this interval after the `WorkerState` has been removed, but before the `op: close` has reached the worker, the scheduler will [get mad at it](https://github.com/dask/distributed/blob/a3414329458e0bb1eb7efa9d948dfe1c91d53bf9/distributed/scheduler.py#L3463-L3466) and the worker will [shut itself down](https://github.com/dask/distributed/blob/a3414329458e0bb1eb7efa9d948dfe1c91d53bf9/distributed/worker.py#L1239-L1245) instead of restarting. Only after it starts to shut down will it receive the `op: close, nanny=True` message from the scheduler, which it will effectively ignore.\r\n\r\nThere are a few ways to address this:\r\n- short-term: the worker should restart when it gets the `missing` message instead of shutting down. This is reasonable to do anyway. https://github.com/dask/distributed/issues/6387\r\n- medium-term: https://github.com/dask/distributed/issues/6390\r\n\r\n@jrbourbeau discovered this, and we initially thought it was an issue with the fact that `Worker.close` `await`s a bunch of things before [turning off its heartbeats](https://github.com/dask/distributed/blob/a3414329458e0bb1eb7efa9d948dfe1c91d53bf9/distributed/worker.py#L1537-L1538) to the scheduler. We thought that the worker was partway through the closing process, but still heartbeating. It's true that this can happen, and it probably shouldn't. However, if an extraneous heartbeat does occur while closing, and the scheduler replies with `missing`, then the `Worker.close()` call in response to that will just [jump on the bandwagon](https://github.com/dask/distributed/blob/a3414329458e0bb1eb7efa9d948dfe1c91d53bf9/distributed/worker.py#L1473-L1475) of the first close call that's already running, so it won't actually cause a shutdown if the first call was doing a restart.\r\n\r\nTherefore, I think this is purely about the race condition on the scheduler.\r\n\r\ncc @fjetter", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6494/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6494/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6437", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6437/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6437/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6437/events", "html_url": "https://github.com/dask/distributed/issues/6437", "id": 1247370794, "node_id": "I_kwDOAocYk85KWV4q", "number": 6437, "title": "Preload teardown failure interferes with scheduler idle timeout", "user": {"login": "jrbourbeau", "id": 11656932, "node_id": "MDQ6VXNlcjExNjU2OTMy", "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrbourbeau", "html_url": "https://github.com/jrbourbeau", "followers_url": "https://api.github.com/users/jrbourbeau/followers", "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}", "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions", "organizations_url": "https://api.github.com/users/jrbourbeau/orgs", "repos_url": "https://api.github.com/users/jrbourbeau/repos", "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}", "received_events_url": "https://api.github.com/users/jrbourbeau/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2022-05-25T02:31:56Z", "updated_at": "2022-05-26T19:53:46Z", "closed_at": "2022-05-26T19:53:46Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Earlier today I ran into a situation where a scheduler preload script had a teardown method failure which caused the scheduler's automatic idle timeout to not work. Here's a quick reproducer in the form of a test:\r\n\r\n```python\r\nimport asyncio\r\n\r\nfrom distributed.utils_test import gen_cluster\r\nfrom distributed.core import Status\r\n\r\nbad_preload_text = \"\"\"\r\ndef dask_setup(scheduler):\r\n    scheduler.foo = 'setup'\r\n\r\ndef dask_teardown(scheduler):\r\n    raise ValueError('ASDF')\r\n\"\"\"\r\n\r\n@gen_cluster(\r\n    client=True,\r\n    config={\r\n        \"distributed.scheduler.idle-timeout\": \"5s\",\r\n        \"distributed.scheduler.preload\": [bad_preload_text],\r\n    },\r\n)\r\nasync def test_idle_timeout_preload_error(c, s, a, b):\r\n    assert s.foo == \"setup\"  # Confirm preload was run on setup\r\n\r\n    while s.status != Status.closed:\r\n        print(f\"{s.status = }\")\r\n        await asyncio.sleep(0.1)\r\n```\r\n\r\nCurrently this test hangs due to the `ValueError` raised in the preload `dask_teardown` method. If you replace, for example, `raise ValueError('ASDF')` with a simple `return` then the test passes ", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6437/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6437/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6356", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6356/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6356/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6356/events", "html_url": "https://github.com/dask/distributed/issues/6356", "id": 1238851592, "node_id": "I_kwDOAocYk85J12AI", "number": 6356, "title": "Deadlock: tasks stolen to old `WorkerState` instance of a reconnected worker", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3621597059, "node_id": "LA_kwDOAocYk87X3S-D", "url": "https://api.github.com/repos/dask/distributed/labels/stability", "name": "stability", "color": "310DBB", "default": false, "description": "Issue or feature related to cluster stability (e.g. deadlock)"}, {"id": 4057255458, "node_id": "LA_kwDOAocYk87x1M4i", "url": "https://api.github.com/repos/dask/distributed/labels/deadlock", "name": "deadlock", "color": "B60205", "default": false, "description": "The cluster appears to not make any progress"}], "state": "closed", "locked": false, "assignee": {"login": "fjetter", "id": 8629629, "node_id": "MDQ6VXNlcjg2Mjk2Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjetter", "html_url": "https://github.com/fjetter", "followers_url": "https://api.github.com/users/fjetter/followers", "following_url": "https://api.github.com/users/fjetter/following{/other_user}", "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions", "organizations_url": "https://api.github.com/users/fjetter/orgs", "repos_url": "https://api.github.com/users/fjetter/repos", "events_url": "https://api.github.com/users/fjetter/events{/privacy}", "received_events_url": "https://api.github.com/users/fjetter/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fjetter", "id": 8629629, "node_id": "MDQ6VXNlcjg2Mjk2Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjetter", "html_url": "https://github.com/fjetter", "followers_url": "https://api.github.com/users/fjetter/followers", "following_url": "https://api.github.com/users/fjetter/following{/other_user}", "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions", "organizations_url": "https://api.github.com/users/fjetter/orgs", "repos_url": "https://api.github.com/users/fjetter/repos", "events_url": "https://api.github.com/users/fjetter/events{/privacy}", "received_events_url": "https://api.github.com/users/fjetter/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2022-05-17T15:55:14Z", "updated_at": "2022-06-22T17:47:39Z", "closed_at": "2022-06-22T17:47:39Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Task stealing keeps references to `WorkerState` objects: https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/stealing.py#L245-L251\r\n\r\nIf a worker disconnects, then reconnects from the address (it restarts, or the shutdown-reconnect bug happens https://github.com/dask/distributed/issues/6354), task stealing can hold a reference to the old `WorkerState` object for that address, while the scheduler is working with the new `WorkerState` object for that address.\r\n\r\nIf tasks are assigned to this old, stale `WorkerState`, and then the worker leaves, the tasks will be forever stuck in processing (because they're not recognized as being on the worker that just left).\r\n\r\n<details><summary>Full trace-through</summary>\r\n\r\nCopied from https://github.com/dask/distributed/issues/6263#issuecomment-1128390113\r\n\r\n1. Stealing decides to move a task to worker X.\r\n    1. It [queues a `steal-request`](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/stealing.py#L242-L244) to worker Y (where the task is currently queued), asking it to cancel the task.\r\n    1. [Stores a reference](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/stealing.py#L245-L247) to the `victim` and `thief` `WorkerState`s (not addresses) in `WorkStealing.in_flight`\r\n2. Worker X gets removed by the scheduler.\r\n3. Its `WorkerState` instance\u2014the one currently referenced `WorkStealing.in_flight`\u2014is removed from `Scheduler.workers`.\r\n4. Worker X heartbeats to the scheduler, reconnecting (bug described above).\r\n4. A _new_ `WorkerState` instance for it is added to `Scheduler.workers`, at the same address. The scheduler thinks nothing is processing on it.\r\n5. Worker Y finally replies, \"hey yeah, it's all cool if you steal that task\".\r\n6. `move_task_confirm` handles this, and [pops info](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/stealing.py#L275) about the stealing operation from `WorkStealing.in_flight`.\r\n7. This info contains a [reference to the `thief`](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/stealing.py#L284) `WorkerState` object. This is the old `WorkerState` instance, which is no longer in `Scheduler.workers`.\r\n8. The `thief`'s [_address_ is in `scheduler.workers`](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/stealing.py#L308), even though the `theif` object isn't.\r\n9. The task [gets assigned](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/stealing.py#L331) to a worker that, to the scheduler, no longer exists.\r\n10. When worker X actually shuts itself down, `Scheduler.remove_worker` [goes to reschedule any tasks it's processing](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/scheduler.py#L4240-L4242). But it's looking at the new `WorkerState` instance, and the task was assigned to the old one, so the task is never rescheduled.\r\n\r\n---------\r\n\r\n</details>\r\n\r\nI think work stealing should either:\r\n* Store the address and `id` of the `WorkerState` instance, instead of a direct reference. Verify that `id(scheduler.workers[addr]) == expected_id`. An advantage is that this avoids reference leaks of `WorkerState` objects (though they should [eventually be cleaned up](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/stealing.py#L144) when the task completes) https://github.com/dask/distributed/issues/6250.\r\n* Just verify that `d[\"thief\"] is self.scheduler.workers[theif.address]`\r\n\r\nIn combination with https://github.com/dask/distributed/issues/6354, causes https://github.com/dask/distributed/issues/6263, https://github.com/dask/distributed/issues/6198.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6356/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6356/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6354", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6354/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6354/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6354/events", "html_url": "https://github.com/dask/distributed/issues/6354", "id": 1238822933, "node_id": "I_kwDOAocYk85J1vAV", "number": 6354, "title": "Worker may message or reconnect to scheduler while worker is closing", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3621597059, "node_id": "LA_kwDOAocYk87X3S-D", "url": "https://api.github.com/repos/dask/distributed/labels/stability", "name": "stability", "color": "310DBB", "default": false, "description": "Issue or feature related to cluster stability (e.g. deadlock)"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-05-17T15:33:05Z", "updated_at": "2022-05-24T20:10:42Z", "closed_at": "2022-05-24T20:10:40Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "There is a race condition where, after `Scheduler.remove_worker` completes and the worker has been removed from the scheduler's perspective, comms with the worker are still open. So:\r\n* Subsequent heartbeats from the worker cause the scheduler to treat it as a new worker connecting\r\n* Any messages might send will still be processed, and could cause errors (if handlers expect the worker to exist, and it doesn't).\r\n\r\nBasically `remove_worker` results in partially-removed state (`WorkerState` and other objects are removed, but connections are not).\r\n\r\n<details><summary>Full trace-through</summary>\r\n\r\nCopied from https://github.com/dask/distributed/issues/6263#issuecomment-1128334969\r\n\r\n1. `close_worker` calls `remove_worker`. (It also [redundantly enqueues a close message to the worker](https://github.com/gjoseph92/distributed/blob/31c97cedbb0fab0bca60a12cb27deb58c59c707d/distributed/scheduler.py#L3431)\u2014`remove_worker` is about to do the same\u2014though I don't think this makes a difference here.)\r\n4. `remove_worker` does a bunch of stuff, including [deleting the `BatchedSend` to that worker](https://github.com/gjoseph92/distributed/blob/31c97cedbb0fab0bca60a12cb27deb58c59c707d/distributed/scheduler.py#L4236) _which has the `{op: close}` message queued on it_. It does not wait until the BatchedSend queue is flushed.\r\n5. `remove_worker` [removes the `WorkerState` entry for that worker](https://github.com/gjoseph92/distributed/blob/31c97cedbb0fab0bca60a12cb27deb58c59c707d/distributed/scheduler.py#L4240)\r\n6. At this point, the scheduler has removed all knowledge of the worker. However, the `close` message hasn't even been sent to it yet\u2014it's still queued inside a BatchedSend, which may not send the message for up to 5ms more. And even after the message has been sent, then `Worker.close` still has to get scheduled on the worker event loop and run (which could take a while https://github.com/dask/distributed/issues/6325). There are multiple sources of truth for when a worker is gone. In some places, it's whether `addr in Scheduler.workers`, or `addr in Scheduler.stream_comms`. In others, it's whether a comm to that worker is closed. The entry being removed from the dict and the comm being closed are disjoint events.\r\n\r\n    **Thus, between when `Scheduler.remove_worker` ends and all comms to the worker comm actually close, we are in a degenerate state and exposed to race conditions. The scheduler forgets about the worker before closing communications to that worker.** Or even confirming that the worker has received the message to close. [Explicitly flushing and closing the `BatchedSend`](https://github.com/dask/distributed/pull/6329/files#diff-bbcf2e505bf2f9dd0dc25de4582115ee4ed4a6e80997affc7b22122912cc6591R4276-R4277) would alleviate this, though not resolve it: while waiting for our send side to close, we could still receive messages from the now-removed worker.\r\n\r\n    Simply moving the flush-and-close to the _top_ of `Scheduler.remove_worker`\u2014before anything else happens\u2014I think would fix the problem. Then, we wouldn't be removing state related to the worker until we were guaranteed the worker connection was closed.\r\n7. **After `remove_worker` has run, but before the close message actually gets sent over the BatchedSend / `Worker.close` starts running, the worker send another heartbeat to the scheduler.**\r\n8. From the scheduler's perspective, this [worker doesn't exist, so it replies \"missing\"](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/scheduler.py#L3454-L3456) to the worker.\r\n9. Though the worker is in state `closing_gracefully`, it [still tries to re-register](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/worker.py#L1208-L1209).\r\n    1. Fun fact (not relevant to this, but yet another broken thing in BatchedSend land): this is going to call `Worker.batched_stream.start` with a new comm object, even though `Worker.batched_stream` is already running with the previous comm object. This the double-start bug I pointed out in https://github.com/dask/distributed/pull/6272/files#r866082750. This will [swap out the `comm`](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/batched.py#L61) that the `BatchedSend` is using, and [launch a second `background_send` coroutine](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/batched.py#L62). Surprisingly, I think having multiple background_sends racing to process one BatchedSend is still safe, just silly.\r\n    2. The `BatchedSend` has now lost its reference to the original comm. However, one [`Worker.handle_scheduler`](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/worker.py#L1233-L1245) is still running with that original comm, and now one is running with the new comm too. Since there's still a reference to the old comm, it [isn't closed](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/comm/tcp.py#L200-L205).\r\n    3. Because the worker's old comm wasn't closed, the old [`Scheduler.handle_worker` is still running](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/scheduler.py#L4821-L4826). Even though, from the scheduler's perspective, the worker it's supposed to be handling doesn't exist anymore. Yay https://github.com/dask/distributed/issues/6201 \u2014 if we had a handle on this coroutine, we could have cancelled it in `remove_worker`.\r\n10. The scheduler [handles this \"new\" worker connecting](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/scheduler.py#L3547-L3725). (This passes through the buggy code of #6341, though I don't think that actually makes a difference in this case.) This is where all of the [\"Unexpected worker completed task\"](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/scheduler.py#L1957-L1963) messages [come from](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/scheduler.py#L3661-L3678), followed by another [\"Register worker\"](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/scheduler.py#L3701) and [\"Starting worker compute stream\"](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/scheduler.py#L4820).\r\n11. Eventually, `Worker.close` [actually closes](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/worker.py#L1542-L1546) (one of) its batched comms to the scheduler.\r\n12. This [triggers a second `remove_worker`](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/scheduler.py#L4823-L4826) on the scheduler, removing the entry for the \"new\" worker.\r\n    1. There's probably still a `Scheduler.handle_worker` running for the old comm too. I presume it eventually stops when the worker actually shuts down and severs the connection? When it does, it probably won't run `remove_worker` another time, because the [address is already gone from `Scheduler.stream_comms`](https://github.com/dask/distributed/blob/b8b45c6e641f298f465d47139f1fb297c4a5799a/distributed/scheduler.py#L4824-L4826).\r\n\r\n------\r\n\r\n</details>\r\n\r\nEven if we do https://github.com/dask/distributed/issues/6350 (which will resolve the heartbeat-reconnect issue), we still need to ensure that `remove_worker` closes the connection to the worker before removing other state.\r\n\r\nI think the easiest way to do that is simply to `await self.stream_comms[address].close()` at the top of `remove_worker`, before removing other state. There are still some race conditions with concurrent `remove_worker` calls to work out, though.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6354/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6340", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6340/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6340/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6340/events", "html_url": "https://github.com/dask/distributed/issues/6340", "id": 1235486041, "node_id": "I_kwDOAocYk85JpAVZ", "number": 6340, "title": "Scheduler.reschedule() does not work", "user": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2022-05-13T17:08:27Z", "updated_at": "2022-06-30T13:57:21Z", "closed_at": "2022-06-30T13:57:21Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Scheduler.reschedule() is called:\r\n1. from Worker.transition_long_running_rescheduled\r\n2. from Worker.transition_executing_rescheduled\r\n3. from stealing.py\r\n4. directly from a bunch of unit tests\r\n\r\nIf you add \"assert False\" at the top of the method, only the tests that invoke it directly fail.\r\nNotably, among them there's test_reschedule - in other words, there is nothing at the moment that tests that worker transitions and work stealing are successfully using the method.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6340/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6277", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6277/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6277/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6277/events", "html_url": "https://github.com/dask/distributed/issues/6277", "id": 1226762777, "node_id": "I_kwDOAocYk85JHuoZ", "number": 6277, "title": "P2P shuffle returns incorrect results if errors occur receiving data (also asyncio tasks are leaked)", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4691806577, "node_id": "LA_kwDOAocYk88AAAABF6dJcQ", "url": "https://api.github.com/repos/dask/distributed/labels/shuffle", "name": "shuffle", "color": "E338E3", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2022-05-05T14:37:26Z", "updated_at": "2022-11-11T15:45:23Z", "closed_at": "2022-11-11T15:45:22Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "If an error occurs deserializing, regrouping, serializing, or writing data to disk, and the shuffle isn't in \"backpressure mode\", that data will simply be lost and the shuffle will still succeed\r\nhttps://github.com/dask/distributed/blob/7bd64425225af4847e2fbf045df31fb60a253e7d/distributed/shuffle/shuffle_extension.py#L252-L258\r\n\r\nWhen `task` isn't awaited, the asyncio task is leaked. Any error that occurred in it is also lost (besides being logged).\r\n\r\n@graingert and I tried to fix this, but we couldn't get something working:\r\n* The dumb way (keep leaking tasks, wrap all of `receive` in a try/except, set `self._exception`, raise `self._exception` in `add_partition`, `get_output_partition`, `inputs_done`) fails tests because of leaking tasks\r\n* The \"proper\" asyncio way causes a CancelledError to pop out in some unexpected place and seems to shut down the whole worker?\r\n\r\nxref #6201 ", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6277/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6263", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6263/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6263/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6263/events", "html_url": "https://github.com/dask/distributed/issues/6263", "id": 1224113312, "node_id": "I_kwDOAocYk85I9nyg", "number": 6263, "title": "Task stuck in \"processing\" on closed worker", "user": {"login": "bnaul", "id": 903655, "node_id": "MDQ6VXNlcjkwMzY1NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/903655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bnaul", "html_url": "https://github.com/bnaul", "followers_url": "https://api.github.com/users/bnaul/followers", "following_url": "https://api.github.com/users/bnaul/following{/other_user}", "gists_url": "https://api.github.com/users/bnaul/gists{/gist_id}", "starred_url": "https://api.github.com/users/bnaul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bnaul/subscriptions", "organizations_url": "https://api.github.com/users/bnaul/orgs", "repos_url": "https://api.github.com/users/bnaul/repos", "events_url": "https://api.github.com/users/bnaul/events{/privacy}", "received_events_url": "https://api.github.com/users/bnaul/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4057255458, "node_id": "LA_kwDOAocYk87x1M4i", "url": "https://api.github.com/repos/dask/distributed/labels/deadlock", "name": "deadlock", "color": "B60205", "default": false, "description": "The cluster appears to not make any progress"}], "state": "closed", "locked": false, "assignee": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 17, "created_at": "2022-05-03T13:34:14Z", "updated_at": "2022-06-22T17:47:39Z", "closed_at": "2022-06-22T17:47:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Similar at a high level to #6198 but a slightly different manifestation: dashboard shows 9 remaining tasks (one is a parent task that spawned the other 8 by calling `dd.read_parquet`), but the Info page shows only the one parent task processing.\r\n![image](https://user-images.githubusercontent.com/903655/166461035-daf26200-fd48-4fea-a69c-d78affb1dd00.png)\r\n\r\nIn the case of #6198 the worker showed up in the scheduler Info page (but would 404 when you tried to click through to its info); here the scheduler knows the workers are gone, but there are still tasks assigned to them anyway:\r\n```\r\n# Just the one parent task\r\nIn [33]: {k: v for k, v in c.processing().items() if v}\r\nOut[33]: {'tcp://10.124.10.46:43097': ('partition_inputs-ea98882db0754e4497b1dcdd7d22e236',)}\r\n\r\n# Here all the stragglers show up; each one is on a worker with status=\"closed\"\r\nIn [34]: c.run_on_scheduler(lambda dask_scheduler: {ts.key: str(ts.processing_on) for ts in dask_scheduler.tasks.values() if ts.state in (\"processing\", \"waiting\")})\r\nOut[34]:\r\n{'partition_inputs-ea98882db0754e4497b1dcdd7d22e236': \"<WorkerState 'tcp://10.124.10.46:43097', name: hardisty-2bab80b9-daskworkers-98d64ff7-28n8w, status: running, memory: 179, processing: 1>\",\r\n \"('_split-ef14f4c130766e401c7f9804e1c5a7bd', 11771)\": 'None',\r\n \"('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 11771)\": \"<WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closed, memory: 0, processing: 18>\",\r\n \"('_split-ef14f4c130766e401c7f9804e1c5a7bd', 7365)\": 'None',\r\n \"('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 7365)\": \"<WorkerState 'tcp://10.126.233.26:37341', name: 125, status: closed, memory: 0, processing: 23>\",\r\n \"('_split-ef14f4c130766e401c7f9804e1c5a7bd', 3225)\": 'None',\r\n \"('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 3225)\": \"<WorkerState 'tcp://10.126.167.29:36945', name: 232, status: closed, memory: 0, processing: 10>\",\r\n \"('_split-ef14f4c130766e401c7f9804e1c5a7bd', 7711)\": 'None',\r\n \"('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 7711)\": \"<WorkerState 'tcp://10.126.167.29:36945', name: 232, status: closed, memory: 0, processing: 10>\",\r\n \"('_split-ef14f4c130766e401c7f9804e1c5a7bd', 4873)\": 'None',\r\n \"('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 4873)\": \"<WorkerState 'tcp://10.127.22.29:43185', name: 412, status: closed, memory: 0, processing: 10>\",\r\n \"('_split-ef14f4c130766e401c7f9804e1c5a7bd', 3331)\": 'None',\r\n \"('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 3331)\": \"<WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closed, memory: 0, processing: 18>\",\r\n \"('_split-ef14f4c130766e401c7f9804e1c5a7bd', 11393)\": 'None',\r\n \"('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 11393)\": \"<WorkerState 'tcp://10.126.71.26:32909', name: 250, status: closed, memory: 0, processing: 10>\",\r\n \"('_split-ef14f4c130766e401c7f9804e1c5a7bd', 10716)\": 'None',\r\n \"('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 10716)\": \"<WorkerState 'tcp://10.126.71.26:32909', name: 250, status: closed, memory: 0, processing: 10>\"}\r\n```\r\n\r\nZooming in on the first closed worker `10.126.160.29:33011`, relevant scheduler logs:\r\n```\r\n(.venv) \u279c  model git:(master) \u2717 kl hardisty-2bab80b9-daskscheduler-5c84ddcfd4-hfplp  | grep 10.126.160.29:33011\r\n2022-05-03 02:05:00,218 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: undefined, memory: 0, processing: 0>\r\n2022-05-03 02:05:00,219 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.126.160.29:33011\r\n2022-05-03 02:07:38,344 - distributed.scheduler - INFO - Retiring worker tcp://10.126.160.29:33011\r\n2022-05-03 02:07:38,684 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.126.160.29:33011; 21 keys are being moved away.\r\n2022-05-03 02:07:39,572 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.126.160.29:33011; 21 keys are being moved away.\r\n2022-05-03 02:07:42,554 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.126.160.29:33011; 10 keys are being moved away.\r\n2022-05-03 02:07:43,888 - distributed.scheduler - INFO - Closing worker tcp://10.126.160.29:33011\r\n2022-05-03 02:07:43,888 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 13, processing: 16>\r\n2022-05-03 02:07:43,888 - distributed.core - INFO - Removing comms to tcp://10.126.160.29:33011\r\n2022-05-03 02:07:43,895 - distributed.scheduler - INFO - Retired worker tcp://10.126.160.29:33011\r\n2022-05-03 02:07:51,117 - distributed.scheduler - INFO - Unexpected worker completed task. Expected: <WorkerState 'tcp://10.127.207.8:34749', name: 2314, status: running, memory: 11, processing: 5>, Got: <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 12, processing: 0>, Key: ('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 7489)\r\n2022-05-03 02:07:51,118 - distributed.scheduler - INFO - Unexpected worker completed task. Expected: <WorkerState 'tcp://10.127.199.4:32777', name: 619, status: running, memory: 13, processing: 7>, Got: <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 14, processing: 0>, Key: ('_split-ef14f4c130766e401c7f9804e1c5a7bd', 7535)\r\n2022-05-03 02:07:51,119 - distributed.scheduler - INFO - Unexpected worker completed task. Expected: <WorkerState 'tcp://10.127.7.24:33955', name: 1049, status: running, memory: 6, processing: 5>, Got: <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 14, processing: 0>, Key: ('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 12280)\r\n2022-05-03 02:07:51,121 - distributed.scheduler - INFO - Unexpected worker completed task. Expected: <WorkerState 'tcp://10.127.113.32:41691', name: 572, status: running, memory: 8, processing: 5>, Got: <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 15, processing: 0>, Key: ('_split-ef14f4c130766e401c7f9804e1c5a7bd', 2679)\r\n2022-05-03 02:07:51,121 - distributed.scheduler - INFO - Unexpected worker completed task. Expected: <WorkerState 'tcp://10.127.199.4:32777', name: 619, status: running, memory: 12, processing: 6>, Got: <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 15, processing: 0>, Key: ('_split-ef14f4c130766e401c7f9804e1c5a7bd', 6989)\r\n2022-05-03 02:07:51,121 - distributed.scheduler - INFO - Unexpected worker completed task. Expected: <WorkerState 'tcp://10.127.133.31:42383', name: 2377, status: running, memory: 8, processing: 5>, Got: <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 15, processing: 0>, Key: ('_split-ef14f4c130766e401c7f9804e1c5a7bd', 844)\r\n2022-05-03 02:07:51,121 - distributed.scheduler - INFO - Unexpected worker completed task. Expected: <WorkerState 'tcp://10.126.226.29:45983', name: 2094, status: running, memory: 7, processing: 6>, Got: <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 15, processing: 0>, Key: ('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 10595)\r\n2022-05-03 02:07:51,123 - distributed.scheduler - INFO - Unexpected worker completed task. Expected: <WorkerState 'tcp://10.127.195.7:42145', name: 2338, status: running, memory: 5, processing: 5>, Got: <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 16, processing: 0>, Key: ('_split-ef14f4c130766e401c7f9804e1c5a7bd', 11009)\r\n2022-05-03 02:07:51,123 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 16, processing: 0>\r\n2022-05-03 02:07:51,128 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.126.160.29:33011\r\n2022-05-03 02:07:55,963 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.126.160.29:33011', name: 77, status: closing_gracefully, memory: 5, processing: 0>\r\n2022-05-03 02:07:55,963 - distributed.core - INFO - Removing comms to tcp://10.126.160.29:33011\r\n```\r\n\r\nAnd worker logs:\r\n```\r\n2022-05-03 02:04:58,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.126.160.29:45303'\r\n2022-05-03 02:04:59,723 - distributed.worker - INFO -       Start worker at:  tcp://10.126.160.29:33011\r\n2022-05-03 02:04:59,723 - distributed.worker - INFO -          Listening to:  tcp://10.126.160.29:33011\r\n2022-05-03 02:04:59,723 - distributed.worker - INFO -          dashboard at:        10.126.160.29:39677\r\n2022-05-03 02:04:59,723 - distributed.worker - INFO - Waiting to connect to: tcp://hardisty-2bab80b9-daskscheduler:8786\r\n2022-05-03 02:04:59,723 - distributed.worker - INFO - -------------------------------------------------\r\n2022-05-03 02:04:59,723 - distributed.worker - INFO -               Threads:                          4\r\n2022-05-03 02:04:59,723 - distributed.worker - INFO -                Memory:                   7.82 GiB\r\n2022-05-03 02:04:59,723 - distributed.worker - INFO -       Local Directory: /src/dask-worker-space/worker-7181cv4y\r\n2022-05-03 02:04:59,723 - distributed.worker - INFO - -------------------------------------------------\r\n2022-05-03 02:05:00,219 - distributed.worker - INFO -         Registered to: tcp://hardisty-2bab80b9-daskscheduler:8786\r\n2022-05-03 02:05:00,220 - distributed.worker - INFO - -------------------------------------------------\r\n2022-05-03 02:05:00,221 - distributed.core - INFO - Starting established connection\r\n2022-05-03 02:07:02,854 - distributed.utils_perf - INFO - full garbage collection released 473.14 MiB from 28 reference cycles (threshold: 9.54 MiB)\r\n2022-05-03 02:07:44,775 - distributed.worker - INFO - -------------------------------------------------\r\n2022-05-03 02:07:46,212 - distributed.worker - INFO - Stopping worker at tcp://10.126.160.29:33011\r\n2022-05-03 02:07:46,212 - distributed.worker - INFO - Not reporting worker closure to scheduler\r\n2022-05-03 02:07:46,219 - distributed.worker - INFO - Connection to scheduler broken. Closing without reporting.  Status: Status.closing\r\n2022-05-03 02:07:51,128 - distributed.worker - INFO -         Registered to: tcp://hardisty-2bab80b9-daskscheduler:8786\r\n2022-05-03 02:07:51,129 - distributed.worker - INFO - -------------------------------------------------\r\n2022-05-03 02:07:51,129 - distributed.core - INFO - Starting established connection\r\n2022-05-03 02:07:52,639 - distributed.worker - ERROR - Exception during execution of task ('read-parquet-14e1bf051c27f27510e4873d5022c6a8', 3331).\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/distributed/worker.py\", line 3677, in execute\r\n    result = await self.loop.run_in_executor(\r\n  File \"/usr/local/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 257, in run_in_executor\r\n    return self.asyncio_loop.run_in_executor(executor, func, *args)\r\n  File \"/usr/local/lib/python3.9/asyncio/base_events.py\", line 814, in run_in_executor\r\n    executor.submit(func, *args), loop=self)\r\n  File \"/usr/local/lib/python3.9/site-packages/distributed/_concurrent_futures_thread.py\", line 127, in submit\r\n    raise RuntimeError(\"cannot schedule new futures after shutdown\")\r\nRuntimeError: cannot schedule new futures after shutdown\r\n2022-05-03 02:07:53,622 - distributed.nanny - INFO - Worker closed\r\n2022-05-03 02:07:53,623 - distributed.nanny - ERROR - Worker process died unexpectedly\r\n2022-05-03 02:07:54,097 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.126.160.29:45303'. Report closure to scheduler: None\r\n2022-05-03 02:07:54,098 - distributed.dask_worker - INFO - End worker\r\n```\r\n\r\nAlso finally got a successful [cluster dump](https://drive.google.com/file/d/1i9o16BHlglpgwP_-TA4J442IYrCwmMEu/view?usp=sharing) \ud83c\udf89 \r\n\r\ncc @gjoseph92 @fjetter @crusaderky @mrocklin ", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6263/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6263/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6244", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6244/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6244/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6244/events", "html_url": "https://github.com/dask/distributed/issues/6244", "id": 1219424678, "node_id": "I_kwDOAocYk85IrvGm", "number": 6244, "title": "Deadlock fetching key from retiring worker, when scheduler thinks we already have the key", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4057255458, "node_id": "LA_kwDOAocYk87x1M4i", "url": "https://api.github.com/repos/dask/distributed/labels/deadlock", "name": "deadlock", "color": "B60205", "default": false, "description": "The cluster appears to not make any progress"}], "state": "closed", "locked": false, "assignee": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2022-04-28T23:06:53Z", "updated_at": "2022-05-06T21:26:02Z", "closed_at": "2022-05-06T21:26:02Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "This is somewhat conjecture based on a partial cluster dump from @bnaul (full one wasn't working for some reason). Hopefully we can attach the dump at some point so others can see. I only got to see the dump of a worker. If we could see the scheduler state, we could confirm this theory.\r\n\r\n<details><summary>This was the worker story of the bad key</summary>\r\n\r\n```python\r\nIn [9]: dump.worker_stories(\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\")\r\nOut[9]: \r\n{'tcp://10.125.88.48:37099': [[\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'ensure-task-exists',\r\n   'released',\r\n   'active_memory_manager-1651174357.5785856',\r\n   datetime.datetime(2022, 4, 28, 13, 32, 37, 586432)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'released',\r\n   'fetch',\r\n   'fetch',\r\n   {},\r\n   'active_memory_manager-1651174357.5785856',\r\n   datetime.datetime(2022, 4, 28, 13, 32, 37, 586584)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'compute-task',\r\n   'compute-task-1651174375.3724043',\r\n   datetime.datetime(2022, 4, 28, 13, 32, 55, 378109)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'release-key',\r\n   'compute-task-1651174375.3724043',\r\n   datetime.datetime(2022, 4, 28, 13, 32, 55, 378176)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'fetch',\r\n   'released',\r\n   'released',\r\n   {\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\": 'forgotten'},\r\n   'compute-task-1651174375.3724043',\r\n   datetime.datetime(2022, 4, 28, 13, 32, 55, 378205)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'released',\r\n   'forgotten',\r\n   'forgotten',\r\n   {},\r\n   'compute-task-1651174375.3724043',\r\n   datetime.datetime(2022, 4, 28, 13, 32, 55, 378212)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'fetch',\r\n   'waiting',\r\n   'forgotten',\r\n   {\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\": 'forgotten'},\r\n   'compute-task-1651174375.3724043',\r\n   datetime.datetime(2022, 4, 28, 13, 32, 55, 378214)]],\r\n 'tcp://10.124.15.5:35871': [[\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'ensure-task-exists',\r\n   'released',\r\n   'active_memory_manager-1651174147.6869376',\r\n   datetime.datetime(2022, 4, 28, 13, 29, 7, 714652)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'released',\r\n   'fetch',\r\n   'fetch',\r\n   {},\r\n   'active_memory_manager-1651174147.6869376',\r\n   datetime.datetime(2022, 4, 28, 13, 29, 7, 714735)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'ensure-task-exists',\r\n   'fetch',\r\n   'active_memory_manager-1651174149.5793703',\r\n   datetime.datetime(2022, 4, 28, 13, 29, 9, 587408)],\r\n  ['receive-dep-failed',\r\n   'tcp://10.124.227.26:35167',\r\n   [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 2764)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 5726)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 9555)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 7277)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 257)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 8041)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 2732)\"],\r\n   'ensure-communicating-1651174136.9395597',\r\n   datetime.datetime(2022, 4, 28, 13, 31, 0, 999395)],\r\n  ['missing-who-has',\r\n   'tcp://10.124.227.26:35167',\r\n   \"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'ensure-communicating-1651174136.9395597',\r\n   datetime.datetime(2022, 4, 28, 13, 31, 0, 999403)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'fetch',\r\n   'missing',\r\n   'missing',\r\n   {},\r\n   'ensure-communicating-1651174136.9395597',\r\n   datetime.datetime(2022, 4, 28, 13, 31, 0, 999475)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'missing',\r\n   'fetch',\r\n   'fetch',\r\n   {},\r\n   'find-missing-1651174261.5426853',\r\n   datetime.datetime(2022, 4, 28, 13, 31, 1, 556907)],\r\n  ['gather-dependencies',\r\n   'tcp://10.124.227.26:35167',\r\n   [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 2764)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 5726)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 9555)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 7277)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 257)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 8041)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 2732)\"],\r\n   'ensure-communicating-1651174261.556927',\r\n   datetime.datetime(2022, 4, 28, 13, 31, 1, 557029)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'fetch',\r\n   'flight',\r\n   'flight',\r\n   {},\r\n   'ensure-communicating-1651174261.556927',\r\n   datetime.datetime(2022, 4, 28, 13, 31, 1, 557074)],\r\n  ['request-dep',\r\n   'tcp://10.124.227.26:35167',\r\n   [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 5726)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 9555)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 7277)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 257)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 2764)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 2732)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 8041)\"],\r\n   'ensure-communicating-1651174261.556927',\r\n   datetime.datetime(2022, 4, 28, 13, 31, 1, 557221)],\r\n  ['receive-dep-failed',\r\n   'tcp://10.124.227.26:35167',\r\n   [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 5726)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 9555)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 7277)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 257)\",\r\n    \"('read-parquet-4eae21bcb0e0252ce122c72c6dfb099a', 2764)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 2732)\",\r\n    \"('_split-bec215abf9d605cb60c27727e9a88b7e', 8041)\"],\r\n   'ensure-communicating-1651174261.556927',\r\n   datetime.datetime(2022, 4, 28, 13, 33, 5, 778931)],\r\n  ['missing-who-has',\r\n   'tcp://10.124.227.26:35167',\r\n   \"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'ensure-communicating-1651174261.556927',\r\n   datetime.datetime(2022, 4, 28, 13, 33, 5, 778938)],\r\n  [\"('_split-bec215abf9d605cb60c27727e9a88b7e', 1126)\",\r\n   'flight',\r\n   'missing',\r\n   'missing',\r\n   {},\r\n   'ensure-communicating-1651174261.556927',\r\n   datetime.datetime(2022, 4, 28, 13, 33, 5, 779073)]]}\r\n```\r\n\r\n-----\r\n\r\n</details>\r\n\r\nIn https://github.com/dask/distributed/pull/6112, we fixed `gather_dep` logic to transition all keys to missing that a particular worker holds, if communication with that worker fails. We assume the [`find_missing`](https://github.com/dask/distributed/blob/b8370030994b36c0625763f3d7c970e1ad059cae/distributed/worker.py#L3190-L3217) PeriodicCallback will talk to the scheduler and ask where to fetch them from next.\r\n\r\nIn the case of other errors, we would inform the scheduler of this: https://github.com/dask/distributed/blob/b8370030994b36c0625763f3d7c970e1ad059cae/distributed/worker.py#L3156-L3167\r\n\r\nHowever, in the [OSError case](https://github.com/dask/distributed/blob/b8370030994b36c0625763f3d7c970e1ad059cae/distributed/worker.py#L3100-L3114), we don't send the `missing-data` message (because we've already added recommendations for those keys to `missing`).\r\n\r\n-----\r\n\r\nSimultaneously, AMM is now used for `retire_workers`. When retiring a worker, we try to move all its keys onto other workers. To do this, AMM calls `Scheduler.request_acquire_replicas`: https://github.com/dask/distributed/blob/b8370030994b36c0625763f3d7c970e1ad059cae/distributed/scheduler.py#L7209-L7226\r\n\r\n`request_acquire_replicas` _optimistically_ adds the key to the worker's `who_has`, before the worker has actually confirmed it's received it. So imagine this flow:\r\n\r\n* Worker A is retiring gracefully. It holds `key-foo`\r\n* AMM calls `request_acquire_replicas(\"worker-b-addr\", [\"key-foo\"])`.\r\n  * `key-foo` is immediately assigned to worker B from the scheduler's perspective\r\n  * Scheduler tells worker B: \"go fetch `key-foo` from worker A\"\r\n* Next AMM cycle runs. AMM thinks, \"both worker A and B hold `key-foo` now\" (not actually true). \"I'll tell worker A to drop its copy.\" `request_remove_replicas` also _eagerly_ updates scheduler state so worker A no longer holds the key (this is also not true, but it's more reasonable).\r\n* Next AMM cycle runs. AMM thinks, \"worker A holds no keys anymore. It can shut down\"\r\n* Worker A shuts down\r\n* Worker B was busy, so it didn't get around to requesting `key-foo` from worker A until too late. Worker A is already shut down. The key is lost.\r\n* The `OSError` happens trying to fetch the key. The key goes to `missing` on worker B.\r\n* In [`find_missing`](https://github.com/dask/distributed/blob/b8370030994b36c0625763f3d7c970e1ad059cae/distributed/worker.py#L3190-L3217), Worker B asks the scheduler, \"who has `key-foo`\"?\r\n* Scheduler replies, \"just you do!\"\r\n* Worker B: \ud83e\udd14  \r\n\r\nI can't quite figure out what happens from here.  I would have hoped that eventually worker B would try to `gather_dep` from itself, it would reply to itself \"I don't have that key\", and then it would finally send the `missing-data` signal to the scheduler, letting the scheduler realize the key is actually missing. That doesn't seem to be happening, from the cluster dump I'm looking at where this situation happened.\r\n\r\nA few things to consider changing:\r\n* `request_acquire_replicas` should not be so optimistic, and assume the key is on a new worker before we've confirmed it's there. This creates a race condition, where we may drop the key from the sender before the receiver has gotten it. This means `RetireWorker` (and all AMM policies) may lead to lost data, when they shouldn't. cc @crusaderky\r\n* _maybe_ the OSError handling logic should also send `missing-data` to the scheduler [here-ish](https://github.com/dask/distributed/blob/b8370030994b36c0625763f3d7c970e1ad059cae/distributed/worker.py#L3110-L3114) (not in a for-loop though)? cc @mrocklin\r\n* `Worker.find_missing` should definitely take action if the scheduler reports nobody (besides it) holds the key that's missing. That's a clear state mismatch, and we shouldn't assume anything else will resolve it. cc @fjetter @mrocklin\r\n* https://github.com/dask/distributed/issues/6243\r\n\r\n---\r\n\r\nI think _maybe_ a change like this would be a good approach (not this doesn't plumb `stimulus_id` through in other calls to `update_who_has`, so it will fail as is. Also not sure if `update_who_has` is always the right place to do this.):\r\n```diff\r\ndiff --git a/distributed/worker.py b/distributed/worker.py\r\nindex e5563ed0..04c6bfd6 100644\r\n--- a/distributed/worker.py\r\n+++ b/distributed/worker.py\r\n@@ -3202,7 +3202,7 @@ class Worker(ServerNode):\r\n                 keys=[ts.key for ts in self._missing_dep_flight],\r\n             )\r\n             who_has = {k: v for k, v in who_has.items() if v}\r\n-            self.update_who_has(who_has)\r\n+            self.update_who_has(who_has, stimulus_id=stimulus_id)\r\n             recommendations: Recs = {}\r\n             for ts in self._missing_dep_flight:\r\n                 if ts.who_has:\r\n@@ -3216,7 +3216,9 @@ class Worker(ServerNode):\r\n             ].callback_time = self.periodic_callbacks[\"heartbeat\"].callback_time\r\n             self.ensure_communicating()\r\n \r\n-    def update_who_has(self, who_has: dict[str, Collection[str]]) -> None:\r\n+    def update_who_has(\r\n+        self, who_has: dict[str, Collection[str]], *, stimulus_id: str\r\n+    ) -> None:\r\n         try:\r\n             for dep, workers in who_has.items():\r\n                 if not workers:\r\n@@ -3232,6 +3234,21 @@ class Worker(ServerNode):\r\n                         )\r\n                         # Do not mutate the input dict. That's rude\r\n                         workers = set(workers) - {self.address}\r\n+                        if not workers:\r\n+                            logger.warning(\r\n+                                f\"Scheduler claims this worker {self.address} is the only one holding {dep!r}, which is not true. \"\r\n+                                f\"{dep!r} likely needs to be recomputed.\"\r\n+                            )\r\n+                            self.batched_stream.send(\r\n+                                {\r\n+                                    \"op\": \"missing-data\",\r\n+                                    \"errant_worker\": self.address,\r\n+                                    \"key\": dep,\r\n+                                    \"stimulus_id\": stimulus_id,\r\n+                                }\r\n+                            )\r\n+                            continue\r\n+\r\n                     dep_ts.who_has.update(workers)\r\n \r\n                     for worker in workers:\r\n```", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6244/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6244/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6239", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6239/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6239/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6239/events", "html_url": "https://github.com/dask/distributed/issues/6239", "id": 1219220853, "node_id": "I_kwDOAocYk85Iq9V1", "number": 6239, "title": "Don't block event loop in `Worker.close` waiting for executor to shut down", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3798450396, "node_id": "LA_kwDOAocYk87iZ8Dc", "url": "https://api.github.com/repos/dask/distributed/labels/tests", "name": "tests", "color": "a0f9b4", "default": false, "description": "Unit tests and/or continuous integration"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-04-28T20:03:01Z", "updated_at": "2022-04-29T13:20:33Z", "closed_at": "2022-04-29T13:20:33Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "https://github.com/dask/distributed/blob/1cbee7f18717f060c65b2ac44445745135c8dd46/distributed/worker.py#L1517-L1521\r\n\r\n`shutdown` is a blocking call that generally calls `join` on the thread/process. If the executor takes a long time to shut down (say there's a function running that's itself blocked on something), this can block the worker event loop for 30s, or whatever the timeout is.\r\n\r\nThis is particularly important for our test suite, because the worker event loop is actually the _only_ event loop.\r\n\r\nI discovered this trying to run a test something like\r\n\r\n```python\r\nevent = distributed.Event()\r\nf = client.submit(event.wait, workers=[a.address])\r\nt = asyncio.create_task(a.close())\r\nawait asyncio.sleep(1)\r\nawait event.set()  # this hangs!\r\n# because the whole event loop is blocked waiting for the ThreadPoolExecutor to shut down,\r\n# which is waiting for the event to be set... which is waiting for the event loop to be free\r\n```\r\n\r\nThis _probably_ won't affect real-life clusters that much, but would be good to clean up.\r\n\r\ncc @graingert", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6239/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6239/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6223", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6223/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6223/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6223/events", "html_url": "https://github.com/dask/distributed/issues/6223", "id": 1217499800, "node_id": "I_kwDOAocYk85IkZKY", "number": 6223, "title": "Worker stuck in closing_gracefully state", "user": {"login": "bnaul", "id": 903655, "node_id": "MDQ6VXNlcjkwMzY1NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/903655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bnaul", "html_url": "https://github.com/bnaul", "followers_url": "https://api.github.com/users/bnaul/followers", "following_url": "https://api.github.com/users/bnaul/following{/other_user}", "gists_url": "https://api.github.com/users/bnaul/gists{/gist_id}", "starred_url": "https://api.github.com/users/bnaul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bnaul/subscriptions", "organizations_url": "https://api.github.com/users/bnaul/orgs", "repos_url": "https://api.github.com/users/bnaul/repos", "events_url": "https://api.github.com/users/bnaul/events{/privacy}", "received_events_url": "https://api.github.com/users/bnaul/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 4057255458, "node_id": "LA_kwDOAocYk87x1M4i", "url": "https://api.github.com/repos/dask/distributed/labels/deadlock", "name": "deadlock", "color": "B60205", "default": false, "description": "The cluster appears to not make any progress"}], "state": "closed", "locked": false, "assignee": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 25, "created_at": "2022-04-27T15:12:11Z", "updated_at": "2022-04-28T22:49:14Z", "closed_at": "2022-04-28T22:49:14Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "(Note: @mrocklin heavily edited this issue, the words are his (sorry Brett to impersonate you) but the logs are coming from Brett)\r\n\r\nWe've found that workers can be stuck in a `closing_gracefully` state indefinitely.  Here is some context:\r\n\r\n### Cluster\r\nWe're running an adaptively scaled cluster of roughly 4000 workers.  \r\n\r\n```\r\ncluster = KubeHelmCluster(release_name=release_name)  # custom wrapper around dask_kubernetes.KubeCluster\r\ncluster.adapt(minimum=0, maximum=4000, interval=\"10s\", target_duration=\"60s\")\r\n```\r\n\r\n### Computation\r\n\r\nOur workload is pretty simple.  We're just reading a bunch of data, and then calling a map_partitions call and that's it.  We see that the cluster scales up pretty fast, and then scales down pretty fast.\r\n\r\n```\r\ndd.read_parquet(\"gs://.../*.parquet\").map_partitions(len).compute()  # ~11k parquet files\r\n```\r\n\r\n### Some findings\r\n\r\nBoth the scheduler and the worker agree that it is in a closing gracefully state (this is after a long while)\r\n\r\n```\r\nIn [30]: c.run_on_scheduler(lambda dask_scheduler: dask_scheduler.workers[w].status)\r\nOut[30]: <Status.closing_gracefully: 'closing_gracefully'>\r\n\r\nIn [31]: c.run(lambda dask_worker: str(dask_worker), workers=[\"tcp://10.36.228.19:34273\"])\r\nOut[31]: {'tcp://10.36.228.19:34273': \"<Worker 'tcp://10.36.228.19:34273', name: 128, status: closing_gracefully, stored: 1, running: 0/4, ready: 3, comm: 0, waiting: 0>\"}\r\n```\r\n\r\nInterestingly, they both also agree that there are three tasks that are ready to go.  An artificial call to `Worker._ensure_computing` at this point doesn't do anything because the first check in that method is `if self.status != Status.Running: return`.  \r\n\r\nHere are the logs on the scheduler that refer to this worker.  \r\n\r\n```\r\n(.venv) \u279c  model git:(ta_backfill) \u2717 kl  brett-f4d63eb0-daskscheduler-667b4fc95-f897l | grep '10.36.228.19:34273'\r\n2022-04-27 13:22:17,287 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.36.228.19:34273', name: 128, status: undefined, memory: 0, processing: 0>\r\n2022-04-27 13:22:17,291 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.36.228.19:34273\r\n2022-04-27 13:23:01,001 - distributed.scheduler - INFO - Retiring worker tcp://10.36.228.19:34273\r\n2022-04-27 13:23:01,448 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.36.228.19:34273; 3 keys are being moved away.\r\n2022-04-27 13:23:02,341 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.36.228.19:34273; 3 keys are being moved away.\r\n2022-04-27 13:23:03,716 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.36.228.19:34273; 2 keys are being moved away.\r\n2022-04-27 13:23:05,544 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.36.228.19:34273; no unique keys need to be moved away.\r\n```\r\n\r\nAnd the events from the scheduler's perspective\r\n\r\n```\r\nIn [32]: c.run_on_scheduler(lambda dask_scheduler: dask_scheduler.events[w])\r\nOut[32]:\r\ndeque([(1651065737.2876427, {'action': 'add-worker'}),\r\n       (1651065737.629527,\r\n        {'action': 'worker-status-change',\r\n         'prev-status': 'undefined',\r\n         'status': 'running'}),\r\n       (1651065793.225713,\r\n        {'action': 'missing-data',\r\n         'key': \"('_split-659a14d513522415a58d9ec3d470e072', 8087)\"}),\r\n       (1651065793.2970057,\r\n        {'action': 'missing-data',\r\n         'key': \"('_split-659a14d513522415a58d9ec3d470e072', 8087)\"})])\r\n```\r\n\r\nAnd the logs on the worker side\r\n\r\n```\r\n(.venv) \u279c  model git:(ta_backfill) \u2717 kl -f brett-f4d63eb0-daskworkers-584d467f76-9zctq\r\n/usr/src/python/distributed/distributed/cli/dask_worker.py:319: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.\r\n  warnings.warn(\r\n2022-04-27 13:22:12,055 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.36.228.19:46595'\r\n2022-04-27 13:22:13,553 - distributed.worker - INFO -       Start worker at:   tcp://10.36.228.19:34273\r\n2022-04-27 13:22:13,553 - distributed.worker - INFO -          Listening to:   tcp://10.36.228.19:34273\r\n2022-04-27 13:22:13,553 - distributed.worker - INFO -          dashboard at:         10.36.228.19:33865\r\n2022-04-27 13:22:13,553 - distributed.worker - INFO - Waiting to connect to: tcp://brett-f4d63eb0-daskscheduler:8786\r\n2022-04-27 13:22:13,553 - distributed.worker - INFO - -------------------------------------------------\r\n2022-04-27 13:22:13,553 - distributed.worker - INFO -               Threads:                          4\r\n2022-04-27 13:22:13,553 - distributed.worker - INFO -                Memory:                   7.82 GiB\r\n2022-04-27 13:22:13,553 - distributed.worker - INFO -       Local Directory: /src/dask-worker-space/worker-ykvkqgn_\r\n2022-04-27 13:22:13,554 - distributed.worker - INFO - -------------------------------------------------\r\n2022-04-27 13:22:17,292 - distributed.worker - INFO -         Registered to: tcp://brett-f4d63eb0-daskscheduler:8786\r\n2022-04-27 13:22:17,292 - distributed.worker - INFO - -------------------------------------------------\r\n2022-04-27 13:22:17,293 - distributed.core - INFO - Starting established connection\r\n2022-04-27 13:22:56,885 - distributed.utils_perf - INFO - full garbage collection released 105.93 MiB from 0 reference cycles (threshold: 9.54 MiB)\r\n```\r\n\r\n### Thoughts\r\n\r\nMy guess is that when entering a `closing_gracefully` state on the worker side we don't immediately clear out ready work, when maybe we should.  \r\n\r\nAlso, we should have some sanity checks so that `close_gracefully` can't go on for forever.  \r\n\r\ncc @crusaderky @fjetter ", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6223/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6223/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6159", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6159/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6159/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6159/events", "html_url": "https://github.com/dask/distributed/issues/6159", "id": 1208911966, "node_id": "I_kwDOAocYk85IDohe", "number": 6159, "title": "Deadlock stealing a `resumed` task", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3621597059, "node_id": "LA_kwDOAocYk87X3S-D", "url": "https://api.github.com/repos/dask/distributed/labels/stability", "name": "stability", "color": "310DBB", "default": false, "description": "Issue or feature related to cluster stability (e.g. deadlock)"}, {"id": 4057255458, "node_id": "LA_kwDOAocYk87x1M4i", "url": "https://api.github.com/repos/dask/distributed/labels/deadlock", "name": "deadlock", "color": "B60205", "default": false, "description": "The cluster appears to not make any progress"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2022-04-19T22:16:05Z", "updated_at": "2022-06-03T21:49:39Z", "closed_at": "2022-06-03T21:49:39Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "* Worker starts fetching key `abcd` from a peer\r\n* The fetch gets cancelled\r\n* Scheduler now asks the worker to _compute_ the key `abcd`\r\n* Deadlock\r\n\r\nHere's the (annotated) worker story for the key in question:\r\n```yaml\r\n# Initially we hear about the key as a dependency to fetch\r\n- - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 1))\r\n  - ensure-task-exists\r\n  - released\r\n  - compute-task-1650391863.1634912\r\n  - 2022-04-19 11:11:03.369210\r\n- - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 1))\r\n  - released\r\n  - fetch\r\n  - fetch\r\n  - {}\r\n  - compute-task-1650391863.1634912\r\n  - 2022-04-19 11:11:03.369474\r\n- - gather-dependencies\r\n  - tls://10.0.13.152:38353\r\n  - - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 8))\r\n    - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 1))\r\n  - ensure-communicating-1650391867.838159\r\n  - 2022-04-19 11:11:07.838874\r\n- - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 1))\r\n  - fetch\r\n  - flight\r\n  - flight\r\n  - {}\r\n  - ensure-communicating-1650391867.838159\r\n  - 2022-04-19 11:11:07.838897\r\n# We go try to fetch it. I think we're talking to a worker that's still alive,\r\n# but so locked up under memory pressure it never responds\u2014see https://github.com/dask/distributed/issues/6110\r\n- - request-dep\r\n  - tls://10.0.13.152:38353\r\n  - - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 8))\r\n    - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 1))\r\n  - ensure-communicating-1650391867.838159\r\n  - 2022-04-19 11:11:07.848737\r\n# This is really weird. After the stuck worker's TTL expires (https://github.com/dask/distributed/issues/6110#issuecomment-1102959742),\r\n# the scheduler removes it. But the log we're looking at _isn't from the stuck worker_.\r\n# So why is `Scheduler.transition_processing_released` sending a `free-keys` to this worker?\r\n# That would indicate the scheduler's `processing_on` pointed to this worker.\r\n- - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 1))\r\n  - flight\r\n  - released\r\n  - cancelled\r\n  - {}\r\n  - processing-released-1650392050.5300016\r\n  - 2022-04-19 11:14:10.571274\r\n# Regardless, the scheduler then asks us to compute (not fetch!) this task.\r\n- - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 1))\r\n  - compute-task\r\n  - compute-task-1650392058.1483586\r\n  - 2022-04-19 11:14:18.165346\r\n# We already know about it---the fetch was just cancelled by `processing-released`---so it stays in cancelled,\r\n# with a recommendation to `(resumed, waiting)`\r\n- - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 1))\r\n  - cancelled\r\n  - waiting\r\n  - cancelled\r\n  - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 1)):\r\n    - resumed\r\n    - waiting\r\n  - compute-task-1650392058.1483586\r\n  - 2022-04-19 11:14:18.165884\r\n# It goes into resumed and nothing ever happens again\r\n- - ('split-shuffle-1-9698a72b7e1aad6e40a6567d3c06d355', 5, (7, 1))\r\n  - cancelled\r\n  - resumed\r\n  - resumed\r\n  - {}\r\n  - compute-task-1650392058.1483586\r\n  - 2022-04-19 11:14:18.165897\r\n```\r\n\r\nThere's definitely something weird about the `processing-released` message arriving right before the `compute-task` message. I can't find an obvious reason in scheduler code why that would happen.\r\n\r\nBut let's ignore that oddity for a second. Pretend it was just a normal work-stealing request that caused the task to be cancelled.\r\n\r\nI find it odd that if a worker is told to _compute_ a task it was previously fetching, that it'll resume the fetch:\r\nhttps://github.com/dask/distributed/blob/c9dcbe7ee87be83fde1156f18e88ebe2da992c0c/distributed/worker.py#L2269-L2271\r\n\r\nIf previously we were fetching a key, but now we're being asked to compute it, it seems almost certain that the fetch is going to fail. The compute request should probably take precedence.\r\n\r\nI imagine here that we're assuming the `gather_dep` will error out sometime in the future, and when it does, _then_ the key will go from `resumed` to `waiting`?\r\n\r\nAlso, this is coming from the https://github.com/dask/distributed/issues/6110 scenario. That's an unusual one in that the TCP connection to the stuck worker doesn't get broken, it's just unresponsive. So I'm also wondering if perhaps `gather_dep` to the stuck worker will hang forever? for 300s (seems to go much longer than that)? for 300s * some retries? Basically, could it be that this isn't _quite_ a deadlock, but a very, very, very long wait for a dependency fetch that might never return until the other worker properly dies? If we don't have any explicit timeouts on `gather_dep` already, maybe we should.\r\n(All that said, I still think the proper fix would be to not have `transition_cancelled_waiting` try to resume the fetch, but instead go down the compute path. The timeout might be something in addition.)\r\n\r\ncc @fjetter", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6159/timeline", "performed_via_github_app": null, "state_reason": "not_planned"}, {"url": "https://api.github.com/repos/dask/distributed/issues/6087", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/6087/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/6087/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/6087/events", "html_url": "https://github.com/dask/distributed/issues/6087", "id": 1196507700, "node_id": "I_kwDOAocYk85HUUI0", "number": 6087, "title": "Error closing a local cluster when client still running", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3798450386, "node_id": "LA_kwDOAocYk87iZ8DS", "url": "https://api.github.com/repos/dask/distributed/labels/p1", "name": "p1", "color": "ff5233", "default": false, "description": "Affects a large population and inhibits work"}, {"id": 3918158031, "node_id": "LA_kwDOAocYk87piljP", "url": "https://api.github.com/repos/dask/distributed/labels/regression", "name": "regression", "color": "B60205", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2022-04-07T19:57:14Z", "updated_at": "2022-07-22T07:17:27Z", "closed_at": "2022-04-15T11:02:18Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "**What happened**:\r\n\r\nWhen a local cluster (using processes) shuts down, a ton of errors are now spewed about scheduling new futures after shutdown.\r\n\r\nI can't replicate it in my distributed dev environment, but in a different environment (which is quite similar, also running dask & distributed from main\u2014just py py3.9.1 instead of py3.9.5?) the process hangs and never terminates until a ctrl-C. In my distributed dev environment, the same errors are spewed, but it exits (with code 0 no the less).\r\n\r\n`git bisect` implicates https://github.com/dask/distributed/pull/6031 @graingert.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\n# repro.py\r\nimport distributed\r\nfrom distributed.deploy.local import LocalCluster\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cluster = LocalCluster(n_workers=1, threads_per_worker=1, processes=True)\r\n    client = distributed.Client(cluster)\r\n```\r\n\r\n```python-traceback\r\n(dask-distributed) gabe dev/distributed \u2039f4c52e9a\u203a \u00bb python repro.py\r\n2022-04-07 15:47:02,414 - distributed.utils - ERROR - cannot schedule new futures after shutdown\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 226, in read\r\n    frames_nbytes = await stream.read_bytes(fmt_size)\r\ntornado.iostream.StreamClosedError: Stream is closed\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1395, in _handle_report\r\n    msgs = await self.scheduler_comm.comm.read()\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 242, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 150, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed) Client->Scheduler local=tcp://127.0.0.1:58800 remote=tcp://127.0.0.1:58791>: Stream is closed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/dev/distributed/distributed/utils.py\", line 693, in log_errors\r\n    yield\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1225, in _reconnect\r\n    await self._ensure_connected(timeout=timeout)\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1254, in _ensure_connected\r\n    comm = await connect(\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/core.py\", line 289, in connect\r\n    comm = await asyncio.wait_for(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/tasks.py\", line 481, in wait_for\r\n    return fut.result()\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 439, in connect\r\n    stream = await self.client.connect(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/tcpclient.py\", line 265, in connect\r\n    addrinfo = await self.resolver.resolve(host, port, af)\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 424, in resolve\r\n    for fam, _, _, _, address in await asyncio.get_running_loop().getaddrinfo(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/base_events.py\", line 856, in getaddrinfo\r\n    return await self.run_in_executor(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/base_events.py\", line 814, in run_in_executor\r\n    executor.submit(func, *args), loop=self)\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/concurrent/futures/thread.py\", line 161, in submit\r\n    raise RuntimeError('cannot schedule new futures after shutdown')\r\nRuntimeError: cannot schedule new futures after shutdown\r\n2022-04-07 15:47:02,417 - distributed.utils - ERROR - cannot schedule new futures after shutdown\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 226, in read\r\n    frames_nbytes = await stream.read_bytes(fmt_size)\r\ntornado.iostream.StreamClosedError: Stream is closed\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1395, in _handle_report\r\n    msgs = await self.scheduler_comm.comm.read()\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 242, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 150, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed) Client->Scheduler local=tcp://127.0.0.1:58800 remote=tcp://127.0.0.1:58791>: Stream is closed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/dev/distributed/distributed/utils.py\", line 693, in log_errors\r\n    yield\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1401, in _handle_report\r\n    await self._reconnect()\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1225, in _reconnect\r\n    await self._ensure_connected(timeout=timeout)\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1254, in _ensure_connected\r\n    comm = await connect(\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/core.py\", line 289, in connect\r\n    comm = await asyncio.wait_for(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/tasks.py\", line 481, in wait_for\r\n    return fut.result()\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 439, in connect\r\n    stream = await self.client.connect(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/tcpclient.py\", line 265, in connect\r\n    addrinfo = await self.resolver.resolve(host, port, af)\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 424, in resolve\r\n    for fam, _, _, _, address in await asyncio.get_running_loop().getaddrinfo(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/base_events.py\", line 856, in getaddrinfo\r\n    return await self.run_in_executor(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/base_events.py\", line 814, in run_in_executor\r\n    executor.submit(func, *args), loop=self)\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/concurrent/futures/thread.py\", line 161, in submit\r\n    raise RuntimeError('cannot schedule new futures after shutdown')\r\nRuntimeError: cannot schedule new futures after shutdown\r\n2022-04-07 15:47:02,471 - distributed.utils - ERROR - cannot schedule new futures after shutdown\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 226, in read\r\n    frames_nbytes = await stream.read_bytes(fmt_size)\r\ntornado.iostream.StreamClosedError: Stream is closed\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1395, in _handle_report\r\n    msgs = await self.scheduler_comm.comm.read()\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 242, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 150, in convert_stream_closed_error\r\n    raise CommClosedError(f\"in {obj}: {exc}\") from exc\r\ndistributed.comm.core.CommClosedError: in <TCP (closed) Client->Scheduler local=tcp://127.0.0.1:58800 remote=tcp://127.0.0.1:58791>: Stream is closed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/dev/distributed/distributed/utils.py\", line 693, in log_errors\r\n    yield\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1521, in _close\r\n    await asyncio.wait_for(asyncio.shield(handle_report_task), 0.1)\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/tasks.py\", line 481, in wait_for\r\n    return fut.result()\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1401, in _handle_report\r\n    await self._reconnect()\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1225, in _reconnect\r\n    await self._ensure_connected(timeout=timeout)\r\n  File \"/Users/gabe/dev/distributed/distributed/client.py\", line 1254, in _ensure_connected\r\n    comm = await connect(\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/core.py\", line 289, in connect\r\n    comm = await asyncio.wait_for(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/tasks.py\", line 481, in wait_for\r\n    return fut.result()\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 439, in connect\r\n    stream = await self.client.connect(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/tcpclient.py\", line 265, in connect\r\n    addrinfo = await self.resolver.resolve(host, port, af)\r\n  File \"/Users/gabe/dev/distributed/distributed/comm/tcp.py\", line 424, in resolve\r\n    for fam, _, _, _, address in await asyncio.get_running_loop().getaddrinfo(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/base_events.py\", line 856, in getaddrinfo\r\n    return await self.run_in_executor(\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/base_events.py\", line 814, in run_in_executor\r\n    executor.submit(func, *args), loop=self)\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/concurrent/futures/thread.py\", line 161, in submit\r\n    raise RuntimeError('cannot schedule new futures after shutdown')\r\n```\r\n\r\n**Environment**:\r\n\r\n- Dask version: 6e307662be633695f57bce79458b77e9be2e5702\r\n- Python version: 3.9.5\r\n- Operating System: macOS\r\n- Install method (conda, pip, source): source", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/6087/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/6087/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5973", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5973/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5973/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5973/events", "html_url": "https://github.com/dask/distributed/issues/5973", "id": 1175496452, "node_id": "I_kwDOAocYk85GEKcE", "number": 5973, "title": "the \"distributed.worker\" logger gains ~thousand DequeHandler instances during a pytest run", "user": {"login": "graingert", "id": 413772, "node_id": "MDQ6VXNlcjQxMzc3Mg==", "avatar_url": "https://avatars.githubusercontent.com/u/413772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/graingert", "html_url": "https://github.com/graingert", "followers_url": "https://api.github.com/users/graingert/followers", "following_url": "https://api.github.com/users/graingert/following{/other_user}", "gists_url": "https://api.github.com/users/graingert/gists{/gist_id}", "starred_url": "https://api.github.com/users/graingert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/graingert/subscriptions", "organizations_url": "https://api.github.com/users/graingert/orgs", "repos_url": "https://api.github.com/users/graingert/repos", "events_url": "https://api.github.com/users/graingert/events{/privacy}", "received_events_url": "https://api.github.com/users/graingert/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2022-03-21T14:48:16Z", "updated_at": "2022-03-24T10:18:34Z", "closed_at": "2022-03-24T10:18:34Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\nrunning `len(logging.getLogger(\"distributed.worker\").handlers)` shows a few thousand `DequeHandler` instances in there.\r\nThis means that logging messages from the worker or adding handlers to the worker gradually gets slower and slower as the test suite progresses.\r\n\r\nthe handlers are supposed to be cleared when the worker is GC'd by\r\nhttps://github.com/dask/distributed/blob/1da51994cc84c0897589a4476cd24073147054b6/distributed/node.py#L88 however https://github.com/dask/distributed/blob/1da51994cc84c0897589a4476cd24073147054b6/distributed/utils.py#L1131-L1142 keeps a permanent strong reference to the worker:\r\n\r\n![image](https://user-images.githubusercontent.com/413772/159285688-6305720a-ee4c-4bef-a375-09a6c07fc36b.png)\r\n\r\nsee https://github.com/dask/distributed/pull/4481 and https://github.com/dask/distributed/issues/4469\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5973/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5971", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5971/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5971/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5971/events", "html_url": "https://github.com/dask/distributed/issues/5971", "id": 1175288584, "node_id": "I_kwDOAocYk85GDXsI", "number": 5971, "title": "distributed.nanny.environ.MALLOC_TRIM_THRESHOLD_ is ineffective", "user": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3965379432, "node_id": "LA_kwDOAocYk87sWuNo", "url": "https://api.github.com/repos/dask/distributed/labels/memory", "name": "memory", "color": "5158E6", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2022-03-21T12:06:13Z", "updated_at": "2022-07-07T17:02:37Z", "closed_at": "2022-07-07T17:02:37Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Ubuntu 21.10 x86/64\r\ndistributed 2022.3.0\r\n\r\nThe MALLOC_TRIM_THRESHOLD_ env variable seems to be effective at making memory deallocation more reactive.\r\nHowever, the config variable that sets it doesn't seem to do anything - which indicates that the variable is being set after the worker process is started, whereas it should be set before spawning it.\r\n\r\n```python\r\nimport dask.array\r\nimport distributed\r\n\r\nclient = distributed.Client(n_workers=1, memory_limit=\"2 GiB\")\r\n\r\nN = 7_000\r\nS = 160 * 1024\r\n\r\na = dask.array.random.random(N * S // 8, chunks=S // 8)\r\na = a.persist()\r\ndistributed.wait(a)\r\ndel a\r\n```\r\nResult:\r\nManaged: 0\r\nUnmanaged: 1.16 GiB\r\n\r\n```python\r\nimport os\r\nimport dask.array\r\nimport dask.config\r\nimport distributed\r\n\r\nos.environ[\"MALLOC_TRIM_THRESHOLD_\"] = str(dask.config.get(\"distributed.nanny.environ.MALLOC_TRIM_THRESHOLD_\"))\r\nclient = distributed.Client(n_workers=1, memory_limit=\"2 GiB\")\r\n\r\nN = 7_000\r\nS = 160 * 1024\r\n\r\na = dask.array.random.random(N * S // 8, chunks=S // 8)\r\na = a.persist()\r\ndistributed.wait(a)\r\ndel a\r\n```\r\nResult:\r\nManaged: 0\r\nUnmanaged: 151 MiB\r\n\r\n# Production Workaround\r\nSet the env variable on the shell, before starting ``dask-worker``:\r\n\r\n```bash\r\nexport MALLOC_TRIM_THRESHOLD_=65536\r\ndask-worker <address>\r\n```", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5971/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5958", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5958/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5958/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5958/events", "html_url": "https://github.com/dask/distributed/issues/5958", "id": 1172719129, "node_id": "I_kwDOAocYk85F5kYZ", "number": 5958, "title": "`BaseException` in task leads to task never completing", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3621597059, "node_id": "LA_kwDOAocYk87X3S-D", "url": "https://api.github.com/repos/dask/distributed/labels/stability", "name": "stability", "color": "310DBB", "default": false, "description": "Issue or feature related to cluster stability (e.g. deadlock)"}, {"id": 4057255458, "node_id": "LA_kwDOAocYk87x1M4i", "url": "https://api.github.com/repos/dask/distributed/labels/deadlock", "name": "deadlock", "color": "B60205", "default": false, "description": "The cluster appears to not make any progress"}], "state": "closed", "locked": false, "assignee": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2022-03-17T18:22:49Z", "updated_at": "2022-11-29T11:43:18Z", "closed_at": "2022-11-29T11:43:18Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "**What happened**:\r\n\r\nIf a task raises a `BaseException` (`KeyboardInterrupt`, `SystemExit`, etc.), the task will appear to be `processing` forever.\r\n\r\n**What you expected to happen**:\r\n\r\nThe task should definitely not deadlock. But what should actually happen, I'm not sure. Could go two ways:\r\n1. The entire worker should shut down gracefully.\r\n2. We should catch them just like any other exceptions and error the task.\r\n\r\nMore discussion in comments.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nIn [1]: import distributed\r\n\r\nIn [2]: client = distributed.Client(n_workers=1)\r\n\r\nIn [3]: def raiser():\r\n   ...:     raise BaseException(\"this could be a KeyboardInterrupt!\")\r\n   ...: \r\n\r\nIn [4]: f = client.submit(raiser)\r\n\r\nIn [5]: Exception in callback IOLoop.add_future.<locals>.<lambda>(<Task finishe...dInterrupt!')>) at /Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/ioloop.py:688\r\nhandle: <Handle IOLoop.add_future.<locals>.<lambda>(<Task finishe...dInterrupt!')>) at /Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/ioloop.py:688>\r\nTraceback (most recent call last):\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/events.py\", line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/ioloop.py\", line 688, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\r\n    future.result()\r\n  File \"/Users/gabe/dev/distributed/distributed/worker.py\", line 3504, in execute\r\n    result = await self.loop.run_in_executor(\r\n  File \"/Users/gabe/dev/distributed/distributed/_concurrent_futures_thread.py\", line 65, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/Users/gabe/dev/distributed/distributed/worker.py\", line 4503, in apply_function\r\n    msg = apply_function_simple(function, args, kwargs, time_delay)\r\n  File \"/Users/gabe/dev/distributed/distributed/worker.py\", line 4525, in apply_function_simple\r\n    result = function(*args, **kwargs)\r\n  File \"<ipython-input-3-7f701e80695b>\", line 2, in raiser\r\nBaseException: this could be a KeyboardInterrupt!\r\nIn [5]: \r\n\r\nIn [5]: client.processing()\r\nOut[5]: {'tcp://127.0.0.1:58316': ('raiser-e3b7ab59305f9e4ddb4ecddd75c55f85',)}\r\n\r\nIn [6]: client.call_stack()\r\nOut[6]: \r\n{'tcp://127.0.0.1:58316': {'raiser-e3b7ab59305f9e4ddb4ecddd75c55f85': ('  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/threading.py\", line 912, in _bootstrap\\n\\tself._bootstrap_inner()\\n',\r\n   '  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/threading.py\", line 954, in _bootstrap_inner\\n\\tself.run()\\n',\r\n   '  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/threading.py\", line 892, in run\\n\\tself._target(*self._args, **self._kwargs)\\n',\r\n   '  File \"/Users/gabe/dev/distributed/distributed/threadpoolexecutor.py\", line 51, in _worker\\n\\ttask = work_queue.get(timeout=1)\\n',\r\n   '  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/queue.py\", line 180, in get\\n\\tself.not_empty.wait(remaining)\\n',\r\n   '  File \"/Users/gabe/miniconda3/envs/dask-distributed/lib/python3.9/threading.py\", line 316, in wait\\n\\tgotit = waiter.acquire(True, timeout)\\n')}}\r\n\r\nIn [7]: client.submit(lambda: 1).result(timeout=5)  # the worker still works fine; just that task is stuck now\r\nOut[7]: 1\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2022.2.1\r\n- Python version: 3.9.5\r\n- Operating System: macOS\r\n- Install method (conda, pip, source): source\r\n\r\ncc @fjetter @graingert @crusaderky ", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5958/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5958/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5951", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5951/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5951/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5951/events", "html_url": "https://github.com/dask/distributed/issues/5951", "id": 1171115439, "node_id": "I_kwDOAocYk85Fzc2v", "number": 5951, "title": "Worker <-> Worker Communication Failures bring Cluster in inconsistent State", "user": {"login": "nils-braun", "id": 6116188, "node_id": "MDQ6VXNlcjYxMTYxODg=", "avatar_url": "https://avatars.githubusercontent.com/u/6116188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nils-braun", "html_url": "https://github.com/nils-braun", "followers_url": "https://api.github.com/users/nils-braun/followers", "following_url": "https://api.github.com/users/nils-braun/following{/other_user}", "gists_url": "https://api.github.com/users/nils-braun/gists{/gist_id}", "starred_url": "https://api.github.com/users/nils-braun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nils-braun/subscriptions", "organizations_url": "https://api.github.com/users/nils-braun/orgs", "repos_url": "https://api.github.com/users/nils-braun/repos", "events_url": "https://api.github.com/users/nils-braun/events{/privacy}", "received_events_url": "https://api.github.com/users/nils-braun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 3621597059, "node_id": "LA_kwDOAocYk87X3S-D", "url": "https://api.github.com/repos/dask/distributed/labels/stability", "name": "stability", "color": "310DBB", "default": false, "description": "Issue or feature related to cluster stability (e.g. deadlock)"}, {"id": 4057255458, "node_id": "LA_kwDOAocYk87x1M4i", "url": "https://api.github.com/repos/dask/distributed/labels/deadlock", "name": "deadlock", "color": "B60205", "default": false, "description": "The cluster appears to not make any progress"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2022-03-16T14:38:42Z", "updated_at": "2022-04-21T14:58:18Z", "closed_at": "2022-04-13T20:42:18Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Due to network issues, overloaded workers or just bad luck, the worker-to-worker communication needed for getting task dependencies from other workers might fail (`distributed.worker.get_data_from_worker`). This is (in principle) successfully caught by the worker and scheduler and reacted on. During this however a race-condition can be triggered, which brings the cluster in an inconsistent and stuck state.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\nThe following code will introduce a random failure in `distributed.worker.get_data_from_worker` with higher probability than a real-world use-case might have, just to demonstrate:\r\n\r\n```python\r\nfrom unittest.mock import patch\r\nfrom random import randint\r\n\r\nimport dask.bag as db\r\nfrom distributed import Client\r\n\r\n# Calling this function on the worker will temporarily replace\r\n# the `get_data_from_worker` function with a version, which\r\n# fails in 1 out of 4 cases. In the \"real world\" the \r\n# probability of a failure due to communication problems is\r\n# of course a lot smaller, but it might happen.\r\ndef replace_get_data_from_worker_function(dask_worker):\r\n    from distributed.worker import get_data_from_worker\r\n    \r\n    dask_worker.my_counter = 0\r\n\r\n    async def get_data_from_worker_function(*args, **kwargs):\r\n        # \"Fail\" in 1 out of 4 cases\r\n        if randint(0, 3) == 1:\r\n            dask_worker.my_counter += 1\r\n            raise OSError\r\n\r\n        return await get_data_from_worker(*args, **kwargs)\r\n            \r\n    p = patch(\"distributed.worker.get_data_from_worker\", \r\n              get_data_from_worker_function)\r\n    p.start()\r\n\r\nif __name__ == \"__main__\":\r\n    client = Client()\r\n    client.run(replace_get_data_from_worker_function)\r\n\r\n    PARTITIONS = 20\r\n\r\n    # Just some arbitrary computation, which takes\r\n    # reasonably long and (most importantly) creates tasks\r\n    # with many dependencies, so that we need a lot of communication\r\n    data = db.from_sequence(range(PARTITIONS), npartitions=PARTITIONS)\\\r\n             .map_partitions(lambda x: range(100_000))\\\r\n             .groupby(lambda x: x % PARTITIONS)\\\r\n             .map_partitions(lambda x: len(list(x)))\r\n    data.compute()\r\n```\r\n\r\nRunning this code, will sometimes not finish the computation, but one (or multiple) workers are stuck while waiting in fetching the dependencies for a specific task.\r\nNote: it is a race-condition, so you might need to run the code multiple times until it is stuck...\r\n\r\nHere are some observations I have made using the worker's and scheduler's properties and the transition log.\r\nLet's say worker `A` is stuck while processing task `T1`, which depends on task `T2` owned by worker `B`.\r\n* the scheduler task state shows that `T2` is present on `B`\r\n* `B` has the task in its memory (`dask_worker.data`)\r\n* `A` however thinks, that no-one owns the task `dask_worker.tasks[T1].who_owns == {}`, so `A` does not even start asking `B` for the data.\r\n\r\nFrom the transition log, this is what I *think* that happens (but I am happy if someone with more knowledge on the way the worker works could confirm my observations):\r\n* some time before that, worker `A` wants to process another task, `T3`, which needs a dependency `T4` also from `B`\r\n* it calls `gather_dep`, which calls `get_data_from_worker`. This fails (either due to a real network issue or due to our patched function above).\r\n* in the meantime/around this time, the scheduler also tells `A` to do `T1`, which depends on `T2` (owned by `B`).\r\n* during the error handling for an `OSError` of [`gather_dep`](https://github.com/dask/distributed/blob/main/distributed/worker.py#L3030), the local state of the worker `A` is changed in a way, so that all tasks owned by `B` are marked as not owned by `B` anymore. In our case, that is `T2` and `T4`. \r\n* However (and I think this is where the bug is), only the initially requested dependencies are notified as missing to the scheduler, in this case `T4`. (see [here](https://github.com/dask/distributed/blob/main/distributed/worker.py#L3062))\r\n\r\nThe final state is, that the worker `A` thinks no one would own the data for `T2`, while the scheduler will not re-distributed the task (as it was never marked as missing).\r\n\r\nOne last comment: using the setting `DASK_DISTRIBUTED__COMM__RETRY__COUNT` it is possible to make the failures\r\nof the `get_data_from_worker` function less likely. But unfortunately, this will just decrease the probability, not fix the problem.\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2022.2.1\r\n- Python version: 3.8\r\n- Operating System: macOS\r\n- Install method (conda, pip, source): pip", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5951/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5565", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5565/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5565/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5565/events", "html_url": "https://github.com/dask/distributed/issues/5565", "id": 1072322483, "node_id": "I_kwDOAocYk84_6lez", "number": 5565, "title": "Work Stealing fails for heterogeneous victim/thief resources", "user": {"login": "xuevin", "id": 296101, "node_id": "MDQ6VXNlcjI5NjEwMQ==", "avatar_url": "https://avatars.githubusercontent.com/u/296101?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuevin", "html_url": "https://github.com/xuevin", "followers_url": "https://api.github.com/users/xuevin/followers", "following_url": "https://api.github.com/users/xuevin/following{/other_user}", "gists_url": "https://api.github.com/users/xuevin/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuevin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuevin/subscriptions", "organizations_url": "https://api.github.com/users/xuevin/orgs", "repos_url": "https://api.github.com/users/xuevin/repos", "events_url": "https://api.github.com/users/xuevin/events{/privacy}", "received_events_url": "https://api.github.com/users/xuevin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-12-06T16:05:07Z", "updated_at": "2021-12-14T18:42:20Z", "closed_at": "2021-12-14T18:42:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Work stealing has a small bug where `_can_steal` checks on the victim's resources, and not the task's resource restrictions.\r\n\r\nThis can lead to a case where a thief cannot steal task from a victim where the resource_restrictions are a superset of the thief's. \r\n\r\nie. if the task has restrictions\r\nTask\r\n`CPU=1, MEMORY=4`\r\n\r\nVictim\r\n`CPU=1, MEMORY=4, GPU=1`\r\n\r\nThief\r\n`CPU=1, MEMORY=4`\r\n\r\nThe thief cannot steal from the victim. Any easy patch is found below to change \r\n\r\nhttps://github.com/dask/distributed/blob/8d93d8aac339ac858c2cd8461765692ab5f322c8/distributed/stealing.py#L509\r\n\r\n```\r\nfor resource, value in ts.resource_restrictions.items():\r\n```", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5565/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5564", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5564/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5564/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5564/events", "html_url": "https://github.com/dask/distributed/issues/5564", "id": 1071029826, "node_id": "I_kwDOAocYk84_1p5C", "number": 5564, "title": "Task stealing regression in 2021-11-0+ (preventing task load balancing)", "user": {"login": "arnaudsj", "id": 9855, "node_id": "MDQ6VXNlcjk4NTU=", "avatar_url": "https://avatars.githubusercontent.com/u/9855?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arnaudsj", "html_url": "https://github.com/arnaudsj", "followers_url": "https://api.github.com/users/arnaudsj/followers", "following_url": "https://api.github.com/users/arnaudsj/following{/other_user}", "gists_url": "https://api.github.com/users/arnaudsj/gists{/gist_id}", "starred_url": "https://api.github.com/users/arnaudsj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arnaudsj/subscriptions", "organizations_url": "https://api.github.com/users/arnaudsj/orgs", "repos_url": "https://api.github.com/users/arnaudsj/repos", "events_url": "https://api.github.com/users/arnaudsj/events{/privacy}", "received_events_url": "https://api.github.com/users/arnaudsj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2021-12-03T22:48:38Z", "updated_at": "2021-12-10T13:25:00Z", "closed_at": "2021-12-10T13:25:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**What happened**: There seems to be an issue with the task scheduling. When the cluster starts, and the first worker grabs the first task, it appears to block the rest of the workers until it completes the first task. This is particularly an issue when large clusters are spun in the cloud, as the scheduler start to send tasks when not all the workers are ready. This minimum working example below also demonstrate how one of the workers appears to keep all the tasks assigned and only distribute them as needed throughout the length of the computation.\r\n\r\n**What you expected to happen**: Workers should start processing tasks immediatly and be able to steal tasks from the 1st worker. This is appears to be a regresssion from pre dask-2021-11-0 (I confirmed for sure it works as expected in 2021-9-2)\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport numpy as np\r\nimport time\r\nimport dask.bag as db\r\n\r\ndef slow_function(input):\r\n    time.sleep(30)\r\n    return input\r\n\r\nbag = db.from_sequence(np.random.rand(1000, 1), npartitions=1000)\r\nbag.map(slow_function).compute()\r\n```\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\nThis is a minimized example of an issue our team ran into at scale on Coiled (cc @gjoseph92)\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2021.11.2\r\n- Python version: 3.9.7\r\n- Operating System: Linux / Coiled\r\n- Install method (conda, pip, source): conda\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5564/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5564/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5552", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5552/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5552/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5552/events", "html_url": "https://github.com/dask/distributed/issues/5552", "id": 1068051071, "node_id": "I_kwDOAocYk84_qSp_", "number": 5552, "title": "`Worker.log_event` is not threadsafe / `BatchedSend.send` not threadsafe", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 1342348975, "node_id": "MDU6TGFiZWwxMzQyMzQ4OTc1", "url": "https://api.github.com/repos/dask/distributed/labels/diagnostics", "name": "diagnostics", "color": "d93f0b", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-12-01T07:18:43Z", "updated_at": "2022-03-16T12:39:35Z", "closed_at": "2022-03-16T12:39:35Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "**What happened**:\r\n\r\nCalling `get_worker().log_event(...)` from multiple worker threads can fail (or probably corrupt the BatchedSend), since `BatchedSend.send` is not thread-safe.\r\n\r\nI saw a traceback like\r\n```python-tracebak\r\n/opt/conda/envs/coiled/lib/python3.8/site-packages/distributed/worker.py in log_event()\r\n\r\n/opt/conda/envs/coiled/lib/python3.8/site-packages/distributed/batched.py in send()\r\n\r\n/opt/conda/envs/coiled/lib/python3.8/site-packages/tornado/locks.py in set()\r\n\r\nRuntimeError: Set changed size during iteration\r\n```\r\n\r\nThis is probably coming from the `_background_send` coroutine calling `wait` on the tornado Event (adding to its `_waiters` set) while the `send` call in a worker thread calls `set` (iterating over the `_waiters` set).\r\n\r\n**What you expected to happen**:\r\n\r\nUsers don't have to manage thread-safety of this API; it locks automatically.\r\n\r\n**Anything else we need to know?**:\r\n\r\nThis is certainly a rarely-used API, but it is advertised in the docs: http://distributed.dask.org/en/stable/logging.html#structured-logs.\r\n\r\nEasy to fix (`sync`?), though not a high priority, just wanted to note it for posterity.\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2021.11.2\r\n- Python version: 3.8.7\r\n- Operating System: linux\r\n- Install method (conda, pip, source): pip", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5552/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5527", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5527/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5527/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5527/events", "html_url": "https://github.com/dask/distributed/issues/5527", "id": 1057924493, "node_id": "I_kwDOAocYk84_DqWN", "number": 5527, "title": "`distributed/tests/test_cancelled_state.py` failures", "user": {"login": "jrbourbeau", "id": 11656932, "node_id": "MDQ6VXNlcjExNjU2OTMy", "avatar_url": "https://avatars.githubusercontent.com/u/11656932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrbourbeau", "html_url": "https://github.com/jrbourbeau", "followers_url": "https://api.github.com/users/jrbourbeau/followers", "following_url": "https://api.github.com/users/jrbourbeau/following{/other_user}", "gists_url": "https://api.github.com/users/jrbourbeau/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrbourbeau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrbourbeau/subscriptions", "organizations_url": "https://api.github.com/users/jrbourbeau/orgs", "repos_url": "https://api.github.com/users/jrbourbeau/repos", "events_url": "https://api.github.com/users/jrbourbeau/events{/privacy}", "received_events_url": "https://api.github.com/users/jrbourbeau/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": {"login": "fjetter", "id": 8629629, "node_id": "MDQ6VXNlcjg2Mjk2Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjetter", "html_url": "https://github.com/fjetter", "followers_url": "https://api.github.com/users/fjetter/followers", "following_url": "https://api.github.com/users/fjetter/following{/other_user}", "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions", "organizations_url": "https://api.github.com/users/fjetter/orgs", "repos_url": "https://api.github.com/users/fjetter/repos", "events_url": "https://api.github.com/users/fjetter/events{/privacy}", "received_events_url": "https://api.github.com/users/fjetter/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fjetter", "id": 8629629, "node_id": "MDQ6VXNlcjg2Mjk2Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjetter", "html_url": "https://github.com/fjetter", "followers_url": "https://api.github.com/users/fjetter/followers", "following_url": "https://api.github.com/users/fjetter/following{/other_user}", "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions", "organizations_url": "https://api.github.com/users/fjetter/orgs", "repos_url": "https://api.github.com/users/fjetter/repos", "events_url": "https://api.github.com/users/fjetter/events{/privacy}", "received_events_url": "https://api.github.com/users/fjetter/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2021-11-18T22:53:41Z", "updated_at": "2021-11-19T16:54:51Z", "closed_at": "2021-11-19T16:54:51Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "We recently merged https://github.com/dask/distributed/pull/5525, https://github.com/dask/distributed/pull/5503, and https://github.com/dask/distributed/pull/5507 which contain various deadlock-related fixes. Tests passed in each individual PR (outside of a few flaky tests), however both `distributed/tests/test_cancelled_state.py::test_executing_cancelled_error` and `distributed/tests/test_cancelled_state.py::test_flight_cancelled_error`, which were added in https://github.com/dask/distributed/pull/5503, are failing consistently on `main` with an `asyncio.TimeoutError` (I'm also able to reproduce locally). This indicates that, while each PR may have been okay on its own, they are not interacting well with each other. \r\n\r\nSee [this CI build](https://github.com/dask/distributed/runs/4257423889?check_suite_focus=true) for an example of the test failures -- I've included full tracebacks below, though note they aren't super insightful. \r\n\r\n<details>\r\n<summary>Full tracebacks:</summary>\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n________________________ test_executing_cancelled_error ________________________\r\n\r\nfut = <Future cancelled>, timeout = 30\r\n\r\n    async def wait_for(fut, timeout, *, loop=None):\r\n        \"\"\"Wait for the single Future or coroutine to complete, with timeout.\r\n    \r\n        Coroutine will be wrapped in Task.\r\n    \r\n        Returns result of the Future or coroutine.  When a timeout occurs,\r\n        it cancels the task and raises TimeoutError.  To avoid the task\r\n        cancellation, wrap it in shield().\r\n    \r\n        If the wait is cancelled, the task is also cancelled.\r\n    \r\n        This function is a coroutine.\r\n        \"\"\"\r\n        if loop is None:\r\n            loop = events.get_running_loop()\r\n        else:\r\n            warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\r\n                          \"and scheduled for removal in Python 3.10.\",\r\n                          DeprecationWarning, stacklevel=2)\r\n    \r\n        if timeout is None:\r\n            return await fut\r\n    \r\n        if timeout <= 0:\r\n            fut = ensure_future(fut, loop=loop)\r\n    \r\n            if fut.done():\r\n                return fut.result()\r\n    \r\n            await _cancel_and_wait(fut, loop=loop)\r\n            try:\r\n                fut.result()\r\n            except exceptions.CancelledError as exc:\r\n                raise exceptions.TimeoutError() from exc\r\n            else:\r\n                raise exceptions.TimeoutError()\r\n    \r\n        waiter = loop.create_future()\r\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\r\n        cb = functools.partial(_release_waiter, waiter)\r\n    \r\n        fut = ensure_future(fut, loop=loop)\r\n        fut.add_done_callback(cb)\r\n    \r\n        try:\r\n            # wait until the future completes or the timeout\r\n            try:\r\n                await waiter\r\n            except exceptions.CancelledError:\r\n                if fut.done():\r\n                    return fut.result()\r\n                else:\r\n                    fut.remove_done_callback(cb)\r\n                    # We must ensure that the task is not running\r\n                    # after wait_for() returns.\r\n                    # See https://bugs.python.org/issue32751\r\n                    await _cancel_and_wait(fut, loop=loop)\r\n                    raise\r\n    \r\n            if fut.done():\r\n                return fut.result()\r\n            else:\r\n                fut.remove_done_callback(cb)\r\n                # We must ensure that the task is not running\r\n                # after wait_for() returns.\r\n                # See https://bugs.python.org/issue32751\r\n                await _cancel_and_wait(fut, loop=loop)\r\n                # In case task cancellation failed with some\r\n                # exception, we should re-raise it\r\n                # See https://bugs.python.org/issue40607\r\n                try:\r\n>                   fut.result()\r\nE                   asyncio.exceptions.CancelledError\r\n\r\n/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/tasks.py:492: CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\n    async def coro():\r\n        with dask.config.set(config):\r\n            s = False\r\n            for _ in range(60):\r\n                try:\r\n                    s, ws = await start_cluster(\r\n                        nthreads,\r\n                        scheduler,\r\n                        loop,\r\n                        security=security,\r\n                        Worker=Worker,\r\n                        scheduler_kwargs=scheduler_kwargs,\r\n                        worker_kwargs=worker_kwargs,\r\n                    )\r\n                except Exception as e:\r\n                    logger.error(\r\n                        \"Failed to start gen_cluster: \"\r\n                        f\"{e.__class__.__name__}: {e}; retrying\",\r\n                        exc_info=True,\r\n                    )\r\n                    await asyncio.sleep(1)\r\n                else:\r\n                    workers[:] = ws\r\n                    args = [s] + workers\r\n                    break\r\n            if s is False:\r\n                raise Exception(\"Could not start cluster\")\r\n            if client:\r\n                c = await Client(\r\n                    s.address,\r\n                    loop=loop,\r\n                    security=security,\r\n                    asynchronous=True,\r\n                    **client_kwargs,\r\n                )\r\n                args = [c] + args\r\n            try:\r\n                coro = func(*args, *outer_args, **kwargs)\r\n                task = asyncio.create_task(coro)\r\n    \r\n                coro2 = asyncio.wait_for(asyncio.shield(task), timeout)\r\n>               result = await coro2\r\n\r\ndistributed/utils_test.py:975: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nfut = <Future cancelled>, timeout = 30\r\n\r\n    async def wait_for(fut, timeout, *, loop=None):\r\n        \"\"\"Wait for the single Future or coroutine to complete, with timeout.\r\n    \r\n        Coroutine will be wrapped in Task.\r\n    \r\n        Returns result of the Future or coroutine.  When a timeout occurs,\r\n        it cancels the task and raises TimeoutError.  To avoid the task\r\n        cancellation, wrap it in shield().\r\n    \r\n        If the wait is cancelled, the task is also cancelled.\r\n    \r\n        This function is a coroutine.\r\n        \"\"\"\r\n        if loop is None:\r\n            loop = events.get_running_loop()\r\n        else:\r\n            warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\r\n                          \"and scheduled for removal in Python 3.10.\",\r\n                          DeprecationWarning, stacklevel=2)\r\n    \r\n        if timeout is None:\r\n            return await fut\r\n    \r\n        if timeout <= 0:\r\n            fut = ensure_future(fut, loop=loop)\r\n    \r\n            if fut.done():\r\n                return fut.result()\r\n    \r\n            await _cancel_and_wait(fut, loop=loop)\r\n            try:\r\n                fut.result()\r\n            except exceptions.CancelledError as exc:\r\n                raise exceptions.TimeoutError() from exc\r\n            else:\r\n                raise exceptions.TimeoutError()\r\n    \r\n        waiter = loop.create_future()\r\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\r\n        cb = functools.partial(_release_waiter, waiter)\r\n    \r\n        fut = ensure_future(fut, loop=loop)\r\n        fut.add_done_callback(cb)\r\n    \r\n        try:\r\n            # wait until the future completes or the timeout\r\n            try:\r\n                await waiter\r\n            except exceptions.CancelledError:\r\n                if fut.done():\r\n                    return fut.result()\r\n                else:\r\n                    fut.remove_done_callback(cb)\r\n                    # We must ensure that the task is not running\r\n                    # after wait_for() returns.\r\n                    # See https://bugs.python.org/issue32751\r\n                    await _cancel_and_wait(fut, loop=loop)\r\n                    raise\r\n    \r\n            if fut.done():\r\n                return fut.result()\r\n            else:\r\n                fut.remove_done_callback(cb)\r\n                # We must ensure that the task is not running\r\n                # after wait_for() returns.\r\n                # See https://bugs.python.org/issue32751\r\n                await _cancel_and_wait(fut, loop=loop)\r\n                # In case task cancellation failed with some\r\n                # exception, we should re-raise it\r\n                # See https://bugs.python.org/issue40607\r\n                try:\r\n                    fut.result()\r\n                except exceptions.CancelledError as exc:\r\n>                   raise exceptions.TimeoutError() from exc\r\nE                   asyncio.exceptions.TimeoutError\r\n\r\n/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/tasks.py:494: TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nouter_args = (), kwargs = {}, result = None\r\ncoro = <function gen_cluster.<locals>._.<locals>.test_func.<locals>.coro at 0x7f0762dad430>\r\n\r\n    @functools.wraps(func)\r\n    def test_func(*outer_args, **kwargs):\r\n        result = None\r\n        workers = []\r\n        with clean(timeout=active_rpc_timeout, **clean_kwargs) as loop:\r\n    \r\n            async def coro():\r\n                with dask.config.set(config):\r\n                    s = False\r\n                    for _ in range(60):\r\n                        try:\r\n                            s, ws = await start_cluster(\r\n                                nthreads,\r\n                                scheduler,\r\n                                loop,\r\n                                security=security,\r\n                                Worker=Worker,\r\n                                scheduler_kwargs=scheduler_kwargs,\r\n                                worker_kwargs=worker_kwargs,\r\n                            )\r\n                        except Exception as e:\r\n                            logger.error(\r\n                                \"Failed to start gen_cluster: \"\r\n                                f\"{e.__class__.__name__}: {e}; retrying\",\r\n                                exc_info=True,\r\n                            )\r\n                            await asyncio.sleep(1)\r\n                        else:\r\n                            workers[:] = ws\r\n                            args = [s] + workers\r\n                            break\r\n                    if s is False:\r\n                        raise Exception(\"Could not start cluster\")\r\n                    if client:\r\n                        c = await Client(\r\n                            s.address,\r\n                            loop=loop,\r\n                            security=security,\r\n                            asynchronous=True,\r\n                            **client_kwargs,\r\n                        )\r\n                        args = [c] + args\r\n                    try:\r\n                        coro = func(*args, *outer_args, **kwargs)\r\n                        task = asyncio.create_task(coro)\r\n    \r\n                        coro2 = asyncio.wait_for(asyncio.shield(task), timeout)\r\n                        result = await coro2\r\n                        if s.validate:\r\n                            s.validate_state()\r\n                    except asyncio.TimeoutError as e:\r\n                        assert task\r\n                        buffer = io.StringIO()\r\n                        # This stack indicates where the coro/test is suspended\r\n                        task.print_stack(file=buffer)\r\n    \r\n                        if client:\r\n                            assert c\r\n                            try:\r\n                                if cluster_dump_directory:\r\n                                    if not os.path.exists(cluster_dump_directory):\r\n                                        os.makedirs(cluster_dump_directory)\r\n                                    filename = os.path.join(\r\n                                        cluster_dump_directory, func.__name__\r\n                                    )\r\n                                    fut = c.dump_cluster_state(\r\n                                        filename,\r\n                                        # Test dumps should be small enough that\r\n                                        # there is no need for a compressed\r\n                                        # binary representation and readability\r\n                                        # is more important\r\n                                        format=\"yaml\",\r\n                                    )\r\n                                    assert fut is not None\r\n                                    await fut\r\n                            except Exception:\r\n                                print(\r\n                                    f\"Exception {sys.exc_info()} while trying to dump cluster state.\"\r\n                                )\r\n    \r\n                        task.cancel()\r\n                        while not task.cancelled():\r\n                            await asyncio.sleep(0.01)\r\n                        raise TimeoutError(\r\n                            f\"Test timeout after {timeout}s.\\n{buffer.getvalue()}\"\r\n                        ) from e\r\n                    finally:\r\n                        if client and c.status not in (\"closing\", \"closed\"):\r\n                            await c._close(fast=s.status == Status.closed)\r\n                        await end_cluster(s, workers)\r\n                        await asyncio.wait_for(cleanup_global_workers(), 1)\r\n    \r\n                    try:\r\n                        c = await default_client()\r\n                    except ValueError:\r\n                        pass\r\n                    else:\r\n                        await c._close(fast=True)\r\n    \r\n                    def get_unclosed():\r\n                        return [c for c in Comm._instances if not c.closed()] + [\r\n                            c\r\n                            for c in _global_clients.values()\r\n                            if c.status != \"closed\"\r\n                        ]\r\n    \r\n                    try:\r\n                        start = time()\r\n                        while time() < start + 60:\r\n                            gc.collect()\r\n                            if not get_unclosed():\r\n                                break\r\n                            await asyncio.sleep(0.05)\r\n                        else:\r\n                            if allow_unclosed:\r\n                                print(f\"Unclosed Comms: {get_unclosed()}\")\r\n                            else:\r\n                                raise RuntimeError(\"Unclosed Comms\", get_unclosed())\r\n                    finally:\r\n                        Comm._instances.clear()\r\n                        _global_clients.clear()\r\n    \r\n                    return result\r\n    \r\n>           result = loop.run_sync(\r\n                coro, timeout=timeout * 2 if timeout else timeout\r\n            )\r\n\r\ndistributed/utils_test.py:1052: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/ioloop.py:530: in run_sync\r\n    return future_cell[0].result()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n    async def coro():\r\n        with dask.config.set(config):\r\n            s = False\r\n            for _ in range(60):\r\n                try:\r\n                    s, ws = await start_cluster(\r\n                        nthreads,\r\n                        scheduler,\r\n                        loop,\r\n                        security=security,\r\n                        Worker=Worker,\r\n                        scheduler_kwargs=scheduler_kwargs,\r\n                        worker_kwargs=worker_kwargs,\r\n                    )\r\n                except Exception as e:\r\n                    logger.error(\r\n                        \"Failed to start gen_cluster: \"\r\n                        f\"{e.__class__.__name__}: {e}; retrying\",\r\n                        exc_info=True,\r\n                    )\r\n                    await asyncio.sleep(1)\r\n                else:\r\n                    workers[:] = ws\r\n                    args = [s] + workers\r\n                    break\r\n            if s is False:\r\n                raise Exception(\"Could not start cluster\")\r\n            if client:\r\n                c = await Client(\r\n                    s.address,\r\n                    loop=loop,\r\n                    security=security,\r\n                    asynchronous=True,\r\n                    **client_kwargs,\r\n                )\r\n                args = [c] + args\r\n            try:\r\n                coro = func(*args, *outer_args, **kwargs)\r\n                task = asyncio.create_task(coro)\r\n    \r\n                coro2 = asyncio.wait_for(asyncio.shield(task), timeout)\r\n                result = await coro2\r\n                if s.validate:\r\n                    s.validate_state()\r\n            except asyncio.TimeoutError as e:\r\n                assert task\r\n                buffer = io.StringIO()\r\n                # This stack indicates where the coro/test is suspended\r\n                task.print_stack(file=buffer)\r\n    \r\n                if client:\r\n                    assert c\r\n                    try:\r\n                        if cluster_dump_directory:\r\n                            if not os.path.exists(cluster_dump_directory):\r\n                                os.makedirs(cluster_dump_directory)\r\n                            filename = os.path.join(\r\n                                cluster_dump_directory, func.__name__\r\n                            )\r\n                            fut = c.dump_cluster_state(\r\n                                filename,\r\n                                # Test dumps should be small enough that\r\n                                # there is no need for a compressed\r\n                                # binary representation and readability\r\n                                # is more important\r\n                                format=\"yaml\",\r\n                            )\r\n                            assert fut is not None\r\n                            await fut\r\n                    except Exception:\r\n                        print(\r\n                            f\"Exception {sys.exc_info()} while trying to dump cluster state.\"\r\n                        )\r\n    \r\n                task.cancel()\r\n                while not task.cancelled():\r\n                    await asyncio.sleep(0.01)\r\n>               raise TimeoutError(\r\n                    f\"Test timeout after {timeout}s.\\n{buffer.getvalue()}\"\r\n                ) from e\r\nE               asyncio.exceptions.TimeoutError: Test timeout after 30s.\r\nE               Stack for <Task pending name='Task-64578' coro=<test_executing_cancelled_error() running at /home/runner/work/distributed/distributed/distributed/tests/test_cancelled_state.py:163> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x7f0762704b80>()]>> (most recent call last):\r\nE                 File \"/home/runner/work/distributed/distributed/distributed/tests/test_cancelled_state.py\", line 163, in test_executing_cancelled_error\r\nE                   await asyncio.sleep(0.01)\r\n\r\ndistributed/utils_test.py:1011: TimeoutError\r\n----------------------------- Captured stderr call -----------------------------\r\ndistributed.worker - WARNING - Compute Failed\r\nFunction:  wait_and_raise\r\nargs:      ()\r\nkwargs:    {}\r\nException: 'RuntimeError()'\r\n\r\n_________________________ test_flight_cancelled_error __________________________\r\n\r\nfut = <Future cancelled>, timeout = 30\r\n\r\n    async def wait_for(fut, timeout, *, loop=None):\r\n        \"\"\"Wait for the single Future or coroutine to complete, with timeout.\r\n    \r\n        Coroutine will be wrapped in Task.\r\n    \r\n        Returns result of the Future or coroutine.  When a timeout occurs,\r\n        it cancels the task and raises TimeoutError.  To avoid the task\r\n        cancellation, wrap it in shield().\r\n    \r\n        If the wait is cancelled, the task is also cancelled.\r\n    \r\n        This function is a coroutine.\r\n        \"\"\"\r\n        if loop is None:\r\n            loop = events.get_running_loop()\r\n        else:\r\n            warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\r\n                          \"and scheduled for removal in Python 3.10.\",\r\n                          DeprecationWarning, stacklevel=2)\r\n    \r\n        if timeout is None:\r\n            return await fut\r\n    \r\n        if timeout <= 0:\r\n            fut = ensure_future(fut, loop=loop)\r\n    \r\n            if fut.done():\r\n                return fut.result()\r\n    \r\n            await _cancel_and_wait(fut, loop=loop)\r\n            try:\r\n                fut.result()\r\n            except exceptions.CancelledError as exc:\r\n                raise exceptions.TimeoutError() from exc\r\n            else:\r\n                raise exceptions.TimeoutError()\r\n    \r\n        waiter = loop.create_future()\r\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\r\n        cb = functools.partial(_release_waiter, waiter)\r\n    \r\n        fut = ensure_future(fut, loop=loop)\r\n        fut.add_done_callback(cb)\r\n    \r\n        try:\r\n            # wait until the future completes or the timeout\r\n            try:\r\n                await waiter\r\n            except exceptions.CancelledError:\r\n                if fut.done():\r\n                    return fut.result()\r\n                else:\r\n                    fut.remove_done_callback(cb)\r\n                    # We must ensure that the task is not running\r\n                    # after wait_for() returns.\r\n                    # See https://bugs.python.org/issue32751\r\n                    await _cancel_and_wait(fut, loop=loop)\r\n                    raise\r\n    \r\n            if fut.done():\r\n                return fut.result()\r\n            else:\r\n                fut.remove_done_callback(cb)\r\n                # We must ensure that the task is not running\r\n                # after wait_for() returns.\r\n                # See https://bugs.python.org/issue32751\r\n                await _cancel_and_wait(fut, loop=loop)\r\n                # In case task cancellation failed with some\r\n                # exception, we should re-raise it\r\n                # See https://bugs.python.org/issue40607\r\n                try:\r\n>                   fut.result()\r\nE                   asyncio.exceptions.CancelledError\r\n\r\n/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/tasks.py:492: CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\n    async def coro():\r\n        with dask.config.set(config):\r\n            s = False\r\n            for _ in range(60):\r\n                try:\r\n                    s, ws = await start_cluster(\r\n                        nthreads,\r\n                        scheduler,\r\n                        loop,\r\n                        security=security,\r\n                        Worker=Worker,\r\n                        scheduler_kwargs=scheduler_kwargs,\r\n                        worker_kwargs=worker_kwargs,\r\n                    )\r\n                except Exception as e:\r\n                    logger.error(\r\n                        \"Failed to start gen_cluster: \"\r\n                        f\"{e.__class__.__name__}: {e}; retrying\",\r\n                        exc_info=True,\r\n                    )\r\n                    await asyncio.sleep(1)\r\n                else:\r\n                    workers[:] = ws\r\n                    args = [s] + workers\r\n                    break\r\n            if s is False:\r\n                raise Exception(\"Could not start cluster\")\r\n            if client:\r\n                c = await Client(\r\n                    s.address,\r\n                    loop=loop,\r\n                    security=security,\r\n                    asynchronous=True,\r\n                    **client_kwargs,\r\n                )\r\n                args = [c] + args\r\n            try:\r\n                coro = func(*args, *outer_args, **kwargs)\r\n                task = asyncio.create_task(coro)\r\n    \r\n                coro2 = asyncio.wait_for(asyncio.shield(task), timeout)\r\n>               result = await coro2\r\n\r\ndistributed/utils_test.py:975: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nfut = <Future cancelled>, timeout = 30\r\n\r\n    async def wait_for(fut, timeout, *, loop=None):\r\n        \"\"\"Wait for the single Future or coroutine to complete, with timeout.\r\n    \r\n        Coroutine will be wrapped in Task.\r\n    \r\n        Returns result of the Future or coroutine.  When a timeout occurs,\r\n        it cancels the task and raises TimeoutError.  To avoid the task\r\n        cancellation, wrap it in shield().\r\n    \r\n        If the wait is cancelled, the task is also cancelled.\r\n    \r\n        This function is a coroutine.\r\n        \"\"\"\r\n        if loop is None:\r\n            loop = events.get_running_loop()\r\n        else:\r\n            warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\r\n                          \"and scheduled for removal in Python 3.10.\",\r\n                          DeprecationWarning, stacklevel=2)\r\n    \r\n        if timeout is None:\r\n            return await fut\r\n    \r\n        if timeout <= 0:\r\n            fut = ensure_future(fut, loop=loop)\r\n    \r\n            if fut.done():\r\n                return fut.result()\r\n    \r\n            await _cancel_and_wait(fut, loop=loop)\r\n            try:\r\n                fut.result()\r\n            except exceptions.CancelledError as exc:\r\n                raise exceptions.TimeoutError() from exc\r\n            else:\r\n                raise exceptions.TimeoutError()\r\n    \r\n        waiter = loop.create_future()\r\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\r\n        cb = functools.partial(_release_waiter, waiter)\r\n    \r\n        fut = ensure_future(fut, loop=loop)\r\n        fut.add_done_callback(cb)\r\n    \r\n        try:\r\n            # wait until the future completes or the timeout\r\n            try:\r\n                await waiter\r\n            except exceptions.CancelledError:\r\n                if fut.done():\r\n                    return fut.result()\r\n                else:\r\n                    fut.remove_done_callback(cb)\r\n                    # We must ensure that the task is not running\r\n                    # after wait_for() returns.\r\n                    # See https://bugs.python.org/issue32751\r\n                    await _cancel_and_wait(fut, loop=loop)\r\n                    raise\r\n    \r\n            if fut.done():\r\n                return fut.result()\r\n            else:\r\n                fut.remove_done_callback(cb)\r\n                # We must ensure that the task is not running\r\n                # after wait_for() returns.\r\n                # See https://bugs.python.org/issue32751\r\n                await _cancel_and_wait(fut, loop=loop)\r\n                # In case task cancellation failed with some\r\n                # exception, we should re-raise it\r\n                # See https://bugs.python.org/issue40607\r\n                try:\r\n                    fut.result()\r\n                except exceptions.CancelledError as exc:\r\n>                   raise exceptions.TimeoutError() from exc\r\nE                   asyncio.exceptions.TimeoutError\r\n\r\n/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/asyncio/tasks.py:494: TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nouter_args = (), kwargs = {}, result = None\r\ncoro = <function gen_cluster.<locals>._.<locals>.test_func.<locals>.coro at 0x7f0762b23310>\r\n\r\n    @functools.wraps(func)\r\n    def test_func(*outer_args, **kwargs):\r\n        result = None\r\n        workers = []\r\n        with clean(timeout=active_rpc_timeout, **clean_kwargs) as loop:\r\n    \r\n            async def coro():\r\n                with dask.config.set(config):\r\n                    s = False\r\n                    for _ in range(60):\r\n                        try:\r\n                            s, ws = await start_cluster(\r\n                                nthreads,\r\n                                scheduler,\r\n                                loop,\r\n                                security=security,\r\n                                Worker=Worker,\r\n                                scheduler_kwargs=scheduler_kwargs,\r\n                                worker_kwargs=worker_kwargs,\r\n                            )\r\n                        except Exception as e:\r\n                            logger.error(\r\n                                \"Failed to start gen_cluster: \"\r\n                                f\"{e.__class__.__name__}: {e}; retrying\",\r\n                                exc_info=True,\r\n                            )\r\n                            await asyncio.sleep(1)\r\n                        else:\r\n                            workers[:] = ws\r\n                            args = [s] + workers\r\n                            break\r\n                    if s is False:\r\n                        raise Exception(\"Could not start cluster\")\r\n                    if client:\r\n                        c = await Client(\r\n                            s.address,\r\n                            loop=loop,\r\n                            security=security,\r\n                            asynchronous=True,\r\n                            **client_kwargs,\r\n                        )\r\n                        args = [c] + args\r\n                    try:\r\n                        coro = func(*args, *outer_args, **kwargs)\r\n                        task = asyncio.create_task(coro)\r\n    \r\n                        coro2 = asyncio.wait_for(asyncio.shield(task), timeout)\r\n                        result = await coro2\r\n                        if s.validate:\r\n                            s.validate_state()\r\n                    except asyncio.TimeoutError as e:\r\n                        assert task\r\n                        buffer = io.StringIO()\r\n                        # This stack indicates where the coro/test is suspended\r\n                        task.print_stack(file=buffer)\r\n    \r\n                        if client:\r\n                            assert c\r\n                            try:\r\n                                if cluster_dump_directory:\r\n                                    if not os.path.exists(cluster_dump_directory):\r\n                                        os.makedirs(cluster_dump_directory)\r\n                                    filename = os.path.join(\r\n                                        cluster_dump_directory, func.__name__\r\n                                    )\r\n                                    fut = c.dump_cluster_state(\r\n                                        filename,\r\n                                        # Test dumps should be small enough that\r\n                                        # there is no need for a compressed\r\n                                        # binary representation and readability\r\n                                        # is more important\r\n                                        format=\"yaml\",\r\n                                    )\r\n                                    assert fut is not None\r\n                                    await fut\r\n                            except Exception:\r\n                                print(\r\n                                    f\"Exception {sys.exc_info()} while trying to dump cluster state.\"\r\n                                )\r\n    \r\n                        task.cancel()\r\n                        while not task.cancelled():\r\n                            await asyncio.sleep(0.01)\r\n                        raise TimeoutError(\r\n                            f\"Test timeout after {timeout}s.\\n{buffer.getvalue()}\"\r\n                        ) from e\r\n                    finally:\r\n                        if client and c.status not in (\"closing\", \"closed\"):\r\n                            await c._close(fast=s.status == Status.closed)\r\n                        await end_cluster(s, workers)\r\n                        await asyncio.wait_for(cleanup_global_workers(), 1)\r\n    \r\n                    try:\r\n                        c = await default_client()\r\n                    except ValueError:\r\n                        pass\r\n                    else:\r\n                        await c._close(fast=True)\r\n    \r\n                    def get_unclosed():\r\n                        return [c for c in Comm._instances if not c.closed()] + [\r\n                            c\r\n                            for c in _global_clients.values()\r\n                            if c.status != \"closed\"\r\n                        ]\r\n    \r\n                    try:\r\n                        start = time()\r\n                        while time() < start + 60:\r\n                            gc.collect()\r\n                            if not get_unclosed():\r\n                                break\r\n                            await asyncio.sleep(0.05)\r\n                        else:\r\n                            if allow_unclosed:\r\n                                print(f\"Unclosed Comms: {get_unclosed()}\")\r\n                            else:\r\n                                raise RuntimeError(\"Unclosed Comms\", get_unclosed())\r\n                    finally:\r\n                        Comm._instances.clear()\r\n                        _global_clients.clear()\r\n    \r\n                    return result\r\n    \r\n>           result = loop.run_sync(\r\n                coro, timeout=timeout * 2 if timeout else timeout\r\n            )\r\n\r\ndistributed/utils_test.py:1052: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/ioloop.py:530: in run_sync\r\n    return future_cell[0].result()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n    async def coro():\r\n        with dask.config.set(config):\r\n            s = False\r\n            for _ in range(60):\r\n                try:\r\n                    s, ws = await start_cluster(\r\n                        nthreads,\r\n                        scheduler,\r\n                        loop,\r\n                        security=security,\r\n                        Worker=Worker,\r\n                        scheduler_kwargs=scheduler_kwargs,\r\n                        worker_kwargs=worker_kwargs,\r\n                    )\r\n                except Exception as e:\r\n                    logger.error(\r\n                        \"Failed to start gen_cluster: \"\r\n                        f\"{e.__class__.__name__}: {e}; retrying\",\r\n                        exc_info=True,\r\n                    )\r\n                    await asyncio.sleep(1)\r\n                else:\r\n                    workers[:] = ws\r\n                    args = [s] + workers\r\n                    break\r\n            if s is False:\r\n                raise Exception(\"Could not start cluster\")\r\n            if client:\r\n                c = await Client(\r\n                    s.address,\r\n                    loop=loop,\r\n                    security=security,\r\n                    asynchronous=True,\r\n                    **client_kwargs,\r\n                )\r\n                args = [c] + args\r\n            try:\r\n                coro = func(*args, *outer_args, **kwargs)\r\n                task = asyncio.create_task(coro)\r\n    \r\n                coro2 = asyncio.wait_for(asyncio.shield(task), timeout)\r\n                result = await coro2\r\n                if s.validate:\r\n                    s.validate_state()\r\n            except asyncio.TimeoutError as e:\r\n                assert task\r\n                buffer = io.StringIO()\r\n                # This stack indicates where the coro/test is suspended\r\n                task.print_stack(file=buffer)\r\n    \r\n                if client:\r\n                    assert c\r\n                    try:\r\n                        if cluster_dump_directory:\r\n                            if not os.path.exists(cluster_dump_directory):\r\n                                os.makedirs(cluster_dump_directory)\r\n                            filename = os.path.join(\r\n                                cluster_dump_directory, func.__name__\r\n                            )\r\n                            fut = c.dump_cluster_state(\r\n                                filename,\r\n                                # Test dumps should be small enough that\r\n                                # there is no need for a compressed\r\n                                # binary representation and readability\r\n                                # is more important\r\n                                format=\"yaml\",\r\n                            )\r\n                            assert fut is not None\r\n                            await fut\r\n                    except Exception:\r\n                        print(\r\n                            f\"Exception {sys.exc_info()} while trying to dump cluster state.\"\r\n                        )\r\n    \r\n                task.cancel()\r\n                while not task.cancelled():\r\n                    await asyncio.sleep(0.01)\r\n>               raise TimeoutError(\r\n                    f\"Test timeout after {timeout}s.\\n{buffer.getvalue()}\"\r\n                ) from e\r\nE               asyncio.exceptions.TimeoutError: Test timeout after 30s.\r\nE               Stack for <Task pending name='Task-64948' coro=<test_flight_cancelled_error() running at /home/runner/work/distributed/distributed/distributed/tests/test_cancelled_state.py:208> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x7f075f63eeb0>()]>> (most recent call last):\r\nE                 File \"/home/runner/work/distributed/distributed/distributed/tests/test_cancelled_state.py\", line 208, in test_flight_cancelled_error\r\nE                   await asyncio.sleep(0.01)\r\n\r\ndistributed/utils_test.py:1011: TimeoutError\r\n----------------------------- Captured stderr call -----------------------------\r\ndistributed.worker - ERROR - \r\nTraceback (most recent call last):\r\n  File \"/home/runner/work/distributed/distributed/distributed/worker.py\", line 2914, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/unittest/mock.py\", line 2165, in _execute_mock_call\r\n    result = await effect(*args, **kwargs)\r\n  File \"/home/runner/work/distributed/distributed/distributed/tests/test_cancelled_state.py\", line 187, in wait_and_raise\r\n    raise RuntimeError()\r\nRuntimeError\r\ndistributed.utils - ERROR - \r\nTraceback (most recent call last):\r\n  File \"/home/runner/work/distributed/distributed/distributed/utils.py\", line 653, in log_errors\r\n    yield\r\n  File \"/home/runner/work/distributed/distributed/distributed/worker.py\", line 2914, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/unittest/mock.py\", line 2165, in _execute_mock_call\r\n    result = await effect(*args, **kwargs)\r\n  File \"/home/runner/work/distributed/distributed/distributed/tests/test_cancelled_state.py\", line 187, in wait_and_raise\r\n    raise RuntimeError()\r\nRuntimeError\r\ntornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7f075df18dc0>>, <Task finished name='Task-64956' coro=<Worker.gather_dep() done, defined at /home/runner/work/distributed/distributed/distributed/worker.py:2858> exception=RuntimeError()>)\r\nTraceback (most recent call last):\r\n  File \"/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\r\n    future.result()\r\n  File \"/home/runner/work/distributed/distributed/distributed/worker.py\", line 2914, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/usr/share/miniconda3/envs/dask-distributed/lib/python3.9/unittest/mock.py\", line 2165, in _execute_mock_call\r\n    result = await effect(*args, **kwargs)\r\n  File \"/home/runner/work/distributed/distributed/distributed/tests/test_cancelled_state.py\", line 187, in wait_and_raise\r\n    raise RuntimeError()\r\nRuntimeError\r\n```\r\n\r\n</details>\r\n\r\ncc @fjetter ", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5527/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 1}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5527/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5522", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5522/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5522/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5522/events", "html_url": "https://github.com/dask/distributed/issues/5522", "id": 1056904273, "node_id": "I_kwDOAocYk84-_xRR", "number": 5522, "title": "Dumping cluster state causes infinite recursion", "user": {"login": "gjoseph92", "id": 3309802, "node_id": "MDQ6VXNlcjMzMDk4MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3309802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gjoseph92", "html_url": "https://github.com/gjoseph92", "followers_url": "https://api.github.com/users/gjoseph92/followers", "following_url": "https://api.github.com/users/gjoseph92/following{/other_user}", "gists_url": "https://api.github.com/users/gjoseph92/gists{/gist_id}", "starred_url": "https://api.github.com/users/gjoseph92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gjoseph92/subscriptions", "organizations_url": "https://api.github.com/users/gjoseph92/orgs", "repos_url": "https://api.github.com/users/gjoseph92/repos", "events_url": "https://api.github.com/users/gjoseph92/events{/privacy}", "received_events_url": "https://api.github.com/users/gjoseph92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2021-11-18T03:49:37Z", "updated_at": "2021-12-03T09:32:58Z", "closed_at": "2021-12-03T09:32:58Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "The auto-dump of cluster state on timed out tests is breaking for me.\r\n\r\ncc @fjetter \r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```diff\r\ndiff --git a/distributed/tests/test_utils_test.py b/distributed/tests/test_utils_test.py\r\nindex 4b7a1c27..f01caf51 100755\r\n--- a/distributed/tests/test_utils_test.py\r\n+++ b/distributed/tests/test_utils_test.py\r\n@@ -360,6 +360,8 @@ def test_dump_cluster_state_timeout(tmp_path):\r\n     sleep_time = 30\r\n \r\n     async def inner_test(c, s, a, b):\r\n+        f1 = c.submit(inc, 1)\r\n+        f2 = c.submit(inc, f1)\r\n         await asyncio.sleep(sleep_time)\r\n \r\n     # This timeout includes cluster startup and teardown which sometimes can\r\n```\r\n\r\nI'm not going to include the full traceback (because it's... recursive) but the loop looks like:\r\n```python-traceback\r\n  File \"/Users/gabe/dev/distributed/distributed/worker.py\", line 248, in _to_dict\r\n    return recursive_to_dict(\r\n  File \"/Users/gabe/dev/distributed/distributed/utils.py\", line 1542, in recursive_to_dict\r\n    v = recursive_to_dict(v, exclude=exclude, seen=seen)\r\n  File \"/Users/gabe/dev/distributed/distributed/utils.py\", line 1525, in recursive_to_dict\r\n    return tuple(\r\n  File \"/Users/gabe/dev/distributed/distributed/utils.py\", line 1526, in <genexpr>\r\n    recursive_to_dict(\r\n  File \"/Users/gabe/dev/distributed/distributed/utils.py\", line 1521, in recursive_to_dict\r\n    return obj._to_dict(exclude=exclude)\r\n  File \"/Users/gabe/dev/distributed/distributed/worker.py\", line 248, in _to_dict\r\n    return recursive_to_dict(\r\n```\r\n\r\nIt appears that `worker.TaskState._to_dict` will cause mutual recursion with any `dependents`/`dependencies`, since within those it finds an object with a `_to_dict` method (the other `TaskState`) and calls it without passing in `seen`.\r\n\r\nI think the `_to_dict` interface should probably support passing in `seen`. I accidentally found a different infinite recursion path on the scheduler, where `scheduler.TaskState` traversed into `__annotations__`, which somehow created some circular references. Trying to prune these down seems like a poor use of time; let's just pass `seen` through.\r\n\r\n**Environment**:\r\n\r\n- Dask version: d840e6eb918d2bc6d12be28d88806e9cc3012fdd\r\n- Python version: 3.9.5\r\n- Operating System: macOS\r\n- Install method (conda, pip, source): source\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5522/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5504", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5504/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5504/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5504/events", "html_url": "https://github.com/dask/distributed/issues/5504", "id": 1047583217, "node_id": "I_kwDOAocYk84-cNnx", "number": 5504, "title": "xarray.DataArray map_blocks failed to deserialize ", "user": {"login": "rsignell-usgs", "id": 1872600, "node_id": "MDQ6VXNlcjE4NzI2MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/1872600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsignell-usgs", "html_url": "https://github.com/rsignell-usgs", "followers_url": "https://api.github.com/users/rsignell-usgs/followers", "following_url": "https://api.github.com/users/rsignell-usgs/following{/other_user}", "gists_url": "https://api.github.com/users/rsignell-usgs/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsignell-usgs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsignell-usgs/subscriptions", "organizations_url": "https://api.github.com/users/rsignell-usgs/orgs", "repos_url": "https://api.github.com/users/rsignell-usgs/repos", "events_url": "https://api.github.com/users/rsignell-usgs/events{/privacy}", "received_events_url": "https://api.github.com/users/rsignell-usgs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-11-08T15:23:42Z", "updated_at": "2021-12-09T11:29:13Z", "closed_at": "2021-12-09T11:29:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "**What happened**\r\n\r\nMy xarray map_blocks call failed in pickling with a deserialization error:\r\n```python-traceback\r\ndistributed.protocol.core - CRITICAL - Failed to deserialize\r\n...\r\nTypeError: __init__() missing 1 required positional argument: 'code'\r\n```\r\n**What you expected to happen**:\r\n\r\nThe code should work (and it works if you close the client and just use Dask, not distributed).\r\n\r\n**Not-Quite-Minimal but Complete and Verifiable Example**\r\n\r\n```python\r\nimport xarray as xr\r\nfrom cartopy import crs as ccrs\r\nimport numpy as np\r\nfrom dask.distributed import Client\r\n\r\nclient = Client()\r\n\r\nnx = 10000\r\nny = 10000\r\n\r\nx = (np.linspace(121940., 574180., nx))\r\ny = (np.linspace(4250700., 4659150., ny))[::-1]\r\n\r\ncrs_from = ccrs.epsg(26917)\r\n\r\nda = xr.DataArray(\r\n    data=np.ones((ny,nx)),\r\n    dims=[\"y\", \"x\"],\r\n    coords=dict(\r\n        x=([\"x\"], x),\r\n        y=([\"y\"], y))).chunk({'x':5120, 'y':5120})\r\n\r\ncrs_to = ccrs.PlateCarree()  \r\n\r\ndef xy_to_lonlat(da):\r\n    x, y = np.meshgrid(da.x, da.y)\r\n    ll = crs_to.transform_points(crs_from, x, y)\r\n    lon = ll[:,:,0]\r\n    lat = ll[:,:,1]\r\n    da = da.assign_coords(dict(\r\n        lon=([\"y\", \"x\"], lon),\r\n        lat=([\"y\", \"x\"], lat)))\r\n    return da\r\n\r\nda2 = da.map_blocks(xy_to_lonlat).compute()\r\n```\r\n\r\n**Anything else we need to know?**:\r\nMay be related to https://github.com/dask/dask/issues/8355 and/or https://github.com/dask/distributed/issues/5495 ?\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2021.11.0\r\n- Python version: 3.8.10\r\n- Operating System: Linux\r\n- Install method (conda, pip, source): conda\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5504/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5504/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5478", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5478/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5478/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5478/events", "html_url": "https://github.com/dask/distributed/issues/5478", "id": 1039413311, "node_id": "I_kwDOAocYk8499DA_", "number": 5478, "title": "numpydoc eats whitespaces on parameters spec", "user": {"login": "crusaderky", "id": 6213168, "node_id": "MDQ6VXNlcjYyMTMxNjg=", "avatar_url": "https://avatars.githubusercontent.com/u/6213168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crusaderky", "html_url": "https://github.com/crusaderky", "followers_url": "https://api.github.com/users/crusaderky/followers", "following_url": "https://api.github.com/users/crusaderky/following{/other_user}", "gists_url": "https://api.github.com/users/crusaderky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crusaderky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crusaderky/subscriptions", "organizations_url": "https://api.github.com/users/crusaderky/orgs", "repos_url": "https://api.github.com/users/crusaderky/repos", "events_url": "https://api.github.com/users/crusaderky/events{/privacy}", "received_events_url": "https://api.github.com/users/crusaderky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 729975096, "node_id": "MDU6TGFiZWw3Mjk5NzUwOTY=", "url": "https://api.github.com/repos/dask/distributed/labels/documentation", "name": "documentation", "color": "f9d0c4", "default": true, "description": "Improve or add to documentation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-10-29T10:19:44Z", "updated_at": "2021-11-01T16:40:47Z", "closed_at": "2021-11-01T16:40:47Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "In client.py:\r\n\r\n```python\r\n    def scatter(...):\r\n    \"\"\"\r\n    ...\r\n\r\n        Parameters\r\n        ----------\r\n        data : list, dict, or object\r\n            Data to scatter out to workers.  Output type matches input type.\r\n        workers : list of tuples (optional)\r\n    \"\"\"\r\n```\r\n\r\nin the rendered HTML, the separation between parameter name and type is gone:\r\nhttps://distributed.dask.org/en/latest/api.html?highlight=scatter#distributed.Client.scatter\r\n\r\n![Screenshot from 2021-10-29 11-16-47](https://user-images.githubusercontent.com/6213168/139418346-3772c1e6-2ee1-4b2f-9b48-792c1f685aae.png)\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5478/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/5109", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/5109/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/5109/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/5109/events", "html_url": "https://github.com/dask/distributed/issues/5109", "id": 951504835, "node_id": "MDU6SXNzdWU5NTE1MDQ4MzU=", "number": 5109, "title": "RecursionError: maximum recursion depth exceeded while calling a Python object", "user": {"login": "fjetter", "id": 8629629, "node_id": "MDQ6VXNlcjg2Mjk2Mjk=", "avatar_url": "https://avatars.githubusercontent.com/u/8629629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjetter", "html_url": "https://github.com/fjetter", "followers_url": "https://api.github.com/users/fjetter/followers", "following_url": "https://api.github.com/users/fjetter/following{/other_user}", "gists_url": "https://api.github.com/users/fjetter/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjetter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjetter/subscriptions", "organizations_url": "https://api.github.com/users/fjetter/orgs", "repos_url": "https://api.github.com/users/fjetter/repos", "events_url": "https://api.github.com/users/fjetter/events{/privacy}", "received_events_url": "https://api.github.com/users/fjetter/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 1889912634, "node_id": "MDU6TGFiZWwxODg5OTEyNjM0", "url": "https://api.github.com/repos/dask/distributed/labels/flaky%20test", "name": "flaky test", "color": "f9d0c4", "default": false, "description": "Intermittent failures on CI."}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-07-23T11:43:19Z", "updated_at": "2022-02-16T13:13:44Z", "closed_at": "2022-02-16T13:13:44Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "I've seen some RecursionErrors popping up in CI while distributed profile is running, see https://github.com/dask/distributed/pull/5103/checks?check_run_id=3142776970", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/5109/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/5109/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/4931", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/4931/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/4931/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/4931/events", "html_url": "https://github.com/dask/distributed/issues/4931", "id": 924961521, "node_id": "MDU6SXNzdWU5MjQ5NjE1MjE=", "number": 4931, "title": "Getting error concurrent.futures._base.CancelledError: single_cutoff_forecast using fbprophet", "user": {"login": "luanmota", "id": 50594316, "node_id": "MDQ6VXNlcjUwNTk0MzE2", "avatar_url": "https://avatars.githubusercontent.com/u/50594316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luanmota", "html_url": "https://github.com/luanmota", "followers_url": "https://api.github.com/users/luanmota/followers", "following_url": "https://api.github.com/users/luanmota/following{/other_user}", "gists_url": "https://api.github.com/users/luanmota/gists{/gist_id}", "starred_url": "https://api.github.com/users/luanmota/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luanmota/subscriptions", "organizations_url": "https://api.github.com/users/luanmota/orgs", "repos_url": "https://api.github.com/users/luanmota/repos", "events_url": "https://api.github.com/users/luanmota/events{/privacy}", "received_events_url": "https://api.github.com/users/luanmota/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 1655256870, "node_id": "MDU6TGFiZWwxNjU1MjU2ODcw", "url": "https://api.github.com/repos/dask/distributed/labels/needs%20info", "name": "needs info", "color": "d0cfcc", "default": false, "description": "Needs further information from the user"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2021-06-18T14:28:57Z", "updated_at": "2021-10-14T14:39:20Z", "closed_at": "2021-10-14T14:39:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey guys! So I'm using prophet with dask and I'm getting stuck in this error:\r\n\r\n```\r\nImporting plotly failed. Interactive plots will not work.\r\nC:\\Users\\luanm\\Anaconda3\\envs\\modelos-de-forecast\\lib\\site-packages\\distributed\\client.py:1130: VersionMismatchWarning: Mismatched versions found\r\n\r\n+---------+----------------+---------------+---------------+\r\n| Package | client         | scheduler     | workers       |\r\n+---------+----------------+---------------+---------------+\r\n| numpy   | 1.20.2         | 1.19.1        | 1.19.1        |\r\n| python  | 3.8.10.final.0 | 3.8.5.final.0 | 3.8.5.final.0 |\r\n+---------+----------------+---------------+---------------+\r\n  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\r\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\r\nInitial log joint probability = -24.2201\r\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \r\n      81       507.814  9.69009e-009       4988.52      0.2792      0.2792      101   \r\nOptimization terminated normally: \r\n  Convergence detected: absolute parameter change was below tolerance\r\nINFO:prophet:Making 17 forecasts with cutoffs between 2012-02-08 00:00:00 and 2019-12-28 00:00:00\r\nINFO:prophet:Applying in parallel with <Client: 'tls://10.6.61.225:46688' processes=2 threads=4, memory=8.59 GB>\r\nTraceback (most recent call last):\r\n  File \"C:/Users/luanm/Desktop/SES/modelos-de-forecast/app/main.py\", line 5, in <module>\r\n    resposta = prophet_coss_validation()\r\n  File \"C:\\Users\\luanm\\Desktop\\SES\\modelos-de-forecast\\app\\Prophet.py\", line 53, in prophet_coss_validation\r\n    df_cv = prophet.diagnostics.cross_validation(m, initial='730 days', period='180 days', horizon = '365 days', parallel='dask')\r\n  File \"C:\\Users\\luanm\\Anaconda3\\envs\\modelos-de-forecast\\lib\\site-packages\\prophet\\diagnostics.py\", line 193, in cross_validation\r\n    predicts = pool.gather(predicts)\r\n  File \"C:\\Users\\luanm\\Anaconda3\\envs\\modelos-de-forecast\\lib\\site-packages\\distributed\\client.py\", line 1987, in gather\r\n    return self.sync(\r\n  File \"C:\\Users\\luanm\\Anaconda3\\envs\\modelos-de-forecast\\lib\\site-packages\\distributed\\client.py\", line 833, in sync\r\n    return sync(\r\n  File \"C:\\Users\\luanm\\Anaconda3\\envs\\modelos-de-forecast\\lib\\site-packages\\distributed\\utils.py\", line 339, in sync\r\n    raise exc.with_traceback(tb)\r\n  File \"C:\\Users\\luanm\\Anaconda3\\envs\\modelos-de-forecast\\lib\\site-packages\\distributed\\utils.py\", line 323, in f\r\n    result[0] = yield future\r\n  File \"C:\\Users\\luanm\\Anaconda3\\envs\\modelos-de-forecast\\lib\\site-packages\\tornado\\gen.py\", line 735, in run\r\n    value = future.result()\r\n  File \"C:\\Users\\luanm\\Anaconda3\\envs\\modelos-de-forecast\\lib\\site-packages\\distributed\\client.py\", line 1853, in _gather\r\n    raise exc\r\nconcurrent.futures._base.CancelledError: single_cutoff_forecast-9c16d68592673fa9596cbf0d96de9258\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n\r\nMy distributed version is 2.26.0, and I'm using the example code from prophet site to hyperparameter tuning:\r\n\r\n```Python\r\nimport itertools\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nparam_grid = {  \r\n    'changepoint_prior_scale': [0.001, 0.01, 0.1, 0.5],\r\n    'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0],\r\n}\r\n\r\n#Generate all combinations of parameters\r\nall_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\r\nrmses = []  # Store the RMSEs for each params here\r\n\r\n#Use cross validation to evaluate all parameters\r\nfor params in all_params:\r\n    m = Prophet(**params).fit(df)  # Fit model with given params\r\n    df_cv = cross_validation(m, cutoffs=cutoffs, horizon='30 days', parallel=\"dask\")\r\n    df_p = performance_metrics(df_cv, rolling_window=1)\r\n    rmses.append(df_p['rmse'].values[0])\r\n\r\n#Find the best parameters\r\ntuning_results = pd.DataFrame(all_params)\r\ntuning_results['rmse'] = rmses\r\nprint(tuning_results)\r\n```\r\n\r\n- Prophet version: 1.0.1\r\n- Dask version: 2.26.0\r\n- Python version: 3.8.10.final.0\r\n- Operating System: Windows\r\n- Install method (conda, pip, source): conda\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/4931/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/4931/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/4884", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/4884/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/4884/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/4884/events", "html_url": "https://github.com/dask/distributed/issues/4884", "id": 912493211, "node_id": "MDU6SXNzdWU5MTI0OTMyMTE=", "number": 4884, "title": "Restart worker on CommClosedError", "user": {"login": "chrisroat", "id": 1053153, "node_id": "MDQ6VXNlcjEwNTMxNTM=", "avatar_url": "https://avatars.githubusercontent.com/u/1053153?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chrisroat", "html_url": "https://github.com/chrisroat", "followers_url": "https://api.github.com/users/chrisroat/followers", "following_url": "https://api.github.com/users/chrisroat/following{/other_user}", "gists_url": "https://api.github.com/users/chrisroat/gists{/gist_id}", "starred_url": "https://api.github.com/users/chrisroat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chrisroat/subscriptions", "organizations_url": "https://api.github.com/users/chrisroat/orgs", "repos_url": "https://api.github.com/users/chrisroat/repos", "events_url": "https://api.github.com/users/chrisroat/events{/privacy}", "received_events_url": "https://api.github.com/users/chrisroat/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 1655256870, "node_id": "MDU6TGFiZWwxNjU1MjU2ODcw", "url": "https://api.github.com/repos/dask/distributed/labels/needs%20info", "name": "needs info", "color": "d0cfcc", "default": false, "description": "Needs further information from the user"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2021-06-05T23:08:55Z", "updated_at": "2021-07-22T20:46:04Z", "closed_at": "2021-07-22T20:46:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "On a dask-gateway GKE cluster, I have a few workers that stop processing and the logs show some combination of CommClosedError, missing dependency warnings, and garbage collection.  The scheduler seems happy with the worker, as the \"Last seen\" remains up to date.  \r\n\r\nI've attached an example log.  This worker was still in \"processing\" for its current task, which I think is sub-second, after 30 minutes.   I killed the worker and the graph backed up to redo the lost work, and  eventually completed.\r\n\r\nI have written dask-cluster-manager jobs which restart schedulers leaking memory.  I see there is a `client.get_scheduler_logs()`, which could be parsed to detect this.  Or is there some way to detect this and restart the worker (besides me searching the GKE dashboard and manually doing it)?  Is there a setting that can be used to somehow mitigate this?  The `lifetime.{duration,stagger,restart}` seem like a last resort.   \r\n\r\n<details>\r\n\r\n<summary>\r\nLogs\r\n</summary>\r\n\r\n<pre>\r\n\r\n2021-06-05T22:10:08.218039578Z + '[' '' ']'\r\n2021-06-05T22:10:08.218953428Z + '[' -e /opt/app/environment.yml ']'\r\n2021-06-05T22:10:08.219179401Z no environment.yml\r\n2021-06-05T22:10:08.219232124Z + echo 'no environment.yml'\r\n2021-06-05T22:10:08.219250341Z + '[' '' ']'\r\n2021-06-05T22:10:08.219255783Z + '[' '' ']'\r\n2021-06-05T22:10:08.219260917Z + exec dask-worker tls://dask-e85d926daa1646c2a782e2cf4ea00f25.starmap:8786 --dashboard-address :8787 --name dask-worker-e85d926daa1646c2a782e2cf4ea00f25-kp6lp --nthreads 1 --memory-limit 7516192768\r\n2021-06-05T22:10:09.406322052Z distributed.nanny - INFO -         Start Nanny at: 'tls://10.4.91.2:44355'\r\n2021-06-05T22:10:11.295639846Z distributed.worker - INFO -       Start worker at:      tls://10.4.91.2:46259\r\n2021-06-05T22:10:11.296092345Z distributed.worker - INFO -          Listening to:      tls://10.4.91.2:46259\r\n2021-06-05T22:10:11.296168743Z distributed.worker - INFO -          dashboard at:             10.4.91.2:8787\r\n2021-06-05T22:10:11.296178429Z distributed.worker - INFO - Waiting to connect to: tls://dask-e85d926daa1646c2a782e2cf4ea00f25.starmap:8786\r\n2021-06-05T22:10:11.296184092Z distributed.worker - INFO - -------------------------------------------------\r\n2021-06-05T22:10:11.296189147Z distributed.worker - INFO -               Threads:                          1\r\n2021-06-05T22:10:11.296222187Z distributed.worker - INFO -                Memory:                   7.00 GiB\r\n2021-06-05T22:10:11.296260851Z distributed.worker - INFO -       Local Directory: /workdir/dask-worker-space/worker-jrhd2vn0\r\n2021-06-05T22:10:11.296284668Z distributed.worker - INFO - -------------------------------------------------\r\n2021-06-05T22:10:11.344733302Z distributed.worker - INFO - Starting Worker plugin <distributed.client._WorkerSetupPlugin object at 0-bfd8f69b-5b98-4ebd-a664-bfd8b2f9bc5d\r\n2021-06-05T22:10:11.346146079Z distributed.worker - INFO -         Registered to: tls://dask-e85d926daa1646c2a782e2cf4ea00f25.starmap:8786\r\n2021-06-05T22:10:11.346253619Z distributed.worker - INFO - -------------------------------------------------\r\n2021-06-05T22:10:11.347449156Z distributed.core - INFO - Starting established connection\r\n2021-06-05T22:10:40.508654171Z distributed.comm.tcp - INFO - Connection closed before handshake completed\r\n2021-06-05T22:13:31.952504492Z distributed.utils_perf - INFO - full garbage collection released 160.61 MiB from 472 reference cycles (threshold: 9.54 MiB)\r\n2021-06-05T22:13:38.009084289Z distributed.comm.tcp - INFO - Connection closed before handshake completed\r\n2021-06-05T22:13:43.654490665Z distributed.comm.tcp - INFO - Connection closed before handshake completed\r\n2021-06-05T22:14:45.191432862Z distributed.worker - ERROR - Worker stream died during communication: tls://10.4.86.3:45971\r\n2021-06-05T22:14:45.191488484Z Traceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 200, in read\r\n    n = await stream.read_into(frames)\r\ntornado.iostream.StreamClosedError: Stream is closed\r\n2021-06-05T22:14:45.191531357Z \r\n2021-06-05T22:14:45.191536179Z The above exception was the direct cause of the following exception:\r\n2021-06-05T22:14:45.191541381Z \r\n2021-06-05T22:14:45.191546152Z Traceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/worker.py\", line 2189, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/worker.py\", line 3475, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/worker.py\", line 3455, in _get_data\r\n    response = await send_recv(\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/core.py\", line 645, in send_recv\r\n    response = await comm.read(deserializers=deserializers)\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 206, in read\r\n    convert_stream_closed_error(self, e)\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 128, in convert_stream_closed_error\r\n    raise CommClosedError(\"in %s: %s\" % (obj, exc)) from exc\r\ndistributed.comm.core.CommClosedError: in <closed TLS>: Stream is closed\r\n2021-06-05T22:14:45.191741414Z distributed.worker - INFO - Can't find dependencies for key ('rechunk-merge-transpose-654fb163d718a0167f4124e19d50f10c', 5, 2, 3, 0)\r\n2021-06-05T22:14:45.202056511Z distributed.worker - INFO - Can't find dependencies for key ('rechunk-split-1661c99969731fa78455440409ca42b2', 5746)\r\n2021-06-05T22:14:46.034760748Z distributed.worker - INFO - Dependent not found: ('getitem-421b304eb1d88a03fa4a18a7ad28ada7', 1, 0, 12, 15) 0 .  Asking scheduler\r\n2021-06-05T22:15:14.977635691Z distributed.utils_perf - INFO - full garbage collection released 60.92 MiB from 667 reference cycles (threshold: 9.54 MiB)\r\n2021-06-05T22:15:16.577678892Z distributed.worker - ERROR - Worker stream died during communication: tls://10.4.86.2:44013\r\n2021-06-05T22:15:16.577729765Z OSError: [Errno 113] No route to host\r\n\r\n2021-06-05T22:15:16.577771666Z The above exception was the direct cause of the following exception:\r\n2021-06-05T22:15:16.577776322Z \r\n2021-06-05T22:15:16.577781272Z Traceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/comm/core.py\", line 285, in connect\r\n    comm = await asyncio.wait_for(\r\n  File \"/opt/conda/lib/python3.8/asyncio/tasks.py\", line 494, in wait_for\r\n    return fut.result()\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 391, in connect\r\n    convert_stream_closed_error(self, e)\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/comm/tcp.py\", line 124, in convert_stream_closed_error\r\n    raise CommClosedError(\r\ndistributed.comm.core.CommClosedError: in <distributed.comm.tcp.TLSConnector object at 0x7f2acadb4a60>: OSError: [Errno 113] No route to host\r\n2021-06-05T22:15:16.577837957Z \r\n2021-06-05T22:15:16.577842845Z The above exception was the direct cause of the following exception:\r\n2021-06-05T22:15:16.577850215Z \r\n2021-06-05T22:15:16.577860139Z Traceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/worker.py\", line 2189, in gather_dep\r\n    response = await get_data_from_worker(\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/worker.py\", line 3475, in get_data_from_worker\r\n    return await retry_operation(_get_data, operation=\"get_data_from_worker\")\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/utils_comm.py\", line 385, in retry_operation\r\n    return await retry(\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/utils_comm.py\", line 370, in retry\r\n    return await coro()\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/worker.py\", line 3452, in _get_data\r\n    comm = await rpc.connect(worker)\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/core.py\", line 1010, in connect\r\n    comm = await connect(\r\n  File \"/opt/conda/lib/python3.8/site-packages/distributed/comm/core.py\", line 309, in connect\r\n    raise IOError(\r\nOSError: Timed out trying to connect to tls://10.4.86.2:44013 after 10 s\r\n2021-06-05T22:15:24.435351939Z distributed.utils_perf - INFO - full garbage collection released 229.12 MiB from 278 reference cycles (threshold: 9.54 MiB)\r\n2021-06-05T22:15:41.568841350Z distributed.utils_perf - INFO - full garbage collection released 66.56 MiB from 373 reference cycles (threshold: 9.54 MiB)\r\n2021-06-05T22:15:52.363204788Z distributed.utils_perf - INFO - full garbage collection released 143.41 MiB from 114 reference cycles (threshold: 9.54 MiB)\r\n2021-06-05T22:16:00.234860246Z distributed.comm.tcp - WARNING - Listener on 'tls://10.4.91.2:46259': TLS handshake failed with remote 'tls://10.4.95.4:41278': EOF occurred in violation of protocol (_ssl.c:1131)\r\n\r\n</pre>\r\n\r\n</details>", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/4884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/4884/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/4724", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/4724/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/4724/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/4724/events", "html_url": "https://github.com/dask/distributed/issues/4724", "id": 863154513, "node_id": "MDU6SXNzdWU4NjMxNTQ1MTM=", "number": 4724, "title": "Cluster hangs with a few tasks in \"processing\" state but no cpu load on any workers", "user": {"login": "GFleishman", "id": 8507206, "node_id": "MDQ6VXNlcjg1MDcyMDY=", "avatar_url": "https://avatars.githubusercontent.com/u/8507206?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GFleishman", "html_url": "https://github.com/GFleishman", "followers_url": "https://api.github.com/users/GFleishman/followers", "following_url": "https://api.github.com/users/GFleishman/following{/other_user}", "gists_url": "https://api.github.com/users/GFleishman/gists{/gist_id}", "starred_url": "https://api.github.com/users/GFleishman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GFleishman/subscriptions", "organizations_url": "https://api.github.com/users/GFleishman/orgs", "repos_url": "https://api.github.com/users/GFleishman/repos", "events_url": "https://api.github.com/users/GFleishman/events{/privacy}", "received_events_url": "https://api.github.com/users/GFleishman/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 1655256870, "node_id": "MDU6TGFiZWwxNjU1MjU2ODcw", "url": "https://api.github.com/repos/dask/distributed/labels/needs%20info", "name": "needs info", "color": "d0cfcc", "default": false, "description": "Needs further information from the user"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2021-04-09T16:55:10Z", "updated_at": "2021-10-14T14:44:05Z", "closed_at": "2021-10-14T14:44:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "This problem is stochastic. It seems to occur more frequently when there is more sharing of data between workers. `map_overlap` calls seem particularly problematic.\r\n\r\nCluster is set up using `dask-jobqueue.LSFCluster` and `dask.distributed.Client`\r\n\r\n```python\r\ncluster = LSFCluster(\r\n    cores, ncpus, memory, mem,\r\n    walltime=walltime,\r\n    env_extra=env_extra,\r\n    **kwargs,\r\n)\r\nclient = Client(cluster)\r\ncluster.scale(job=njobs)  # number of workers\r\n```\r\n\r\nWorkers are all allocated properly, bash scripts invoking LSF all seem fine. The task graph starts to execute, but then gets hung up and sits indefinitely in this type of state:\r\n\r\n<img width=\"1619\" alt=\"Screen Shot 2021-04-09 at 12 26 36 PM\" src=\"https://user-images.githubusercontent.com/8507206/114213199-b212f980-9930-11eb-9e02-2eff750519a0.png\">\r\n\r\n<img width=\"1619\" alt=\"Screen Shot 2021-04-09 at 12 27 24 PM\" src=\"https://user-images.githubusercontent.com/8507206/114213258-c3f49c80-9930-11eb-8251-4d2cc510e849.png\">\r\n\r\nNo workers show any cpu activity (2-4% for all workers). `env_extra` above makes sure all MKL, BLAS, and OpenMP environment variables are set to 2 threads per core (should be fine with hyper threading?).\r\n\r\nWhen I click on the red task on the left of the graph I see:\r\n[hung_cluster_last_task_left.pdf](https://github.com/dask/dask/files/6287304/hung_cluster_last_task_left.pdf)\r\n\r\nWhen I click on the red task on the right of the graph (second to last column) I see:\r\n[hung_cluster_last_task.pdf](https://github.com/dask/dask/files/6287305/hung_cluster_last_task.pdf)\r\n\r\nFor the red task on the right, the two \"workers with data\" show:\r\n\r\n<img width=\"1619\" alt=\"Screen Shot 2021-04-09 at 12 28 30 PM\" src=\"https://user-images.githubusercontent.com/8507206/114214276-1aaea600-9932-11eb-86dc-2f18e70b78fa.png\">\r\n\r\n<img width=\"1619\" alt=\"Screen Shot 2021-04-09 at 12 28 32 PM\" src=\"https://user-images.githubusercontent.com/8507206/114214283-1d110000-9932-11eb-9411-1ebe5942278b.png\">\r\n\r\n\r\nI've let these hang for upwards of 30 minutes with no meaningful cpu activity on any workers before killing the cluster manually. I can't let it run any longer because I'm paying for cluster time so I don't know if it's just (intractably) slow or totally hung. Comparatively the entire rest of the task graph was executed in less than 180 seconds.\r\n\r\nAny pointers as to what could be causing this or how to permanently avoid it would be really appreciated.\r\n\r\n\r\n- Dask version:    2020.12.0\r\n- Python version:    3.8.5\r\n- Operating System:    CentOS\r\n- Install method (conda, pip, source):    pip\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/4724/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/4724/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/4566", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/4566/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/4566/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/4566/events", "html_url": "https://github.com/dask/distributed/issues/4566", "id": 824398888, "node_id": "MDU6SXNzdWU4MjQzOTg4ODg=", "number": 4566, "title": "ECSCluster raises error on exit for scheduler that has already stopped", "user": {"login": "kinghuang", "id": 27340, "node_id": "MDQ6VXNlcjI3MzQw", "avatar_url": "https://avatars.githubusercontent.com/u/27340?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kinghuang", "html_url": "https://github.com/kinghuang", "followers_url": "https://api.github.com/users/kinghuang/followers", "following_url": "https://api.github.com/users/kinghuang/following{/other_user}", "gists_url": "https://api.github.com/users/kinghuang/gists{/gist_id}", "starred_url": "https://api.github.com/users/kinghuang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kinghuang/subscriptions", "organizations_url": "https://api.github.com/users/kinghuang/orgs", "repos_url": "https://api.github.com/users/kinghuang/repos", "events_url": "https://api.github.com/users/kinghuang/events{/privacy}", "received_events_url": "https://api.github.com/users/kinghuang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2021-03-05T22:53:50Z", "updated_at": "2021-03-29T20:49:56Z", "closed_at": "2021-03-29T20:49:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "**What happened**:\r\n\r\nUsing a Python REPL, if I create a Dask cluster using ECSCluster, wait for the scheduler to shut down from inactivity, then exit Python, an error is thrown by the exit handler because the scheduler is no longer reachable.\r\n\r\n```python\r\n>>> exit()\r\ntornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7fb576979580>>, <Task finished name='Task-509' coro=<SpecCluster._correct_state_internal() done, defined at /usr/local/lib/python3.8/site-packages/distributed/deploy/spec.py:325> exception=OSError('Timed out trying to connect to tcp://10.53.13.246:8786 after 10 s')>)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/comm/core.py\", line 286, in connect\r\n    comm = await asyncio.wait_for(\r\n  File \"/usr/local/lib/python3.8/asyncio/tasks.py\", line 498, in wait_for\r\n    raise exceptions.TimeoutError()\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/tornado/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"/usr/local/lib/python3.8/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\r\n    future.result()\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 406, in _close\r\n    await self._correct_state()\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 333, in _correct_state_internal\r\n    await self.scheduler_comm.retire_workers(workers=list(to_close))\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/core.py\", line 788, in send_recv_from_rpc\r\n    comm = await self.live_comm()\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/core.py\", line 746, in live_comm\r\n    comm = await connect(\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/comm/core.py\", line 308, in connect\r\n    raise IOError(\r\nOSError: Timed out trying to connect to tcp://10.53.13.246:8786 after 10 s\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/comm/core.py\", line 286, in connect\r\n    comm = await asyncio.wait_for(\r\n  File \"/usr/local/lib/python3.8/asyncio/tasks.py\", line 498, in wait_for\r\n    raise exceptions.TimeoutError()\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 652, in close_clusters\r\n    cluster.close(timeout=10)\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/deploy/cluster.py\", line 110, in close\r\n    return self.sync(self._close, callback_timeout=timeout)\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/deploy/cluster.py\", line 189, in sync\r\n    return sync(self.loop, func, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/utils.py\", line 340, in sync\r\n    raise exc.with_traceback(tb)\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/utils.py\", line 324, in f\r\n    result[0] = yield future\r\n  File \"/usr/local/lib/python3.8/site-packages/tornado/gen.py\", line 762, in run\r\n    value = future.result()\r\n  File \"/usr/local/lib/python3.8/site-packages/tornado/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"/usr/local/lib/python3.8/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\r\n    future.result()\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 406, in _close\r\n    await self._correct_state()\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 333, in _correct_state_internal\r\n    await self.scheduler_comm.retire_workers(workers=list(to_close))\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/core.py\", line 788, in send_recv_from_rpc\r\n    comm = await self.live_comm()\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/core.py\", line 746, in live_comm\r\n    comm = await connect(\r\n  File \"/usr/local/lib/python3.8/site-packages/distributed/comm/core.py\", line 308, in connect\r\n    raise IOError(\r\nOSError: Timed out trying to connect to tcp://10.53.13.246:8786 after 10 s\r\n```\r\n\r\n**What you expected to happen**:\r\n\r\nThe exit handler should ignore the already finished scheduler task, and continue with any other clean up as necessary.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\ncluster = ECSCluster(\u2026)\r\n\r\n# wait for scheduler to exit from inactivity\r\n\r\nexit()\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nNone.\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2012.2.0\r\n- Python version: 3.8.6\r\n- Operating System: Debian (python:3.8 Docker image)\r\n- Install method (conda, pip, source): pip", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/4566/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/4566/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/4446", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/4446/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/4446/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/4446/events", "html_url": "https://github.com/dask/distributed/issues/4446", "id": 791352438, "node_id": "MDU6SXNzdWU3OTEzNTI0Mzg=", "number": 4446, "title": "Resource constraints and work stealing not working as expected, the cluster ends up stuck", "user": {"login": "dtenchurin", "id": 50564343, "node_id": "MDQ6VXNlcjUwNTY0MzQz", "avatar_url": "https://avatars.githubusercontent.com/u/50564343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dtenchurin", "html_url": "https://github.com/dtenchurin", "followers_url": "https://api.github.com/users/dtenchurin/followers", "following_url": "https://api.github.com/users/dtenchurin/following{/other_user}", "gists_url": "https://api.github.com/users/dtenchurin/gists{/gist_id}", "starred_url": "https://api.github.com/users/dtenchurin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dtenchurin/subscriptions", "organizations_url": "https://api.github.com/users/dtenchurin/orgs", "repos_url": "https://api.github.com/users/dtenchurin/repos", "events_url": "https://api.github.com/users/dtenchurin/events{/privacy}", "received_events_url": "https://api.github.com/users/dtenchurin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 1655256870, "node_id": "MDU6TGFiZWwxNjU1MjU2ODcw", "url": "https://api.github.com/repos/dask/distributed/labels/needs%20info", "name": "needs info", "color": "d0cfcc", "default": false, "description": "Needs further information from the user"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2021-01-21T17:26:22Z", "updated_at": "2021-10-14T14:41:50Z", "closed_at": "2021-10-14T14:41:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nSince it is pretty hard to reproduce this at the moment, but the problem is happening often enough for us to stop ignoring it. \r\n\r\nSetup: \r\n1) 100 16 core boxes with single worker each started with --resources 'cpu=16'\r\n2) 3 task types: A: {'cpu':16}, B: {'cpu':4}, C: {'cpu':1}, tasks B and C are python subprocess calls, and task A is a function with a call to sklearn .fit().\r\n3) work stealing is enabled\r\n4) there are about 2000+ tasks in the queue normally\r\n\r\n\r\nObserved bug:\r\nAt some point the available_resources dictionary in the worker object becomes incorrect. Not sure what leads to this state but something is broken in the state transition mechanism so that it would look like a particular worker has more available resources than it should. This cases overscheduling of tasks into the worker.\r\n\r\nExample:\r\n2021-01-21 07:52:02.021 INFO:  {'tcp://172.24.0.144:39033': (9,)}, available_resources: {'cpu': 3.0}, used_calculated_outside_dask: 24\r\n2021-01-21 07:52:02.021 INFO:   i('A-f29492ccfd701faea20c09e4193c0db9', 'executing')\r\n2021-01-21 07:52:02.021 INFO:   i('C-2636fbf9c5cf07db2eb3088985835730', 'executing')\r\n2021-01-21 07:52:02.021 INFO:   i('C-28deaee4c9494ef2b376f8246894e5cb', 'executing')\r\n2021-01-21 07:52:02.021 INFO:   i('C-2561aa2b50164e905a0582810c91d8c7', 'executing')\r\n2021-01-21 07:52:02.021 INFO:   i('C-c75dcc64cdc7a19890fd516fede9fa28', 'executing')\r\n2021-01-21 07:52:02.021 INFO:   i('C-75251f721ba1b411c80e7bb08fa79245', 'executing')\r\n2021-01-21 07:52:02.021 INFO:   i('C-3d87f41690c6f1a08941628371c6e491', 'executing')\r\n2021-01-21 07:52:02.021 INFO:   i('C-2fc85a2cee66cefff07fa4d6db5e38bc', 'executing')\r\n2021-01-21 07:52:02.021 INFO:   i('C-7d4de834d8bb02db9a4f097e8815c5cc', 'executing')\r\n\r\nAn even bigger problem is that after some time of running these many tasks, the worker becomes unresponsive, and the scheduler starts reporting:\r\n`OSError: Timed out during handshake while connecting to tcp://172.24.0.147:35091 after 25 s`\r\nwhile clients get:\r\n`OSError: Timed out trying to connect to tcp://172.24.0.114:42141 after 25 s`\r\nAt this point the whole cluster becomes unusable, until we kill the timed out worker (which seems to be ok load/ram wise)\r\n\r\nWhen I logged in into worker and ran strace on the dask-worker process, I got this:\r\n\r\nLooks like similar to this issue: https://github.com/dask/distributed/issues/2880\r\nor this https://github.com/dask/distributed/issues/2446\r\n \r\nI will try to work towards a minimal reproducible example but it might be tough, and certainly will take a while. \r\n\r\nMaybe you have some suggestions in the meanwhile?\r\n\r\n**Environment**:\r\nPython 3.7.6 (default, Jan 30 2020, 03:53:38) \r\n[GCC 7.3.1 20180712 (Red Hat 7.3.1-6)] on linux\r\ndask.__version__\r\n'2021.01.0'`\r\n\r\n- Install method (conda, pip, source): pip", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/4446/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/4446/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/4415", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/4415/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/4415/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/4415/events", "html_url": "https://github.com/dask/distributed/issues/4415", "id": 783235757, "node_id": "MDU6SXNzdWU3ODMyMzU3NTc=", "number": 4415, "title": "CI blocked on `test_broken_worker_during_computation`", "user": {"login": "jacobtomlinson", "id": 1610850, "node_id": "MDQ6VXNlcjE2MTA4NTA=", "avatar_url": "https://avatars.githubusercontent.com/u/1610850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacobtomlinson", "html_url": "https://github.com/jacobtomlinson", "followers_url": "https://api.github.com/users/jacobtomlinson/followers", "following_url": "https://api.github.com/users/jacobtomlinson/following{/other_user}", "gists_url": "https://api.github.com/users/jacobtomlinson/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacobtomlinson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacobtomlinson/subscriptions", "organizations_url": "https://api.github.com/users/jacobtomlinson/orgs", "repos_url": "https://api.github.com/users/jacobtomlinson/repos", "events_url": "https://api.github.com/users/jacobtomlinson/events{/privacy}", "received_events_url": "https://api.github.com/users/jacobtomlinson/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2021-01-11T10:08:56Z", "updated_at": "2021-01-11T14:59:53Z", "closed_at": "2021-01-11T14:59:53Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Looks like CI is still blocked with `distributed/tests/test_failed_workers.py::test_broken_worker_during_computation` failing on Travis only.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/4415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/4415/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/4404", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/4404/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/4404/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/4404/events", "html_url": "https://github.com/dask/distributed/issues/4404", "id": 780463305, "node_id": "MDU6SXNzdWU3ODA0NjMzMDU=", "number": 4404, "title": "CI failing test_auto_normalize_collection ", "user": {"login": "jacobtomlinson", "id": 1610850, "node_id": "MDQ6VXNlcjE2MTA4NTA=", "avatar_url": "https://avatars.githubusercontent.com/u/1610850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacobtomlinson", "html_url": "https://github.com/jacobtomlinson", "followers_url": "https://api.github.com/users/jacobtomlinson/followers", "following_url": "https://api.github.com/users/jacobtomlinson/following{/other_user}", "gists_url": "https://api.github.com/users/jacobtomlinson/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacobtomlinson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacobtomlinson/subscriptions", "organizations_url": "https://api.github.com/users/jacobtomlinson/orgs", "repos_url": "https://api.github.com/users/jacobtomlinson/repos", "events_url": "https://api.github.com/users/jacobtomlinson/events{/privacy}", "received_events_url": "https://api.github.com/users/jacobtomlinson/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2021-01-06T10:53:04Z", "updated_at": "2021-01-07T23:15:45Z", "closed_at": "2021-01-07T23:15:45Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "The CI on master is currently failing. The bad test is `distributed/tests/test_client.py::test_auto_normalize_collection`.\r\n\r\nThe problem appears to be in `dask/dask`. Running a `git bisect` shows that this has been failing since dask/dask#7016.\r\n\r\nWith dask and distributed `2020.12.0` the test passes. But with `dask@master` all distributed versions that I've tested fail.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/4404/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/4404/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/4063", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/4063/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/4063/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/4063/events", "html_url": "https://github.com/dask/distributed/issues/4063", "id": 683635813, "node_id": "MDU6SXNzdWU2ODM2MzU4MTM=", "number": 4063, "title": "SSHCluster expects conda environment to be at the same path on all systems", "user": {"login": "jacobtomlinson", "id": 1610850, "node_id": "MDQ6VXNlcjE2MTA4NTA=", "avatar_url": "https://avatars.githubusercontent.com/u/1610850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacobtomlinson", "html_url": "https://github.com/jacobtomlinson", "followers_url": "https://api.github.com/users/jacobtomlinson/followers", "following_url": "https://api.github.com/users/jacobtomlinson/following{/other_user}", "gists_url": "https://api.github.com/users/jacobtomlinson/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacobtomlinson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacobtomlinson/subscriptions", "organizations_url": "https://api.github.com/users/jacobtomlinson/orgs", "repos_url": "https://api.github.com/users/jacobtomlinson/repos", "events_url": "https://api.github.com/users/jacobtomlinson/events{/privacy}", "received_events_url": "https://api.github.com/users/jacobtomlinson/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-21T15:18:07Z", "updated_at": "2020-09-17T14:34:30Z", "closed_at": "2020-09-17T14:34:30Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "**What happened**:\r\nWhen using `SSHCluster` on machines with different conda paths things fail.\r\n\r\n**What you expected to happen**:\r\nThe correct conda environment should be activated.\r\n\r\nIt seems that the `SSHCluster` tries to call the python executable directly at the same path that the current Python is running at. It may be more robust to activate the conda environment with the same name as the current one and use the `python` provided with that. It would also be good to be able to specify a conda environment in the kwargs.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```shell\r\n# On Host A\r\nconda create -n test -p /tmp/condaA python ipython dask\r\nconda activate /tmp/condaA/test\r\n\r\n# On Host B\r\nconda create -n test -p /tmp/condaB python ipython dask\r\nconda activate /tmp/condaB/test\r\n```\r\n\r\n```python\r\n# On Host A\r\n\r\nfrom dask.distributed import SSHCluster\r\ncluster = SSHCluster([\"localhost\", \"HostB\"]\r\n```\r\n\r\n```python-traceback\r\n...\r\ndistributed.deploy.ssh - INFO - env: \u2018/tmp/condaA/test/bin/python\u2019: No such file or directory\r\n...\r\n```\r\n\r\n<details>\r\n<summary>Full Example Traceback</summary>\r\n\r\nThis traceback is from a real example to the paths don't quite match the simplified example above.\r\n\r\n```python-traceback\r\ndistributed.deploy.ssh - INFO - distributed.scheduler - INFO - -----------------------------------------------\r\ndistributed.deploy.ssh - INFO - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\r\ndistributed.deploy.ssh - INFO - distributed.scheduler - INFO - -----------------------------------------------\r\ndistributed.deploy.ssh - INFO - distributed.scheduler - INFO - Clear task state\r\ndistributed.deploy.ssh - INFO - distributed.scheduler - INFO -   Scheduler at:   tcp://10.51.100.15:8786\r\ndistributed.deploy.ssh - INFO - env: \u2018/Users/jtomlinson/miniconda3/envs/coiledstream/bin/python\u2019: No such file or directory\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-45' coro=<_wrap_awaitable() done, defined at /Users/jtomlinson/miniconda3/envs/coiledstream/lib/python3.8/asyncio/tasks.py:677> exception=Exception('Worker failed to start')>\r\nTraceback (most recent call last):\r\n  File \"/Users/jtomlinson/miniconda3/envs/coiledstream/lib/python3.8/asyncio/tasks.py\", line 684, in _wrap_awaitable\r\n    return (yield from awaitable.__await__())\r\n  File \"/Users/jtomlinson/miniconda3/envs/coiledstream/lib/python3.8/site-packages/distributed/deploy/spec.py\", line 50, in _\r\n    await self.start()\r\n  File \"/Users/jtomlinson/miniconda3/envs/coiledstream/lib/python3.8/site-packages/distributed/deploy/ssh.py\", line 129, in start\r\n    raise Exception(\"Worker failed to start\")\r\nException: Worker failed to start\r\ndistributed.deploy.ssh - INFO - env: \u2018/Users/jtomlinson/miniconda3/envs/coiledstream/bin/python\u2019: No such file or directory\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-2-dbe5c7142de0> in <module>\r\n----> 1 cluster = SSHCluster([\"localhost\", \"10.51.0.32\"], connect_options=[{}, {\"username\": \"jacob\"}])\r\n\r\n~/miniconda3/envs/coiledstream/lib/python3.8/site-packages/distributed/deploy/ssh.py in SSHCluster(hosts, connect_options, worker_options, scheduler_options, worker_module, remote_python, **kwargs)\r\n    352         for i, host in enumerate(hosts[1:])\r\n    353     }\r\n--> 354     return SpecCluster(workers, scheduler, name=\"SSHCluster\", **kwargs)\r\n\r\n~/miniconda3/envs/coiledstream/lib/python3.8/site-packages/distributed/deploy/spec.py in __init__(self, workers, scheduler, worker, asynchronous, loop, security, silence_logs, name)\r\n    255             self._loop_runner.start()\r\n    256             self.sync(self._start)\r\n--> 257             self.sync(self._correct_state)\r\n    258\r\n    259     async def _start(self):\r\n\r\n~/miniconda3/envs/coiledstream/lib/python3.8/site-packages/distributed/deploy/cluster.py in sync(self, func, asynchronous, callback_timeout, *args, **kwargs)\r\n    167             return future\r\n    168         else:\r\n--> 169             return sync(self.loop, func, *args, **kwargs)\r\n    170\r\n    171     async def _get_logs(self, scheduler=True, workers=True):\r\n\r\n~/miniconda3/envs/coiledstream/lib/python3.8/site-packages/distributed/utils.py in sync(loop, func, callback_timeout, *args, **kwargs)\r\n    337     if error[0]:\r\n    338         typ, exc, tb = error[0]\r\n--> 339         raise exc.with_traceback(tb)\r\n    340     else:\r\n    341         return result[0]\r\n\r\n~/miniconda3/envs/coiledstream/lib/python3.8/site-packages/distributed/utils.py in f()\r\n    321             if callback_timeout is not None:\r\n    322                 future = asyncio.wait_for(future, callback_timeout)\r\n--> 323             result[0] = yield future\r\n    324         except Exception as exc:\r\n    325             error[0] = sys.exc_info()\r\n\r\n~/miniconda3/envs/coiledstream/lib/python3.8/site-packages/tornado/gen.py in run(self)\r\n    733\r\n    734                     try:\r\n--> 735                         value = future.result()\r\n    736                     except Exception:\r\n    737                         exc_info = sys.exc_info()\r\n\r\n~/miniconda3/envs/coiledstream/lib/python3.8/site-packages/distributed/deploy/spec.py in _correct_state_internal(self)\r\n    333                 for w in workers:\r\n    334                     w._cluster = weakref.ref(self)\r\n--> 335                     await w  # for tornado gen.coroutine support\r\n    336             self.workers.update(dict(zip(to_open, workers)))\r\n    337\r\n\r\n~/miniconda3/envs/coiledstream/lib/python3.8/site-packages/distributed/deploy/spec.py in _()\r\n     48             async with self.lock:\r\n     49                 if self.status == \"created\":\r\n---> 50                     await self.start()\r\n     51                     assert self.status == \"running\"\r\n     52             return self\r\n\r\n~/miniconda3/envs/coiledstream/lib/python3.8/site-packages/distributed/deploy/ssh.py in start(self)\r\n    127             line = await self.proc.stderr.readline()\r\n    128             if not line.strip():\r\n--> 129                 raise Exception(\"Worker failed to start\")\r\n    130             logger.info(line.strip())\r\n    131             if \"worker at\" in line:\r\n\r\nException: Worker failed to start\r\n```\r\n\r\n</details>\r\n\r\n**Environment**:\r\n\r\n- Dask version: 2.23.0\r\n- Python version: 3.8.5\r\n- Operating System: macOS 10.14 and Ubuntu 10.04\r\n- Install method (conda, pip, source): conda\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/4063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/4063/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/3839", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/3839/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/3839/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/3839/events", "html_url": "https://github.com/dask/distributed/issues/3839", "id": 627131295, "node_id": "MDU6SXNzdWU2MjcxMzEyOTU=", "number": 3839, "title": "Add helpful error message when passing a bad argument to Client", "user": {"login": "jacobtomlinson", "id": 1610850, "node_id": "MDQ6VXNlcjE2MTA4NTA=", "avatar_url": "https://avatars.githubusercontent.com/u/1610850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacobtomlinson", "html_url": "https://github.com/jacobtomlinson", "followers_url": "https://api.github.com/users/jacobtomlinson/followers", "following_url": "https://api.github.com/users/jacobtomlinson/following{/other_user}", "gists_url": "https://api.github.com/users/jacobtomlinson/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacobtomlinson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacobtomlinson/subscriptions", "organizations_url": "https://api.github.com/users/jacobtomlinson/orgs", "repos_url": "https://api.github.com/users/jacobtomlinson/repos", "events_url": "https://api.github.com/users/jacobtomlinson/events{/privacy}", "received_events_url": "https://api.github.com/users/jacobtomlinson/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 259867385, "node_id": "MDU6TGFiZWwyNTk4NjczODU=", "url": "https://api.github.com/repos/dask/distributed/labels/help%20wanted", "name": "help wanted", "color": "159818", "default": true, "description": null}, {"id": 969453080, "node_id": "MDU6TGFiZWw5Njk0NTMwODA=", "url": "https://api.github.com/repos/dask/distributed/labels/good%20first%20issue", "name": "good first issue", "color": "159818", "default": true, "description": "Clearly described and easy to accomplish. Good for beginners to the project."}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-29T09:27:12Z", "updated_at": "2020-06-07T17:05:28Z", "closed_at": "2020-06-07T17:05:28Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Today I made a typo which resulted in an unhelpful error message. I created a `Client` from a `LocalCluster` but forgot to actually instantiate the `LocalCluster`. I assume I'm not the first and won't be the last.\r\n\r\n```python\r\nfrom dask.distributed import LocalCluster, Client\r\ncluster = LocalCluster  # Note the missing ()\r\nclient = Client(cluster)\r\n```\r\n\r\nThis results in a `TypeError` complaining about expecting a string but get a property.\r\n\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-82a29784114b> in <module>\r\n      1 cluster = LocalCluster\r\n----> 2 client = Client(cluster)\r\n\r\n~/Projects/dask/distributed/distributed/client.py in __init__(self, address, loop, timeout, set_as_default, scheduler_file, security, asynchronous, name, heartbeat_interval, serializers, deserializers, extensions, direct_to_workers, connection_limit, **kwargs)\r\n    734             ext(self)\r\n    735 \r\n--> 736         self.start(timeout=timeout)\r\n    737         Client._instances.add(self)\r\n    738 \r\n\r\n~/Projects/dask/distributed/distributed/client.py in start(self, **kwargs)\r\n    938             self._started = asyncio.ensure_future(self._start(**kwargs))\r\n    939         else:\r\n--> 940             sync(self.loop, self._start, **kwargs)\r\n    941 \r\n    942     def __await__(self):\r\n\r\n~/Projects/dask/distributed/distributed/utils.py in sync(loop, func, callback_timeout, *args, **kwargs)\r\n    337     if error[0]:\r\n    338         typ, exc, tb = error[0]\r\n--> 339         raise exc.with_traceback(tb)\r\n    340     else:\r\n    341         return result[0]\r\n\r\n~/Projects/dask/distributed/distributed/utils.py in f()\r\n    321             if callback_timeout is not None:\r\n    322                 future = asyncio.wait_for(future, callback_timeout)\r\n--> 323             result[0] = yield future\r\n    324         except Exception as exc:\r\n    325             error[0] = sys.exc_info()\r\n\r\n~/miniconda3/envs/dask/lib/python3.7/site-packages/tornado/gen.py in run(self)\r\n    733 \r\n    734                     try:\r\n--> 735                         value = future.result()\r\n    736                     except Exception:\r\n    737                         exc_info = sys.exc_info()\r\n\r\n~/Projects/dask/distributed/distributed/client.py in _start(self, timeout, **kwargs)\r\n   1031 \r\n   1032         if self.scheduler is None:\r\n-> 1033             self.scheduler = self.rpc(address)\r\n   1034         self.scheduler_comm = None\r\n   1035 \r\n\r\n~/Projects/dask/distributed/distributed/core.py in __call__(self, addr, ip, port)\r\n    915     def __call__(self, addr=None, ip=None, port=None):\r\n    916         \"\"\" Cached rpc objects \"\"\"\r\n--> 917         addr = addr_from_args(addr=addr, ip=ip, port=port)\r\n    918         return PooledRPCCall(\r\n    919             addr, self, serializers=self.serializers, deserializers=self.deserializers\r\n\r\n~/Projects/dask/distributed/distributed/core.py in addr_from_args(addr, ip, port)\r\n    616     if isinstance(addr, tuple):\r\n    617         addr = unparse_host_port(*addr)\r\n--> 618     return normalize_address(addr)\r\n    619 \r\n    620 \r\n\r\n~/Projects/dask/distributed/distributed/comm/addressing.py in normalize_address(addr)\r\n     52     'tcp://[::1]'\r\n     53     \"\"\"\r\n---> 54     return unparse_address(*parse_address(addr))\r\n     55 \r\n     56 \r\n\r\n~/Projects/dask/distributed/distributed/comm/addressing.py in parse_address(addr, strict)\r\n     19     \"\"\"\r\n     20     if not isinstance(addr, str):\r\n---> 21         raise TypeError(\"expected str, got %r\" % addr.__class__.__name__)\r\n     22     scheme, sep, loc = addr.rpartition(\"://\")\r\n     23     if strict and not sep:\r\n\r\nTypeError: expected str, got 'property'\r\n```\r\n\r\nIt would be nice to catch this and raise a more helpful error about the kind of things that can be passed to `Client`. Perhaps even test if the thing passed in is a `Cluster` class rather than an instance of the class and give an error about instantiating.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/3839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/3839/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/3808", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/3808/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/3808/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/3808/events", "html_url": "https://github.com/dask/distributed/issues/3808", "id": 619873333, "node_id": "MDU6SXNzdWU2MTk4NzMzMzM=", "number": 3808, "title": "Incorrect GPU Memory Totals ", "user": {"login": "quasiben", "id": 1403768, "node_id": "MDQ6VXNlcjE0MDM3Njg=", "avatar_url": "https://avatars.githubusercontent.com/u/1403768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/quasiben", "html_url": "https://github.com/quasiben", "followers_url": "https://api.github.com/users/quasiben/followers", "following_url": "https://api.github.com/users/quasiben/following{/other_user}", "gists_url": "https://api.github.com/users/quasiben/gists{/gist_id}", "starred_url": "https://api.github.com/users/quasiben/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/quasiben/subscriptions", "organizations_url": "https://api.github.com/users/quasiben/orgs", "repos_url": "https://api.github.com/users/quasiben/repos", "events_url": "https://api.github.com/users/quasiben/events{/privacy}", "received_events_url": "https://api.github.com/users/quasiben/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-05-18T03:00:51Z", "updated_at": "2020-10-06T14:25:06Z", "closed_at": "2020-10-06T14:25:06Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "On the `/individual-gpu-memory` page, the GPU memory title value far exceeds what is used and available:\r\n\r\n<img width=\"1470\" alt=\"Screen Shot 2020-05-17 at 10 50 09 PM\" src=\"https://user-images.githubusercontent.com/1403768/82170031-4f84bb00-9891-11ea-997d-b065d4d213bb.png\">\r\n\r\nThis was executed on a DGX1 with 256GBs of \r\nGPU memory, not 2TBs.  Perhaps a counting issue within https://github.com/dask/distributed/blob/1bcbaeee4a92d57fb2d399c9580fce6480a6d7c9/distributed/dashboard/components/nvml.py#L136-L153", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/3808/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/3808/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/3380", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/3380/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/3380/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/3380/events", "html_url": "https://github.com/dask/distributed/issues/3380", "id": 551178218, "node_id": "MDU6SXNzdWU1NTExNzgyMTg=", "number": 3380, "title": "Cluster Map fails in JupyterLab", "user": {"login": "mrocklin", "id": 306380, "node_id": "MDQ6VXNlcjMwNjM4MA==", "avatar_url": "https://avatars.githubusercontent.com/u/306380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrocklin", "html_url": "https://github.com/mrocklin", "followers_url": "https://api.github.com/users/mrocklin/followers", "following_url": "https://api.github.com/users/mrocklin/following{/other_user}", "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions", "organizations_url": "https://api.github.com/users/mrocklin/orgs", "repos_url": "https://api.github.com/users/mrocklin/repos", "events_url": "https://api.github.com/users/mrocklin/events{/privacy}", "received_events_url": "https://api.github.com/users/mrocklin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": {"login": "jacobtomlinson", "id": 1610850, "node_id": "MDQ6VXNlcjE2MTA4NTA=", "avatar_url": "https://avatars.githubusercontent.com/u/1610850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacobtomlinson", "html_url": "https://github.com/jacobtomlinson", "followers_url": "https://api.github.com/users/jacobtomlinson/followers", "following_url": "https://api.github.com/users/jacobtomlinson/following{/other_user}", "gists_url": "https://api.github.com/users/jacobtomlinson/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacobtomlinson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacobtomlinson/subscriptions", "organizations_url": "https://api.github.com/users/jacobtomlinson/orgs", "repos_url": "https://api.github.com/users/jacobtomlinson/repos", "events_url": "https://api.github.com/users/jacobtomlinson/events{/privacy}", "received_events_url": "https://api.github.com/users/jacobtomlinson/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jacobtomlinson", "id": 1610850, "node_id": "MDQ6VXNlcjE2MTA4NTA=", "avatar_url": "https://avatars.githubusercontent.com/u/1610850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacobtomlinson", "html_url": "https://github.com/jacobtomlinson", "followers_url": "https://api.github.com/users/jacobtomlinson/followers", "following_url": "https://api.github.com/users/jacobtomlinson/following{/other_user}", "gists_url": "https://api.github.com/users/jacobtomlinson/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacobtomlinson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacobtomlinson/subscriptions", "organizations_url": "https://api.github.com/users/jacobtomlinson/orgs", "repos_url": "https://api.github.com/users/jacobtomlinson/repos", "events_url": "https://api.github.com/users/jacobtomlinson/events{/privacy}", "received_events_url": "https://api.github.com/users/jacobtomlinson/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-01-17T03:31:04Z", "updated_at": "2020-01-17T14:03:09Z", "closed_at": "2020-01-17T14:03:09Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "```python-traceback\r\n[I 19:28:51.865 LabApp] Websocket connection established to ws://127.0.0.1:8787/individual-workers/ws?bokeh-protocol-version=1.0&bokeh-session-id=AAmKR8Xkchs3PnkcPiYXu8lXPqe4YNvX2QLTPp5ZKMe7\r\nLabApp - ERROR - Uncaught exception \r\nTraceback (most recent call last):\r\n  File \"/home/mrocklin/miniconda/lib/python3.7/site-packages/tornado/websocket.py\", line 649, in _run_callback\r\n    result = callback(*args, **kwargs)\r\n  File \"/home/mrocklin/miniconda/lib/python3.7/site-packages/tornado/websocket.py\", line 1528, in on_message\r\n    return self._on_message(message)\r\n  File \"/home/mrocklin/miniconda/lib/python3.7/site-packages/tornado/websocket.py\", line 1534, in _on_message\r\n    self._on_message_callback(message)\r\n  File \"/home/mrocklin/miniconda/lib/python3.7/site-packages/jupyter_server_proxy/handlers.py\", line 271, in message_cb\r\n    self.write_message(message, binary=isinstance(message, bytes))\r\n  File \"/home/mrocklin/miniconda/lib/python3.7/site-packages/tornado/websocket.py\", line 339, in write_message\r\n    raise WebSocketClosedError()\r\ntornado.websocket.WebSocketClosedError\r\n```\r\n\r\n![Screenshot from 2020-01-16 19-29-15](https://user-images.githubusercontent.com/306380/72582156-b455e780-3896-11ea-9376-5c6ea5ab1267.png)\r\n\r\n\r\ncc @jacobtomlinson ", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/3380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/3380/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/2722", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/2722/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/2722/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/2722/events", "html_url": "https://github.com/dask/distributed/issues/2722", "id": 447210059, "node_id": "MDU6SXNzdWU0NDcyMTAwNTk=", "number": 2722, "title": "Info Page for Worker Dashboard returns 404", "user": {"login": "quasiben", "id": 1403768, "node_id": "MDQ6VXNlcjE0MDM3Njg=", "avatar_url": "https://avatars.githubusercontent.com/u/1403768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/quasiben", "html_url": "https://github.com/quasiben", "followers_url": "https://api.github.com/users/quasiben/followers", "following_url": "https://api.github.com/users/quasiben/following{/other_user}", "gists_url": "https://api.github.com/users/quasiben/gists{/gist_id}", "starred_url": "https://api.github.com/users/quasiben/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/quasiben/subscriptions", "organizations_url": "https://api.github.com/users/quasiben/orgs", "repos_url": "https://api.github.com/users/quasiben/repos", "events_url": "https://api.github.com/users/quasiben/events{/privacy}", "received_events_url": "https://api.github.com/users/quasiben/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-22T15:40:32Z", "updated_at": "2019-06-05T17:09:28Z", "closed_at": "2019-06-05T17:09:28Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "The info page (/info/main/workers.html) on for the worker dashboard is not working currently returns 404.   After https://github.com/dask/distributed/pull/2715 is merged we should track down the error and fix", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/2722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/2722/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/2696", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/2696/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/2696/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/2696/events", "html_url": "https://github.com/dask/distributed/issues/2696", "id": 443826842, "node_id": "MDU6SXNzdWU0NDM4MjY4NDI=", "number": 2696, "title": "Computing delayed None fails", "user": {"login": "alex-marty", "id": 2773493, "node_id": "MDQ6VXNlcjI3NzM0OTM=", "avatar_url": "https://avatars.githubusercontent.com/u/2773493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alex-marty", "html_url": "https://github.com/alex-marty", "followers_url": "https://api.github.com/users/alex-marty/followers", "following_url": "https://api.github.com/users/alex-marty/following{/other_user}", "gists_url": "https://api.github.com/users/alex-marty/gists{/gist_id}", "starred_url": "https://api.github.com/users/alex-marty/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alex-marty/subscriptions", "organizations_url": "https://api.github.com/users/alex-marty/orgs", "repos_url": "https://api.github.com/users/alex-marty/repos", "events_url": "https://api.github.com/users/alex-marty/events{/privacy}", "received_events_url": "https://api.github.com/users/alex-marty/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 969453080, "node_id": "MDU6TGFiZWw5Njk0NTMwODA=", "url": "https://api.github.com/repos/dask/distributed/labels/good%20first%20issue", "name": "good first issue", "color": "159818", "default": true, "description": "Clearly described and easy to accomplish. Good for beginners to the project."}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-05-14T10:08:38Z", "updated_at": "2019-05-15T14:35:44Z", "closed_at": "2019-05-15T14:35:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "I regularly find myself delaying data. I encountered an issue when delaying None: it works fine with simple dask schedulers like processes, but fails with distributed, at least with a local cluster:\r\n\r\n```\r\nfrom dask import delayed\r\nfrom dask.distributed import Client, LocalCluster\r\n\r\nclient = Client(LocalCluster())\r\n\r\nx = delayed(12)\r\nx.compute(scheduler=\"processes\")  # Returns: 12\r\nx.compute(scheduler=client)  # Returns: 12\r\n\r\ny = delayed(None)\r\ny.compute(scheduler=\"processes\")  # Returns: None\r\ny.compute(scheduler=client)  # Error: TypeError: 'NoneType' object is not callable\r\n```\r\n\r\nPassing the delayed value as an input to a delayed function yields the same behavior:\r\n\r\n```\r\ndelayed(str)(y).compute(scheduler=\"processes\")  # Returns: 'None'\r\ndelayed(str)(y).compute(scheduler=client)  # Error: TypeError: 'NoneType' object is not callable\r\n```\r\n\r\nFor now, the workaround I use for the purpose of my project is to detect and replace None values with a delayed function that returns None:\r\n\r\n```\r\ndef none_func():\r\n    return None\r\n\r\nz = delayed(none_func)()\r\nz.compute(scheduler=\"processes\")  # Returns: None\r\nz.compute(scheduler=client)  # Returns: None\r\n```\r\n\r\nBut I would be happy to switch back to simpler code.", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/2696/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/2696/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/2597", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/2597/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/2597/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/2597/events", "html_url": "https://github.com/dask/distributed/issues/2597", "id": 429992068, "node_id": "MDU6SXNzdWU0Mjk5OTIwNjg=", "number": 2597, "title": "\"distributed.protocol.core - CRITICAL - Failed to Serialize\" with PyArrow 0.13", "user": {"login": "johnbensnyder", "id": 7110081, "node_id": "MDQ6VXNlcjcxMTAwODE=", "avatar_url": "https://avatars.githubusercontent.com/u/7110081?v=4", "gravatar_id": "", "url": "https://api.github.com/users/johnbensnyder", "html_url": "https://github.com/johnbensnyder", "followers_url": "https://api.github.com/users/johnbensnyder/followers", "following_url": "https://api.github.com/users/johnbensnyder/following{/other_user}", "gists_url": "https://api.github.com/users/johnbensnyder/gists{/gist_id}", "starred_url": "https://api.github.com/users/johnbensnyder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/johnbensnyder/subscriptions", "organizations_url": "https://api.github.com/users/johnbensnyder/orgs", "repos_url": "https://api.github.com/users/johnbensnyder/repos", "events_url": "https://api.github.com/users/johnbensnyder/events{/privacy}", "received_events_url": "https://api.github.com/users/johnbensnyder/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 259867385, "node_id": "MDU6TGFiZWwyNTk4NjczODU=", "url": "https://api.github.com/repos/dask/distributed/labels/help%20wanted", "name": "help wanted", "color": "159818", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 56, "created_at": "2019-04-06T03:20:27Z", "updated_at": "2020-07-27T13:03:28Z", "closed_at": "2019-09-11T15:13:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Reading from Parquet is failing with PyArrow 0.13. Downgrading to PyArrow 0.12.1 seems to fix the problem. I've only encountered this when using the distributed client. Using a Dask dataframe by itself does not appear to be affected.\r\n\r\nFor example, \r\n\r\n```\r\nfrom distributed import LocalCluster, Client\r\nimport dask.dataframe as dd\r\nclient = Client(LocalCluster(diagnostics_port=('0.0.0.0', 8889), n_workers = 4))\r\nddf = dd.read_parquet('DATA/parquet/', engine = 'pyarrow')\r\nddf.set_index('index')\r\n```\r\n\r\nGives\r\n\r\n```\r\ndistributed.protocol.core - CRITICAL - Failed to Serialize\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/protocol/core.py\", line 54, in dumps\r\n    for key, value in data.items()\r\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/protocol/core.py\", line 55, in <dictcomp>\r\n    if type(value) is Serialize}\r\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/protocol/serialize.py\", line 163, in serialize\r\n    raise TypeError(msg, str(x)[:10000])\r\n```\r\n\r\n```\r\ndistributed.batched - ERROR - Error in batched write\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/batched.py\", line 94, in _background_send\r\n    on_error='raise')\r\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 729, in run\r\n    value = future.result()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 736, in run\r\n    yielded = self.gen.throw(*exc_info)  # type: ignore\r\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/tcp.py\", line 224, in write\r\n    'recipient': self._peer_addr})\r\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 729, in run\r\n    value = future.result()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 736, in run\r\n    yielded = self.gen.throw(*exc_info)  # type: ignore\r\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/utils.py\", line 50, in to_frames\r\n    res = yield offload(_to_frames)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 729, in run\r\n    value = future.result()\r\n  File \"/opt/conda/lib/python3.6/concurrent/futures/_base.py\", line 425, in result\r\n    return self.__get_result()\r\n  File \"/opt/conda/lib/python3.6/concurrent/futures/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\n  File \"/opt/conda/lib/python3.6/concurrent/futures/thread.py\", line 56, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/utils.py\", line 43, in _to_frames\r\n    context=context))\r\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/protocol/core.py\", line 54, in dumps\r\n    for key, value in data.items()\r\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/protocol/core.py\", line 55, in <dictcomp>\r\n    if type(value) is Serialize}\r\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/protocol/serialize.py\", line 163, in serialize\r\n    raise TypeError(msg, str(x)[:10000])\r\n```\r\n\r\nSimilarly, \r\n\r\n```\r\nfrom distributed import LocalCluster, Client\r\nimport dask.dataframe as dd\r\nclient = Client(LocalCluster(diagnostics_port=('0.0.0.0', 8889), n_workers = 4))\r\nddf = dd.read_parquet('DATA/parquet/', engine = 'pyarrow')\r\nddf.compute()\r\n```\r\n\r\nCauses this error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/opt/conda/lib/python3.6/site-packages/distributed/protocol/pickle.py in dumps(x)\r\n     37     try:\r\n---> 38         result = pickle.dumps(x, protocol=pickle.HIGHEST_PROTOCOL)\r\n     39         if len(result) < 1000:\r\n\r\nAttributeError: Can't pickle local object 'ParquetDataset._get_open_file_func.<locals>.open_file'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-a90291ebde1e> in <module>\r\n      1 ft_inp_ddf = dd.read_parquet('DATA/ft_14_15_inputs_parquet/', engine = 'pyarrow')\r\n----> 2 ft_inp_ddf.compute()\r\n\r\n/opt/conda/lib/python3.6/site-packages/dask/base.py in compute(self, **kwargs)\r\n    154         dask.base.compute\r\n    155         \"\"\"\r\n--> 156         (result,) = compute(self, traverse=False, **kwargs)\r\n    157         return result\r\n    158 \r\n\r\n/opt/conda/lib/python3.6/site-packages/dask/base.py in compute(*args, **kwargs)\r\n    396     keys = [x.__dask_keys__() for x in collections]\r\n    397     postcomputes = [x.__dask_postcompute__() for x in collections]\r\n--> 398     results = schedule(dsk, keys, **kwargs)\r\n    399     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    400 \r\n\r\n/opt/conda/lib/python3.6/site-packages/distributed/client.py in get(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\r\n   2318             retries=retries,\r\n   2319             user_priority=priority,\r\n-> 2320             actors=actors,\r\n   2321         )\r\n   2322         packed = pack_data(keys, futures)\r\n\r\n/opt/conda/lib/python3.6/site-packages/distributed/client.py in _graph_to_futures(self, dsk, keys, restrictions, loose_restrictions, priority, user_priority, resources, retries, fifo_timeout, actors)\r\n   2259 \r\n   2260             self._send_to_scheduler({'op': 'update-graph',\r\n-> 2261                                      'tasks': valmap(dumps_task, dsk3),\r\n   2262                                      'dependencies': dependencies,\r\n   2263                                      'keys': list(flatkeys),\r\n\r\n/opt/conda/lib/python3.6/site-packages/cytoolz/dicttoolz.pyx in cytoolz.dicttoolz.valmap()\r\n\r\n/opt/conda/lib/python3.6/site-packages/cytoolz/dicttoolz.pyx in cytoolz.dicttoolz.valmap()\r\n\r\n/opt/conda/lib/python3.6/site-packages/distributed/worker.py in dumps_task(task)\r\n   2769         elif not any(map(_maybe_complex, task[1:])):\r\n   2770             return {'function': dumps_function(task[0]),\r\n-> 2771                     'args': warn_dumps(task[1:])}\r\n   2772     return to_serialize(task)\r\n   2773 \r\n\r\n/opt/conda/lib/python3.6/site-packages/distributed/worker.py in warn_dumps(obj, dumps, limit)\r\n   2778 def warn_dumps(obj, dumps=pickle.dumps, limit=1e6):\r\n   2779     \"\"\" Dump an object to bytes, warn if those bytes are large \"\"\"\r\n-> 2780     b = dumps(obj)\r\n   2781     if not _warn_dumps_warned[0] and len(b) > limit:\r\n   2782         _warn_dumps_warned[0] = True\r\n\r\n/opt/conda/lib/python3.6/site-packages/distributed/protocol/pickle.py in dumps(x)\r\n     49     except Exception:\r\n     50         try:\r\n---> 51             return cloudpickle.dumps(x, protocol=pickle.HIGHEST_PROTOCOL)\r\n     52         except Exception as e:\r\n     53             logger.info(\"Failed to serialize %s. Exception: %s\", x, e)\r\n\r\n/opt/conda/lib/python3.6/site-packages/cloudpickle/cloudpickle.py in dumps(obj, protocol)\r\n    959     try:\r\n    960         cp = CloudPickler(file, protocol=protocol)\r\n--> 961         cp.dump(obj)\r\n    962         return file.getvalue()\r\n    963     finally:\r\n\r\n/opt/conda/lib/python3.6/site-packages/cloudpickle/cloudpickle.py in dump(self, obj)\r\n    265         self.inject_addons()\r\n    266         try:\r\n--> 267             return Pickler.dump(self, obj)\r\n    268         except RuntimeError as e:\r\n    269             if 'recursion' in e.args[0]:\r\n\r\n/opt/conda/lib/python3.6/pickle.py in dump(self, obj)\r\n    407         if self.proto >= 4:\r\n    408             self.framer.start_framing()\r\n--> 409         self.save(obj)\r\n    410         self.write(STOP)\r\n    411         self.framer.end_framing()\r\n\r\n/opt/conda/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/opt/conda/lib/python3.6/pickle.py in save_tuple(self, obj)\r\n    749         write(MARK)\r\n    750         for element in obj:\r\n--> 751             save(element)\r\n    752 \r\n    753         if id(obj) in memo:\r\n\r\n/opt/conda/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    519 \r\n    520         # Save the reduce() output and finally memoize the object\r\n--> 521         self.save_reduce(obj=obj, *rv)\r\n    522 \r\n    523     def persistent_id(self, obj):\r\n\r\n/opt/conda/lib/python3.6/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    632 \r\n    633         if state is not None:\r\n--> 634             save(state)\r\n    635             write(BUILD)\r\n    636 \r\n\r\n/opt/conda/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/opt/conda/lib/python3.6/pickle.py in save_dict(self, obj)\r\n    819 \r\n    820         self.memoize(obj)\r\n--> 821         self._batch_setitems(obj.items())\r\n    822 \r\n    823     dispatch[dict] = save_dict\r\n\r\n/opt/conda/lib/python3.6/pickle.py in _batch_setitems(self, items)\r\n    845                 for k, v in tmp:\r\n    846                     save(k)\r\n--> 847                     save(v)\r\n    848                 write(SETITEMS)\r\n    849             elif n:\r\n\r\n/opt/conda/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/opt/conda/lib/python3.6/site-packages/cloudpickle/cloudpickle.py in save_function(self, obj, name)\r\n    398             # func is nested\r\n    399             if lookedup_by_name is None or lookedup_by_name is not obj:\r\n--> 400                 self.save_function_tuple(obj)\r\n    401                 return\r\n    402 \r\n\r\n/opt/conda/lib/python3.6/site-packages/cloudpickle/cloudpickle.py in save_function_tuple(self, func)\r\n    592         if hasattr(func, '__qualname__'):\r\n    593             state['qualname'] = func.__qualname__\r\n--> 594         save(state)\r\n    595         write(pickle.TUPLE)\r\n    596         write(pickle.REDUCE)  # applies _fill_function on the tuple\r\n\r\n/opt/conda/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/opt/conda/lib/python3.6/pickle.py in save_dict(self, obj)\r\n    819 \r\n    820         self.memoize(obj)\r\n--> 821         self._batch_setitems(obj.items())\r\n    822 \r\n    823     dispatch[dict] = save_dict\r\n\r\n/opt/conda/lib/python3.6/pickle.py in _batch_setitems(self, items)\r\n    845                 for k, v in tmp:\r\n    846                     save(k)\r\n--> 847                     save(v)\r\n    848                 write(SETITEMS)\r\n    849             elif n:\r\n\r\n/opt/conda/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/opt/conda/lib/python3.6/pickle.py in save_list(self, obj)\r\n    779 \r\n    780         self.memoize(obj)\r\n--> 781         self._batch_appends(obj)\r\n    782 \r\n    783     dispatch[list] = save_list\r\n\r\n/opt/conda/lib/python3.6/pickle.py in _batch_appends(self, items)\r\n    806                 write(APPENDS)\r\n    807             elif n:\r\n--> 808                 save(tmp[0])\r\n    809                 write(APPEND)\r\n    810             # else tmp is empty, and we're done\r\n\r\n/opt/conda/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    519 \r\n    520         # Save the reduce() output and finally memoize the object\r\n--> 521         self.save_reduce(obj=obj, *rv)\r\n    522 \r\n    523     def persistent_id(self, obj):\r\n\r\n/opt/conda/lib/python3.6/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    632 \r\n    633         if state is not None:\r\n--> 634             save(state)\r\n    635             write(BUILD)\r\n    636 \r\n\r\n/opt/conda/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/opt/conda/lib/python3.6/pickle.py in save_dict(self, obj)\r\n    819 \r\n    820         self.memoize(obj)\r\n--> 821         self._batch_setitems(obj.items())\r\n    822 \r\n    823     dispatch[dict] = save_dict\r\n\r\n/opt/conda/lib/python3.6/pickle.py in _batch_setitems(self, items)\r\n    845                 for k, v in tmp:\r\n    846                     save(k)\r\n--> 847                     save(v)\r\n    848                 write(SETITEMS)\r\n    849             elif n:\r\n\r\n/opt/conda/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    494             reduce = getattr(obj, \"__reduce_ex__\", None)\r\n    495             if reduce is not None:\r\n--> 496                 rv = reduce(self.proto)\r\n    497             else:\r\n    498                 reduce = getattr(obj, \"__reduce__\", None)\r\n\r\n/opt/conda/lib/python3.6/site-packages/pyarrow/_parquet.cpython-36m-x86_64-linux-gnu.so in pyarrow._parquet.ParquetSchema.__reduce_cython__()\r\n\r\nTypeError: no default __reduce__ due to non-trivial __cinit__\r\n```", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/2597/reactions", "total_count": 4, "+1": 4, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/2597/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/2473", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/2473/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/2473/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/2473/events", "html_url": "https://github.com/dask/distributed/issues/2473", "id": 401541951, "node_id": "MDU6SXNzdWU0MDE1NDE5NTE=", "number": 2473, "title": "Closing a client sets dask.config 'shuffle' and 'scheduler' to None", "user": {"login": "gsakkis", "id": 291289, "node_id": "MDQ6VXNlcjI5MTI4OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/291289?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gsakkis", "html_url": "https://github.com/gsakkis", "followers_url": "https://api.github.com/users/gsakkis/followers", "following_url": "https://api.github.com/users/gsakkis/following{/other_user}", "gists_url": "https://api.github.com/users/gsakkis/gists{/gist_id}", "starred_url": "https://api.github.com/users/gsakkis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gsakkis/subscriptions", "organizations_url": "https://api.github.com/users/gsakkis/orgs", "repos_url": "https://api.github.com/users/gsakkis/repos", "events_url": "https://api.github.com/users/gsakkis/events{/privacy}", "received_events_url": "https://api.github.com/users/gsakkis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}, {"id": 969453080, "node_id": "MDU6TGFiZWw5Njk0NTMwODA=", "url": "https://api.github.com/repos/dask/distributed/labels/good%20first%20issue", "name": "good first issue", "color": "159818", "default": true, "description": "Clearly described and easy to accomplish. Good for beginners to the project."}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-01-21T23:23:08Z", "updated_at": "2019-01-28T17:11:58Z", "closed_at": "2019-01-28T17:11:58Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "```\r\nimport dask\r\nimport dask.dataframe as dd\r\nimport distributed\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n\r\ndef set_index(msg=''):\r\n    print(\"{} shuffle: {}\".format(msg, dask.config.get('shuffle', 'MISSING')))\r\n    print(\"{} scheduler: {}\".format(msg, dask.config.get('scheduler', 'MISSING')))\r\n    ddf.set_index('A').compute()\r\n\r\n\r\nif __name__ == '__main__':\r\n    df = pd.DataFrame({'A': np.random.rand(100)})\r\n    ddf = dd.from_pandas(df, npartitions=2)\r\n    set_index(\"before client\")\r\n    with distributed.Client():\r\n        set_index(\"inside client\")\r\n    set_index(\"after client\")\r\n```\r\n```\r\n$ python tmp/dask/shuffle_none.py \r\nbefore client shuffle: MISSING\r\nbefore client scheduler: MISSING\r\ninside client shuffle: tasks\r\ninside client scheduler: dask.distributed\r\nafter client shuffle: None\r\nafter client scheduler: None\r\nTraceback (most recent call last):\r\n...\r\nNotImplementedError: Unknown shuffle method None\r\n```", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/2473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/2473/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/1622", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/1622/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/1622/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/1622/events", "html_url": "https://github.com/dask/distributed/issues/1622", "id": 280846502, "node_id": "MDU6SXNzdWUyODA4NDY1MDI=", "number": 1622, "title": "Test failure on Tornado git master", "user": {"login": "pitrou", "id": 1721820, "node_id": "MDQ6VXNlcjE3MjE4MjA=", "avatar_url": "https://avatars.githubusercontent.com/u/1721820?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pitrou", "html_url": "https://github.com/pitrou", "followers_url": "https://api.github.com/users/pitrou/followers", "following_url": "https://api.github.com/users/pitrou/following{/other_user}", "gists_url": "https://api.github.com/users/pitrou/gists{/gist_id}", "starred_url": "https://api.github.com/users/pitrou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pitrou/subscriptions", "organizations_url": "https://api.github.com/users/pitrou/orgs", "repos_url": "https://api.github.com/users/pitrou/repos", "events_url": "https://api.github.com/users/pitrou/events{/privacy}", "received_events_url": "https://api.github.com/users/pitrou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-10T21:26:20Z", "updated_at": "2021-10-18T01:09:31Z", "closed_at": "2021-10-18T01:09:31Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "There is a single test failure when running the test suite with Tornado git master (excluding Bokeh-and IPython-related tests), both under Python 2.7 and 3.6:\r\n```\r\n========================================================================= FAILURES =========================================================================\r\n__________________________________________________________________ test_workers_to_close ___________________________________________________________________\r\nTraceback (most recent call last):\r\n  File \"/home/antoine/distributed/distributed/utils_test.py\", line 723, in test_func\r\n    result = loop.run_sync(coro, timeout=timeout)\r\n  File \"/home/antoine/tornado/tornado/ioloop.py\", line 521, in run_sync\r\n    raise TimeoutError('Operation timed out after %s seconds' % timeout)\r\ntornado.util.TimeoutError: Operation timed out after 10 seconds\r\n------------------------------------------------------------------- Captured stderr call -------------------------------------------------------------------\r\ndistributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:33977\r\ndistributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35501\r\ndistributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35501\r\ndistributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33977\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.worker - INFO -               Threads:                          1\r\ndistributed.worker - INFO -                Memory:                    8.35 GB\r\ndistributed.worker - INFO -       Local Directory: /home/antoine/distributed/dask-worker-space/worker-ssw4ovw1\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43253\r\ndistributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43253\r\ndistributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33977\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.worker - INFO -               Threads:                          1\r\ndistributed.worker - INFO -                Memory:                    8.35 GB\r\ndistributed.worker - INFO -       Local Directory: /home/antoine/distributed/dask-worker-space/worker-7p76rlrp\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42055\r\ndistributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42055\r\ndistributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33977\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.worker - INFO -               Threads:                          1\r\ndistributed.worker - INFO -                Memory:                    8.35 GB\r\ndistributed.worker - INFO -       Local Directory: /home/antoine/distributed/dask-worker-space/worker-0cuh8_fi\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36653\r\ndistributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36653\r\ndistributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33977\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.worker - INFO -               Threads:                          1\r\ndistributed.worker - INFO -                Memory:                    8.35 GB\r\ndistributed.worker - INFO -       Local Directory: /home/antoine/distributed/dask-worker-space/worker-om218ztt\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.scheduler - INFO - Register tcp://127.0.0.1:35501\r\ndistributed.scheduler - INFO - Register tcp://127.0.0.1:43253\r\ndistributed.scheduler - INFO - Register tcp://127.0.0.1:42055\r\ndistributed.scheduler - INFO - Register tcp://127.0.0.1:36653\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35501\r\ndistributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33977\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43253\r\ndistributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33977\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42055\r\ndistributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33977\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36653\r\ndistributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33977\r\ndistributed.worker - INFO - -------------------------------------------------\r\ndistributed.scheduler - INFO - Receive client connection: Client-aa2ae17a-dddb-11e7-9e65-14dae9deda7e\r\n===================================================================== warnings summary =====================================================================\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/1622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/1622/timeline", "performed_via_github_app": null, "state_reason": "completed"}, {"url": "https://api.github.com/repos/dask/distributed/issues/1100", "repository_url": "https://api.github.com/repos/dask/distributed", "labels_url": "https://api.github.com/repos/dask/distributed/issues/1100/labels{/name}", "comments_url": "https://api.github.com/repos/dask/distributed/issues/1100/comments", "events_url": "https://api.github.com/repos/dask/distributed/issues/1100/events", "html_url": "https://github.com/dask/distributed/issues/1100", "id": 231282794, "node_id": "MDU6SXNzdWUyMzEyODI3OTQ=", "number": 1100, "title": "_reevaluate_occupancy_worker() resets processing estimate", "user": {"login": "pitrou", "id": 1721820, "node_id": "MDQ6VXNlcjE3MjE4MjA=", "avatar_url": "https://avatars.githubusercontent.com/u/1721820?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pitrou", "html_url": "https://github.com/pitrou", "followers_url": "https://api.github.com/users/pitrou/followers", "following_url": "https://api.github.com/users/pitrou/following{/other_user}", "gists_url": "https://api.github.com/users/pitrou/gists{/gist_id}", "starred_url": "https://api.github.com/users/pitrou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pitrou/subscriptions", "organizations_url": "https://api.github.com/users/pitrou/orgs", "repos_url": "https://api.github.com/users/pitrou/repos", "events_url": "https://api.github.com/users/pitrou/events{/privacy}", "received_events_url": "https://api.github.com/users/pitrou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259867382, "node_id": "MDU6TGFiZWwyNTk4NjczODI=", "url": "https://api.github.com/repos/dask/distributed/labels/bug", "name": "bug", "color": "faadaf", "default": true, "description": "Something is broken"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-25T09:11:56Z", "updated_at": "2017-05-25T10:50:20Z", "closed_at": "2017-05-25T10:50:20Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "This is an intermittent test failure:\r\nhttps://travis-ci.org/dask/distributed/jobs/235929242#L1472-L1521\r\n\r\nBasically, `transition_waiting_processing` updates `processing` with the expected task duration plus the estimated communication cost, but `_reevaluate_occupancy_worker` can run just after and reset the `processing` estimate to its former value from `task_duration`.\r\n\r\nI admit I don't understand why `_reevaluate_occupancy_worker` does this. Why does it mutate `processing` at all? `processing` is already maintained in real time in various state transitions...", "reactions": {"url": "https://api.github.com/repos/dask/distributed/issues/1100/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/dask/distributed/issues/1100/timeline", "performed_via_github_app": null, "state_reason": "completed"}]